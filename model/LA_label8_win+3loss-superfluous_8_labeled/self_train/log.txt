[20:31:31.611] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:31:32.746] 2 iterations per epoch
[20:31:35.673] iteration 1 : loss: 0.151975, loss_a: 0.089397, loss_b: 0.000000
[20:33:29.549] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:33:30.696] 2 iterations per epoch
[20:33:56.408] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:33:57.549] 2 iterations per epoch
[20:34:00.493] iteration 1 : loss: 0.151975, loss_a: 0.089397
[20:35:28.171] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:35:29.314] 2 iterations per epoch
[20:36:13.213] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:36:14.372] 2 iterations per epoch
[20:37:03.008] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:37:04.156] 2 iterations per epoch
[20:37:07.127] iteration 1 : loss: 0.151975, loss_a: 0.089397
[20:38:19.634] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:38:20.800] 2 iterations per epoch
[20:38:23.927] iteration 1 : loss: 0.133327, loss_a: 0.088885, loss_b: 0.000000
[20:38:26.229] iteration 2 : loss: 0.270815, loss_a: 0.180543, loss_b: 0.000000
[20:38:27.562] iteration 3 : loss: 0.148125, loss_a: 0.098750, loss_b: 0.000000
[20:38:28.360] iteration 4 : loss: 0.173658, loss_a: 0.115772, loss_b: 0.000000
[20:38:29.752] iteration 5 : loss: 0.161890, loss_a: 0.107927, loss_b: 0.000000
[20:44:09.578] Namespace(root_path='/home/vigil/Desktop/BCP-main/data/byh_data/SSNet_data/LA', exp='label8_win+3loss-superfluous', model='VNet', pre_max_iteration=15000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.1, deterministic=1, labelnum=8, gpu='1', seed=1337, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[20:44:10.707] 2 iterations per epoch
[20:44:13.640] iteration 1 : loss: 0.151975, loss_a: 0.089397
[20:44:15.902] iteration 2 : loss: 0.383775, loss_a: 0.225750
[20:44:17.272] iteration 3 : loss: 0.168826, loss_a: 0.099310
[20:44:18.033] iteration 4 : loss: 0.205342, loss_a: 0.120790
[20:44:19.390] iteration 5 : loss: 0.172709, loss_a: 0.101593
[20:44:20.133] iteration 6 : loss: 0.144978, loss_a: 0.085281
[20:44:21.478] iteration 7 : loss: 0.255787, loss_a: 0.150463
[20:44:22.275] iteration 8 : loss: 0.154494, loss_a: 0.090879
[20:44:23.624] iteration 9 : loss: 0.123195, loss_a: 0.072468
[20:44:24.407] iteration 10 : loss: 0.170894, loss_a: 0.100526
[20:44:25.775] iteration 11 : loss: 0.080023, loss_a: 0.047072
[20:44:26.550] iteration 12 : loss: 0.095440, loss_a: 0.056141
[20:44:27.857] iteration 13 : loss: 0.099268, loss_a: 0.058393
[20:44:28.601] iteration 14 : loss: 0.203687, loss_a: 0.119816
[20:44:29.909] iteration 15 : loss: 0.123206, loss_a: 0.072474
[20:44:30.660] iteration 16 : loss: 0.089496, loss_a: 0.052645
[20:44:31.996] iteration 17 : loss: 0.080050, loss_a: 0.047088
[20:44:32.735] iteration 18 : loss: 0.109602, loss_a: 0.064472
[20:44:34.037] iteration 19 : loss: 0.128602, loss_a: 0.075648
[20:44:34.786] iteration 20 : loss: 0.122049, loss_a: 0.071793
[20:44:36.133] iteration 21 : loss: 0.181754, loss_a: 0.106914
[20:44:36.888] iteration 22 : loss: 0.197237, loss_a: 0.116022
[20:44:38.202] iteration 23 : loss: 0.089427, loss_a: 0.052604
[20:44:38.940] iteration 24 : loss: 0.054042, loss_a: 0.031790
[20:44:40.262] iteration 25 : loss: 0.088127, loss_a: 0.051839
[20:44:40.995] iteration 26 : loss: 0.133911, loss_a: 0.078771
[20:44:42.302] iteration 27 : loss: 0.133294, loss_a: 0.078408
[20:44:43.039] iteration 28 : loss: 0.072673, loss_a: 0.042749
[20:44:44.344] iteration 29 : loss: 0.115945, loss_a: 0.068203
[20:44:45.097] iteration 30 : loss: 0.133931, loss_a: 0.078783
[20:44:46.391] iteration 31 : loss: 0.092679, loss_a: 0.054517
[20:44:47.130] iteration 32 : loss: 0.082815, loss_a: 0.048715
[20:44:48.453] iteration 33 : loss: 0.097234, loss_a: 0.057197
[20:44:49.187] iteration 34 : loss: 0.085810, loss_a: 0.050476
[20:44:50.498] iteration 35 : loss: 0.147233, loss_a: 0.086608
[20:44:51.278] iteration 36 : loss: 0.103145, loss_a: 0.060673
[20:44:52.619] iteration 37 : loss: 0.068827, loss_a: 0.040487
[20:44:53.386] iteration 38 : loss: 0.095412, loss_a: 0.056125
[20:44:54.696] iteration 39 : loss: 0.091977, loss_a: 0.054104
[20:44:55.436] iteration 40 : loss: 0.138029, loss_a: 0.081193
[20:44:56.765] iteration 41 : loss: 0.247810, loss_a: 0.145771
[20:44:57.603] iteration 42 : loss: 0.121783, loss_a: 0.071637
[20:44:58.976] iteration 43 : loss: 0.119134, loss_a: 0.070079
[20:44:59.714] iteration 44 : loss: 0.109937, loss_a: 0.064669
[20:45:01.089] iteration 45 : loss: 0.131887, loss_a: 0.077581
[20:45:01.831] iteration 46 : loss: 0.160846, loss_a: 0.094615
[20:45:03.218] iteration 47 : loss: 0.157951, loss_a: 0.092913
[20:45:03.963] iteration 48 : loss: 0.077510, loss_a: 0.045594
[20:45:05.284] iteration 49 : loss: 0.119511, loss_a: 0.070300
[20:45:06.037] iteration 50 : loss: 0.088047, loss_a: 0.051792
[20:45:07.366] iteration 51 : loss: 0.088675, loss_a: 0.052161
[20:45:08.106] iteration 52 : loss: 0.100111, loss_a: 0.058889
[20:45:09.417] iteration 53 : loss: 0.072068, loss_a: 0.042393
[20:45:10.150] iteration 54 : loss: 0.115076, loss_a: 0.067692
[20:45:11.476] iteration 55 : loss: 0.074810, loss_a: 0.044006
[20:45:12.232] iteration 56 : loss: 0.144441, loss_a: 0.084965
[20:45:13.585] iteration 57 : loss: 0.073795, loss_a: 0.043409
[20:45:14.318] iteration 58 : loss: 0.146394, loss_a: 0.086114
[20:45:15.647] iteration 59 : loss: 0.096735, loss_a: 0.056903
[20:45:16.383] iteration 60 : loss: 0.187014, loss_a: 0.110008
[20:45:17.704] iteration 61 : loss: 0.052633, loss_a: 0.030960
[20:45:18.440] iteration 62 : loss: 0.057696, loss_a: 0.033939
[20:45:19.734] iteration 63 : loss: 0.088753, loss_a: 0.052208
[20:45:20.469] iteration 64 : loss: 0.063495, loss_a: 0.037350
[20:45:21.813] iteration 65 : loss: 0.085236, loss_a: 0.050139
[20:45:22.547] iteration 66 : loss: 0.097356, loss_a: 0.057268
[20:45:23.859] iteration 67 : loss: 0.109784, loss_a: 0.064579
[20:45:24.602] iteration 68 : loss: 0.069334, loss_a: 0.040785
[20:45:25.926] iteration 69 : loss: 0.300262, loss_a: 0.176625
[20:45:26.676] iteration 70 : loss: 0.067393, loss_a: 0.039643
[20:45:27.988] iteration 71 : loss: 0.201236, loss_a: 0.118374
[20:45:28.727] iteration 72 : loss: 0.148414, loss_a: 0.087303
[20:45:30.040] iteration 73 : loss: 0.148257, loss_a: 0.087210
[20:45:30.778] iteration 74 : loss: 0.135142, loss_a: 0.079496
[20:45:32.123] iteration 75 : loss: 0.105285, loss_a: 0.061932
[20:45:32.868] iteration 76 : loss: 0.077254, loss_a: 0.045443
[20:45:34.182] iteration 77 : loss: 0.142190, loss_a: 0.083641
[20:45:34.923] iteration 78 : loss: 0.203587, loss_a: 0.119757
[20:45:36.225] iteration 79 : loss: 0.066411, loss_a: 0.039065
[20:45:36.963] iteration 80 : loss: 0.120241, loss_a: 0.070730
[20:45:38.296] iteration 81 : loss: 0.097051, loss_a: 0.057089
[20:45:39.032] iteration 82 : loss: 0.094274, loss_a: 0.055455
[20:45:40.371] iteration 83 : loss: 0.119628, loss_a: 0.070370
[20:45:41.112] iteration 84 : loss: 0.068485, loss_a: 0.040285
[20:45:42.443] iteration 85 : loss: 0.130536, loss_a: 0.076786
[20:45:43.185] iteration 86 : loss: 0.112110, loss_a: 0.065947
[20:45:44.516] iteration 87 : loss: 0.125366, loss_a: 0.073745
[20:45:45.247] iteration 88 : loss: 0.101702, loss_a: 0.059825
[20:45:46.550] iteration 89 : loss: 0.070198, loss_a: 0.041293
[20:45:47.284] iteration 90 : loss: 0.071808, loss_a: 0.042240
[20:45:48.595] iteration 91 : loss: 0.143375, loss_a: 0.084338
[20:45:49.331] iteration 92 : loss: 0.064662, loss_a: 0.038037
[20:45:50.677] iteration 93 : loss: 0.134227, loss_a: 0.078957
[20:45:51.420] iteration 94 : loss: 0.140311, loss_a: 0.082536
[20:45:52.725] iteration 95 : loss: 0.093455, loss_a: 0.054973
[20:45:53.459] iteration 96 : loss: 0.089152, loss_a: 0.052443
[20:45:54.808] iteration 97 : loss: 0.140322, loss_a: 0.082542
[20:45:55.552] iteration 98 : loss: 0.145098, loss_a: 0.085352
[20:45:56.904] iteration 99 : loss: 0.138194, loss_a: 0.081291
[20:45:57.643] iteration 100 : loss: 0.106716, loss_a: 0.062774
[20:45:59.040] iteration 101 : loss: 0.528955, loss_a: 0.311150
[20:45:59.770] iteration 102 : loss: 0.045590, loss_a: 0.026818
[20:46:01.146] iteration 103 : loss: 0.076934, loss_a: 0.045255
[20:46:01.889] iteration 104 : loss: 0.559541, loss_a: 0.329142
[20:46:03.237] iteration 105 : loss: 0.125860, loss_a: 0.074035
[20:46:03.975] iteration 106 : loss: 0.092379, loss_a: 0.054341
[20:46:05.283] iteration 107 : loss: 0.075666, loss_a: 0.044510
[20:46:06.022] iteration 108 : loss: 0.091787, loss_a: 0.053992
[20:46:07.337] iteration 109 : loss: 0.111487, loss_a: 0.065581
[20:46:08.084] iteration 110 : loss: 0.105088, loss_a: 0.061816
[20:46:09.419] iteration 111 : loss: 0.183474, loss_a: 0.107926
[20:46:10.152] iteration 112 : loss: 0.152658, loss_a: 0.089799
[20:46:11.493] iteration 113 : loss: 0.105343, loss_a: 0.061967
[20:46:12.230] iteration 114 : loss: 0.082582, loss_a: 0.048577
[20:46:13.566] iteration 115 : loss: 0.108742, loss_a: 0.063966
[20:46:14.304] iteration 116 : loss: 0.060562, loss_a: 0.035625
[20:46:15.643] iteration 117 : loss: 0.077668, loss_a: 0.045687
[20:46:16.384] iteration 118 : loss: 0.076246, loss_a: 0.044850
[20:46:17.694] iteration 119 : loss: 0.069027, loss_a: 0.040604
[20:46:18.434] iteration 120 : loss: 0.095400, loss_a: 0.056118
[20:46:19.741] iteration 121 : loss: 0.138444, loss_a: 0.081438
[20:46:20.481] iteration 122 : loss: 0.089428, loss_a: 0.052605
[20:46:21.792] iteration 123 : loss: 0.091098, loss_a: 0.053587
[20:46:22.526] iteration 124 : loss: 0.067155, loss_a: 0.039503
[20:46:23.861] iteration 125 : loss: 0.058289, loss_a: 0.034288
[20:46:24.604] iteration 126 : loss: 0.083496, loss_a: 0.049115
[20:46:25.933] iteration 127 : loss: 0.056319, loss_a: 0.033129
[20:46:26.667] iteration 128 : loss: 0.142455, loss_a: 0.083797
[20:46:28.004] iteration 129 : loss: 0.145354, loss_a: 0.085502
[20:46:28.750] iteration 130 : loss: 0.076845, loss_a: 0.045203
[20:46:30.087] iteration 131 : loss: 0.092491, loss_a: 0.054407
[20:46:30.820] iteration 132 : loss: 0.071302, loss_a: 0.041942
[20:46:32.122] iteration 133 : loss: 0.099963, loss_a: 0.058802
[20:46:32.866] iteration 134 : loss: 0.066206, loss_a: 0.038945
[20:46:34.210] iteration 135 : loss: 0.100506, loss_a: 0.059121
[20:46:34.949] iteration 136 : loss: 0.112939, loss_a: 0.066434
[20:46:36.303] iteration 137 : loss: 0.132265, loss_a: 0.077803
[20:46:37.045] iteration 138 : loss: 0.119211, loss_a: 0.070124
[20:46:38.378] iteration 139 : loss: 0.076603, loss_a: 0.045061
[20:46:39.115] iteration 140 : loss: 0.050526, loss_a: 0.029721
[20:46:40.448] iteration 141 : loss: 0.081042, loss_a: 0.047672
[20:46:41.200] iteration 142 : loss: 0.152902, loss_a: 0.089942
[20:46:42.556] iteration 143 : loss: 0.123407, loss_a: 0.072592
[20:46:43.308] iteration 144 : loss: 0.073503, loss_a: 0.043237
[20:46:44.654] iteration 145 : loss: 0.137482, loss_a: 0.080872
[20:46:45.400] iteration 146 : loss: 0.074601, loss_a: 0.043883
[20:46:46.733] iteration 147 : loss: 0.081097, loss_a: 0.047704
[20:46:47.469] iteration 148 : loss: 0.091528, loss_a: 0.053840
[20:46:48.787] iteration 149 : loss: 0.061915, loss_a: 0.036421
[20:46:49.530] iteration 150 : loss: 0.160100, loss_a: 0.094176
[20:46:50.865] iteration 151 : loss: 0.071713, loss_a: 0.042184
[20:46:51.599] iteration 152 : loss: 0.079501, loss_a: 0.046766
[20:46:52.901] iteration 153 : loss: 0.075405, loss_a: 0.044356
[20:46:53.628] iteration 154 : loss: 0.076248, loss_a: 0.044852
[20:46:54.954] iteration 155 : loss: 0.088500, loss_a: 0.052059
[20:46:55.699] iteration 156 : loss: 0.087683, loss_a: 0.051578
[20:46:57.009] iteration 157 : loss: 0.065141, loss_a: 0.038318
[20:46:57.745] iteration 158 : loss: 0.093445, loss_a: 0.054968
[20:46:59.071] iteration 159 : loss: 0.076583, loss_a: 0.045049
[20:46:59.802] iteration 160 : loss: 0.074530, loss_a: 0.043841
[20:47:01.107] iteration 161 : loss: 0.055315, loss_a: 0.032538
[20:47:01.845] iteration 162 : loss: 0.080974, loss_a: 0.047631
[20:47:03.173] iteration 163 : loss: 0.115365, loss_a: 0.067862
[20:47:03.917] iteration 164 : loss: 0.162857, loss_a: 0.095798
[20:47:05.270] iteration 165 : loss: 0.619546, loss_a: 0.364439
[20:47:06.014] iteration 166 : loss: 0.061349, loss_a: 0.036088
[20:47:07.331] iteration 167 : loss: 0.049603, loss_a: 0.029178
[20:47:08.071] iteration 168 : loss: 0.072333, loss_a: 0.042549
[20:47:09.378] iteration 169 : loss: 0.078437, loss_a: 0.046139
[20:47:10.107] iteration 170 : loss: 0.091894, loss_a: 0.054055
[20:47:11.446] iteration 171 : loss: 0.135904, loss_a: 0.079944
[20:47:12.191] iteration 172 : loss: 0.086943, loss_a: 0.051143
[20:47:13.517] iteration 173 : loss: 0.077938, loss_a: 0.045846
[20:47:14.252] iteration 174 : loss: 0.059052, loss_a: 0.034736
[20:47:15.564] iteration 175 : loss: 0.057558, loss_a: 0.033857
[20:47:16.317] iteration 176 : loss: 0.054964, loss_a: 0.032332
[20:47:17.619] iteration 177 : loss: 0.061635, loss_a: 0.036256
[20:47:18.350] iteration 178 : loss: 0.105215, loss_a: 0.061891
[20:47:19.678] iteration 179 : loss: 0.109620, loss_a: 0.064482
[20:47:20.414] iteration 180 : loss: 0.169331, loss_a: 0.099607
[20:47:21.744] iteration 181 : loss: 0.155735, loss_a: 0.091609
[20:47:22.485] iteration 182 : loss: 0.073897, loss_a: 0.043469
[20:47:23.813] iteration 183 : loss: 0.127703, loss_a: 0.075119
[20:47:24.543] iteration 184 : loss: 0.067132, loss_a: 0.039490
[20:47:25.879] iteration 185 : loss: 0.079348, loss_a: 0.046675
[20:47:26.612] iteration 186 : loss: 0.084840, loss_a: 0.049906
[20:47:27.944] iteration 187 : loss: 0.115506, loss_a: 0.067945
[20:47:28.687] iteration 188 : loss: 0.116180, loss_a: 0.068341
[20:47:30.003] iteration 189 : loss: 0.142729, loss_a: 0.083958
[20:47:30.757] iteration 190 : loss: 0.245489, loss_a: 0.144405
[20:47:32.099] iteration 191 : loss: 0.126757, loss_a: 0.074563
[20:47:32.844] iteration 192 : loss: 0.080905, loss_a: 0.047591
[20:47:34.174] iteration 193 : loss: 0.081565, loss_a: 0.047979
[20:47:34.921] iteration 194 : loss: 0.069723, loss_a: 0.041013
[20:47:36.231] iteration 195 : loss: 0.094567, loss_a: 0.055628
[20:47:36.984] iteration 196 : loss: 0.106623, loss_a: 0.062719
[20:47:38.290] iteration 197 : loss: 0.124938, loss_a: 0.073493
[20:47:39.030] iteration 198 : loss: 0.070295, loss_a: 0.041350
[20:47:40.373] iteration 199 : loss: 0.105335, loss_a: 0.061961
[20:47:41.110] iteration 200 : loss: 0.137625, loss_a: 0.080956
[20:48:04.556] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_200_dice_0.7517.pth
[20:48:05.916] iteration 201 : loss: 0.089282, loss_a: 0.052519
[20:48:08.160] iteration 202 : loss: 0.058750, loss_a: 0.034559
[20:48:09.465] iteration 203 : loss: 0.071096, loss_a: 0.041821
[20:48:10.208] iteration 204 : loss: 0.129592, loss_a: 0.076230
[20:48:11.541] iteration 205 : loss: 0.066504, loss_a: 0.039120
[20:48:12.278] iteration 206 : loss: 0.055764, loss_a: 0.032802
[20:48:13.590] iteration 207 : loss: 0.135836, loss_a: 0.079904
[20:48:14.333] iteration 208 : loss: 0.069375, loss_a: 0.040809
[20:48:15.673] iteration 209 : loss: 0.078535, loss_a: 0.046197
[20:48:16.413] iteration 210 : loss: 0.115859, loss_a: 0.068152
[20:48:17.731] iteration 211 : loss: 0.143460, loss_a: 0.084388
[20:48:18.467] iteration 212 : loss: 0.037782, loss_a: 0.022224
[20:48:19.812] iteration 213 : loss: 0.101507, loss_a: 0.059710
[20:48:20.554] iteration 214 : loss: 0.143290, loss_a: 0.084288
[20:48:21.904] iteration 215 : loss: 0.100456, loss_a: 0.059092
[20:48:22.647] iteration 216 : loss: 0.053200, loss_a: 0.031294
[20:48:23.999] iteration 217 : loss: 0.094976, loss_a: 0.055868
[20:48:24.735] iteration 218 : loss: 0.040792, loss_a: 0.023995
[20:48:26.062] iteration 219 : loss: 0.114577, loss_a: 0.067398
[20:48:26.811] iteration 220 : loss: 0.085865, loss_a: 0.050509
[20:48:28.145] iteration 221 : loss: 0.199563, loss_a: 0.117390
[20:48:28.889] iteration 222 : loss: 0.042242, loss_a: 0.024848
[20:48:30.236] iteration 223 : loss: 0.182529, loss_a: 0.107370
[20:48:30.976] iteration 224 : loss: 0.065597, loss_a: 0.038587
[20:48:32.292] iteration 225 : loss: 0.100909, loss_a: 0.059358
[20:48:33.026] iteration 226 : loss: 0.053320, loss_a: 0.031365
[20:48:34.363] iteration 227 : loss: 0.065089, loss_a: 0.038288
[20:48:35.104] iteration 228 : loss: 0.052959, loss_a: 0.031152
[20:48:36.416] iteration 229 : loss: 0.071644, loss_a: 0.042144
[20:48:37.160] iteration 230 : loss: 0.096904, loss_a: 0.057002
[20:48:38.455] iteration 231 : loss: 0.063476, loss_a: 0.037339
[20:48:39.191] iteration 232 : loss: 0.048427, loss_a: 0.028486
[20:48:40.519] iteration 233 : loss: 0.041897, loss_a: 0.024645
[20:48:41.260] iteration 234 : loss: 0.054505, loss_a: 0.032062
[20:48:42.640] iteration 235 : loss: 0.067409, loss_a: 0.039652
[20:48:43.388] iteration 236 : loss: 0.086162, loss_a: 0.050684
[20:48:44.713] iteration 237 : loss: 0.047233, loss_a: 0.027784
[20:48:45.464] iteration 238 : loss: 0.101961, loss_a: 0.059977
[20:48:46.759] iteration 239 : loss: 0.040961, loss_a: 0.024095
[20:48:47.504] iteration 240 : loss: 0.126512, loss_a: 0.074419
[20:48:48.836] iteration 241 : loss: 0.069320, loss_a: 0.040776
[20:48:49.575] iteration 242 : loss: 0.102219, loss_a: 0.060129
[20:48:50.906] iteration 243 : loss: 0.098145, loss_a: 0.057732
[20:48:51.649] iteration 244 : loss: 0.126765, loss_a: 0.074568
[20:48:53.020] iteration 245 : loss: 0.108329, loss_a: 0.063723
[20:48:53.766] iteration 246 : loss: 0.058495, loss_a: 0.034409
[20:48:55.098] iteration 247 : loss: 0.065137, loss_a: 0.038316
[20:48:55.839] iteration 248 : loss: 0.068802, loss_a: 0.040472
[20:48:57.162] iteration 249 : loss: 0.066133, loss_a: 0.038902
[20:48:57.906] iteration 250 : loss: 0.095809, loss_a: 0.056358
[20:48:59.219] iteration 251 : loss: 0.031179, loss_a: 0.018341
[20:48:59.959] iteration 252 : loss: 0.053953, loss_a: 0.031737
[20:49:01.323] iteration 253 : loss: 0.100970, loss_a: 0.059394
[20:49:02.068] iteration 254 : loss: 0.089379, loss_a: 0.052576
[20:49:03.408] iteration 255 : loss: 0.095653, loss_a: 0.056267
[20:49:04.158] iteration 256 : loss: 0.049196, loss_a: 0.028939
[20:49:05.474] iteration 257 : loss: 0.087377, loss_a: 0.051398
[20:49:06.210] iteration 258 : loss: 0.064669, loss_a: 0.038040
[20:49:07.595] iteration 259 : loss: 0.180262, loss_a: 0.106036
[20:49:08.347] iteration 260 : loss: 0.094835, loss_a: 0.055786
[20:49:09.669] iteration 261 : loss: 0.064618, loss_a: 0.038011
[20:49:10.413] iteration 262 : loss: 0.103260, loss_a: 0.060741
[20:49:11.763] iteration 263 : loss: 0.072242, loss_a: 0.042495
[20:49:12.505] iteration 264 : loss: 0.075740, loss_a: 0.044553
[20:49:13.874] iteration 265 : loss: 0.102343, loss_a: 0.060202
[20:49:14.610] iteration 266 : loss: 0.113146, loss_a: 0.066557
[20:49:15.960] iteration 267 : loss: 0.075059, loss_a: 0.044152
[20:49:16.703] iteration 268 : loss: 0.059626, loss_a: 0.035074
[20:49:18.046] iteration 269 : loss: 0.134046, loss_a: 0.078850
[20:49:18.809] iteration 270 : loss: 0.115632, loss_a: 0.068019
[20:49:20.139] iteration 271 : loss: 0.049185, loss_a: 0.028932
[20:49:20.884] iteration 272 : loss: 0.040475, loss_a: 0.023809
[20:49:22.235] iteration 273 : loss: 0.056665, loss_a: 0.033332
[20:49:22.975] iteration 274 : loss: 0.048322, loss_a: 0.028425
[20:49:24.306] iteration 275 : loss: 0.059523, loss_a: 0.035013
[20:49:25.043] iteration 276 : loss: 0.077720, loss_a: 0.045718
[20:49:26.393] iteration 277 : loss: 0.065327, loss_a: 0.038428
[20:49:27.149] iteration 278 : loss: 0.047541, loss_a: 0.027965
[20:49:28.478] iteration 279 : loss: 0.087270, loss_a: 0.051335
[20:49:29.224] iteration 280 : loss: 0.146584, loss_a: 0.086226
[20:49:30.545] iteration 281 : loss: 0.082868, loss_a: 0.048746
[20:49:31.286] iteration 282 : loss: 0.053688, loss_a: 0.031581
[20:49:32.643] iteration 283 : loss: 0.071750, loss_a: 0.042206
[20:49:33.397] iteration 284 : loss: 0.077431, loss_a: 0.045548
[20:49:34.738] iteration 285 : loss: 0.085568, loss_a: 0.050334
[20:49:35.487] iteration 286 : loss: 0.089348, loss_a: 0.052558
[20:49:36.808] iteration 287 : loss: 0.048065, loss_a: 0.028274
[20:49:37.562] iteration 288 : loss: 0.091793, loss_a: 0.053996
[20:49:38.888] iteration 289 : loss: 0.037291, loss_a: 0.021936
[20:49:39.633] iteration 290 : loss: 0.086320, loss_a: 0.050776
[20:49:40.967] iteration 291 : loss: 0.132693, loss_a: 0.078054
[20:49:41.710] iteration 292 : loss: 0.104026, loss_a: 0.061192
[20:49:43.077] iteration 293 : loss: 0.086781, loss_a: 0.051048
[20:49:43.827] iteration 294 : loss: 0.069153, loss_a: 0.040678
[20:49:45.147] iteration 295 : loss: 0.042476, loss_a: 0.024986
[20:49:45.889] iteration 296 : loss: 0.098157, loss_a: 0.057739
[20:49:47.220] iteration 297 : loss: 0.071179, loss_a: 0.041870
[20:49:47.971] iteration 298 : loss: 0.036325, loss_a: 0.021368
[20:49:49.327] iteration 299 : loss: 0.110080, loss_a: 0.064753
[20:49:50.069] iteration 300 : loss: 0.073541, loss_a: 0.043260
[20:49:51.395] iteration 301 : loss: 0.035752, loss_a: 0.021031
[20:49:52.148] iteration 302 : loss: 0.054311, loss_a: 0.031948
[20:49:53.465] iteration 303 : loss: 0.073139, loss_a: 0.043023
[20:49:54.206] iteration 304 : loss: 0.047503, loss_a: 0.027943
[20:49:55.532] iteration 305 : loss: 0.100466, loss_a: 0.059097
[20:49:56.270] iteration 306 : loss: 0.072153, loss_a: 0.042443
[20:49:57.626] iteration 307 : loss: 0.086695, loss_a: 0.050997
[20:49:58.366] iteration 308 : loss: 0.091054, loss_a: 0.053561
[20:49:59.692] iteration 309 : loss: 0.045212, loss_a: 0.026595
[20:50:00.441] iteration 310 : loss: 0.045000, loss_a: 0.026471
[20:50:01.786] iteration 311 : loss: 0.046694, loss_a: 0.027467
[20:50:02.540] iteration 312 : loss: 0.067557, loss_a: 0.039740
[20:50:03.851] iteration 313 : loss: 0.034711, loss_a: 0.020418
[20:50:04.603] iteration 314 : loss: 0.163341, loss_a: 0.096083
[20:50:05.955] iteration 315 : loss: 0.072718, loss_a: 0.042775
[20:50:06.706] iteration 316 : loss: 0.063419, loss_a: 0.037305
[20:50:08.030] iteration 317 : loss: 0.093746, loss_a: 0.055145
[20:50:08.774] iteration 318 : loss: 0.044986, loss_a: 0.026462
[20:50:10.089] iteration 319 : loss: 0.042537, loss_a: 0.025022
[20:50:10.848] iteration 320 : loss: 0.129541, loss_a: 0.076200
[20:50:12.212] iteration 321 : loss: 0.059062, loss_a: 0.034742
[20:50:12.966] iteration 322 : loss: 0.053654, loss_a: 0.031561
[20:50:14.296] iteration 323 : loss: 0.078554, loss_a: 0.046208
[20:50:15.046] iteration 324 : loss: 0.056548, loss_a: 0.033263
[20:50:16.427] iteration 325 : loss: 0.136466, loss_a: 0.080274
[20:50:17.177] iteration 326 : loss: 0.076217, loss_a: 0.044834
[20:50:18.510] iteration 327 : loss: 0.074234, loss_a: 0.043667
[20:50:19.259] iteration 328 : loss: 0.054755, loss_a: 0.032209
[20:50:20.570] iteration 329 : loss: 0.072478, loss_a: 0.042634
[20:50:21.313] iteration 330 : loss: 0.082425, loss_a: 0.048485
[20:50:22.623] iteration 331 : loss: 0.070559, loss_a: 0.041506
[20:50:23.368] iteration 332 : loss: 0.047975, loss_a: 0.028221
[20:50:24.691] iteration 333 : loss: 0.089184, loss_a: 0.052461
[20:50:25.439] iteration 334 : loss: 0.060003, loss_a: 0.035296
[20:50:26.800] iteration 335 : loss: 0.079859, loss_a: 0.046976
[20:50:27.540] iteration 336 : loss: 0.107083, loss_a: 0.062990
[20:50:28.856] iteration 337 : loss: 0.041523, loss_a: 0.024425
[20:50:29.598] iteration 338 : loss: 0.077312, loss_a: 0.045478
[20:50:30.922] iteration 339 : loss: 0.094301, loss_a: 0.055471
[20:50:31.673] iteration 340 : loss: 0.035097, loss_a: 0.020645
[20:50:33.003] iteration 341 : loss: 0.052448, loss_a: 0.030852
[20:50:33.751] iteration 342 : loss: 0.087566, loss_a: 0.051510
[20:50:35.058] iteration 343 : loss: 0.057356, loss_a: 0.033739
[20:50:35.803] iteration 344 : loss: 0.056127, loss_a: 0.033016
[20:50:37.147] iteration 345 : loss: 0.057733, loss_a: 0.033960
[20:50:37.877] iteration 346 : loss: 0.122935, loss_a: 0.072315
[20:50:39.243] iteration 347 : loss: 0.067128, loss_a: 0.039487
[20:50:39.986] iteration 348 : loss: 0.048048, loss_a: 0.028264
[20:50:41.337] iteration 349 : loss: 0.091754, loss_a: 0.053973
[20:50:42.075] iteration 350 : loss: 0.040522, loss_a: 0.023836
[20:50:43.399] iteration 351 : loss: 0.027850, loss_a: 0.016382
[20:50:44.154] iteration 352 : loss: 0.067839, loss_a: 0.039905
[20:50:45.512] iteration 353 : loss: 0.192515, loss_a: 0.113244
[20:50:46.268] iteration 354 : loss: 0.079625, loss_a: 0.046838
[20:50:47.616] iteration 355 : loss: 0.093302, loss_a: 0.054884
[20:50:48.356] iteration 356 : loss: 0.051482, loss_a: 0.030284
[20:50:49.694] iteration 357 : loss: 0.067473, loss_a: 0.039690
[20:50:50.433] iteration 358 : loss: 0.057807, loss_a: 0.034004
[20:50:51.793] iteration 359 : loss: 0.091321, loss_a: 0.053718
[20:50:52.542] iteration 360 : loss: 0.041630, loss_a: 0.024488
[20:50:53.897] iteration 361 : loss: 0.084290, loss_a: 0.049582
[20:50:54.637] iteration 362 : loss: 0.125655, loss_a: 0.073915
[20:50:55.944] iteration 363 : loss: 0.054979, loss_a: 0.032340
[20:50:56.684] iteration 364 : loss: 0.066083, loss_a: 0.038872
[20:50:58.033] iteration 365 : loss: 0.068124, loss_a: 0.040073
[20:50:58.767] iteration 366 : loss: 0.043179, loss_a: 0.025399
[20:51:00.084] iteration 367 : loss: 0.040846, loss_a: 0.024027
[20:51:00.828] iteration 368 : loss: 0.060394, loss_a: 0.035526
[20:51:02.150] iteration 369 : loss: 0.060060, loss_a: 0.035330
[20:51:02.891] iteration 370 : loss: 0.072875, loss_a: 0.042868
[20:51:04.221] iteration 371 : loss: 0.039963, loss_a: 0.023508
[20:51:04.967] iteration 372 : loss: 0.093623, loss_a: 0.055072
[20:51:06.306] iteration 373 : loss: 0.067982, loss_a: 0.039989
[20:51:07.050] iteration 374 : loss: 0.062275, loss_a: 0.036632
[20:51:08.379] iteration 375 : loss: 0.057043, loss_a: 0.033555
[20:51:09.121] iteration 376 : loss: 0.064651, loss_a: 0.038030
[20:51:10.448] iteration 377 : loss: 0.044991, loss_a: 0.026465
[20:51:11.181] iteration 378 : loss: 0.039163, loss_a: 0.023037
[20:51:12.525] iteration 379 : loss: 0.026257, loss_a: 0.015445
[20:51:13.268] iteration 380 : loss: 0.043794, loss_a: 0.025761
[20:51:14.643] iteration 381 : loss: 0.090014, loss_a: 0.052949
[20:51:15.387] iteration 382 : loss: 0.067015, loss_a: 0.039421
[20:51:16.756] iteration 383 : loss: 0.093360, loss_a: 0.054918
[20:51:17.504] iteration 384 : loss: 0.051390, loss_a: 0.030229
[20:51:18.853] iteration 385 : loss: 0.162448, loss_a: 0.095558
[20:51:19.592] iteration 386 : loss: 0.070508, loss_a: 0.041475
[20:51:20.912] iteration 387 : loss: 0.102986, loss_a: 0.060580
[20:51:21.657] iteration 388 : loss: 0.100558, loss_a: 0.059152
[20:51:23.000] iteration 389 : loss: 0.101606, loss_a: 0.059768
[20:51:23.733] iteration 390 : loss: 0.049869, loss_a: 0.029335
[20:51:25.067] iteration 391 : loss: 0.129484, loss_a: 0.076167
[20:51:25.816] iteration 392 : loss: 0.040741, loss_a: 0.023965
[20:51:27.151] iteration 393 : loss: 0.049728, loss_a: 0.029251
[20:51:27.925] iteration 394 : loss: 0.055227, loss_a: 0.032486
[20:51:29.260] iteration 395 : loss: 0.088947, loss_a: 0.052322
[20:51:30.011] iteration 396 : loss: 0.088658, loss_a: 0.052152
[20:51:31.351] iteration 397 : loss: 0.067641, loss_a: 0.039789
[20:51:32.093] iteration 398 : loss: 0.092799, loss_a: 0.054588
[20:51:33.448] iteration 399 : loss: 0.086623, loss_a: 0.050955
[20:51:34.186] iteration 400 : loss: 0.074929, loss_a: 0.044076
[20:51:57.726] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_400_dice_0.8446.pth
[20:51:59.071] iteration 401 : loss: 0.045729, loss_a: 0.026899
[20:52:01.331] iteration 402 : loss: 0.050267, loss_a: 0.029569
[20:52:02.684] iteration 403 : loss: 0.058164, loss_a: 0.034214
[20:52:03.425] iteration 404 : loss: 0.104307, loss_a: 0.061357
[20:52:04.768] iteration 405 : loss: 0.079850, loss_a: 0.046970
[20:52:05.511] iteration 406 : loss: 0.050326, loss_a: 0.029603
[20:52:06.825] iteration 407 : loss: 0.074246, loss_a: 0.043674
[20:52:07.574] iteration 408 : loss: 0.062781, loss_a: 0.036930
[20:52:08.901] iteration 409 : loss: 0.067246, loss_a: 0.039556
[20:52:09.637] iteration 410 : loss: 0.081055, loss_a: 0.047680
[20:52:10.967] iteration 411 : loss: 0.072281, loss_a: 0.042518
[20:52:11.711] iteration 412 : loss: 0.091774, loss_a: 0.053985
[20:52:13.074] iteration 413 : loss: 0.056203, loss_a: 0.033061
[20:52:13.804] iteration 414 : loss: 0.046954, loss_a: 0.027620
[20:52:15.138] iteration 415 : loss: 0.036241, loss_a: 0.021318
[20:52:15.885] iteration 416 : loss: 0.090007, loss_a: 0.052945
[20:52:17.197] iteration 417 : loss: 0.048192, loss_a: 0.028349
[20:52:17.941] iteration 418 : loss: 0.060178, loss_a: 0.035399
[20:52:19.257] iteration 419 : loss: 0.077846, loss_a: 0.045792
[20:52:19.996] iteration 420 : loss: 0.081800, loss_a: 0.048118
[20:52:21.331] iteration 421 : loss: 0.084325, loss_a: 0.049603
[20:52:22.071] iteration 422 : loss: 0.096905, loss_a: 0.057003
[20:52:23.423] iteration 423 : loss: 0.043735, loss_a: 0.025727
[20:52:24.165] iteration 424 : loss: 0.088495, loss_a: 0.052056
[20:52:25.509] iteration 425 : loss: 0.077849, loss_a: 0.045794
[20:52:26.244] iteration 426 : loss: 0.047264, loss_a: 0.027802
[20:52:27.587] iteration 427 : loss: 0.148507, loss_a: 0.087357
[20:52:28.325] iteration 428 : loss: 0.044573, loss_a: 0.026220
[20:52:29.673] iteration 429 : loss: 0.059545, loss_a: 0.035026
[20:52:30.403] iteration 430 : loss: 0.047918, loss_a: 0.028187
[20:52:31.755] iteration 431 : loss: 0.036344, loss_a: 0.021379
[20:52:32.505] iteration 432 : loss: 0.080296, loss_a: 0.047233
[20:52:33.850] iteration 433 : loss: 0.043099, loss_a: 0.025353
[20:52:34.583] iteration 434 : loss: 0.051334, loss_a: 0.030196
[20:52:35.919] iteration 435 : loss: 0.053483, loss_a: 0.031461
[20:52:36.660] iteration 436 : loss: 0.056617, loss_a: 0.033304
[20:52:37.966] iteration 437 : loss: 0.072441, loss_a: 0.042613
[20:52:38.696] iteration 438 : loss: 0.041979, loss_a: 0.024693
[20:52:40.044] iteration 439 : loss: 0.054076, loss_a: 0.031809
[20:52:40.795] iteration 440 : loss: 0.107467, loss_a: 0.063216
[20:52:42.154] iteration 441 : loss: 0.050236, loss_a: 0.029550
[20:52:42.895] iteration 442 : loss: 0.058930, loss_a: 0.034664
[20:52:44.239] iteration 443 : loss: 0.073968, loss_a: 0.043510
[20:52:44.985] iteration 444 : loss: 0.119488, loss_a: 0.070287
[20:52:46.334] iteration 445 : loss: 0.030285, loss_a: 0.017815
[20:52:47.089] iteration 446 : loss: 0.074526, loss_a: 0.043839
[20:52:48.447] iteration 447 : loss: 0.035777, loss_a: 0.021045
[20:52:49.204] iteration 448 : loss: 0.072456, loss_a: 0.042621
[20:52:50.515] iteration 449 : loss: 0.042083, loss_a: 0.024755
[20:52:51.255] iteration 450 : loss: 0.049815, loss_a: 0.029303
[20:52:52.577] iteration 451 : loss: 0.033586, loss_a: 0.019757
[20:52:53.324] iteration 452 : loss: 0.079474, loss_a: 0.046750
[20:52:54.686] iteration 453 : loss: 0.071289, loss_a: 0.041935
[20:52:55.421] iteration 454 : loss: 0.031883, loss_a: 0.018755
[20:52:56.818] iteration 455 : loss: 0.034007, loss_a: 0.020004
[20:52:57.551] iteration 456 : loss: 0.024142, loss_a: 0.014201
[20:52:58.906] iteration 457 : loss: 0.096907, loss_a: 0.057004
[20:52:59.641] iteration 458 : loss: 0.074260, loss_a: 0.043682
[20:53:01.015] iteration 459 : loss: 0.043436, loss_a: 0.025551
[20:53:01.757] iteration 460 : loss: 0.038524, loss_a: 0.022661
[20:53:03.081] iteration 461 : loss: 0.036687, loss_a: 0.021581
[20:53:03.819] iteration 462 : loss: 0.049586, loss_a: 0.029168
[20:53:05.166] iteration 463 : loss: 0.035495, loss_a: 0.020880
[20:53:05.898] iteration 464 : loss: 0.079833, loss_a: 0.046960
[20:53:07.219] iteration 465 : loss: 0.041499, loss_a: 0.024411
[20:53:07.961] iteration 466 : loss: 0.069141, loss_a: 0.040671
[20:53:09.311] iteration 467 : loss: 0.030200, loss_a: 0.017765
[20:53:10.055] iteration 468 : loss: 0.125616, loss_a: 0.073892
[20:53:11.354] iteration 469 : loss: 0.037316, loss_a: 0.021951
[20:53:12.100] iteration 470 : loss: 0.052887, loss_a: 0.031110
[20:53:13.425] iteration 471 : loss: 0.073001, loss_a: 0.042942
[20:53:14.177] iteration 472 : loss: 0.096719, loss_a: 0.056894
[20:53:15.528] iteration 473 : loss: 0.035822, loss_a: 0.021072
[20:53:16.273] iteration 474 : loss: 0.105069, loss_a: 0.061805
[20:53:17.577] iteration 475 : loss: 0.042006, loss_a: 0.024710
[20:53:18.317] iteration 476 : loss: 0.063141, loss_a: 0.037142
[20:53:19.667] iteration 477 : loss: 0.050005, loss_a: 0.029415
[20:53:20.454] iteration 478 : loss: 0.071337, loss_a: 0.041963
[20:53:21.806] iteration 479 : loss: 0.037513, loss_a: 0.022067
[20:53:22.540] iteration 480 : loss: 0.050300, loss_a: 0.029588
[20:53:23.872] iteration 481 : loss: 0.025413, loss_a: 0.014949
[20:53:24.607] iteration 482 : loss: 0.044446, loss_a: 0.026145
[20:53:25.931] iteration 483 : loss: 0.052178, loss_a: 0.030693
[20:53:26.680] iteration 484 : loss: 0.066472, loss_a: 0.039101
[20:53:28.032] iteration 485 : loss: 0.074891, loss_a: 0.044054
[20:53:28.770] iteration 486 : loss: 0.044558, loss_a: 0.026211
[20:53:30.080] iteration 487 : loss: 0.045037, loss_a: 0.026492
[20:53:30.820] iteration 488 : loss: 0.115274, loss_a: 0.067808
[20:53:32.144] iteration 489 : loss: 0.064488, loss_a: 0.037934
[20:53:32.873] iteration 490 : loss: 0.039185, loss_a: 0.023050
[20:53:34.238] iteration 491 : loss: 0.038235, loss_a: 0.022491
[20:53:34.982] iteration 492 : loss: 0.027489, loss_a: 0.016170
[20:53:36.308] iteration 493 : loss: 0.061273, loss_a: 0.036043
[20:53:37.045] iteration 494 : loss: 0.048195, loss_a: 0.028350
[20:53:38.379] iteration 495 : loss: 0.067925, loss_a: 0.039956
[20:53:39.116] iteration 496 : loss: 0.079029, loss_a: 0.046488
[20:53:40.473] iteration 497 : loss: 0.061541, loss_a: 0.036200
[20:53:41.212] iteration 498 : loss: 0.053287, loss_a: 0.031345
[20:53:42.571] iteration 499 : loss: 0.031057, loss_a: 0.018269
[20:53:43.304] iteration 500 : loss: 0.030343, loss_a: 0.017849
[20:53:44.629] iteration 501 : loss: 0.070113, loss_a: 0.041243
[20:53:45.374] iteration 502 : loss: 0.040370, loss_a: 0.023747
[20:53:46.725] iteration 503 : loss: 0.097335, loss_a: 0.057256
[20:53:47.471] iteration 504 : loss: 0.050049, loss_a: 0.029440
[20:53:48.830] iteration 505 : loss: 0.090948, loss_a: 0.053499
[20:53:49.574] iteration 506 : loss: 0.060426, loss_a: 0.035544
[20:53:50.903] iteration 507 : loss: 0.066343, loss_a: 0.039025
[20:53:51.656] iteration 508 : loss: 0.092897, loss_a: 0.054645
[20:53:52.997] iteration 509 : loss: 0.071001, loss_a: 0.041765
[20:53:53.739] iteration 510 : loss: 0.049519, loss_a: 0.029129
[20:53:55.058] iteration 511 : loss: 0.043919, loss_a: 0.025835
[20:53:55.798] iteration 512 : loss: 0.063315, loss_a: 0.037244
[20:53:57.147] iteration 513 : loss: 0.086982, loss_a: 0.051166
[20:53:57.897] iteration 514 : loss: 0.109852, loss_a: 0.064619
[20:53:59.249] iteration 515 : loss: 0.070816, loss_a: 0.041656
[20:53:59.990] iteration 516 : loss: 0.038404, loss_a: 0.022590
[20:54:01.301] iteration 517 : loss: 0.050306, loss_a: 0.029592
[20:54:02.053] iteration 518 : loss: 0.052278, loss_a: 0.030752
[20:54:03.386] iteration 519 : loss: 0.043439, loss_a: 0.025553
[20:54:04.130] iteration 520 : loss: 0.084033, loss_a: 0.049431
[20:54:05.471] iteration 521 : loss: 0.081913, loss_a: 0.048184
[20:54:06.210] iteration 522 : loss: 0.059939, loss_a: 0.035258
[20:54:07.548] iteration 523 : loss: 0.058535, loss_a: 0.034432
[20:54:08.298] iteration 524 : loss: 0.071584, loss_a: 0.042108
[20:54:09.640] iteration 525 : loss: 0.064454, loss_a: 0.037914
[20:54:10.390] iteration 526 : loss: 0.063976, loss_a: 0.037633
[20:54:11.707] iteration 527 : loss: 0.033771, loss_a: 0.019865
[20:54:12.465] iteration 528 : loss: 0.075627, loss_a: 0.044487
[20:54:13.810] iteration 529 : loss: 0.037202, loss_a: 0.021884
[20:54:14.542] iteration 530 : loss: 0.056837, loss_a: 0.033434
[20:54:15.864] iteration 531 : loss: 0.077752, loss_a: 0.045736
[20:54:16.603] iteration 532 : loss: 0.073943, loss_a: 0.043496
[20:54:17.914] iteration 533 : loss: 0.066921, loss_a: 0.039365
[20:54:18.659] iteration 534 : loss: 0.051181, loss_a: 0.030107
[20:54:19.998] iteration 535 : loss: 0.041014, loss_a: 0.024126
[20:54:20.733] iteration 536 : loss: 0.036844, loss_a: 0.021673
[20:54:22.089] iteration 537 : loss: 0.065611, loss_a: 0.038595
[20:54:22.828] iteration 538 : loss: 0.063282, loss_a: 0.037225
[20:54:24.180] iteration 539 : loss: 0.045011, loss_a: 0.026477
[20:54:24.908] iteration 540 : loss: 0.035015, loss_a: 0.020597
[20:54:26.260] iteration 541 : loss: 0.111573, loss_a: 0.065631
[20:54:27.010] iteration 542 : loss: 0.107842, loss_a: 0.063436
[20:54:28.328] iteration 543 : loss: 0.061103, loss_a: 0.035943
[20:54:29.061] iteration 544 : loss: 0.029990, loss_a: 0.017641
[20:54:30.387] iteration 545 : loss: 0.062003, loss_a: 0.036473
[20:54:31.127] iteration 546 : loss: 0.027869, loss_a: 0.016393
[20:54:32.438] iteration 547 : loss: 0.046142, loss_a: 0.027142
[20:54:33.173] iteration 548 : loss: 0.024095, loss_a: 0.014174
[20:54:34.511] iteration 549 : loss: 0.226065, loss_a: 0.132979
[20:54:35.254] iteration 550 : loss: 0.032558, loss_a: 0.019152
[20:54:36.594] iteration 551 : loss: 0.076916, loss_a: 0.045245
[20:54:37.325] iteration 552 : loss: 0.106029, loss_a: 0.062370
[20:54:38.668] iteration 553 : loss: 0.062419, loss_a: 0.036717
[20:54:39.401] iteration 554 : loss: 0.049298, loss_a: 0.028999
[20:54:40.728] iteration 555 : loss: 0.059158, loss_a: 0.034799
[20:54:41.460] iteration 556 : loss: 0.087246, loss_a: 0.051321
[20:54:42.812] iteration 557 : loss: 0.180619, loss_a: 0.106246
[20:54:43.564] iteration 558 : loss: 0.058986, loss_a: 0.034698
[20:54:44.934] iteration 559 : loss: 0.071545, loss_a: 0.042085
[20:54:45.674] iteration 560 : loss: 0.044243, loss_a: 0.026026
[20:54:47.015] iteration 561 : loss: 0.087355, loss_a: 0.051385
[20:54:47.747] iteration 562 : loss: 0.035096, loss_a: 0.020644
[20:54:49.098] iteration 563 : loss: 0.062689, loss_a: 0.036876
[20:54:49.843] iteration 564 : loss: 0.080503, loss_a: 0.047355
[20:54:51.181] iteration 565 : loss: 0.049047, loss_a: 0.028851
[20:54:51.930] iteration 566 : loss: 0.098901, loss_a: 0.058177
[20:54:53.265] iteration 567 : loss: 0.069746, loss_a: 0.041027
[20:54:54.006] iteration 568 : loss: 0.048441, loss_a: 0.028494
[20:54:55.322] iteration 569 : loss: 0.024699, loss_a: 0.014529
[20:54:56.063] iteration 570 : loss: 0.032123, loss_a: 0.018896
[20:54:57.472] iteration 571 : loss: 0.064477, loss_a: 0.037928
[20:54:58.207] iteration 572 : loss: 0.040359, loss_a: 0.023740
[20:54:59.543] iteration 573 : loss: 0.057279, loss_a: 0.033693
[20:55:00.281] iteration 574 : loss: 0.040307, loss_a: 0.023710
[20:55:01.602] iteration 575 : loss: 0.043264, loss_a: 0.025449
[20:55:02.358] iteration 576 : loss: 0.166747, loss_a: 0.098087
[20:55:03.684] iteration 577 : loss: 0.072809, loss_a: 0.042829
[20:55:04.433] iteration 578 : loss: 0.062219, loss_a: 0.036600
[20:55:05.747] iteration 579 : loss: 0.079366, loss_a: 0.046686
[20:55:06.491] iteration 580 : loss: 0.078834, loss_a: 0.046373
[20:55:07.832] iteration 581 : loss: 0.042623, loss_a: 0.025073
[20:55:08.574] iteration 582 : loss: 0.035726, loss_a: 0.021015
[20:55:09.881] iteration 583 : loss: 0.082904, loss_a: 0.048767
[20:55:10.634] iteration 584 : loss: 0.087262, loss_a: 0.051330
[20:55:11.967] iteration 585 : loss: 0.045293, loss_a: 0.026643
[20:55:12.706] iteration 586 : loss: 0.042376, loss_a: 0.024927
[20:55:14.030] iteration 587 : loss: 0.045226, loss_a: 0.026603
[20:55:14.777] iteration 588 : loss: 0.108588, loss_a: 0.063875
[20:55:16.103] iteration 589 : loss: 0.047356, loss_a: 0.027856
[20:55:16.845] iteration 590 : loss: 0.081220, loss_a: 0.047776
[20:55:18.151] iteration 591 : loss: 0.029130, loss_a: 0.017135
[20:55:18.899] iteration 592 : loss: 0.056409, loss_a: 0.033182
[20:55:20.212] iteration 593 : loss: 0.038491, loss_a: 0.022642
[20:55:20.943] iteration 594 : loss: 0.117882, loss_a: 0.069343
[20:55:22.290] iteration 595 : loss: 0.042702, loss_a: 0.025119
[20:55:23.037] iteration 596 : loss: 0.034377, loss_a: 0.020222
[20:55:24.394] iteration 597 : loss: 0.129808, loss_a: 0.076358
[20:55:25.133] iteration 598 : loss: 0.104326, loss_a: 0.061368
[20:55:26.456] iteration 599 : loss: 0.068888, loss_a: 0.040522
[20:55:27.197] iteration 600 : loss: 0.045974, loss_a: 0.027044
[20:55:51.799] iteration 601 : loss: 0.089168, loss_a: 0.052452
[20:55:53.855] iteration 602 : loss: 0.042791, loss_a: 0.025171
[20:55:55.200] iteration 603 : loss: 0.081746, loss_a: 0.048086
[20:55:55.957] iteration 604 : loss: 0.058672, loss_a: 0.034513
[20:55:57.285] iteration 605 : loss: 0.047506, loss_a: 0.027945
[20:55:58.032] iteration 606 : loss: 0.068306, loss_a: 0.040180
[20:55:59.359] iteration 607 : loss: 0.036385, loss_a: 0.021403
[20:56:00.115] iteration 608 : loss: 0.078642, loss_a: 0.046260
[20:56:01.472] iteration 609 : loss: 0.162206, loss_a: 0.095415
[20:56:02.223] iteration 610 : loss: 0.096213, loss_a: 0.056596
[20:56:03.547] iteration 611 : loss: 0.059143, loss_a: 0.034790
[20:56:04.292] iteration 612 : loss: 0.065157, loss_a: 0.038328
[20:56:05.613] iteration 613 : loss: 0.079570, loss_a: 0.046806
[20:56:06.343] iteration 614 : loss: 0.035345, loss_a: 0.020791
[20:56:07.657] iteration 615 : loss: 0.055746, loss_a: 0.032791
[20:56:08.417] iteration 616 : loss: 0.128751, loss_a: 0.075736
[20:56:09.781] iteration 617 : loss: 0.136957, loss_a: 0.080563
[20:56:10.520] iteration 618 : loss: 0.076061, loss_a: 0.044742
[20:56:11.881] iteration 619 : loss: 0.102182, loss_a: 0.060107
[20:56:12.624] iteration 620 : loss: 0.048803, loss_a: 0.028707
[20:56:13.978] iteration 621 : loss: 0.063371, loss_a: 0.037277
[20:56:14.717] iteration 622 : loss: 0.038135, loss_a: 0.022432
[20:56:16.093] iteration 623 : loss: 0.107261, loss_a: 0.063095
[20:56:16.838] iteration 624 : loss: 0.060961, loss_a: 0.035860
[20:56:18.187] iteration 625 : loss: 0.059897, loss_a: 0.035234
[20:56:18.929] iteration 626 : loss: 0.085177, loss_a: 0.050104
[20:56:20.291] iteration 627 : loss: 0.066359, loss_a: 0.039035
[20:56:21.033] iteration 628 : loss: 0.053048, loss_a: 0.031205
[20:56:22.356] iteration 629 : loss: 0.066340, loss_a: 0.039024
[20:56:23.103] iteration 630 : loss: 0.063256, loss_a: 0.037210
[20:56:24.426] iteration 631 : loss: 0.051445, loss_a: 0.030262
[20:56:25.166] iteration 632 : loss: 0.042417, loss_a: 0.024951
[20:56:26.488] iteration 633 : loss: 0.060054, loss_a: 0.035326
[20:56:27.228] iteration 634 : loss: 0.050633, loss_a: 0.029784
[20:56:28.583] iteration 635 : loss: 0.083645, loss_a: 0.049203
[20:56:29.330] iteration 636 : loss: 0.087331, loss_a: 0.051371
[20:56:30.668] iteration 637 : loss: 0.052197, loss_a: 0.030704
[20:56:31.417] iteration 638 : loss: 0.053396, loss_a: 0.031409
[20:56:32.758] iteration 639 : loss: 0.097461, loss_a: 0.057330
[20:56:33.503] iteration 640 : loss: 0.104857, loss_a: 0.061680
[20:56:34.855] iteration 641 : loss: 0.080574, loss_a: 0.047397
[20:56:35.610] iteration 642 : loss: 0.079324, loss_a: 0.046661
[20:56:36.973] iteration 643 : loss: 0.076681, loss_a: 0.045107
[20:56:37.715] iteration 644 : loss: 0.058542, loss_a: 0.034437
[20:56:39.083] iteration 645 : loss: 0.049855, loss_a: 0.029326
[20:56:39.828] iteration 646 : loss: 0.045876, loss_a: 0.026986
[20:56:41.190] iteration 647 : loss: 0.037920, loss_a: 0.022306
[20:56:41.919] iteration 648 : loss: 0.024435, loss_a: 0.014373
[20:56:43.266] iteration 649 : loss: 0.037338, loss_a: 0.021963
[20:56:44.018] iteration 650 : loss: 0.033255, loss_a: 0.019562
[20:56:45.336] iteration 651 : loss: 0.058591, loss_a: 0.034465
[20:56:46.073] iteration 652 : loss: 0.051042, loss_a: 0.030025
[20:56:47.409] iteration 653 : loss: 0.096540, loss_a: 0.056788
[20:56:48.149] iteration 654 : loss: 0.062497, loss_a: 0.036763
[20:56:49.460] iteration 655 : loss: 0.037153, loss_a: 0.021855
[20:56:50.220] iteration 656 : loss: 0.104644, loss_a: 0.061555
[20:56:51.541] iteration 657 : loss: 0.070405, loss_a: 0.041415
[20:56:52.283] iteration 658 : loss: 0.076889, loss_a: 0.045229
[20:56:53.642] iteration 659 : loss: 0.026962, loss_a: 0.015860
[20:56:54.389] iteration 660 : loss: 0.069301, loss_a: 0.040765
[20:56:55.703] iteration 661 : loss: 0.061413, loss_a: 0.036125
[20:56:56.443] iteration 662 : loss: 0.034098, loss_a: 0.020058
[20:56:57.781] iteration 663 : loss: 0.090856, loss_a: 0.053445
[20:56:58.527] iteration 664 : loss: 0.064625, loss_a: 0.038015
[20:56:59.848] iteration 665 : loss: 0.029394, loss_a: 0.017290
[20:57:00.604] iteration 666 : loss: 0.074581, loss_a: 0.043871
[20:57:01.936] iteration 667 : loss: 0.080089, loss_a: 0.047111
[20:57:02.671] iteration 668 : loss: 0.080400, loss_a: 0.047294
[20:57:04.017] iteration 669 : loss: 0.085722, loss_a: 0.050425
[20:57:04.757] iteration 670 : loss: 0.047910, loss_a: 0.028182
[20:57:06.084] iteration 671 : loss: 0.037965, loss_a: 0.022332
[20:57:06.823] iteration 672 : loss: 0.037276, loss_a: 0.021927
[20:57:08.164] iteration 673 : loss: 0.034184, loss_a: 0.020108
[20:57:08.903] iteration 674 : loss: 0.084640, loss_a: 0.049788
[20:57:10.214] iteration 675 : loss: 0.030611, loss_a: 0.018007
[20:57:10.951] iteration 676 : loss: 0.044584, loss_a: 0.026226
[20:57:12.278] iteration 677 : loss: 0.041537, loss_a: 0.024433
[20:57:13.028] iteration 678 : loss: 0.051614, loss_a: 0.030361
[20:57:14.378] iteration 679 : loss: 0.047094, loss_a: 0.027703
[20:57:15.122] iteration 680 : loss: 0.050784, loss_a: 0.029873
[20:57:16.478] iteration 681 : loss: 0.052320, loss_a: 0.030777
[20:57:17.217] iteration 682 : loss: 0.058553, loss_a: 0.034443
[20:57:18.566] iteration 683 : loss: 0.048737, loss_a: 0.028669
[20:57:19.298] iteration 684 : loss: 0.062991, loss_a: 0.037054
[20:57:20.615] iteration 685 : loss: 0.063702, loss_a: 0.037472
[20:57:21.358] iteration 686 : loss: 0.035777, loss_a: 0.021046
[20:57:22.664] iteration 687 : loss: 0.037108, loss_a: 0.021828
[20:57:23.416] iteration 688 : loss: 0.060269, loss_a: 0.035452
[20:57:24.747] iteration 689 : loss: 0.041626, loss_a: 0.024486
[20:57:25.489] iteration 690 : loss: 0.038318, loss_a: 0.022540
[20:57:26.807] iteration 691 : loss: 0.051043, loss_a: 0.030025
[20:57:27.546] iteration 692 : loss: 0.096063, loss_a: 0.056508
[20:57:28.887] iteration 693 : loss: 0.046009, loss_a: 0.027064
[20:57:29.624] iteration 694 : loss: 0.055483, loss_a: 0.032637
[20:57:30.960] iteration 695 : loss: 0.056936, loss_a: 0.033492
[20:57:31.702] iteration 696 : loss: 0.038913, loss_a: 0.022890
[20:57:33.043] iteration 697 : loss: 0.042416, loss_a: 0.024950
[20:57:33.794] iteration 698 : loss: 0.035357, loss_a: 0.020798
[20:57:35.129] iteration 699 : loss: 0.028973, loss_a: 0.017043
[20:57:35.862] iteration 700 : loss: 0.076480, loss_a: 0.044989
[20:57:37.207] iteration 701 : loss: 0.064410, loss_a: 0.037888
[20:57:37.938] iteration 702 : loss: 0.032868, loss_a: 0.019334
[20:57:39.275] iteration 703 : loss: 0.049503, loss_a: 0.029119
[20:57:40.010] iteration 704 : loss: 0.020986, loss_a: 0.012345
[20:57:41.347] iteration 705 : loss: 0.062711, loss_a: 0.036889
[20:57:42.090] iteration 706 : loss: 0.051040, loss_a: 0.030024
[20:57:43.449] iteration 707 : loss: 0.046738, loss_a: 0.027493
[20:57:44.197] iteration 708 : loss: 0.047616, loss_a: 0.028010
[20:57:45.517] iteration 709 : loss: 0.076317, loss_a: 0.044892
[20:57:46.256] iteration 710 : loss: 0.027516, loss_a: 0.016186
[20:57:47.587] iteration 711 : loss: 0.037659, loss_a: 0.022152
[20:57:48.340] iteration 712 : loss: 0.074921, loss_a: 0.044071
[20:57:49.642] iteration 713 : loss: 0.054823, loss_a: 0.032249
[20:57:50.397] iteration 714 : loss: 0.057015, loss_a: 0.033538
[20:57:51.751] iteration 715 : loss: 0.092801, loss_a: 0.054589
[20:57:52.498] iteration 716 : loss: 0.034571, loss_a: 0.020336
[20:57:53.826] iteration 717 : loss: 0.081107, loss_a: 0.047710
[20:57:54.572] iteration 718 : loss: 0.310230, loss_a: 0.182488
[20:57:55.902] iteration 719 : loss: 0.030858, loss_a: 0.018151
[20:57:56.639] iteration 720 : loss: 0.073175, loss_a: 0.043044
[20:57:57.972] iteration 721 : loss: 0.027042, loss_a: 0.015907
[20:57:58.709] iteration 722 : loss: 0.043360, loss_a: 0.025506
[20:58:00.022] iteration 723 : loss: 0.077876, loss_a: 0.045809
[20:58:00.763] iteration 724 : loss: 0.060668, loss_a: 0.035687
[20:58:02.110] iteration 725 : loss: 0.063538, loss_a: 0.037375
[20:58:02.847] iteration 726 : loss: 0.049066, loss_a: 0.028862
[20:58:04.176] iteration 727 : loss: 0.058162, loss_a: 0.034213
[20:58:04.926] iteration 728 : loss: 0.075333, loss_a: 0.044314
[20:58:06.325] iteration 729 : loss: 0.036162, loss_a: 0.021272
[20:58:07.069] iteration 730 : loss: 0.085177, loss_a: 0.050104
[20:58:08.406] iteration 731 : loss: 0.041832, loss_a: 0.024607
[20:58:09.147] iteration 732 : loss: 0.090270, loss_a: 0.053100
[20:58:10.500] iteration 733 : loss: 0.036070, loss_a: 0.021218
[20:58:11.244] iteration 734 : loss: 0.048080, loss_a: 0.028283
[20:58:12.600] iteration 735 : loss: 0.096159, loss_a: 0.056564
[20:58:13.345] iteration 736 : loss: 0.073071, loss_a: 0.042983
[20:58:14.690] iteration 737 : loss: 0.055721, loss_a: 0.032777
[20:58:15.432] iteration 738 : loss: 0.097407, loss_a: 0.057298
[20:58:16.789] iteration 739 : loss: 0.043433, loss_a: 0.025549
[20:58:17.521] iteration 740 : loss: 0.029832, loss_a: 0.017549
[20:58:18.878] iteration 741 : loss: 0.057506, loss_a: 0.033827
[20:58:19.622] iteration 742 : loss: 0.070559, loss_a: 0.041506
[20:58:20.972] iteration 743 : loss: 0.058379, loss_a: 0.034341
[20:58:21.717] iteration 744 : loss: 0.063580, loss_a: 0.037400
[20:58:23.066] iteration 745 : loss: 0.086437, loss_a: 0.050845
[20:58:23.806] iteration 746 : loss: 0.042716, loss_a: 0.025127
[20:58:25.151] iteration 747 : loss: 0.033035, loss_a: 0.019432
[20:58:25.894] iteration 748 : loss: 0.044806, loss_a: 0.026357
[20:58:27.255] iteration 749 : loss: 0.071091, loss_a: 0.041818
[20:58:27.993] iteration 750 : loss: 0.064666, loss_a: 0.038039
[20:58:29.326] iteration 751 : loss: 0.060043, loss_a: 0.035319
[20:58:30.070] iteration 752 : loss: 0.029726, loss_a: 0.017486
[20:58:31.420] iteration 753 : loss: 0.039509, loss_a: 0.023241
[20:58:32.168] iteration 754 : loss: 0.048114, loss_a: 0.028302
[20:58:33.524] iteration 755 : loss: 0.038826, loss_a: 0.022839
[20:58:34.274] iteration 756 : loss: 0.045068, loss_a: 0.026510
[20:58:35.602] iteration 757 : loss: 0.054214, loss_a: 0.031890
[20:58:36.356] iteration 758 : loss: 0.073237, loss_a: 0.043081
[20:58:37.681] iteration 759 : loss: 0.065437, loss_a: 0.038492
[20:58:38.424] iteration 760 : loss: 0.122573, loss_a: 0.072102
[20:58:39.744] iteration 761 : loss: 0.046429, loss_a: 0.027311
[20:58:40.477] iteration 762 : loss: 0.022427, loss_a: 0.013193
[20:58:41.818] iteration 763 : loss: 0.029235, loss_a: 0.017197
[20:58:42.569] iteration 764 : loss: 0.051833, loss_a: 0.030490
[20:58:43.909] iteration 765 : loss: 0.031835, loss_a: 0.018727
[20:58:44.643] iteration 766 : loss: 0.039369, loss_a: 0.023158
[20:58:45.998] iteration 767 : loss: 0.060516, loss_a: 0.035597
[20:58:46.738] iteration 768 : loss: 0.037186, loss_a: 0.021874
[20:58:48.071] iteration 769 : loss: 0.074008, loss_a: 0.043534
[20:58:48.818] iteration 770 : loss: 0.032178, loss_a: 0.018928
[20:58:50.187] iteration 771 : loss: 0.067526, loss_a: 0.039721
[20:58:50.940] iteration 772 : loss: 0.114388, loss_a: 0.067287
[20:58:52.291] iteration 773 : loss: 0.049233, loss_a: 0.028961
[20:58:53.029] iteration 774 : loss: 0.030734, loss_a: 0.018079
[20:58:54.356] iteration 775 : loss: 0.074584, loss_a: 0.043873
[20:58:55.094] iteration 776 : loss: 0.101579, loss_a: 0.059753
[20:58:56.422] iteration 777 : loss: 0.061305, loss_a: 0.036062
[20:58:57.182] iteration 778 : loss: 0.059123, loss_a: 0.034778
[20:58:58.544] iteration 779 : loss: 0.113375, loss_a: 0.066691
[20:58:59.284] iteration 780 : loss: 0.073729, loss_a: 0.043370
[20:59:00.650] iteration 781 : loss: 0.057553, loss_a: 0.033855
[20:59:01.386] iteration 782 : loss: 0.044633, loss_a: 0.026255
[20:59:02.735] iteration 783 : loss: 0.064528, loss_a: 0.037957
[20:59:03.480] iteration 784 : loss: 0.037500, loss_a: 0.022059
[20:59:04.820] iteration 785 : loss: 0.053945, loss_a: 0.031732
[20:59:05.565] iteration 786 : loss: 0.047857, loss_a: 0.028151
[20:59:06.927] iteration 787 : loss: 0.060589, loss_a: 0.035640
[20:59:07.676] iteration 788 : loss: 0.036764, loss_a: 0.021626
[20:59:09.012] iteration 789 : loss: 0.042914, loss_a: 0.025244
[20:59:09.766] iteration 790 : loss: 0.050563, loss_a: 0.029743
[20:59:11.115] iteration 791 : loss: 0.066626, loss_a: 0.039192
[20:59:11.864] iteration 792 : loss: 0.077197, loss_a: 0.045410
[20:59:13.204] iteration 793 : loss: 0.104360, loss_a: 0.061388
[20:59:13.944] iteration 794 : loss: 0.045824, loss_a: 0.026955
[20:59:15.275] iteration 795 : loss: 0.053863, loss_a: 0.031684
[20:59:16.013] iteration 796 : loss: 0.040441, loss_a: 0.023789
[20:59:17.369] iteration 797 : loss: 0.083838, loss_a: 0.049316
[20:59:18.111] iteration 798 : loss: 0.025243, loss_a: 0.014849
[20:59:19.471] iteration 799 : loss: 0.063408, loss_a: 0.037299
[20:59:20.213] iteration 800 : loss: 0.071592, loss_a: 0.042113
[20:59:43.769] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_800_dice_0.8564.pth
[20:59:45.110] iteration 801 : loss: 0.055447, loss_a: 0.032616
[20:59:47.339] iteration 802 : loss: 0.060187, loss_a: 0.035404
[20:59:48.710] iteration 803 : loss: 0.031494, loss_a: 0.018526
[20:59:49.447] iteration 804 : loss: 0.048491, loss_a: 0.028524
[20:59:50.818] iteration 805 : loss: 0.079292, loss_a: 0.046643
[20:59:51.567] iteration 806 : loss: 0.086763, loss_a: 0.051037
[20:59:52.909] iteration 807 : loss: 0.032427, loss_a: 0.019074
[20:59:53.651] iteration 808 : loss: 0.041706, loss_a: 0.024533
[20:59:55.004] iteration 809 : loss: 0.047253, loss_a: 0.027796
[20:59:55.749] iteration 810 : loss: 0.075305, loss_a: 0.044297
[20:59:57.100] iteration 811 : loss: 0.041899, loss_a: 0.024647
[20:59:57.844] iteration 812 : loss: 0.039477, loss_a: 0.023222
[20:59:59.197] iteration 813 : loss: 0.066290, loss_a: 0.038994
[20:59:59.958] iteration 814 : loss: 0.072730, loss_a: 0.042782
[21:00:01.319] iteration 815 : loss: 0.100405, loss_a: 0.059061
[21:00:02.054] iteration 816 : loss: 0.030387, loss_a: 0.017875
[21:00:03.374] iteration 817 : loss: 0.064758, loss_a: 0.038093
[21:00:04.110] iteration 818 : loss: 0.031718, loss_a: 0.018658
[21:00:05.473] iteration 819 : loss: 0.038773, loss_a: 0.022807
[21:00:06.217] iteration 820 : loss: 0.039493, loss_a: 0.023231
[21:00:07.597] iteration 821 : loss: 0.042493, loss_a: 0.024996
[21:00:08.345] iteration 822 : loss: 0.042211, loss_a: 0.024830
[21:00:09.678] iteration 823 : loss: 0.039224, loss_a: 0.023073
[21:00:10.419] iteration 824 : loss: 0.052296, loss_a: 0.030763
[21:00:11.772] iteration 825 : loss: 0.064226, loss_a: 0.037780
[21:00:12.504] iteration 826 : loss: 0.046821, loss_a: 0.027542
[21:00:13.846] iteration 827 : loss: 0.097673, loss_a: 0.057455
[21:00:14.584] iteration 828 : loss: 0.036769, loss_a: 0.021629
[21:00:15.932] iteration 829 : loss: 0.028768, loss_a: 0.016922
[21:00:16.670] iteration 830 : loss: 0.049744, loss_a: 0.029261
[21:00:18.007] iteration 831 : loss: 0.058273, loss_a: 0.034278
[21:00:18.756] iteration 832 : loss: 0.051625, loss_a: 0.030368
[21:00:20.117] iteration 833 : loss: 0.049530, loss_a: 0.029135
[21:00:20.852] iteration 834 : loss: 0.034300, loss_a: 0.020177
[21:00:22.195] iteration 835 : loss: 0.036592, loss_a: 0.021524
[21:00:22.949] iteration 836 : loss: 0.045432, loss_a: 0.026725
[21:00:24.306] iteration 837 : loss: 0.060987, loss_a: 0.035875
[21:00:25.045] iteration 838 : loss: 0.044490, loss_a: 0.026171
[21:00:26.362] iteration 839 : loss: 0.019762, loss_a: 0.011625
[21:00:27.107] iteration 840 : loss: 0.063736, loss_a: 0.037492
[21:00:28.459] iteration 841 : loss: 0.024934, loss_a: 0.014667
[21:00:29.196] iteration 842 : loss: 0.041508, loss_a: 0.024417
[21:00:30.555] iteration 843 : loss: 0.089238, loss_a: 0.052493
[21:00:31.300] iteration 844 : loss: 0.044260, loss_a: 0.026035
[21:00:32.668] iteration 845 : loss: 0.108244, loss_a: 0.063673
[21:00:33.404] iteration 846 : loss: 0.061184, loss_a: 0.035990
[21:00:34.731] iteration 847 : loss: 0.032370, loss_a: 0.019041
[21:00:35.465] iteration 848 : loss: 0.037340, loss_a: 0.021965
[21:00:36.791] iteration 849 : loss: 0.029924, loss_a: 0.017603
[21:00:37.538] iteration 850 : loss: 0.030186, loss_a: 0.017757
[21:00:38.892] iteration 851 : loss: 0.051078, loss_a: 0.030046
[21:00:39.632] iteration 852 : loss: 0.064474, loss_a: 0.037926
[21:00:40.976] iteration 853 : loss: 0.040572, loss_a: 0.023866
[21:00:41.723] iteration 854 : loss: 0.035853, loss_a: 0.021090
[21:00:43.091] iteration 855 : loss: 0.046028, loss_a: 0.027076
[21:00:43.826] iteration 856 : loss: 0.047737, loss_a: 0.028081
[21:00:45.192] iteration 857 : loss: 0.057125, loss_a: 0.033603
[21:00:45.943] iteration 858 : loss: 0.102835, loss_a: 0.060491
[21:00:47.263] iteration 859 : loss: 0.021831, loss_a: 0.012842
[21:00:47.997] iteration 860 : loss: 0.044919, loss_a: 0.026423
[21:00:49.317] iteration 861 : loss: 0.050298, loss_a: 0.029587
[21:00:50.054] iteration 862 : loss: 0.034628, loss_a: 0.020369
[21:00:51.375] iteration 863 : loss: 0.033432, loss_a: 0.019666
[21:00:52.118] iteration 864 : loss: 0.055533, loss_a: 0.032667
[21:00:53.443] iteration 865 : loss: 0.046390, loss_a: 0.027288
[21:00:54.182] iteration 866 : loss: 0.031149, loss_a: 0.018323
[21:00:55.526] iteration 867 : loss: 0.050639, loss_a: 0.029787
[21:00:56.263] iteration 868 : loss: 0.054985, loss_a: 0.032344
[21:00:57.577] iteration 869 : loss: 0.044795, loss_a: 0.026350
[21:00:58.338] iteration 870 : loss: 0.075335, loss_a: 0.044315
[21:00:59.676] iteration 871 : loss: 0.033255, loss_a: 0.019562
[21:01:00.411] iteration 872 : loss: 0.024971, loss_a: 0.014689
[21:01:01.772] iteration 873 : loss: 0.080573, loss_a: 0.047396
[21:01:02.516] iteration 874 : loss: 0.061348, loss_a: 0.036087
[21:01:03.864] iteration 875 : loss: 0.075455, loss_a: 0.044385
[21:01:04.602] iteration 876 : loss: 0.051824, loss_a: 0.030485
[21:01:05.919] iteration 877 : loss: 0.051999, loss_a: 0.030588
[21:01:06.655] iteration 878 : loss: 0.126819, loss_a: 0.074600
[21:01:07.984] iteration 879 : loss: 0.054435, loss_a: 0.032021
[21:01:08.754] iteration 880 : loss: 0.081004, loss_a: 0.047650
[21:01:10.117] iteration 881 : loss: 0.039040, loss_a: 0.022965
[21:01:10.869] iteration 882 : loss: 0.102465, loss_a: 0.060273
[21:01:12.230] iteration 883 : loss: 0.090897, loss_a: 0.053469
[21:01:12.981] iteration 884 : loss: 0.052006, loss_a: 0.030592
[21:01:14.340] iteration 885 : loss: 0.099182, loss_a: 0.058342
[21:01:15.079] iteration 886 : loss: 0.060739, loss_a: 0.035729
[21:01:16.403] iteration 887 : loss: 0.122781, loss_a: 0.072224
[21:01:17.144] iteration 888 : loss: 0.032127, loss_a: 0.018898
[21:01:18.498] iteration 889 : loss: 0.049109, loss_a: 0.028887
[21:01:19.229] iteration 890 : loss: 0.029155, loss_a: 0.017150
[21:01:20.560] iteration 891 : loss: 0.078798, loss_a: 0.046352
[21:01:21.295] iteration 892 : loss: 0.039415, loss_a: 0.023185
[21:01:22.614] iteration 893 : loss: 0.031261, loss_a: 0.018389
[21:01:23.354] iteration 894 : loss: 0.111021, loss_a: 0.065307
[21:01:24.738] iteration 895 : loss: 0.098015, loss_a: 0.057656
[21:01:25.482] iteration 896 : loss: 0.046331, loss_a: 0.027254
[21:01:26.844] iteration 897 : loss: 0.045585, loss_a: 0.026815
[21:01:27.591] iteration 898 : loss: 0.050833, loss_a: 0.029902
[21:01:28.916] iteration 899 : loss: 0.033898, loss_a: 0.019940
[21:01:29.664] iteration 900 : loss: 0.073168, loss_a: 0.043040
[21:01:30.987] iteration 901 : loss: 0.020413, loss_a: 0.012007
[21:01:31.732] iteration 902 : loss: 0.050717, loss_a: 0.029833
[21:01:33.077] iteration 903 : loss: 0.077546, loss_a: 0.045615
[21:01:33.816] iteration 904 : loss: 0.070070, loss_a: 0.041217
[21:01:35.134] iteration 905 : loss: 0.030006, loss_a: 0.017651
[21:01:35.873] iteration 906 : loss: 0.045165, loss_a: 0.026568
[21:01:37.197] iteration 907 : loss: 0.083015, loss_a: 0.048832
[21:01:37.943] iteration 908 : loss: 0.034725, loss_a: 0.020427
[21:01:39.260] iteration 909 : loss: 0.020301, loss_a: 0.011942
[21:01:39.994] iteration 910 : loss: 0.037324, loss_a: 0.021955
[21:01:41.329] iteration 911 : loss: 0.044502, loss_a: 0.026178
[21:01:42.071] iteration 912 : loss: 0.055134, loss_a: 0.032432
[21:01:43.416] iteration 913 : loss: 0.038131, loss_a: 0.022430
[21:01:44.149] iteration 914 : loss: 0.055469, loss_a: 0.032629
[21:01:45.469] iteration 915 : loss: 0.031955, loss_a: 0.018797
[21:01:46.210] iteration 916 : loss: 0.038182, loss_a: 0.022460
[21:01:47.563] iteration 917 : loss: 0.055798, loss_a: 0.032822
[21:01:48.308] iteration 918 : loss: 0.051387, loss_a: 0.030228
[21:01:49.647] iteration 919 : loss: 0.017291, loss_a: 0.010171
[21:01:50.383] iteration 920 : loss: 0.029728, loss_a: 0.017487
[21:01:51.698] iteration 921 : loss: 0.031023, loss_a: 0.018249
[21:01:52.429] iteration 922 : loss: 0.048956, loss_a: 0.028798
[21:01:53.737] iteration 923 : loss: 0.041528, loss_a: 0.024428
[21:01:54.471] iteration 924 : loss: 0.037828, loss_a: 0.022252
[21:01:55.820] iteration 925 : loss: 0.046971, loss_a: 0.027630
[21:01:56.569] iteration 926 : loss: 0.067068, loss_a: 0.039452
[21:01:57.930] iteration 927 : loss: 0.030643, loss_a: 0.018026
[21:01:58.660] iteration 928 : loss: 0.039294, loss_a: 0.023114
[21:01:59.974] iteration 929 : loss: 0.037510, loss_a: 0.022065
[21:02:00.709] iteration 930 : loss: 0.021973, loss_a: 0.012925
[21:02:02.059] iteration 931 : loss: 0.097877, loss_a: 0.057575
[21:02:02.809] iteration 932 : loss: 0.029002, loss_a: 0.017060
[21:02:04.158] iteration 933 : loss: 0.024050, loss_a: 0.014147
[21:02:04.902] iteration 934 : loss: 0.037099, loss_a: 0.021823
[21:02:06.233] iteration 935 : loss: 0.045290, loss_a: 0.026641
[21:02:06.969] iteration 936 : loss: 0.036187, loss_a: 0.021287
[21:02:08.296] iteration 937 : loss: 0.030155, loss_a: 0.017738
[21:02:09.032] iteration 938 : loss: 0.031089, loss_a: 0.018288
[21:02:10.344] iteration 939 : loss: 0.030210, loss_a: 0.017771
[21:02:11.079] iteration 940 : loss: 0.059192, loss_a: 0.034819
[21:02:12.398] iteration 941 : loss: 0.028347, loss_a: 0.016675
[21:02:13.142] iteration 942 : loss: 0.039997, loss_a: 0.023528
[21:02:14.481] iteration 943 : loss: 0.041695, loss_a: 0.024526
[21:02:15.223] iteration 944 : loss: 0.048173, loss_a: 0.028337
[21:02:16.586] iteration 945 : loss: 0.042241, loss_a: 0.024848
[21:02:17.325] iteration 946 : loss: 0.063307, loss_a: 0.037239
[21:02:18.683] iteration 947 : loss: 0.027365, loss_a: 0.016097
[21:02:19.422] iteration 948 : loss: 0.042108, loss_a: 0.024769
[21:02:20.782] iteration 949 : loss: 0.066953, loss_a: 0.039384
[21:02:21.526] iteration 950 : loss: 0.055926, loss_a: 0.032897
[21:02:22.870] iteration 951 : loss: 0.027056, loss_a: 0.015915
[21:02:23.615] iteration 952 : loss: 0.041916, loss_a: 0.024656
[21:02:24.941] iteration 953 : loss: 0.045022, loss_a: 0.026483
[21:02:25.685] iteration 954 : loss: 0.048816, loss_a: 0.028715
[21:02:27.001] iteration 955 : loss: 0.032579, loss_a: 0.019164
[21:02:27.751] iteration 956 : loss: 0.059110, loss_a: 0.034771
[21:02:29.067] iteration 957 : loss: 0.061625, loss_a: 0.036250
[21:02:29.820] iteration 958 : loss: 0.041986, loss_a: 0.024698
[21:02:31.172] iteration 959 : loss: 0.037264, loss_a: 0.021920
[21:02:31.914] iteration 960 : loss: 0.059361, loss_a: 0.034918
[21:02:33.244] iteration 961 : loss: 0.046937, loss_a: 0.027610
[21:02:33.997] iteration 962 : loss: 0.064706, loss_a: 0.038062
[21:02:35.314] iteration 963 : loss: 0.039021, loss_a: 0.022953
[21:02:36.061] iteration 964 : loss: 0.053766, loss_a: 0.031627
[21:02:37.413] iteration 965 : loss: 0.043742, loss_a: 0.025731
[21:02:38.166] iteration 966 : loss: 0.053936, loss_a: 0.031727
[21:02:39.520] iteration 967 : loss: 0.076005, loss_a: 0.044709
[21:02:40.262] iteration 968 : loss: 0.037693, loss_a: 0.022172
[21:02:41.592] iteration 969 : loss: 0.030742, loss_a: 0.018083
[21:02:42.334] iteration 970 : loss: 0.048751, loss_a: 0.028677
[21:02:43.695] iteration 971 : loss: 0.059531, loss_a: 0.035018
[21:02:44.437] iteration 972 : loss: 0.036776, loss_a: 0.021633
[21:02:45.815] iteration 973 : loss: 0.038210, loss_a: 0.022476
[21:02:46.560] iteration 974 : loss: 0.021788, loss_a: 0.012817
[21:02:47.908] iteration 975 : loss: 0.053582, loss_a: 0.031519
[21:02:48.650] iteration 976 : loss: 0.036705, loss_a: 0.021591
[21:02:49.982] iteration 977 : loss: 0.047418, loss_a: 0.027893
[21:02:50.720] iteration 978 : loss: 0.014658, loss_a: 0.008622
[21:02:52.053] iteration 979 : loss: 0.055110, loss_a: 0.032418
[21:02:52.791] iteration 980 : loss: 0.047000, loss_a: 0.027647
[21:02:54.133] iteration 981 : loss: 0.030903, loss_a: 0.018178
[21:02:54.878] iteration 982 : loss: 0.028656, loss_a: 0.016857
[21:02:56.199] iteration 983 : loss: 0.024712, loss_a: 0.014536
[21:02:56.943] iteration 984 : loss: 0.065714, loss_a: 0.038655
[21:02:58.261] iteration 985 : loss: 0.059573, loss_a: 0.035043
[21:02:59.002] iteration 986 : loss: 0.044772, loss_a: 0.026337
[21:03:00.325] iteration 987 : loss: 0.034459, loss_a: 0.020270
[21:03:01.067] iteration 988 : loss: 0.043552, loss_a: 0.025619
[21:03:02.406] iteration 989 : loss: 0.044387, loss_a: 0.026110
[21:03:03.152] iteration 990 : loss: 0.043809, loss_a: 0.025770
[21:03:04.461] iteration 991 : loss: 0.029480, loss_a: 0.017341
[21:03:05.210] iteration 992 : loss: 0.055229, loss_a: 0.032487
[21:03:06.581] iteration 993 : loss: 0.050362, loss_a: 0.029625
[21:03:07.327] iteration 994 : loss: 0.033905, loss_a: 0.019944
[21:03:08.656] iteration 995 : loss: 0.058333, loss_a: 0.034314
[21:03:09.403] iteration 996 : loss: 0.030070, loss_a: 0.017688
[21:03:10.746] iteration 997 : loss: 0.028002, loss_a: 0.016472
[21:03:11.490] iteration 998 : loss: 0.062841, loss_a: 0.036965
[21:03:12.805] iteration 999 : loss: 0.051300, loss_a: 0.030177
[21:03:13.547] iteration 1000 : loss: 0.033978, loss_a: 0.019987
[21:03:37.137] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_1000_dice_0.8734.pth
[21:03:38.477] iteration 1001 : loss: 0.028890, loss_a: 0.016994
[21:03:40.640] iteration 1002 : loss: 0.054334, loss_a: 0.031961
[21:03:41.945] iteration 1003 : loss: 0.024621, loss_a: 0.014483
[21:03:42.685] iteration 1004 : loss: 0.059052, loss_a: 0.034736
[21:03:44.021] iteration 1005 : loss: 0.080292, loss_a: 0.047231
[21:03:44.754] iteration 1006 : loss: 0.035711, loss_a: 0.021007
[21:03:46.080] iteration 1007 : loss: 0.058250, loss_a: 0.034265
[21:03:46.822] iteration 1008 : loss: 0.087377, loss_a: 0.051399
[21:03:48.140] iteration 1009 : loss: 0.032560, loss_a: 0.019153
[21:03:48.890] iteration 1010 : loss: 0.051724, loss_a: 0.030426
[21:03:50.235] iteration 1011 : loss: 0.028813, loss_a: 0.016949
[21:03:50.977] iteration 1012 : loss: 0.081945, loss_a: 0.048203
[21:03:52.327] iteration 1013 : loss: 0.068171, loss_a: 0.040101
[21:03:53.078] iteration 1014 : loss: 0.059040, loss_a: 0.034729
[21:03:54.392] iteration 1015 : loss: 0.023962, loss_a: 0.014095
[21:03:55.135] iteration 1016 : loss: 0.062255, loss_a: 0.036620
[21:03:56.487] iteration 1017 : loss: 0.065024, loss_a: 0.038249
[21:03:57.230] iteration 1018 : loss: 0.041593, loss_a: 0.024466
[21:03:58.554] iteration 1019 : loss: 0.052003, loss_a: 0.030590
[21:03:59.287] iteration 1020 : loss: 0.081673, loss_a: 0.048043
[21:04:00.673] iteration 1021 : loss: 0.122813, loss_a: 0.072243
[21:04:01.407] iteration 1022 : loss: 0.044656, loss_a: 0.026268
[21:04:02.773] iteration 1023 : loss: 0.043562, loss_a: 0.025625
[21:04:03.516] iteration 1024 : loss: 0.054144, loss_a: 0.031850
[21:04:04.871] iteration 1025 : loss: 0.063607, loss_a: 0.037416
[21:04:05.601] iteration 1026 : loss: 0.020331, loss_a: 0.011959
[21:04:06.954] iteration 1027 : loss: 0.035344, loss_a: 0.020791
[21:04:07.702] iteration 1028 : loss: 0.040653, loss_a: 0.023913
[21:04:09.033] iteration 1029 : loss: 0.023644, loss_a: 0.013908
[21:04:09.777] iteration 1030 : loss: 0.043436, loss_a: 0.025550
[21:04:11.099] iteration 1031 : loss: 0.039896, loss_a: 0.023468
[21:04:11.836] iteration 1032 : loss: 0.024709, loss_a: 0.014535
[21:04:13.194] iteration 1033 : loss: 0.061110, loss_a: 0.035947
[21:04:13.935] iteration 1034 : loss: 0.034855, loss_a: 0.020503
[21:04:15.259] iteration 1035 : loss: 0.025275, loss_a: 0.014868
[21:04:16.004] iteration 1036 : loss: 0.084581, loss_a: 0.049753
[21:04:17.325] iteration 1037 : loss: 0.088494, loss_a: 0.052055
[21:04:18.071] iteration 1038 : loss: 0.045925, loss_a: 0.027015
[21:04:19.382] iteration 1039 : loss: 0.037413, loss_a: 0.022008
[21:04:20.126] iteration 1040 : loss: 0.071402, loss_a: 0.042001
[21:04:21.480] iteration 1041 : loss: 0.060091, loss_a: 0.035347
[21:04:22.220] iteration 1042 : loss: 0.082425, loss_a: 0.048486
[21:04:23.574] iteration 1043 : loss: 0.039719, loss_a: 0.023364
[21:04:24.315] iteration 1044 : loss: 0.046619, loss_a: 0.027423
[21:04:25.655] iteration 1045 : loss: 0.040933, loss_a: 0.024078
[21:04:26.402] iteration 1046 : loss: 0.083651, loss_a: 0.049206
[21:04:27.719] iteration 1047 : loss: 0.055579, loss_a: 0.032693
[21:04:28.460] iteration 1048 : loss: 0.116116, loss_a: 0.068304
[21:04:29.802] iteration 1049 : loss: 0.070863, loss_a: 0.041684
[21:04:30.546] iteration 1050 : loss: 0.030819, loss_a: 0.018129
[21:04:31.885] iteration 1051 : loss: 0.047878, loss_a: 0.028164
[21:04:32.621] iteration 1052 : loss: 0.056417, loss_a: 0.033186
[21:04:33.950] iteration 1053 : loss: 0.088982, loss_a: 0.052343
[21:04:34.699] iteration 1054 : loss: 0.053846, loss_a: 0.031674
[21:04:36.044] iteration 1055 : loss: 0.052779, loss_a: 0.031046
[21:04:36.783] iteration 1056 : loss: 0.092285, loss_a: 0.054285
[21:04:38.123] iteration 1057 : loss: 0.058861, loss_a: 0.034624
[21:04:38.872] iteration 1058 : loss: 0.027191, loss_a: 0.015995
[21:04:40.215] iteration 1059 : loss: 0.041830, loss_a: 0.024606
[21:04:40.960] iteration 1060 : loss: 0.063644, loss_a: 0.037437
[21:04:42.268] iteration 1061 : loss: 0.048794, loss_a: 0.028703
[21:04:43.013] iteration 1062 : loss: 0.026475, loss_a: 0.015574
[21:04:44.382] iteration 1063 : loss: 0.036949, loss_a: 0.021735
[21:04:45.129] iteration 1064 : loss: 0.057053, loss_a: 0.033560
[21:04:46.507] iteration 1065 : loss: 0.079378, loss_a: 0.046693
[21:04:47.260] iteration 1066 : loss: 0.066124, loss_a: 0.038897
[21:04:48.574] iteration 1067 : loss: 0.085121, loss_a: 0.050071
[21:04:49.311] iteration 1068 : loss: 0.032751, loss_a: 0.019265
[21:04:50.650] iteration 1069 : loss: 0.057629, loss_a: 0.033899
[21:04:51.394] iteration 1070 : loss: 0.041887, loss_a: 0.024640
[21:04:52.722] iteration 1071 : loss: 0.077904, loss_a: 0.045826
[21:04:53.462] iteration 1072 : loss: 0.036510, loss_a: 0.021476
[21:04:54.783] iteration 1073 : loss: 0.036347, loss_a: 0.021380
[21:04:55.515] iteration 1074 : loss: 0.031163, loss_a: 0.018331
[21:04:56.839] iteration 1075 : loss: 0.035769, loss_a: 0.021041
[21:04:57.583] iteration 1076 : loss: 0.070393, loss_a: 0.041408
[21:04:58.911] iteration 1077 : loss: 0.031905, loss_a: 0.018767
[21:04:59.668] iteration 1078 : loss: 0.022332, loss_a: 0.013137
[21:05:00.993] iteration 1079 : loss: 0.103849, loss_a: 0.061088
[21:05:01.738] iteration 1080 : loss: 0.044720, loss_a: 0.026306
[21:05:03.082] iteration 1081 : loss: 0.028780, loss_a: 0.016930
[21:05:03.813] iteration 1082 : loss: 0.029429, loss_a: 0.017311
[21:05:05.174] iteration 1083 : loss: 0.104625, loss_a: 0.061544
[21:05:05.923] iteration 1084 : loss: 0.059322, loss_a: 0.034895
[21:05:07.276] iteration 1085 : loss: 0.037911, loss_a: 0.022301
[21:05:08.006] iteration 1086 : loss: 0.034122, loss_a: 0.020072
[21:05:09.333] iteration 1087 : loss: 0.054881, loss_a: 0.032283
[21:05:10.076] iteration 1088 : loss: 0.042040, loss_a: 0.024729
[21:05:11.396] iteration 1089 : loss: 0.044640, loss_a: 0.026259
[21:05:12.143] iteration 1090 : loss: 0.039818, loss_a: 0.023422
[21:05:13.504] iteration 1091 : loss: 0.106213, loss_a: 0.062478
[21:05:14.247] iteration 1092 : loss: 0.046316, loss_a: 0.027245
[21:05:15.603] iteration 1093 : loss: 0.033332, loss_a: 0.019607
[21:05:16.348] iteration 1094 : loss: 0.054885, loss_a: 0.032285
[21:05:17.684] iteration 1095 : loss: 0.034371, loss_a: 0.020218
[21:05:18.424] iteration 1096 : loss: 0.038083, loss_a: 0.022402
[21:05:19.769] iteration 1097 : loss: 0.029270, loss_a: 0.017218
[21:05:20.507] iteration 1098 : loss: 0.038824, loss_a: 0.022837
[21:05:21.851] iteration 1099 : loss: 0.035915, loss_a: 0.021127
[21:05:22.594] iteration 1100 : loss: 0.050497, loss_a: 0.029704
[21:05:23.953] iteration 1101 : loss: 0.062822, loss_a: 0.036954
[21:05:24.689] iteration 1102 : loss: 0.039355, loss_a: 0.023150
[21:05:26.021] iteration 1103 : loss: 0.077408, loss_a: 0.045534
[21:05:26.775] iteration 1104 : loss: 0.100986, loss_a: 0.059403
[21:05:28.115] iteration 1105 : loss: 0.059253, loss_a: 0.034855
[21:05:28.855] iteration 1106 : loss: 0.065543, loss_a: 0.038555
[21:05:30.207] iteration 1107 : loss: 0.062490, loss_a: 0.036759
[21:05:30.951] iteration 1108 : loss: 0.044662, loss_a: 0.026272
[21:05:32.248] iteration 1109 : loss: 0.029902, loss_a: 0.017589
[21:05:32.991] iteration 1110 : loss: 0.034233, loss_a: 0.020137
[21:05:34.343] iteration 1111 : loss: 0.039715, loss_a: 0.023362
[21:05:35.084] iteration 1112 : loss: 0.054972, loss_a: 0.032337
[21:05:36.393] iteration 1113 : loss: 0.035697, loss_a: 0.020998
[21:05:37.137] iteration 1114 : loss: 0.052308, loss_a: 0.030769
[21:05:38.462] iteration 1115 : loss: 0.045057, loss_a: 0.026504
[21:05:39.196] iteration 1116 : loss: 0.029629, loss_a: 0.017429
[21:05:40.553] iteration 1117 : loss: 0.062679, loss_a: 0.036870
[21:05:41.292] iteration 1118 : loss: 0.034827, loss_a: 0.020487
[21:05:42.624] iteration 1119 : loss: 0.045039, loss_a: 0.026493
[21:05:43.362] iteration 1120 : loss: 0.026323, loss_a: 0.015484
[21:05:44.703] iteration 1121 : loss: 0.082938, loss_a: 0.048787
[21:05:45.456] iteration 1122 : loss: 0.040316, loss_a: 0.023715
[21:05:46.777] iteration 1123 : loss: 0.023484, loss_a: 0.013814
[21:05:47.531] iteration 1124 : loss: 0.070953, loss_a: 0.041737
[21:05:48.849] iteration 1125 : loss: 0.037594, loss_a: 0.022114
[21:05:49.589] iteration 1126 : loss: 0.120611, loss_a: 0.070947
[21:05:50.936] iteration 1127 : loss: 0.050039, loss_a: 0.029435
[21:05:51.665] iteration 1128 : loss: 0.051688, loss_a: 0.030405
[21:05:53.027] iteration 1129 : loss: 0.038715, loss_a: 0.022774
[21:05:53.770] iteration 1130 : loss: 0.055940, loss_a: 0.032906
[21:05:55.086] iteration 1131 : loss: 0.064501, loss_a: 0.037941
[21:05:55.836] iteration 1132 : loss: 0.026035, loss_a: 0.015314
[21:05:57.175] iteration 1133 : loss: 0.056582, loss_a: 0.033284
[21:05:57.910] iteration 1134 : loss: 0.036456, loss_a: 0.021445
[21:05:59.241] iteration 1135 : loss: 0.058562, loss_a: 0.034448
[21:05:59.980] iteration 1136 : loss: 0.053970, loss_a: 0.031747
[21:06:01.336] iteration 1137 : loss: 0.113568, loss_a: 0.066805
[21:06:02.074] iteration 1138 : loss: 0.051559, loss_a: 0.030329
[21:06:03.387] iteration 1139 : loss: 0.055072, loss_a: 0.032396
[21:06:04.131] iteration 1140 : loss: 0.033558, loss_a: 0.019740
[21:06:05.456] iteration 1141 : loss: 0.030915, loss_a: 0.018186
[21:06:06.185] iteration 1142 : loss: 0.020747, loss_a: 0.012204
[21:06:07.535] iteration 1143 : loss: 0.051235, loss_a: 0.030138
[21:06:08.276] iteration 1144 : loss: 0.066282, loss_a: 0.038989
[21:06:09.625] iteration 1145 : loss: 0.043457, loss_a: 0.025563
[21:06:10.356] iteration 1146 : loss: 0.061902, loss_a: 0.036413
[21:06:11.710] iteration 1147 : loss: 0.030660, loss_a: 0.018035
[21:06:12.446] iteration 1148 : loss: 0.042069, loss_a: 0.024746
[21:06:13.773] iteration 1149 : loss: 0.039457, loss_a: 0.023210
[21:06:14.504] iteration 1150 : loss: 0.032419, loss_a: 0.019070
[21:06:15.855] iteration 1151 : loss: 0.088858, loss_a: 0.052269
[21:06:16.602] iteration 1152 : loss: 0.093903, loss_a: 0.055237
[21:06:17.975] iteration 1153 : loss: 0.050114, loss_a: 0.029479
[21:06:18.715] iteration 1154 : loss: 0.032480, loss_a: 0.019106
[21:06:20.030] iteration 1155 : loss: 0.085676, loss_a: 0.050397
[21:06:20.774] iteration 1156 : loss: 0.033667, loss_a: 0.019804
[21:06:22.115] iteration 1157 : loss: 0.036525, loss_a: 0.021485
[21:06:22.851] iteration 1158 : loss: 0.023834, loss_a: 0.014020
[21:06:24.156] iteration 1159 : loss: 0.059217, loss_a: 0.034833
[21:06:24.898] iteration 1160 : loss: 0.056384, loss_a: 0.033167
[21:06:26.217] iteration 1161 : loss: 0.048060, loss_a: 0.028271
[21:06:26.960] iteration 1162 : loss: 0.036546, loss_a: 0.021498
[21:06:28.276] iteration 1163 : loss: 0.036795, loss_a: 0.021644
[21:06:29.029] iteration 1164 : loss: 0.036742, loss_a: 0.021613
[21:06:30.333] iteration 1165 : loss: 0.036477, loss_a: 0.021457
[21:06:31.067] iteration 1166 : loss: 0.054359, loss_a: 0.031976
[21:06:32.389] iteration 1167 : loss: 0.048372, loss_a: 0.028454
[21:06:33.133] iteration 1168 : loss: 0.053130, loss_a: 0.031253
[21:06:34.456] iteration 1169 : loss: 0.048463, loss_a: 0.028508
[21:06:35.190] iteration 1170 : loss: 0.034328, loss_a: 0.020193
[21:06:36.531] iteration 1171 : loss: 0.065669, loss_a: 0.038629
[21:06:37.270] iteration 1172 : loss: 0.154740, loss_a: 0.091024
[21:06:38.584] iteration 1173 : loss: 0.036664, loss_a: 0.021567
[21:06:39.328] iteration 1174 : loss: 0.051205, loss_a: 0.030121
[21:06:40.656] iteration 1175 : loss: 0.118993, loss_a: 0.069996
[21:06:41.398] iteration 1176 : loss: 0.040699, loss_a: 0.023941
[21:06:42.729] iteration 1177 : loss: 0.029328, loss_a: 0.017252
[21:06:43.468] iteration 1178 : loss: 0.072429, loss_a: 0.042605
[21:06:44.805] iteration 1179 : loss: 0.072390, loss_a: 0.042582
[21:06:45.539] iteration 1180 : loss: 0.037654, loss_a: 0.022149
[21:06:46.897] iteration 1181 : loss: 0.100758, loss_a: 0.059269
[21:06:47.640] iteration 1182 : loss: 0.034204, loss_a: 0.020120
[21:06:48.998] iteration 1183 : loss: 0.090699, loss_a: 0.053352
[21:06:49.744] iteration 1184 : loss: 0.126808, loss_a: 0.074593
[21:06:51.097] iteration 1185 : loss: 0.053095, loss_a: 0.031232
[21:06:51.843] iteration 1186 : loss: 0.046361, loss_a: 0.027271
[21:06:53.148] iteration 1187 : loss: 0.026219, loss_a: 0.015423
[21:06:53.884] iteration 1188 : loss: 0.066060, loss_a: 0.038859
[21:06:55.230] iteration 1189 : loss: 0.048891, loss_a: 0.028759
[21:06:55.968] iteration 1190 : loss: 0.098910, loss_a: 0.058183
[21:06:57.274] iteration 1191 : loss: 0.058467, loss_a: 0.034392
[21:06:58.022] iteration 1192 : loss: 0.029507, loss_a: 0.017357
[21:06:59.337] iteration 1193 : loss: 0.087770, loss_a: 0.051629
[21:07:00.073] iteration 1194 : loss: 0.043443, loss_a: 0.025555
[21:07:01.397] iteration 1195 : loss: 0.032146, loss_a: 0.018909
[21:07:02.137] iteration 1196 : loss: 0.051491, loss_a: 0.030289
[21:07:03.492] iteration 1197 : loss: 0.046236, loss_a: 0.027198
[21:07:04.231] iteration 1198 : loss: 0.034430, loss_a: 0.020253
[21:07:05.581] iteration 1199 : loss: 0.077436, loss_a: 0.045551
[21:07:06.311] iteration 1200 : loss: 0.029458, loss_a: 0.017328
[21:07:30.910] iteration 1201 : loss: 0.057939, loss_a: 0.034082
[21:07:33.084] iteration 1202 : loss: 0.056721, loss_a: 0.033365
[21:07:34.405] iteration 1203 : loss: 0.067554, loss_a: 0.039737
[21:07:35.150] iteration 1204 : loss: 0.034456, loss_a: 0.020268
[21:07:36.504] iteration 1205 : loss: 0.053182, loss_a: 0.031284
[21:07:37.239] iteration 1206 : loss: 0.037604, loss_a: 0.022120
[21:07:38.575] iteration 1207 : loss: 0.071213, loss_a: 0.041890
[21:07:39.310] iteration 1208 : loss: 0.036146, loss_a: 0.021263
[21:07:40.653] iteration 1209 : loss: 0.026115, loss_a: 0.015362
[21:07:41.405] iteration 1210 : loss: 0.050364, loss_a: 0.029626
[21:07:42.723] iteration 1211 : loss: 0.025901, loss_a: 0.015236
[21:07:43.458] iteration 1212 : loss: 0.040999, loss_a: 0.024117
[21:07:44.831] iteration 1213 : loss: 0.073891, loss_a: 0.043465
[21:07:45.563] iteration 1214 : loss: 0.031157, loss_a: 0.018328
[21:07:46.923] iteration 1215 : loss: 0.076235, loss_a: 0.044844
[21:07:47.658] iteration 1216 : loss: 0.030845, loss_a: 0.018144
[21:07:48.975] iteration 1217 : loss: 0.053966, loss_a: 0.031745
[21:07:49.710] iteration 1218 : loss: 0.045866, loss_a: 0.026980
[21:07:51.036] iteration 1219 : loss: 0.063626, loss_a: 0.037427
[21:07:51.774] iteration 1220 : loss: 0.075124, loss_a: 0.044191
[21:07:53.134] iteration 1221 : loss: 0.118256, loss_a: 0.069562
[21:07:53.878] iteration 1222 : loss: 0.073082, loss_a: 0.042989
[21:07:55.243] iteration 1223 : loss: 0.048781, loss_a: 0.028695
[21:07:55.987] iteration 1224 : loss: 0.078037, loss_a: 0.045904
[21:07:57.307] iteration 1225 : loss: 0.052186, loss_a: 0.030698
[21:07:58.037] iteration 1226 : loss: 0.030482, loss_a: 0.017930
[21:07:59.352] iteration 1227 : loss: 0.034107, loss_a: 0.020063
[21:08:00.095] iteration 1228 : loss: 0.068669, loss_a: 0.040394
[21:08:01.457] iteration 1229 : loss: 0.068069, loss_a: 0.040041
[21:08:02.198] iteration 1230 : loss: 0.044458, loss_a: 0.026152
[21:08:03.536] iteration 1231 : loss: 0.021962, loss_a: 0.012919
[21:08:04.273] iteration 1232 : loss: 0.037218, loss_a: 0.021893
[21:08:05.613] iteration 1233 : loss: 0.021786, loss_a: 0.012815
[21:08:06.356] iteration 1234 : loss: 0.050491, loss_a: 0.029701
[21:08:07.718] iteration 1235 : loss: 0.049873, loss_a: 0.029337
[21:08:08.454] iteration 1236 : loss: 0.031815, loss_a: 0.018715
[21:08:09.770] iteration 1237 : loss: 0.020447, loss_a: 0.012028
[21:08:10.501] iteration 1238 : loss: 0.044108, loss_a: 0.025946
[21:08:11.826] iteration 1239 : loss: 0.034125, loss_a: 0.020074
[21:08:12.564] iteration 1240 : loss: 0.055299, loss_a: 0.032529
[21:08:13.886] iteration 1241 : loss: 0.050918, loss_a: 0.029952
[21:08:14.623] iteration 1242 : loss: 0.049028, loss_a: 0.028840
[21:08:15.958] iteration 1243 : loss: 0.030160, loss_a: 0.017741
[21:08:16.696] iteration 1244 : loss: 0.045385, loss_a: 0.026697
[21:08:18.013] iteration 1245 : loss: 0.052244, loss_a: 0.030732
[21:08:18.739] iteration 1246 : loss: 0.040161, loss_a: 0.023624
[21:08:20.080] iteration 1247 : loss: 0.017254, loss_a: 0.010150
[21:08:20.822] iteration 1248 : loss: 0.042020, loss_a: 0.024717
[21:08:22.125] iteration 1249 : loss: 0.026511, loss_a: 0.015594
[21:08:22.868] iteration 1250 : loss: 0.058757, loss_a: 0.034563
[21:08:24.205] iteration 1251 : loss: 0.034172, loss_a: 0.020101
[21:08:24.940] iteration 1252 : loss: 0.037645, loss_a: 0.022144
[21:08:26.255] iteration 1253 : loss: 0.028850, loss_a: 0.016971
[21:08:26.992] iteration 1254 : loss: 0.078893, loss_a: 0.046408
[21:08:28.339] iteration 1255 : loss: 0.025733, loss_a: 0.015137
[21:08:29.074] iteration 1256 : loss: 0.025499, loss_a: 0.015000
[21:08:30.421] iteration 1257 : loss: 0.120017, loss_a: 0.070598
[21:08:31.164] iteration 1258 : loss: 0.041737, loss_a: 0.024551
[21:08:32.500] iteration 1259 : loss: 0.063934, loss_a: 0.037608
[21:08:33.232] iteration 1260 : loss: 0.054063, loss_a: 0.031802
[21:08:34.600] iteration 1261 : loss: 0.054309, loss_a: 0.031947
[21:08:35.330] iteration 1262 : loss: 0.058440, loss_a: 0.034376
[21:08:36.677] iteration 1263 : loss: 0.033063, loss_a: 0.019449
[21:08:37.409] iteration 1264 : loss: 0.026147, loss_a: 0.015380
[21:08:38.753] iteration 1265 : loss: 0.059706, loss_a: 0.035121
[21:08:39.495] iteration 1266 : loss: 0.042443, loss_a: 0.024967
[21:08:40.818] iteration 1267 : loss: 0.057417, loss_a: 0.033775
[21:08:41.557] iteration 1268 : loss: 0.047252, loss_a: 0.027795
[21:08:42.878] iteration 1269 : loss: 0.057578, loss_a: 0.033870
[21:08:43.611] iteration 1270 : loss: 0.039980, loss_a: 0.023517
[21:08:44.958] iteration 1271 : loss: 0.047436, loss_a: 0.027903
[21:08:45.697] iteration 1272 : loss: 0.056570, loss_a: 0.033276
[21:08:47.049] iteration 1273 : loss: 0.031485, loss_a: 0.018521
[21:08:47.784] iteration 1274 : loss: 0.024993, loss_a: 0.014702
[21:08:49.098] iteration 1275 : loss: 0.024100, loss_a: 0.014176
[21:08:49.830] iteration 1276 : loss: 0.047096, loss_a: 0.027703
[21:08:51.186] iteration 1277 : loss: 0.028169, loss_a: 0.016570
[21:08:51.923] iteration 1278 : loss: 0.027813, loss_a: 0.016361
[21:08:53.248] iteration 1279 : loss: 0.075301, loss_a: 0.044294
[21:08:53.992] iteration 1280 : loss: 0.056674, loss_a: 0.033338
[21:08:55.339] iteration 1281 : loss: 0.062359, loss_a: 0.036682
[21:08:56.080] iteration 1282 : loss: 0.035197, loss_a: 0.020704
[21:08:57.398] iteration 1283 : loss: 0.069717, loss_a: 0.041010
[21:08:58.138] iteration 1284 : loss: 0.039251, loss_a: 0.023089
[21:08:59.475] iteration 1285 : loss: 0.018138, loss_a: 0.010669
[21:09:00.218] iteration 1286 : loss: 0.055320, loss_a: 0.032541
[21:09:01.567] iteration 1287 : loss: 0.045544, loss_a: 0.026791
[21:09:02.308] iteration 1288 : loss: 0.050019, loss_a: 0.029423
[21:09:03.660] iteration 1289 : loss: 0.055481, loss_a: 0.032636
[21:09:04.390] iteration 1290 : loss: 0.032684, loss_a: 0.019226
[21:09:05.709] iteration 1291 : loss: 0.034892, loss_a: 0.020525
[21:09:06.456] iteration 1292 : loss: 0.031764, loss_a: 0.018685
[21:09:07.790] iteration 1293 : loss: 0.032165, loss_a: 0.018921
[21:09:08.521] iteration 1294 : loss: 0.035858, loss_a: 0.021093
[21:09:09.872] iteration 1295 : loss: 0.053526, loss_a: 0.031486
[21:09:10.606] iteration 1296 : loss: 0.048078, loss_a: 0.028281
[21:09:11.968] iteration 1297 : loss: 0.035341, loss_a: 0.020789
[21:09:12.704] iteration 1298 : loss: 0.059280, loss_a: 0.034870
[21:09:14.053] iteration 1299 : loss: 0.053697, loss_a: 0.031586
[21:09:14.788] iteration 1300 : loss: 0.041456, loss_a: 0.024386
[21:09:16.091] iteration 1301 : loss: 0.041778, loss_a: 0.024575
[21:09:16.822] iteration 1302 : loss: 0.036044, loss_a: 0.021203
[21:09:18.170] iteration 1303 : loss: 0.039821, loss_a: 0.023424
[21:09:18.921] iteration 1304 : loss: 0.045748, loss_a: 0.026910
[21:09:20.238] iteration 1305 : loss: 0.053097, loss_a: 0.031233
[21:09:20.972] iteration 1306 : loss: 0.059419, loss_a: 0.034953
[21:09:22.301] iteration 1307 : loss: 0.052428, loss_a: 0.030840
[21:09:23.044] iteration 1308 : loss: 0.058666, loss_a: 0.034509
[21:09:24.389] iteration 1309 : loss: 0.027954, loss_a: 0.016443
[21:09:25.133] iteration 1310 : loss: 0.035898, loss_a: 0.021116
[21:09:26.455] iteration 1311 : loss: 0.094775, loss_a: 0.055750
[21:09:27.202] iteration 1312 : loss: 0.110026, loss_a: 0.064721
[21:09:28.544] iteration 1313 : loss: 0.069426, loss_a: 0.040839
[21:09:29.278] iteration 1314 : loss: 0.025697, loss_a: 0.015116
[21:09:30.598] iteration 1315 : loss: 0.047557, loss_a: 0.027974
[21:09:31.334] iteration 1316 : loss: 0.057965, loss_a: 0.034097
[21:09:32.656] iteration 1317 : loss: 0.088953, loss_a: 0.052325
[21:09:33.393] iteration 1318 : loss: 0.055767, loss_a: 0.032804
[21:09:34.715] iteration 1319 : loss: 0.019446, loss_a: 0.011439
[21:09:35.450] iteration 1320 : loss: 0.054531, loss_a: 0.032077
[21:09:36.800] iteration 1321 : loss: 0.045192, loss_a: 0.026584
[21:09:37.549] iteration 1322 : loss: 0.036188, loss_a: 0.021287
[21:09:38.892] iteration 1323 : loss: 0.049462, loss_a: 0.029095
[21:09:39.621] iteration 1324 : loss: 0.046098, loss_a: 0.027116
[21:09:40.926] iteration 1325 : loss: 0.124730, loss_a: 0.073371
[21:09:41.666] iteration 1326 : loss: 0.039328, loss_a: 0.023134
[21:09:43.023] iteration 1327 : loss: 0.042792, loss_a: 0.025172
[21:09:43.773] iteration 1328 : loss: 0.036948, loss_a: 0.021734
[21:09:45.127] iteration 1329 : loss: 0.039729, loss_a: 0.023370
[21:09:45.875] iteration 1330 : loss: 0.063239, loss_a: 0.037199
[21:09:47.236] iteration 1331 : loss: 0.057705, loss_a: 0.033944
[21:09:47.978] iteration 1332 : loss: 0.032844, loss_a: 0.019320
[21:09:49.316] iteration 1333 : loss: 0.066238, loss_a: 0.038964
[21:09:50.058] iteration 1334 : loss: 0.053797, loss_a: 0.031646
[21:09:51.410] iteration 1335 : loss: 0.051024, loss_a: 0.030014
[21:09:52.151] iteration 1336 : loss: 0.049621, loss_a: 0.029189
[21:09:53.489] iteration 1337 : loss: 0.043642, loss_a: 0.025672
[21:09:54.230] iteration 1338 : loss: 0.044468, loss_a: 0.026158
[21:09:55.593] iteration 1339 : loss: 0.068324, loss_a: 0.040190
[21:09:56.339] iteration 1340 : loss: 0.051700, loss_a: 0.030412
[21:09:57.689] iteration 1341 : loss: 0.019139, loss_a: 0.011258
[21:09:58.424] iteration 1342 : loss: 0.043358, loss_a: 0.025504
[21:09:59.774] iteration 1343 : loss: 0.030723, loss_a: 0.018072
[21:10:00.517] iteration 1344 : loss: 0.045540, loss_a: 0.026788
[21:10:01.865] iteration 1345 : loss: 0.035261, loss_a: 0.020742
[21:10:02.607] iteration 1346 : loss: 0.038020, loss_a: 0.022365
[21:10:03.943] iteration 1347 : loss: 0.038768, loss_a: 0.022805
[21:10:04.689] iteration 1348 : loss: 0.039942, loss_a: 0.023495
[21:10:06.029] iteration 1349 : loss: 0.034121, loss_a: 0.020071
[21:10:06.776] iteration 1350 : loss: 0.064897, loss_a: 0.038175
[21:10:08.111] iteration 1351 : loss: 0.012269, loss_a: 0.007217
[21:10:08.855] iteration 1352 : loss: 0.058444, loss_a: 0.034379
[21:10:10.192] iteration 1353 : loss: 0.025050, loss_a: 0.014735
[21:10:10.944] iteration 1354 : loss: 0.037829, loss_a: 0.022252
[21:10:12.288] iteration 1355 : loss: 0.052046, loss_a: 0.030615
[21:10:13.023] iteration 1356 : loss: 0.033509, loss_a: 0.019711
[21:10:14.355] iteration 1357 : loss: 0.041831, loss_a: 0.024607
[21:10:15.097] iteration 1358 : loss: 0.028596, loss_a: 0.016821
[21:10:16.427] iteration 1359 : loss: 0.031161, loss_a: 0.018330
[21:10:17.171] iteration 1360 : loss: 0.036557, loss_a: 0.021504
[21:10:18.549] iteration 1361 : loss: 0.022204, loss_a: 0.013061
[21:10:19.314] iteration 1362 : loss: 0.045297, loss_a: 0.026645
[21:10:20.662] iteration 1363 : loss: 0.062859, loss_a: 0.036976
[21:10:21.409] iteration 1364 : loss: 0.024885, loss_a: 0.014638
[21:10:22.724] iteration 1365 : loss: 0.024402, loss_a: 0.014354
[21:10:23.467] iteration 1366 : loss: 0.032475, loss_a: 0.019103
[21:10:24.797] iteration 1367 : loss: 0.033238, loss_a: 0.019552
[21:10:25.546] iteration 1368 : loss: 0.020285, loss_a: 0.011932
[21:10:26.890] iteration 1369 : loss: 0.031713, loss_a: 0.018655
[21:10:27.621] iteration 1370 : loss: 0.042573, loss_a: 0.025043
[21:10:28.968] iteration 1371 : loss: 0.076958, loss_a: 0.045270
[21:10:29.703] iteration 1372 : loss: 0.039714, loss_a: 0.023361
[21:10:31.034] iteration 1373 : loss: 0.065045, loss_a: 0.038262
[21:10:31.772] iteration 1374 : loss: 0.034774, loss_a: 0.020455
[21:10:33.119] iteration 1375 : loss: 0.027205, loss_a: 0.016003
[21:10:33.863] iteration 1376 : loss: 0.051271, loss_a: 0.030159
[21:10:35.201] iteration 1377 : loss: 0.036918, loss_a: 0.021716
[21:10:35.948] iteration 1378 : loss: 0.044996, loss_a: 0.026468
[21:10:37.261] iteration 1379 : loss: 0.058734, loss_a: 0.034549
[21:10:37.990] iteration 1380 : loss: 0.022535, loss_a: 0.013256
[21:10:39.309] iteration 1381 : loss: 0.058211, loss_a: 0.034242
[21:10:40.056] iteration 1382 : loss: 0.042690, loss_a: 0.025112
[21:10:41.367] iteration 1383 : loss: 0.071783, loss_a: 0.042225
[21:10:42.097] iteration 1384 : loss: 0.039204, loss_a: 0.023061
[21:10:43.428] iteration 1385 : loss: 0.059704, loss_a: 0.035120
[21:10:44.160] iteration 1386 : loss: 0.032251, loss_a: 0.018971
[21:10:45.465] iteration 1387 : loss: 0.065649, loss_a: 0.038617
[21:10:46.213] iteration 1388 : loss: 0.066129, loss_a: 0.038900
[21:10:47.537] iteration 1389 : loss: 0.021076, loss_a: 0.012398
[21:10:48.272] iteration 1390 : loss: 0.038632, loss_a: 0.022725
[21:10:49.588] iteration 1391 : loss: 0.025589, loss_a: 0.015053
[21:10:50.328] iteration 1392 : loss: 0.024942, loss_a: 0.014672
[21:10:51.657] iteration 1393 : loss: 0.075087, loss_a: 0.044169
[21:10:52.402] iteration 1394 : loss: 0.066693, loss_a: 0.039231
[21:10:53.746] iteration 1395 : loss: 0.042976, loss_a: 0.025280
[21:10:54.478] iteration 1396 : loss: 0.053940, loss_a: 0.031730
[21:10:55.794] iteration 1397 : loss: 0.042636, loss_a: 0.025080
[21:10:56.537] iteration 1398 : loss: 0.050180, loss_a: 0.029518
[21:10:57.884] iteration 1399 : loss: 0.020417, loss_a: 0.012010
[21:10:58.634] iteration 1400 : loss: 0.076520, loss_a: 0.045012
[21:11:23.241] iteration 1401 : loss: 0.034522, loss_a: 0.020307
[21:11:25.453] iteration 1402 : loss: 0.058106, loss_a: 0.034180
[21:11:26.783] iteration 1403 : loss: 0.055864, loss_a: 0.032861
[21:11:27.529] iteration 1404 : loss: 0.044563, loss_a: 0.026214
[21:11:28.884] iteration 1405 : loss: 0.044268, loss_a: 0.026040
[21:11:29.623] iteration 1406 : loss: 0.041449, loss_a: 0.024382
[21:11:30.956] iteration 1407 : loss: 0.037967, loss_a: 0.022333
[21:11:31.710] iteration 1408 : loss: 0.047719, loss_a: 0.028070
[21:11:33.065] iteration 1409 : loss: 0.060574, loss_a: 0.035631
[21:11:33.798] iteration 1410 : loss: 0.041222, loss_a: 0.024248
[21:11:35.154] iteration 1411 : loss: 0.060506, loss_a: 0.035592
[21:11:35.893] iteration 1412 : loss: 0.030685, loss_a: 0.018050
[21:11:37.218] iteration 1413 : loss: 0.025058, loss_a: 0.014740
[21:11:37.959] iteration 1414 : loss: 0.082725, loss_a: 0.048662
[21:11:39.309] iteration 1415 : loss: 0.069149, loss_a: 0.040676
[21:11:40.049] iteration 1416 : loss: 0.053033, loss_a: 0.031196
[21:11:41.389] iteration 1417 : loss: 0.059401, loss_a: 0.034942
[21:11:42.136] iteration 1418 : loss: 0.020311, loss_a: 0.011948
[21:11:43.454] iteration 1419 : loss: 0.060939, loss_a: 0.035847
[21:11:44.201] iteration 1420 : loss: 0.047167, loss_a: 0.027745
[21:11:45.533] iteration 1421 : loss: 0.026237, loss_a: 0.015433
[21:11:46.272] iteration 1422 : loss: 0.037147, loss_a: 0.021851
[21:11:47.602] iteration 1423 : loss: 0.030918, loss_a: 0.018187
[21:11:48.346] iteration 1424 : loss: 0.043948, loss_a: 0.025852
[21:11:49.681] iteration 1425 : loss: 0.087253, loss_a: 0.051325
[21:11:50.437] iteration 1426 : loss: 0.038316, loss_a: 0.022539
[21:11:51.788] iteration 1427 : loss: 0.067709, loss_a: 0.039829
[21:11:52.536] iteration 1428 : loss: 0.033786, loss_a: 0.019874
[21:11:53.902] iteration 1429 : loss: 0.099636, loss_a: 0.058610
[21:11:54.649] iteration 1430 : loss: 0.042735, loss_a: 0.025138
[21:11:55.976] iteration 1431 : loss: 0.078687, loss_a: 0.046287
[21:11:56.726] iteration 1432 : loss: 0.060584, loss_a: 0.035638
[21:11:58.081] iteration 1433 : loss: 0.056279, loss_a: 0.033105
[21:11:58.816] iteration 1434 : loss: 0.029907, loss_a: 0.017592
[21:12:00.169] iteration 1435 : loss: 0.089926, loss_a: 0.052898
[21:12:00.919] iteration 1436 : loss: 0.028677, loss_a: 0.016869
[21:12:02.270] iteration 1437 : loss: 0.046656, loss_a: 0.027445
[21:12:03.008] iteration 1438 : loss: 0.051994, loss_a: 0.030585
[21:12:04.327] iteration 1439 : loss: 0.024090, loss_a: 0.014171
[21:12:05.068] iteration 1440 : loss: 0.047692, loss_a: 0.028054
[21:12:06.392] iteration 1441 : loss: 0.032695, loss_a: 0.019232
[21:12:07.148] iteration 1442 : loss: 0.074678, loss_a: 0.043928
[21:12:08.455] iteration 1443 : loss: 0.018200, loss_a: 0.010706
[21:12:09.201] iteration 1444 : loss: 0.041916, loss_a: 0.024656
[21:12:10.565] iteration 1445 : loss: 0.057115, loss_a: 0.033597
[21:12:11.315] iteration 1446 : loss: 0.048128, loss_a: 0.028311
[21:12:12.655] iteration 1447 : loss: 0.077611, loss_a: 0.045654
[21:12:13.403] iteration 1448 : loss: 0.027581, loss_a: 0.016224
[21:12:14.751] iteration 1449 : loss: 0.039894, loss_a: 0.023467
[21:12:15.506] iteration 1450 : loss: 0.051536, loss_a: 0.030315
[21:12:16.825] iteration 1451 : loss: 0.024641, loss_a: 0.014495
[21:12:17.563] iteration 1452 : loss: 0.045439, loss_a: 0.026729
[21:12:18.913] iteration 1453 : loss: 0.029496, loss_a: 0.017350
[21:12:19.678] iteration 1454 : loss: 0.078620, loss_a: 0.046247
[21:12:21.013] iteration 1455 : loss: 0.042721, loss_a: 0.025130
[21:12:21.760] iteration 1456 : loss: 0.059395, loss_a: 0.034938
[21:12:23.087] iteration 1457 : loss: 0.020830, loss_a: 0.012253
[21:12:23.831] iteration 1458 : loss: 0.057409, loss_a: 0.033770
[21:12:25.180] iteration 1459 : loss: 0.033091, loss_a: 0.019465
[21:12:25.913] iteration 1460 : loss: 0.038736, loss_a: 0.022786
[21:12:27.264] iteration 1461 : loss: 0.042861, loss_a: 0.025212
[21:12:27.997] iteration 1462 : loss: 0.032555, loss_a: 0.019150
[21:12:29.322] iteration 1463 : loss: 0.029539, loss_a: 0.017376
[21:12:30.065] iteration 1464 : loss: 0.052815, loss_a: 0.031068
[21:12:31.394] iteration 1465 : loss: 0.066130, loss_a: 0.038900
[21:12:32.152] iteration 1466 : loss: 0.081971, loss_a: 0.048218
[21:12:33.493] iteration 1467 : loss: 0.036157, loss_a: 0.021269
[21:12:34.243] iteration 1468 : loss: 0.043211, loss_a: 0.025418
[21:12:35.582] iteration 1469 : loss: 0.022884, loss_a: 0.013461
[21:12:36.326] iteration 1470 : loss: 0.025595, loss_a: 0.015056
[21:12:37.647] iteration 1471 : loss: 0.061026, loss_a: 0.035898
[21:12:38.384] iteration 1472 : loss: 0.025291, loss_a: 0.014877
[21:12:39.708] iteration 1473 : loss: 0.037060, loss_a: 0.021800
[21:12:40.443] iteration 1474 : loss: 0.051178, loss_a: 0.030105
[21:12:41.772] iteration 1475 : loss: 0.067514, loss_a: 0.039714
[21:12:42.503] iteration 1476 : loss: 0.130994, loss_a: 0.077055
[21:12:43.820] iteration 1477 : loss: 0.035721, loss_a: 0.021012
[21:12:44.558] iteration 1478 : loss: 0.019951, loss_a: 0.011736
[21:12:45.894] iteration 1479 : loss: 0.024183, loss_a: 0.014225
[21:12:46.627] iteration 1480 : loss: 0.096770, loss_a: 0.056924
[21:12:47.981] iteration 1481 : loss: 0.028499, loss_a: 0.016764
[21:12:48.716] iteration 1482 : loss: 0.070345, loss_a: 0.041379
[21:12:50.083] iteration 1483 : loss: 0.048656, loss_a: 0.028621
[21:12:50.818] iteration 1484 : loss: 0.046417, loss_a: 0.027304
[21:12:52.156] iteration 1485 : loss: 0.045744, loss_a: 0.026908
[21:12:52.901] iteration 1486 : loss: 0.028972, loss_a: 0.017042
[21:12:54.229] iteration 1487 : loss: 0.033299, loss_a: 0.019587
[21:12:54.979] iteration 1488 : loss: 0.079402, loss_a: 0.046707
[21:12:56.291] iteration 1489 : loss: 0.034956, loss_a: 0.020563
[21:12:57.039] iteration 1490 : loss: 0.059213, loss_a: 0.034831
[21:12:58.382] iteration 1491 : loss: 0.073441, loss_a: 0.043200
[21:12:59.118] iteration 1492 : loss: 0.016410, loss_a: 0.009653
[21:13:00.428] iteration 1493 : loss: 0.049469, loss_a: 0.029100
[21:13:01.164] iteration 1494 : loss: 0.034762, loss_a: 0.020448
[21:13:02.472] iteration 1495 : loss: 0.060580, loss_a: 0.035635
[21:13:03.212] iteration 1496 : loss: 0.029037, loss_a: 0.017080
[21:13:04.524] iteration 1497 : loss: 0.039638, loss_a: 0.023316
[21:13:05.256] iteration 1498 : loss: 0.055956, loss_a: 0.032915
[21:13:06.590] iteration 1499 : loss: 0.053535, loss_a: 0.031491
[21:13:07.331] iteration 1500 : loss: 0.050831, loss_a: 0.029900
[21:13:08.663] iteration 1501 : loss: 0.031599, loss_a: 0.018588
[21:13:09.418] iteration 1502 : loss: 0.106626, loss_a: 0.062721
[21:13:10.768] iteration 1503 : loss: 0.047913, loss_a: 0.028184
[21:13:11.504] iteration 1504 : loss: 0.023682, loss_a: 0.013931
[21:13:12.823] iteration 1505 : loss: 0.042688, loss_a: 0.025111
[21:13:13.561] iteration 1506 : loss: 0.040947, loss_a: 0.024087
[21:13:14.878] iteration 1507 : loss: 0.025687, loss_a: 0.015110
[21:13:15.629] iteration 1508 : loss: 0.064650, loss_a: 0.038029
[21:13:16.962] iteration 1509 : loss: 0.048252, loss_a: 0.028383
[21:13:17.710] iteration 1510 : loss: 0.032761, loss_a: 0.019271
[21:13:19.056] iteration 1511 : loss: 0.056097, loss_a: 0.032998
[21:13:19.795] iteration 1512 : loss: 0.047641, loss_a: 0.028024
[21:13:21.133] iteration 1513 : loss: 0.033497, loss_a: 0.019704
[21:13:21.879] iteration 1514 : loss: 0.044243, loss_a: 0.026025
[21:13:23.244] iteration 1515 : loss: 0.101071, loss_a: 0.059453
[21:13:23.979] iteration 1516 : loss: 0.056455, loss_a: 0.033209
[21:13:25.314] iteration 1517 : loss: 0.027322, loss_a: 0.016072
[21:13:26.059] iteration 1518 : loss: 0.041906, loss_a: 0.024651
[21:13:27.398] iteration 1519 : loss: 0.053698, loss_a: 0.031587
[21:13:28.181] iteration 1520 : loss: 0.037950, loss_a: 0.022323
[21:13:29.518] iteration 1521 : loss: 0.064876, loss_a: 0.038162
[21:13:30.269] iteration 1522 : loss: 0.066053, loss_a: 0.038855
[21:13:31.612] iteration 1523 : loss: 0.027403, loss_a: 0.016119
[21:13:32.342] iteration 1524 : loss: 0.032343, loss_a: 0.019025
[21:13:33.672] iteration 1525 : loss: 0.062127, loss_a: 0.036546
[21:13:34.423] iteration 1526 : loss: 0.055966, loss_a: 0.032921
[21:13:35.751] iteration 1527 : loss: 0.024220, loss_a: 0.014247
[21:13:36.500] iteration 1528 : loss: 0.057627, loss_a: 0.033898
[21:13:37.832] iteration 1529 : loss: 0.019246, loss_a: 0.011321
[21:13:38.574] iteration 1530 : loss: 0.038322, loss_a: 0.022543
[21:13:39.887] iteration 1531 : loss: 0.075639, loss_a: 0.044494
[21:13:40.629] iteration 1532 : loss: 0.032099, loss_a: 0.018882
[21:13:41.960] iteration 1533 : loss: 0.039227, loss_a: 0.023075
[21:13:42.704] iteration 1534 : loss: 0.034281, loss_a: 0.020165
[21:13:44.007] iteration 1535 : loss: 0.025923, loss_a: 0.015249
[21:13:44.746] iteration 1536 : loss: 0.032023, loss_a: 0.018837
[21:13:46.094] iteration 1537 : loss: 0.037769, loss_a: 0.022217
[21:13:46.837] iteration 1538 : loss: 0.031336, loss_a: 0.018433
[21:13:48.174] iteration 1539 : loss: 0.035117, loss_a: 0.020657
[21:13:48.919] iteration 1540 : loss: 0.061387, loss_a: 0.036110
[21:13:50.272] iteration 1541 : loss: 0.064189, loss_a: 0.037758
[21:13:51.010] iteration 1542 : loss: 0.028322, loss_a: 0.016660
[21:13:52.373] iteration 1543 : loss: 0.037119, loss_a: 0.021835
[21:13:53.122] iteration 1544 : loss: 0.068874, loss_a: 0.040514
[21:13:54.488] iteration 1545 : loss: 0.041856, loss_a: 0.024621
[21:13:55.227] iteration 1546 : loss: 0.033398, loss_a: 0.019646
[21:13:56.569] iteration 1547 : loss: 0.033115, loss_a: 0.019479
[21:13:57.313] iteration 1548 : loss: 0.049146, loss_a: 0.028910
[21:13:58.633] iteration 1549 : loss: 0.044274, loss_a: 0.026044
[21:13:59.365] iteration 1550 : loss: 0.040770, loss_a: 0.023982
[21:14:00.690] iteration 1551 : loss: 0.076991, loss_a: 0.045289
[21:14:01.437] iteration 1552 : loss: 0.046034, loss_a: 0.027079
[21:14:02.783] iteration 1553 : loss: 0.037284, loss_a: 0.021932
[21:14:03.525] iteration 1554 : loss: 0.020514, loss_a: 0.012067
[21:14:04.852] iteration 1555 : loss: 0.039110, loss_a: 0.023006
[21:14:05.590] iteration 1556 : loss: 0.032632, loss_a: 0.019195
[21:14:06.940] iteration 1557 : loss: 0.110087, loss_a: 0.064757
[21:14:07.679] iteration 1558 : loss: 0.029594, loss_a: 0.017408
[21:14:09.004] iteration 1559 : loss: 0.069096, loss_a: 0.040645
[21:14:09.755] iteration 1560 : loss: 0.052734, loss_a: 0.031020
[21:14:11.105] iteration 1561 : loss: 0.031280, loss_a: 0.018400
[21:14:11.839] iteration 1562 : loss: 0.025201, loss_a: 0.014824
[21:14:13.187] iteration 1563 : loss: 0.044194, loss_a: 0.025996
[21:14:13.929] iteration 1564 : loss: 0.036000, loss_a: 0.021176
[21:14:15.247] iteration 1565 : loss: 0.026612, loss_a: 0.015654
[21:14:15.990] iteration 1566 : loss: 0.087799, loss_a: 0.051647
[21:14:17.299] iteration 1567 : loss: 0.031450, loss_a: 0.018500
[21:14:18.035] iteration 1568 : loss: 0.020705, loss_a: 0.012179
[21:14:19.386] iteration 1569 : loss: 0.037561, loss_a: 0.022095
[21:14:20.119] iteration 1570 : loss: 0.040413, loss_a: 0.023772
[21:14:21.438] iteration 1571 : loss: 0.050744, loss_a: 0.029849
[21:14:22.196] iteration 1572 : loss: 0.066705, loss_a: 0.039238
[21:14:23.517] iteration 1573 : loss: 0.044374, loss_a: 0.026103
[21:14:24.270] iteration 1574 : loss: 0.048331, loss_a: 0.028430
[21:14:25.603] iteration 1575 : loss: 0.084287, loss_a: 0.049580
[21:14:26.353] iteration 1576 : loss: 0.027568, loss_a: 0.016216
[21:14:27.691] iteration 1577 : loss: 0.040553, loss_a: 0.023855
[21:14:28.429] iteration 1578 : loss: 0.049037, loss_a: 0.028845
[21:14:29.768] iteration 1579 : loss: 0.083552, loss_a: 0.049148
[21:14:30.517] iteration 1580 : loss: 0.049729, loss_a: 0.029252
[21:14:31.810] iteration 1581 : loss: 0.031883, loss_a: 0.018755
[21:14:32.543] iteration 1582 : loss: 0.035384, loss_a: 0.020814
[21:14:33.893] iteration 1583 : loss: 0.033420, loss_a: 0.019659
[21:14:34.626] iteration 1584 : loss: 0.022344, loss_a: 0.013144
[21:14:35.945] iteration 1585 : loss: 0.020456, loss_a: 0.012033
[21:14:36.681] iteration 1586 : loss: 0.025907, loss_a: 0.015240
[21:14:38.027] iteration 1587 : loss: 0.020957, loss_a: 0.012328
[21:14:38.759] iteration 1588 : loss: 0.048688, loss_a: 0.028640
[21:14:40.102] iteration 1589 : loss: 0.026570, loss_a: 0.015629
[21:14:40.848] iteration 1590 : loss: 0.057817, loss_a: 0.034010
[21:14:42.191] iteration 1591 : loss: 0.029188, loss_a: 0.017170
[21:14:42.923] iteration 1592 : loss: 0.022763, loss_a: 0.013390
[21:14:44.290] iteration 1593 : loss: 0.051840, loss_a: 0.030494
[21:14:45.023] iteration 1594 : loss: 0.041415, loss_a: 0.024362
[21:14:46.349] iteration 1595 : loss: 0.024068, loss_a: 0.014157
[21:14:47.091] iteration 1596 : loss: 0.041966, loss_a: 0.024686
[21:14:48.395] iteration 1597 : loss: 0.040121, loss_a: 0.023601
[21:14:49.137] iteration 1598 : loss: 0.031821, loss_a: 0.018718
[21:14:50.483] iteration 1599 : loss: 0.041321, loss_a: 0.024307
[21:14:51.220] iteration 1600 : loss: 0.030403, loss_a: 0.017884
[21:15:14.855] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_1600_dice_0.8771.pth
[21:15:16.191] iteration 1601 : loss: 0.037311, loss_a: 0.021948
[21:15:18.467] iteration 1602 : loss: 0.036039, loss_a: 0.021199
[21:15:19.806] iteration 1603 : loss: 0.049097, loss_a: 0.028880
[21:15:20.541] iteration 1604 : loss: 0.029233, loss_a: 0.017196
[21:15:21.887] iteration 1605 : loss: 0.053191, loss_a: 0.031289
[21:15:22.626] iteration 1606 : loss: 0.055545, loss_a: 0.032673
[21:15:23.964] iteration 1607 : loss: 0.040239, loss_a: 0.023670
[21:15:24.703] iteration 1608 : loss: 0.043491, loss_a: 0.025583
[21:15:26.033] iteration 1609 : loss: 0.058424, loss_a: 0.034367
[21:15:26.767] iteration 1610 : loss: 0.028361, loss_a: 0.016683
[21:15:28.129] iteration 1611 : loss: 0.035305, loss_a: 0.020768
[21:15:28.874] iteration 1612 : loss: 0.041214, loss_a: 0.024244
[21:15:30.202] iteration 1613 : loss: 0.019832, loss_a: 0.011666
[21:15:30.946] iteration 1614 : loss: 0.031788, loss_a: 0.018699
[21:15:32.306] iteration 1615 : loss: 0.044193, loss_a: 0.025996
[21:15:33.042] iteration 1616 : loss: 0.031738, loss_a: 0.018669
[21:15:34.372] iteration 1617 : loss: 0.041310, loss_a: 0.024300
[21:15:35.103] iteration 1618 : loss: 0.015177, loss_a: 0.008928
[21:15:36.435] iteration 1619 : loss: 0.032634, loss_a: 0.019197
[21:15:37.177] iteration 1620 : loss: 0.324239, loss_a: 0.190729
[21:15:38.534] iteration 1621 : loss: 0.037660, loss_a: 0.022153
[21:15:39.291] iteration 1622 : loss: 0.048173, loss_a: 0.028337
[21:15:40.625] iteration 1623 : loss: 0.058191, loss_a: 0.034230
[21:15:41.368] iteration 1624 : loss: 0.037615, loss_a: 0.022126
[21:15:42.714] iteration 1625 : loss: 0.097904, loss_a: 0.057590
[21:15:43.455] iteration 1626 : loss: 0.032374, loss_a: 0.019043
[21:15:44.802] iteration 1627 : loss: 0.096103, loss_a: 0.056531
[21:15:45.541] iteration 1628 : loss: 0.029995, loss_a: 0.017644
[21:15:46.875] iteration 1629 : loss: 0.034614, loss_a: 0.020361
[21:15:47.615] iteration 1630 : loss: 0.036174, loss_a: 0.021279
[21:15:48.946] iteration 1631 : loss: 0.069802, loss_a: 0.041060
[21:15:49.676] iteration 1632 : loss: 0.038889, loss_a: 0.022876
[21:15:51.009] iteration 1633 : loss: 0.051557, loss_a: 0.030327
[21:15:51.748] iteration 1634 : loss: 0.105049, loss_a: 0.061793
[21:15:53.093] iteration 1635 : loss: 0.037173, loss_a: 0.021866
[21:15:53.840] iteration 1636 : loss: 0.029774, loss_a: 0.017514
[21:15:55.173] iteration 1637 : loss: 0.030896, loss_a: 0.018174
[21:15:55.922] iteration 1638 : loss: 0.046811, loss_a: 0.027536
[21:15:57.215] iteration 1639 : loss: 0.035131, loss_a: 0.020665
[21:15:57.953] iteration 1640 : loss: 0.024877, loss_a: 0.014633
[21:15:59.275] iteration 1641 : loss: 0.120191, loss_a: 0.070701
[21:16:00.016] iteration 1642 : loss: 0.041339, loss_a: 0.024317
[21:16:01.361] iteration 1643 : loss: 0.061356, loss_a: 0.036092
[21:16:02.100] iteration 1644 : loss: 0.051747, loss_a: 0.030440
[21:16:03.417] iteration 1645 : loss: 0.031218, loss_a: 0.018363
[21:16:04.167] iteration 1646 : loss: 0.075519, loss_a: 0.044423
[21:16:05.508] iteration 1647 : loss: 0.027146, loss_a: 0.015968
[21:16:06.243] iteration 1648 : loss: 0.049324, loss_a: 0.029014
[21:16:07.600] iteration 1649 : loss: 0.065222, loss_a: 0.038366
[21:16:08.347] iteration 1650 : loss: 0.047979, loss_a: 0.028223
[21:16:09.653] iteration 1651 : loss: 0.038467, loss_a: 0.022628
[21:16:10.391] iteration 1652 : loss: 0.045700, loss_a: 0.026883
[21:16:11.720] iteration 1653 : loss: 0.027143, loss_a: 0.015966
[21:16:12.462] iteration 1654 : loss: 0.044770, loss_a: 0.026335
[21:16:13.809] iteration 1655 : loss: 0.047778, loss_a: 0.028105
[21:16:14.550] iteration 1656 : loss: 0.036507, loss_a: 0.021475
[21:16:15.871] iteration 1657 : loss: 0.033275, loss_a: 0.019573
[21:16:16.622] iteration 1658 : loss: 0.039274, loss_a: 0.023102
[21:16:17.958] iteration 1659 : loss: 0.038529, loss_a: 0.022664
[21:16:18.693] iteration 1660 : loss: 0.039037, loss_a: 0.022963
[21:16:20.011] iteration 1661 : loss: 0.041235, loss_a: 0.024256
[21:16:20.750] iteration 1662 : loss: 0.051352, loss_a: 0.030207
[21:16:22.096] iteration 1663 : loss: 0.066980, loss_a: 0.039400
[21:16:22.847] iteration 1664 : loss: 0.079086, loss_a: 0.046521
[21:16:24.180] iteration 1665 : loss: 0.031600, loss_a: 0.018588
[21:16:24.917] iteration 1666 : loss: 0.020895, loss_a: 0.012291
[21:16:26.242] iteration 1667 : loss: 0.100388, loss_a: 0.059052
[21:16:26.979] iteration 1668 : loss: 0.025708, loss_a: 0.015122
[21:16:28.320] iteration 1669 : loss: 0.063368, loss_a: 0.037275
[21:16:29.064] iteration 1670 : loss: 0.099101, loss_a: 0.058295
[21:16:30.408] iteration 1671 : loss: 0.033634, loss_a: 0.019785
[21:16:31.162] iteration 1672 : loss: 0.050546, loss_a: 0.029733
[21:16:32.505] iteration 1673 : loss: 0.026879, loss_a: 0.015811
[21:16:33.244] iteration 1674 : loss: 0.043977, loss_a: 0.025869
[21:16:34.558] iteration 1675 : loss: 0.034651, loss_a: 0.020383
[21:16:35.300] iteration 1676 : loss: 0.073035, loss_a: 0.042962
[21:16:36.651] iteration 1677 : loss: 0.045133, loss_a: 0.026549
[21:16:37.385] iteration 1678 : loss: 0.027271, loss_a: 0.016042
[21:16:38.724] iteration 1679 : loss: 0.054660, loss_a: 0.032153
[21:16:39.460] iteration 1680 : loss: 0.040368, loss_a: 0.023746
[21:16:40.786] iteration 1681 : loss: 0.040963, loss_a: 0.024096
[21:16:41.531] iteration 1682 : loss: 0.040289, loss_a: 0.023699
[21:16:42.879] iteration 1683 : loss: 0.059508, loss_a: 0.035005
[21:16:43.621] iteration 1684 : loss: 0.040490, loss_a: 0.023817
[21:16:44.982] iteration 1685 : loss: 0.083236, loss_a: 0.048962
[21:16:45.730] iteration 1686 : loss: 0.031045, loss_a: 0.018262
[21:16:47.078] iteration 1687 : loss: 0.050628, loss_a: 0.029781
[21:16:47.819] iteration 1688 : loss: 0.026490, loss_a: 0.015582
[21:16:49.143] iteration 1689 : loss: 0.043674, loss_a: 0.025690
[21:16:49.889] iteration 1690 : loss: 0.051924, loss_a: 0.030544
[21:16:51.226] iteration 1691 : loss: 0.053168, loss_a: 0.031276
[21:16:51.957] iteration 1692 : loss: 0.029605, loss_a: 0.017415
[21:16:53.287] iteration 1693 : loss: 0.054315, loss_a: 0.031950
[21:16:54.032] iteration 1694 : loss: 0.036796, loss_a: 0.021645
[21:16:55.349] iteration 1695 : loss: 0.039356, loss_a: 0.023151
[21:16:56.088] iteration 1696 : loss: 0.052440, loss_a: 0.030847
[21:16:57.399] iteration 1697 : loss: 0.025993, loss_a: 0.015290
[21:16:58.140] iteration 1698 : loss: 0.110423, loss_a: 0.064955
[21:16:59.470] iteration 1699 : loss: 0.038706, loss_a: 0.022768
[21:17:00.219] iteration 1700 : loss: 0.039765, loss_a: 0.023391
[21:17:01.561] iteration 1701 : loss: 0.058766, loss_a: 0.034568
[21:17:02.297] iteration 1702 : loss: 0.050217, loss_a: 0.029539
[21:17:03.646] iteration 1703 : loss: 0.029180, loss_a: 0.017165
[21:17:04.383] iteration 1704 : loss: 0.031427, loss_a: 0.018487
[21:17:05.698] iteration 1705 : loss: 0.025201, loss_a: 0.014824
[21:17:06.436] iteration 1706 : loss: 0.060657, loss_a: 0.035681
[21:17:07.783] iteration 1707 : loss: 0.037246, loss_a: 0.021909
[21:17:08.519] iteration 1708 : loss: 0.024696, loss_a: 0.014527
[21:17:09.846] iteration 1709 : loss: 0.029601, loss_a: 0.017412
[21:17:10.582] iteration 1710 : loss: 0.036421, loss_a: 0.021424
[21:17:11.906] iteration 1711 : loss: 0.079428, loss_a: 0.046722
[21:17:12.647] iteration 1712 : loss: 0.063897, loss_a: 0.037587
[21:17:13.976] iteration 1713 : loss: 0.045231, loss_a: 0.026606
[21:17:14.708] iteration 1714 : loss: 0.020013, loss_a: 0.011772
[21:17:16.059] iteration 1715 : loss: 0.034915, loss_a: 0.020538
[21:17:16.805] iteration 1716 : loss: 0.038660, loss_a: 0.022741
[21:17:18.116] iteration 1717 : loss: 0.016239, loss_a: 0.009553
[21:17:18.854] iteration 1718 : loss: 0.025037, loss_a: 0.014728
[21:17:20.180] iteration 1719 : loss: 0.037663, loss_a: 0.022155
[21:17:20.932] iteration 1720 : loss: 0.069909, loss_a: 0.041123
[21:17:22.263] iteration 1721 : loss: 0.040247, loss_a: 0.023675
[21:17:23.024] iteration 1722 : loss: 0.056419, loss_a: 0.033187
[21:17:24.363] iteration 1723 : loss: 0.031620, loss_a: 0.018600
[21:17:25.110] iteration 1724 : loss: 0.078508, loss_a: 0.046181
[21:17:26.482] iteration 1725 : loss: 0.046801, loss_a: 0.027530
[21:17:27.225] iteration 1726 : loss: 0.031179, loss_a: 0.018340
[21:17:28.576] iteration 1727 : loss: 0.042145, loss_a: 0.024791
[21:17:29.316] iteration 1728 : loss: 0.016216, loss_a: 0.009539
[21:17:30.678] iteration 1729 : loss: 0.050301, loss_a: 0.029589
[21:17:31.421] iteration 1730 : loss: 0.057716, loss_a: 0.033950
[21:17:32.776] iteration 1731 : loss: 0.025016, loss_a: 0.014715
[21:17:33.535] iteration 1732 : loss: 0.043936, loss_a: 0.025845
[21:17:34.889] iteration 1733 : loss: 0.066387, loss_a: 0.039051
[21:17:35.621] iteration 1734 : loss: 0.014299, loss_a: 0.008411
[21:17:36.951] iteration 1735 : loss: 0.028512, loss_a: 0.016772
[21:17:37.699] iteration 1736 : loss: 0.025292, loss_a: 0.014878
[21:17:39.024] iteration 1737 : loss: 0.052976, loss_a: 0.031163
[21:17:39.762] iteration 1738 : loss: 0.041575, loss_a: 0.024456
[21:17:41.110] iteration 1739 : loss: 0.013818, loss_a: 0.008128
[21:17:41.843] iteration 1740 : loss: 0.055426, loss_a: 0.032603
[21:17:43.182] iteration 1741 : loss: 0.042141, loss_a: 0.024789
[21:17:43.930] iteration 1742 : loss: 0.029760, loss_a: 0.017506
[21:17:45.244] iteration 1743 : loss: 0.028888, loss_a: 0.016993
[21:17:45.992] iteration 1744 : loss: 0.045952, loss_a: 0.027030
[21:17:47.321] iteration 1745 : loss: 0.065719, loss_a: 0.038658
[21:17:48.056] iteration 1746 : loss: 0.031196, loss_a: 0.018351
[21:17:49.400] iteration 1747 : loss: 0.066564, loss_a: 0.039155
[21:17:50.140] iteration 1748 : loss: 0.047055, loss_a: 0.027679
[21:17:51.445] iteration 1749 : loss: 0.022191, loss_a: 0.013054
[21:17:52.200] iteration 1750 : loss: 0.064841, loss_a: 0.038142
[21:17:53.551] iteration 1751 : loss: 0.058152, loss_a: 0.034207
[21:17:54.286] iteration 1752 : loss: 0.038269, loss_a: 0.022511
[21:17:55.638] iteration 1753 : loss: 0.043775, loss_a: 0.025750
[21:17:56.388] iteration 1754 : loss: 0.032661, loss_a: 0.019212
[21:17:57.700] iteration 1755 : loss: 0.029897, loss_a: 0.017586
[21:17:58.441] iteration 1756 : loss: 0.027397, loss_a: 0.016116
[21:17:59.761] iteration 1757 : loss: 0.024733, loss_a: 0.014549
[21:18:00.507] iteration 1758 : loss: 0.039637, loss_a: 0.023316
[21:18:01.862] iteration 1759 : loss: 0.039327, loss_a: 0.023134
[21:18:02.602] iteration 1760 : loss: 0.018655, loss_a: 0.010973
[21:18:03.919] iteration 1761 : loss: 0.030835, loss_a: 0.018138
[21:18:04.656] iteration 1762 : loss: 0.035441, loss_a: 0.020847
[21:18:05.977] iteration 1763 : loss: 0.055759, loss_a: 0.032799
[21:18:06.711] iteration 1764 : loss: 0.022159, loss_a: 0.013035
[21:18:08.041] iteration 1765 : loss: 0.048855, loss_a: 0.028738
[21:18:08.789] iteration 1766 : loss: 0.028249, loss_a: 0.016617
[21:18:10.110] iteration 1767 : loss: 0.052234, loss_a: 0.030726
[21:18:10.852] iteration 1768 : loss: 0.031267, loss_a: 0.018392
[21:18:12.190] iteration 1769 : loss: 0.033717, loss_a: 0.019833
[21:18:12.933] iteration 1770 : loss: 0.059543, loss_a: 0.035025
[21:18:14.257] iteration 1771 : loss: 0.064518, loss_a: 0.037952
[21:18:15.003] iteration 1772 : loss: 0.041082, loss_a: 0.024166
[21:18:16.343] iteration 1773 : loss: 0.037476, loss_a: 0.022045
[21:18:17.075] iteration 1774 : loss: 0.046567, loss_a: 0.027392
[21:18:18.498] iteration 1775 : loss: 0.073091, loss_a: 0.042995
[21:18:19.241] iteration 1776 : loss: 0.020489, loss_a: 0.012052
[21:18:20.606] iteration 1777 : loss: 0.029695, loss_a: 0.017468
[21:18:21.338] iteration 1778 : loss: 0.038497, loss_a: 0.022645
[21:18:22.643] iteration 1779 : loss: 0.025763, loss_a: 0.015155
[21:18:23.393] iteration 1780 : loss: 0.036005, loss_a: 0.021180
[21:18:24.740] iteration 1781 : loss: 0.045413, loss_a: 0.026714
[21:18:25.477] iteration 1782 : loss: 0.048802, loss_a: 0.028707
[21:18:26.823] iteration 1783 : loss: 0.054157, loss_a: 0.031857
[21:18:27.567] iteration 1784 : loss: 0.065677, loss_a: 0.038633
[21:18:28.900] iteration 1785 : loss: 0.040745, loss_a: 0.023968
[21:18:29.639] iteration 1786 : loss: 0.044191, loss_a: 0.025995
[21:18:30.992] iteration 1787 : loss: 0.025156, loss_a: 0.014797
[21:18:31.735] iteration 1788 : loss: 0.033093, loss_a: 0.019466
[21:18:33.068] iteration 1789 : loss: 0.072214, loss_a: 0.042479
[21:18:33.819] iteration 1790 : loss: 0.044642, loss_a: 0.026260
[21:18:35.138] iteration 1791 : loss: 0.035750, loss_a: 0.021030
[21:18:35.881] iteration 1792 : loss: 0.026394, loss_a: 0.015526
[21:18:37.213] iteration 1793 : loss: 0.054119, loss_a: 0.031834
[21:18:37.958] iteration 1794 : loss: 0.050478, loss_a: 0.029693
[21:18:39.331] iteration 1795 : loss: 0.058088, loss_a: 0.034170
[21:18:40.081] iteration 1796 : loss: 0.032867, loss_a: 0.019333
[21:18:41.426] iteration 1797 : loss: 0.032503, loss_a: 0.019120
[21:18:42.163] iteration 1798 : loss: 0.030038, loss_a: 0.017670
[21:18:43.518] iteration 1799 : loss: 0.026113, loss_a: 0.015360
[21:18:44.256] iteration 1800 : loss: 0.048791, loss_a: 0.028701
[21:19:07.796] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_1800_dice_0.8795.pth
[21:19:09.133] iteration 1801 : loss: 0.046719, loss_a: 0.027482
[21:19:11.408] iteration 1802 : loss: 0.045352, loss_a: 0.026678
[21:19:12.749] iteration 1803 : loss: 0.056473, loss_a: 0.033219
[21:19:13.493] iteration 1804 : loss: 0.052325, loss_a: 0.030780
[21:19:14.803] iteration 1805 : loss: 0.033483, loss_a: 0.019696
[21:19:15.536] iteration 1806 : loss: 0.038024, loss_a: 0.022367
[21:19:16.878] iteration 1807 : loss: 0.025010, loss_a: 0.014712
[21:19:17.625] iteration 1808 : loss: 0.031284, loss_a: 0.018402
[21:19:18.955] iteration 1809 : loss: 0.028768, loss_a: 0.016922
[21:19:19.695] iteration 1810 : loss: 0.069527, loss_a: 0.040899
[21:19:21.033] iteration 1811 : loss: 0.019583, loss_a: 0.011519
[21:19:21.812] iteration 1812 : loss: 0.145109, loss_a: 0.085358
[21:19:23.154] iteration 1813 : loss: 0.106929, loss_a: 0.062899
[21:19:23.898] iteration 1814 : loss: 0.030260, loss_a: 0.017800
[21:19:25.227] iteration 1815 : loss: 0.029288, loss_a: 0.017228
[21:19:25.970] iteration 1816 : loss: 0.028384, loss_a: 0.016696
[21:19:27.321] iteration 1817 : loss: 0.042284, loss_a: 0.024873
[21:19:28.069] iteration 1818 : loss: 0.037280, loss_a: 0.021930
[21:19:29.364] iteration 1819 : loss: 0.026004, loss_a: 0.015296
[21:19:30.098] iteration 1820 : loss: 0.023882, loss_a: 0.014049
[21:19:31.427] iteration 1821 : loss: 0.031505, loss_a: 0.018533
[21:19:32.177] iteration 1822 : loss: 0.049430, loss_a: 0.029077
[21:19:33.491] iteration 1823 : loss: 0.042697, loss_a: 0.025116
[21:19:34.233] iteration 1824 : loss: 0.036849, loss_a: 0.021676
[21:19:35.571] iteration 1825 : loss: 0.047472, loss_a: 0.027925
[21:19:36.303] iteration 1826 : loss: 0.031224, loss_a: 0.018367
[21:19:37.625] iteration 1827 : loss: 0.062504, loss_a: 0.036767
[21:19:38.366] iteration 1828 : loss: 0.038496, loss_a: 0.022645
[21:19:39.692] iteration 1829 : loss: 0.043621, loss_a: 0.025660
[21:19:40.437] iteration 1830 : loss: 0.030495, loss_a: 0.017938
[21:19:41.754] iteration 1831 : loss: 0.027038, loss_a: 0.015905
[21:19:42.498] iteration 1832 : loss: 0.067555, loss_a: 0.039738
[21:19:43.814] iteration 1833 : loss: 0.026962, loss_a: 0.015860
[21:19:44.578] iteration 1834 : loss: 0.058514, loss_a: 0.034420
[21:19:45.900] iteration 1835 : loss: 0.031328, loss_a: 0.018428
[21:19:46.638] iteration 1836 : loss: 0.059583, loss_a: 0.035049
[21:19:47.991] iteration 1837 : loss: 0.053776, loss_a: 0.031633
[21:19:48.727] iteration 1838 : loss: 0.056879, loss_a: 0.033458
[21:19:50.039] iteration 1839 : loss: 0.064405, loss_a: 0.037886
[21:19:50.781] iteration 1840 : loss: 0.054828, loss_a: 0.032252
[21:19:52.108] iteration 1841 : loss: 0.033656, loss_a: 0.019798
[21:19:52.863] iteration 1842 : loss: 0.037192, loss_a: 0.021878
[21:19:54.204] iteration 1843 : loss: 0.025956, loss_a: 0.015268
[21:19:54.947] iteration 1844 : loss: 0.034423, loss_a: 0.020249
[21:19:56.300] iteration 1845 : loss: 0.072761, loss_a: 0.042800
[21:19:57.034] iteration 1846 : loss: 0.033139, loss_a: 0.019494
[21:19:58.353] iteration 1847 : loss: 0.056290, loss_a: 0.033112
[21:19:59.101] iteration 1848 : loss: 0.040552, loss_a: 0.023854
[21:20:00.413] iteration 1849 : loss: 0.028153, loss_a: 0.016560
[21:20:01.158] iteration 1850 : loss: 0.030498, loss_a: 0.017940
[21:20:02.514] iteration 1851 : loss: 0.039494, loss_a: 0.023232
[21:20:03.250] iteration 1852 : loss: 0.033051, loss_a: 0.019442
[21:20:04.610] iteration 1853 : loss: 0.043500, loss_a: 0.025588
[21:20:05.360] iteration 1854 : loss: 0.020699, loss_a: 0.012176
[21:20:06.674] iteration 1855 : loss: 0.042470, loss_a: 0.024983
[21:20:07.415] iteration 1856 : loss: 0.030609, loss_a: 0.018005
[21:20:08.732] iteration 1857 : loss: 0.025796, loss_a: 0.015174
[21:20:09.480] iteration 1858 : loss: 0.048244, loss_a: 0.028379
[21:20:10.831] iteration 1859 : loss: 0.026615, loss_a: 0.015656
[21:20:11.566] iteration 1860 : loss: 0.027526, loss_a: 0.016192
[21:20:12.917] iteration 1861 : loss: 0.016892, loss_a: 0.009936
[21:20:13.655] iteration 1862 : loss: 0.041307, loss_a: 0.024298
[21:20:14.979] iteration 1863 : loss: 0.085414, loss_a: 0.050244
[21:20:15.720] iteration 1864 : loss: 0.034359, loss_a: 0.020211
[21:20:17.062] iteration 1865 : loss: 0.034697, loss_a: 0.020410
[21:20:17.808] iteration 1866 : loss: 0.059439, loss_a: 0.034964
[21:20:19.161] iteration 1867 : loss: 0.048662, loss_a: 0.028625
[21:20:19.899] iteration 1868 : loss: 0.024967, loss_a: 0.014686
[21:20:21.240] iteration 1869 : loss: 0.035180, loss_a: 0.020694
[21:20:21.978] iteration 1870 : loss: 0.045262, loss_a: 0.026625
[21:20:23.307] iteration 1871 : loss: 0.024409, loss_a: 0.014358
[21:20:24.041] iteration 1872 : loss: 0.026669, loss_a: 0.015688
[21:20:25.408] iteration 1873 : loss: 0.046356, loss_a: 0.027268
[21:20:26.161] iteration 1874 : loss: 0.036538, loss_a: 0.021493
[21:20:27.509] iteration 1875 : loss: 0.046556, loss_a: 0.027386
[21:20:28.252] iteration 1876 : loss: 0.017328, loss_a: 0.010193
[21:20:29.596] iteration 1877 : loss: 0.032066, loss_a: 0.018862
[21:20:30.337] iteration 1878 : loss: 0.030535, loss_a: 0.017962
[21:20:31.707] iteration 1879 : loss: 0.042150, loss_a: 0.024794
[21:20:32.445] iteration 1880 : loss: 0.035204, loss_a: 0.020708
[21:20:33.792] iteration 1881 : loss: 0.036961, loss_a: 0.021742
[21:20:34.550] iteration 1882 : loss: 0.043205, loss_a: 0.025415
[21:20:35.896] iteration 1883 : loss: 0.036266, loss_a: 0.021333
[21:20:36.632] iteration 1884 : loss: 0.032338, loss_a: 0.019023
[21:20:37.962] iteration 1885 : loss: 0.027506, loss_a: 0.016180
[21:20:38.713] iteration 1886 : loss: 0.055692, loss_a: 0.032760
[21:20:40.053] iteration 1887 : loss: 0.025302, loss_a: 0.014883
[21:20:40.793] iteration 1888 : loss: 0.037483, loss_a: 0.022049
[21:20:42.143] iteration 1889 : loss: 0.028974, loss_a: 0.017043
[21:20:42.888] iteration 1890 : loss: 0.056369, loss_a: 0.033158
[21:20:44.224] iteration 1891 : loss: 0.026591, loss_a: 0.015642
[21:20:44.962] iteration 1892 : loss: 0.056441, loss_a: 0.033201
[21:20:46.298] iteration 1893 : loss: 0.036061, loss_a: 0.021212
[21:20:47.036] iteration 1894 : loss: 0.017581, loss_a: 0.010342
[21:20:48.353] iteration 1895 : loss: 0.021644, loss_a: 0.012732
[21:20:49.093] iteration 1896 : loss: 0.029930, loss_a: 0.017606
[21:20:50.416] iteration 1897 : loss: 0.023166, loss_a: 0.013627
[21:20:51.158] iteration 1898 : loss: 0.047523, loss_a: 0.027955
[21:20:52.482] iteration 1899 : loss: 0.048063, loss_a: 0.028272
[21:20:53.221] iteration 1900 : loss: 0.031078, loss_a: 0.018281
[21:20:54.554] iteration 1901 : loss: 0.039486, loss_a: 0.023227
[21:20:55.291] iteration 1902 : loss: 0.061327, loss_a: 0.036075
[21:20:56.608] iteration 1903 : loss: 0.027202, loss_a: 0.016001
[21:20:57.342] iteration 1904 : loss: 0.023234, loss_a: 0.013667
[21:20:58.673] iteration 1905 : loss: 0.028016, loss_a: 0.016480
[21:20:59.424] iteration 1906 : loss: 0.039698, loss_a: 0.023351
[21:21:00.768] iteration 1907 : loss: 0.028949, loss_a: 0.017029
[21:21:01.507] iteration 1908 : loss: 0.029114, loss_a: 0.017126
[21:21:02.865] iteration 1909 : loss: 0.071864, loss_a: 0.042273
[21:21:03.610] iteration 1910 : loss: 0.046068, loss_a: 0.027099
[21:21:04.997] iteration 1911 : loss: 0.028781, loss_a: 0.016930
[21:21:05.735] iteration 1912 : loss: 0.034981, loss_a: 0.020577
[21:21:07.080] iteration 1913 : loss: 0.040659, loss_a: 0.023917
[21:21:07.825] iteration 1914 : loss: 0.028956, loss_a: 0.017033
[21:21:09.172] iteration 1915 : loss: 0.028732, loss_a: 0.016901
[21:21:09.916] iteration 1916 : loss: 0.062229, loss_a: 0.036605
[21:21:11.245] iteration 1917 : loss: 0.028446, loss_a: 0.016733
[21:21:11.977] iteration 1918 : loss: 0.026048, loss_a: 0.015322
[21:21:13.303] iteration 1919 : loss: 0.026505, loss_a: 0.015591
[21:21:14.050] iteration 1920 : loss: 0.040129, loss_a: 0.023605
[21:21:15.355] iteration 1921 : loss: 0.047817, loss_a: 0.028127
[21:21:16.091] iteration 1922 : loss: 0.028346, loss_a: 0.016674
[21:21:17.425] iteration 1923 : loss: 0.033307, loss_a: 0.019593
[21:21:18.168] iteration 1924 : loss: 0.030811, loss_a: 0.018124
[21:21:19.470] iteration 1925 : loss: 0.032535, loss_a: 0.019138
[21:21:20.214] iteration 1926 : loss: 0.046258, loss_a: 0.027210
[21:21:21.567] iteration 1927 : loss: 0.065101, loss_a: 0.038295
[21:21:22.306] iteration 1928 : loss: 0.038538, loss_a: 0.022670
[21:21:23.659] iteration 1929 : loss: 0.031657, loss_a: 0.018622
[21:21:24.400] iteration 1930 : loss: 0.039382, loss_a: 0.023166
[21:21:25.752] iteration 1931 : loss: 0.058025, loss_a: 0.034132
[21:21:26.495] iteration 1932 : loss: 0.037719, loss_a: 0.022188
[21:21:27.808] iteration 1933 : loss: 0.029520, loss_a: 0.017365
[21:21:28.565] iteration 1934 : loss: 0.058442, loss_a: 0.034378
[21:21:29.876] iteration 1935 : loss: 0.087029, loss_a: 0.051194
[21:21:30.625] iteration 1936 : loss: 0.040927, loss_a: 0.024075
[21:21:31.976] iteration 1937 : loss: 0.036909, loss_a: 0.021711
[21:21:32.723] iteration 1938 : loss: 0.038365, loss_a: 0.022568
[21:21:34.071] iteration 1939 : loss: 0.019762, loss_a: 0.011625
[21:21:34.812] iteration 1940 : loss: 0.049130, loss_a: 0.028900
[21:21:36.165] iteration 1941 : loss: 0.041753, loss_a: 0.024560
[21:21:36.904] iteration 1942 : loss: 0.017650, loss_a: 0.010382
[21:21:38.219] iteration 1943 : loss: 0.022806, loss_a: 0.013415
[21:21:38.958] iteration 1944 : loss: 0.028775, loss_a: 0.016926
[21:21:40.279] iteration 1945 : loss: 0.028795, loss_a: 0.016938
[21:21:41.030] iteration 1946 : loss: 0.090943, loss_a: 0.053496
[21:21:42.332] iteration 1947 : loss: 0.027411, loss_a: 0.016124
[21:21:43.082] iteration 1948 : loss: 0.064145, loss_a: 0.037733
[21:21:44.426] iteration 1949 : loss: 0.035112, loss_a: 0.020654
[21:21:45.164] iteration 1950 : loss: 0.028666, loss_a: 0.016862
[21:21:46.499] iteration 1951 : loss: 0.025003, loss_a: 0.014708
[21:21:47.242] iteration 1952 : loss: 0.028264, loss_a: 0.016626
[21:21:48.595] iteration 1953 : loss: 0.036368, loss_a: 0.021393
[21:21:49.329] iteration 1954 : loss: 0.070087, loss_a: 0.041228
[21:21:50.655] iteration 1955 : loss: 0.044997, loss_a: 0.026469
[21:21:51.389] iteration 1956 : loss: 0.014823, loss_a: 0.008719
[21:21:52.694] iteration 1957 : loss: 0.032303, loss_a: 0.019002
[21:21:53.436] iteration 1958 : loss: 0.054442, loss_a: 0.032025
[21:21:54.777] iteration 1959 : loss: 0.086317, loss_a: 0.050775
[21:21:55.513] iteration 1960 : loss: 0.018943, loss_a: 0.011143
[21:21:56.857] iteration 1961 : loss: 0.044581, loss_a: 0.026224
[21:21:57.588] iteration 1962 : loss: 0.024563, loss_a: 0.014449
[21:21:58.927] iteration 1963 : loss: 0.020843, loss_a: 0.012261
[21:21:59.667] iteration 1964 : loss: 0.037700, loss_a: 0.022176
[21:22:01.027] iteration 1965 : loss: 0.071068, loss_a: 0.041805
[21:22:01.794] iteration 1966 : loss: 0.030684, loss_a: 0.018050
[21:22:03.155] iteration 1967 : loss: 0.081949, loss_a: 0.048205
[21:22:03.912] iteration 1968 : loss: 0.060220, loss_a: 0.035424
[21:22:05.252] iteration 1969 : loss: 0.023198, loss_a: 0.013646
[21:22:05.993] iteration 1970 : loss: 0.043341, loss_a: 0.025495
[21:22:07.324] iteration 1971 : loss: 0.034114, loss_a: 0.020067
[21:22:08.060] iteration 1972 : loss: 0.028972, loss_a: 0.017042
[21:22:09.372] iteration 1973 : loss: 0.027878, loss_a: 0.016399
[21:22:10.131] iteration 1974 : loss: 0.061995, loss_a: 0.036468
[21:22:11.465] iteration 1975 : loss: 0.048977, loss_a: 0.028810
[21:22:12.211] iteration 1976 : loss: 0.040953, loss_a: 0.024090
[21:22:13.544] iteration 1977 : loss: 0.029519, loss_a: 0.017364
[21:22:14.292] iteration 1978 : loss: 0.061490, loss_a: 0.036170
[21:22:15.656] iteration 1979 : loss: 0.054841, loss_a: 0.032259
[21:22:16.397] iteration 1980 : loss: 0.100125, loss_a: 0.058897
[21:22:17.754] iteration 1981 : loss: 0.095487, loss_a: 0.056169
[21:22:18.501] iteration 1982 : loss: 0.019473, loss_a: 0.011455
[21:22:19.868] iteration 1983 : loss: 0.508546, loss_a: 0.299145
[21:22:20.611] iteration 1984 : loss: 0.039548, loss_a: 0.023264
[21:22:22.001] iteration 1985 : loss: 0.433412, loss_a: 0.254948
[21:22:22.741] iteration 1986 : loss: 0.020740, loss_a: 0.012200
[21:22:24.062] iteration 1987 : loss: 0.025869, loss_a: 0.015217
[21:22:24.802] iteration 1988 : loss: 0.028247, loss_a: 0.016616
[21:22:26.161] iteration 1989 : loss: 0.050021, loss_a: 0.029424
[21:22:26.908] iteration 1990 : loss: 0.034687, loss_a: 0.020404
[21:22:28.240] iteration 1991 : loss: 0.074590, loss_a: 0.043877
[21:22:28.979] iteration 1992 : loss: 0.051779, loss_a: 0.030458
[21:22:30.335] iteration 1993 : loss: 0.054487, loss_a: 0.032051
[21:22:31.069] iteration 1994 : loss: 0.045128, loss_a: 0.026546
[21:22:32.447] iteration 1995 : loss: 0.033113, loss_a: 0.019478
[21:22:33.180] iteration 1996 : loss: 0.045942, loss_a: 0.027025
[21:22:34.529] iteration 1997 : loss: 0.033882, loss_a: 0.019931
[21:22:35.273] iteration 1998 : loss: 0.045971, loss_a: 0.027042
[21:22:36.630] iteration 1999 : loss: 0.078421, loss_a: 0.046130
[21:22:37.372] iteration 2000 : loss: 0.040528, loss_a: 0.023840
[21:23:02.023] iteration 2001 : loss: 0.059446, loss_a: 0.034968
[21:23:04.148] iteration 2002 : loss: 0.028874, loss_a: 0.016985
[21:23:05.511] iteration 2003 : loss: 0.081665, loss_a: 0.048038
[21:23:06.258] iteration 2004 : loss: 0.025454, loss_a: 0.014973
[21:23:07.584] iteration 2005 : loss: 0.027374, loss_a: 0.016102
[21:23:08.333] iteration 2006 : loss: 0.024680, loss_a: 0.014518
[21:23:09.670] iteration 2007 : loss: 0.038481, loss_a: 0.022636
[21:23:10.410] iteration 2008 : loss: 0.022156, loss_a: 0.013033
[21:23:11.738] iteration 2009 : loss: 0.039089, loss_a: 0.022993
[21:23:12.483] iteration 2010 : loss: 0.041744, loss_a: 0.024555
[21:23:13.809] iteration 2011 : loss: 0.063817, loss_a: 0.037540
[21:23:14.547] iteration 2012 : loss: 0.046612, loss_a: 0.027419
[21:23:15.881] iteration 2013 : loss: 0.029326, loss_a: 0.017251
[21:23:16.621] iteration 2014 : loss: 0.030305, loss_a: 0.017827
[21:23:17.995] iteration 2015 : loss: 0.037790, loss_a: 0.022229
[21:23:18.735] iteration 2016 : loss: 0.041313, loss_a: 0.024302
[21:23:20.088] iteration 2017 : loss: 0.035030, loss_a: 0.020606
[21:23:20.825] iteration 2018 : loss: 0.049054, loss_a: 0.028855
[21:23:22.176] iteration 2019 : loss: 0.035183, loss_a: 0.020696
[21:23:22.939] iteration 2020 : loss: 0.049776, loss_a: 0.029280
[21:23:24.289] iteration 2021 : loss: 0.035552, loss_a: 0.020913
[21:23:25.045] iteration 2022 : loss: 0.061323, loss_a: 0.036072
[21:23:26.366] iteration 2023 : loss: 0.056087, loss_a: 0.032992
[21:23:27.114] iteration 2024 : loss: 0.041472, loss_a: 0.024396
[21:23:28.469] iteration 2025 : loss: 0.032683, loss_a: 0.019225
[21:23:29.216] iteration 2026 : loss: 0.045716, loss_a: 0.026892
[21:23:30.537] iteration 2027 : loss: 0.029662, loss_a: 0.017448
[21:23:31.282] iteration 2028 : loss: 0.087939, loss_a: 0.051729
[21:23:32.633] iteration 2029 : loss: 0.040197, loss_a: 0.023646
[21:23:33.365] iteration 2030 : loss: 0.033135, loss_a: 0.019491
[21:23:34.722] iteration 2031 : loss: 0.047731, loss_a: 0.028077
[21:23:35.470] iteration 2032 : loss: 0.086109, loss_a: 0.050653
[21:23:36.787] iteration 2033 : loss: 0.040747, loss_a: 0.023969
[21:23:37.528] iteration 2034 : loss: 0.068819, loss_a: 0.040482
[21:23:38.839] iteration 2035 : loss: 0.022139, loss_a: 0.013023
[21:23:39.584] iteration 2036 : loss: 0.072939, loss_a: 0.042905
[21:23:40.905] iteration 2037 : loss: 0.040377, loss_a: 0.023751
[21:23:41.634] iteration 2038 : loss: 0.018691, loss_a: 0.010995
[21:23:42.963] iteration 2039 : loss: 0.041445, loss_a: 0.024380
[21:23:43.705] iteration 2040 : loss: 0.048205, loss_a: 0.028356
[21:23:45.023] iteration 2041 : loss: 0.040842, loss_a: 0.024025
[21:23:45.767] iteration 2042 : loss: 0.051996, loss_a: 0.030586
[21:23:47.115] iteration 2043 : loss: 0.036781, loss_a: 0.021636
[21:23:47.851] iteration 2044 : loss: 0.061130, loss_a: 0.035959
[21:23:49.168] iteration 2045 : loss: 0.024320, loss_a: 0.014306
[21:23:49.914] iteration 2046 : loss: 0.051219, loss_a: 0.030129
[21:23:51.250] iteration 2047 : loss: 0.052993, loss_a: 0.031173
[21:23:52.007] iteration 2048 : loss: 0.047201, loss_a: 0.027765
[21:23:53.325] iteration 2049 : loss: 0.036335, loss_a: 0.021373
[21:23:54.066] iteration 2050 : loss: 0.019590, loss_a: 0.011524
[21:23:55.400] iteration 2051 : loss: 0.074926, loss_a: 0.044074
[21:23:56.135] iteration 2052 : loss: 0.030217, loss_a: 0.017775
[21:23:57.461] iteration 2053 : loss: 0.053092, loss_a: 0.031231
[21:23:58.197] iteration 2054 : loss: 0.066293, loss_a: 0.038996
[21:23:59.533] iteration 2055 : loss: 0.021212, loss_a: 0.012478
[21:24:00.288] iteration 2056 : loss: 0.060089, loss_a: 0.035346
[21:24:01.634] iteration 2057 : loss: 0.043408, loss_a: 0.025534
[21:24:02.369] iteration 2058 : loss: 0.050457, loss_a: 0.029681
[21:24:03.693] iteration 2059 : loss: 0.047487, loss_a: 0.027934
[21:24:04.433] iteration 2060 : loss: 0.069703, loss_a: 0.041002
[21:24:05.759] iteration 2061 : loss: 0.113908, loss_a: 0.067005
[21:24:06.499] iteration 2062 : loss: 0.043817, loss_a: 0.025775
[21:24:07.817] iteration 2063 : loss: 0.024989, loss_a: 0.014699
[21:24:08.559] iteration 2064 : loss: 0.053432, loss_a: 0.031431
[21:24:09.922] iteration 2065 : loss: 0.040089, loss_a: 0.023582
[21:24:10.668] iteration 2066 : loss: 0.039173, loss_a: 0.023043
[21:24:11.986] iteration 2067 : loss: 0.024114, loss_a: 0.014185
[21:24:12.724] iteration 2068 : loss: 0.041215, loss_a: 0.024244
[21:24:14.069] iteration 2069 : loss: 0.027887, loss_a: 0.016404
[21:24:14.816] iteration 2070 : loss: 0.032254, loss_a: 0.018973
[21:24:16.176] iteration 2071 : loss: 0.045937, loss_a: 0.027022
[21:24:16.919] iteration 2072 : loss: 0.053578, loss_a: 0.031516
[21:24:18.269] iteration 2073 : loss: 0.089931, loss_a: 0.052900
[21:24:19.009] iteration 2074 : loss: 0.037603, loss_a: 0.022119
[21:24:20.367] iteration 2075 : loss: 0.045398, loss_a: 0.026705
[21:24:21.109] iteration 2076 : loss: 0.049786, loss_a: 0.029286
[21:24:22.445] iteration 2077 : loss: 0.043395, loss_a: 0.025526
[21:24:23.182] iteration 2078 : loss: 0.025896, loss_a: 0.015233
[21:24:24.519] iteration 2079 : loss: 0.031631, loss_a: 0.018607
[21:24:25.259] iteration 2080 : loss: 0.025425, loss_a: 0.014956
[21:24:26.609] iteration 2081 : loss: 0.065479, loss_a: 0.038517
[21:24:27.344] iteration 2082 : loss: 0.044701, loss_a: 0.026295
[21:24:28.698] iteration 2083 : loss: 0.029502, loss_a: 0.017354
[21:24:29.437] iteration 2084 : loss: 0.047147, loss_a: 0.027733
[21:24:30.813] iteration 2085 : loss: 0.042864, loss_a: 0.025214
[21:24:31.561] iteration 2086 : loss: 0.022688, loss_a: 0.013346
[21:24:32.908] iteration 2087 : loss: 0.045649, loss_a: 0.026852
[21:24:33.661] iteration 2088 : loss: 0.044611, loss_a: 0.026242
[21:24:35.005] iteration 2089 : loss: 0.033969, loss_a: 0.019982
[21:24:35.750] iteration 2090 : loss: 0.050012, loss_a: 0.029419
[21:24:37.078] iteration 2091 : loss: 0.054049, loss_a: 0.031794
[21:24:37.825] iteration 2092 : loss: 0.035260, loss_a: 0.020741
[21:24:39.168] iteration 2093 : loss: 0.051518, loss_a: 0.030305
[21:24:39.915] iteration 2094 : loss: 0.054342, loss_a: 0.031966
[21:24:41.252] iteration 2095 : loss: 0.045001, loss_a: 0.026471
[21:24:41.981] iteration 2096 : loss: 0.021750, loss_a: 0.012794
[21:24:43.301] iteration 2097 : loss: 0.027700, loss_a: 0.016294
[21:24:44.047] iteration 2098 : loss: 0.078172, loss_a: 0.045984
[21:24:45.379] iteration 2099 : loss: 0.025824, loss_a: 0.015190
[21:24:46.113] iteration 2100 : loss: 0.027657, loss_a: 0.016269
[21:24:47.455] iteration 2101 : loss: 0.025104, loss_a: 0.014767
[21:24:48.200] iteration 2102 : loss: 0.033431, loss_a: 0.019665
[21:24:49.544] iteration 2103 : loss: 0.023739, loss_a: 0.013964
[21:24:50.286] iteration 2104 : loss: 0.039563, loss_a: 0.023272
[21:24:51.633] iteration 2105 : loss: 0.080957, loss_a: 0.047622
[21:24:52.382] iteration 2106 : loss: 0.045923, loss_a: 0.027014
[21:24:53.770] iteration 2107 : loss: 0.047613, loss_a: 0.028008
[21:24:54.514] iteration 2108 : loss: 0.059327, loss_a: 0.034898
[21:24:55.876] iteration 2109 : loss: 0.034657, loss_a: 0.020386
[21:24:56.610] iteration 2110 : loss: 0.055371, loss_a: 0.032571
[21:24:57.943] iteration 2111 : loss: 0.030875, loss_a: 0.018162
[21:24:58.688] iteration 2112 : loss: 0.021059, loss_a: 0.012387
[21:25:00.045] iteration 2113 : loss: 0.044916, loss_a: 0.026421
[21:25:00.802] iteration 2114 : loss: 0.045010, loss_a: 0.026477
[21:25:02.147] iteration 2115 : loss: 0.034951, loss_a: 0.020560
[21:25:02.886] iteration 2116 : loss: 0.032065, loss_a: 0.018862
[21:25:04.243] iteration 2117 : loss: 0.049335, loss_a: 0.029020
[21:25:04.992] iteration 2118 : loss: 0.059446, loss_a: 0.034968
[21:25:06.336] iteration 2119 : loss: 0.051264, loss_a: 0.030155
[21:25:07.080] iteration 2120 : loss: 0.030125, loss_a: 0.017721
[21:25:08.398] iteration 2121 : loss: 0.023338, loss_a: 0.013728
[21:25:09.140] iteration 2122 : loss: 0.045915, loss_a: 0.027009
[21:25:10.447] iteration 2123 : loss: 0.039813, loss_a: 0.023419
[21:25:11.193] iteration 2124 : loss: 0.045202, loss_a: 0.026589
[21:25:12.565] iteration 2125 : loss: 0.055307, loss_a: 0.032534
[21:25:13.310] iteration 2126 : loss: 0.041261, loss_a: 0.024271
[21:25:14.666] iteration 2127 : loss: 0.050924, loss_a: 0.029955
[21:25:15.413] iteration 2128 : loss: 0.044408, loss_a: 0.026122
[21:25:16.779] iteration 2129 : loss: 0.032103, loss_a: 0.018884
[21:25:17.516] iteration 2130 : loss: 0.054552, loss_a: 0.032089
[21:25:18.830] iteration 2131 : loss: 0.041758, loss_a: 0.024563
[21:25:19.566] iteration 2132 : loss: 0.020652, loss_a: 0.012148
[21:25:20.920] iteration 2133 : loss: 0.038559, loss_a: 0.022681
[21:25:21.658] iteration 2134 : loss: 0.024170, loss_a: 0.014217
[21:25:22.955] iteration 2135 : loss: 0.022986, loss_a: 0.013521
[21:25:23.689] iteration 2136 : loss: 0.024574, loss_a: 0.014456
[21:25:25.042] iteration 2137 : loss: 0.032050, loss_a: 0.018853
[21:25:25.780] iteration 2138 : loss: 0.101285, loss_a: 0.059580
[21:25:27.095] iteration 2139 : loss: 0.048132, loss_a: 0.028313
[21:25:27.849] iteration 2140 : loss: 0.050551, loss_a: 0.029736
[21:25:29.156] iteration 2141 : loss: 0.026665, loss_a: 0.015685
[21:25:29.897] iteration 2142 : loss: 0.036698, loss_a: 0.021587
[21:25:31.256] iteration 2143 : loss: 0.063611, loss_a: 0.037418
[21:25:31.998] iteration 2144 : loss: 0.019191, loss_a: 0.011289
[21:25:33.345] iteration 2145 : loss: 0.076169, loss_a: 0.044805
[21:25:34.091] iteration 2146 : loss: 0.056358, loss_a: 0.033152
[21:25:35.407] iteration 2147 : loss: 0.035833, loss_a: 0.021078
[21:25:36.138] iteration 2148 : loss: 0.018059, loss_a: 0.010623
[21:25:37.451] iteration 2149 : loss: 0.066526, loss_a: 0.039133
[21:25:38.201] iteration 2150 : loss: 0.038177, loss_a: 0.022457
[21:25:39.527] iteration 2151 : loss: 0.020584, loss_a: 0.012108
[21:25:40.269] iteration 2152 : loss: 0.031781, loss_a: 0.018695
[21:25:41.640] iteration 2153 : loss: 0.032120, loss_a: 0.018894
[21:25:42.384] iteration 2154 : loss: 0.030711, loss_a: 0.018065
[21:25:43.697] iteration 2155 : loss: 0.023176, loss_a: 0.013633
[21:25:44.441] iteration 2156 : loss: 0.028185, loss_a: 0.016579
[21:25:45.766] iteration 2157 : loss: 0.033729, loss_a: 0.019841
[21:25:46.502] iteration 2158 : loss: 0.024690, loss_a: 0.014523
[21:25:47.855] iteration 2159 : loss: 0.040526, loss_a: 0.023839
[21:25:48.589] iteration 2160 : loss: 0.053883, loss_a: 0.031696
[21:25:49.936] iteration 2161 : loss: 0.061397, loss_a: 0.036116
[21:25:50.676] iteration 2162 : loss: 0.024217, loss_a: 0.014245
[21:25:51.998] iteration 2163 : loss: 0.065114, loss_a: 0.038302
[21:25:52.744] iteration 2164 : loss: 0.023799, loss_a: 0.014000
[21:25:54.055] iteration 2165 : loss: 0.021212, loss_a: 0.012478
[21:25:54.807] iteration 2166 : loss: 0.032944, loss_a: 0.019379
[21:25:56.146] iteration 2167 : loss: 0.023908, loss_a: 0.014063
[21:25:56.897] iteration 2168 : loss: 0.089898, loss_a: 0.052881
[21:25:58.250] iteration 2169 : loss: 0.023914, loss_a: 0.014067
[21:25:58.985] iteration 2170 : loss: 0.016998, loss_a: 0.009999
[21:26:00.305] iteration 2171 : loss: 0.058000, loss_a: 0.034118
[21:26:01.054] iteration 2172 : loss: 0.044780, loss_a: 0.026341
[21:26:02.368] iteration 2173 : loss: 0.022199, loss_a: 0.013058
[21:26:03.103] iteration 2174 : loss: 0.022499, loss_a: 0.013235
[21:26:04.453] iteration 2175 : loss: 0.025060, loss_a: 0.014741
[21:26:05.201] iteration 2176 : loss: 0.034245, loss_a: 0.020144
[21:26:06.527] iteration 2177 : loss: 0.046220, loss_a: 0.027188
[21:26:07.263] iteration 2178 : loss: 0.032092, loss_a: 0.018878
[21:26:08.596] iteration 2179 : loss: 0.058401, loss_a: 0.034354
[21:26:09.335] iteration 2180 : loss: 0.028659, loss_a: 0.016858
[21:26:10.687] iteration 2181 : loss: 0.031905, loss_a: 0.018768
[21:26:11.429] iteration 2182 : loss: 0.040967, loss_a: 0.024098
[21:26:12.741] iteration 2183 : loss: 0.052768, loss_a: 0.031040
[21:26:13.477] iteration 2184 : loss: 0.050080, loss_a: 0.029459
[21:26:14.786] iteration 2185 : loss: 0.024131, loss_a: 0.014194
[21:26:15.519] iteration 2186 : loss: 0.016110, loss_a: 0.009477
[21:26:16.879] iteration 2187 : loss: 0.031728, loss_a: 0.018663
[21:26:17.615] iteration 2188 : loss: 0.029773, loss_a: 0.017513
[21:26:18.937] iteration 2189 : loss: 0.023495, loss_a: 0.013820
[21:26:19.680] iteration 2190 : loss: 0.055541, loss_a: 0.032671
[21:26:21.010] iteration 2191 : loss: 0.022224, loss_a: 0.013073
[21:26:21.746] iteration 2192 : loss: 0.023873, loss_a: 0.014043
[21:26:23.086] iteration 2193 : loss: 0.023423, loss_a: 0.013778
[21:26:23.838] iteration 2194 : loss: 0.024200, loss_a: 0.014236
[21:26:25.186] iteration 2195 : loss: 0.028294, loss_a: 0.016644
[21:26:25.917] iteration 2196 : loss: 0.031163, loss_a: 0.018331
[21:26:27.246] iteration 2197 : loss: 0.036229, loss_a: 0.021311
[21:26:27.985] iteration 2198 : loss: 0.056601, loss_a: 0.033295
[21:26:29.339] iteration 2199 : loss: 0.071148, loss_a: 0.041852
[21:26:30.079] iteration 2200 : loss: 0.029284, loss_a: 0.017226
[21:26:53.640] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_2200_dice_0.8816.pth
[21:26:54.970] iteration 2201 : loss: 0.037813, loss_a: 0.022243
[21:26:57.199] iteration 2202 : loss: 0.031348, loss_a: 0.018440
[21:26:58.550] iteration 2203 : loss: 0.095351, loss_a: 0.056089
[21:26:59.296] iteration 2204 : loss: 0.031250, loss_a: 0.018382
[21:27:00.661] iteration 2205 : loss: 0.036908, loss_a: 0.021711
[21:27:01.403] iteration 2206 : loss: 0.032078, loss_a: 0.018869
[21:27:02.755] iteration 2207 : loss: 0.033025, loss_a: 0.019427
[21:27:03.498] iteration 2208 : loss: 0.029321, loss_a: 0.017248
[21:27:04.846] iteration 2209 : loss: 0.025968, loss_a: 0.015275
[21:27:05.585] iteration 2210 : loss: 0.031032, loss_a: 0.018254
[21:27:06.933] iteration 2211 : loss: 0.032565, loss_a: 0.019156
[21:27:07.672] iteration 2212 : loss: 0.044323, loss_a: 0.026072
[21:27:08.979] iteration 2213 : loss: 0.019643, loss_a: 0.011555
[21:27:09.718] iteration 2214 : loss: 0.029535, loss_a: 0.017373
[21:27:11.059] iteration 2215 : loss: 0.015328, loss_a: 0.009017
[21:27:11.796] iteration 2216 : loss: 0.037190, loss_a: 0.021877
[21:27:13.131] iteration 2217 : loss: 0.025606, loss_a: 0.015063
[21:27:13.875] iteration 2218 : loss: 0.046220, loss_a: 0.027189
[21:27:15.231] iteration 2219 : loss: 0.055689, loss_a: 0.032758
[21:27:15.970] iteration 2220 : loss: 0.032062, loss_a: 0.018860
[21:27:17.288] iteration 2221 : loss: 0.034334, loss_a: 0.020196
[21:27:18.033] iteration 2222 : loss: 0.031171, loss_a: 0.018336
[21:27:19.352] iteration 2223 : loss: 0.019435, loss_a: 0.011432
[21:27:20.089] iteration 2224 : loss: 0.039789, loss_a: 0.023405
[21:27:21.438] iteration 2225 : loss: 0.027603, loss_a: 0.016237
[21:27:22.183] iteration 2226 : loss: 0.074267, loss_a: 0.043686
[21:27:23.511] iteration 2227 : loss: 0.025363, loss_a: 0.014920
[21:27:24.259] iteration 2228 : loss: 0.054768, loss_a: 0.032216
[21:27:25.589] iteration 2229 : loss: 0.038949, loss_a: 0.022911
[21:27:26.336] iteration 2230 : loss: 0.060356, loss_a: 0.035503
[21:27:27.672] iteration 2231 : loss: 0.036552, loss_a: 0.021501
[21:27:28.412] iteration 2232 : loss: 0.080511, loss_a: 0.047359
[21:27:29.749] iteration 2233 : loss: 0.034968, loss_a: 0.020569
[21:27:30.484] iteration 2234 : loss: 0.035545, loss_a: 0.020909
[21:27:31.815] iteration 2235 : loss: 0.057442, loss_a: 0.033789
[21:27:32.545] iteration 2236 : loss: 0.039017, loss_a: 0.022951
[21:27:33.865] iteration 2237 : loss: 0.029704, loss_a: 0.017473
[21:27:34.611] iteration 2238 : loss: 0.019961, loss_a: 0.011742
[21:27:35.964] iteration 2239 : loss: 0.041527, loss_a: 0.024428
[21:27:36.694] iteration 2240 : loss: 0.028111, loss_a: 0.016536
[21:27:38.027] iteration 2241 : loss: 0.086742, loss_a: 0.051024
[21:27:38.766] iteration 2242 : loss: 0.033824, loss_a: 0.019897
[21:27:40.082] iteration 2243 : loss: 0.044484, loss_a: 0.026167
[21:27:40.823] iteration 2244 : loss: 0.062382, loss_a: 0.036696
[21:27:42.194] iteration 2245 : loss: 0.033006, loss_a: 0.019416
[21:27:42.947] iteration 2246 : loss: 0.053970, loss_a: 0.031747
[21:27:44.303] iteration 2247 : loss: 0.036454, loss_a: 0.021443
[21:27:45.044] iteration 2248 : loss: 0.034710, loss_a: 0.020418
[21:27:46.407] iteration 2249 : loss: 0.046062, loss_a: 0.027095
[21:27:47.143] iteration 2250 : loss: 0.031124, loss_a: 0.018308
[21:27:48.472] iteration 2251 : loss: 0.029721, loss_a: 0.017483
[21:27:49.224] iteration 2252 : loss: 0.059788, loss_a: 0.035170
[21:27:50.564] iteration 2253 : loss: 0.022225, loss_a: 0.013074
[21:27:51.293] iteration 2254 : loss: 0.028859, loss_a: 0.016976
[21:27:52.642] iteration 2255 : loss: 0.054740, loss_a: 0.032200
[21:27:53.374] iteration 2256 : loss: 0.026236, loss_a: 0.015433
[21:27:54.709] iteration 2257 : loss: 0.039049, loss_a: 0.022970
[21:27:55.442] iteration 2258 : loss: 0.026900, loss_a: 0.015823
[21:27:56.808] iteration 2259 : loss: 0.032055, loss_a: 0.018856
[21:27:57.554] iteration 2260 : loss: 0.092225, loss_a: 0.054250
[21:27:58.891] iteration 2261 : loss: 0.034253, loss_a: 0.020149
[21:27:59.629] iteration 2262 : loss: 0.052795, loss_a: 0.031056
[21:28:00.967] iteration 2263 : loss: 0.016296, loss_a: 0.009586
[21:28:01.707] iteration 2264 : loss: 0.033650, loss_a: 0.019794
[21:28:03.034] iteration 2265 : loss: 0.033326, loss_a: 0.019604
[21:28:03.767] iteration 2266 : loss: 0.028399, loss_a: 0.016705
[21:28:05.103] iteration 2267 : loss: 0.020433, loss_a: 0.012020
[21:28:05.835] iteration 2268 : loss: 0.038724, loss_a: 0.022779
[21:28:07.144] iteration 2269 : loss: 0.022564, loss_a: 0.013273
[21:28:07.887] iteration 2270 : loss: 0.070037, loss_a: 0.041198
[21:28:09.203] iteration 2271 : loss: 0.055509, loss_a: 0.032652
[21:28:09.948] iteration 2272 : loss: 0.041138, loss_a: 0.024199
[21:28:11.309] iteration 2273 : loss: 0.055282, loss_a: 0.032519
[21:28:12.058] iteration 2274 : loss: 0.063178, loss_a: 0.037164
[21:28:13.390] iteration 2275 : loss: 0.018245, loss_a: 0.010732
[21:28:14.130] iteration 2276 : loss: 0.029932, loss_a: 0.017607
[21:28:15.458] iteration 2277 : loss: 0.041394, loss_a: 0.024350
[21:28:16.216] iteration 2278 : loss: 0.082937, loss_a: 0.048787
[21:28:17.551] iteration 2279 : loss: 0.035927, loss_a: 0.021133
[21:28:18.286] iteration 2280 : loss: 0.030617, loss_a: 0.018010
[21:28:19.594] iteration 2281 : loss: 0.020549, loss_a: 0.012088
[21:28:20.334] iteration 2282 : loss: 0.026058, loss_a: 0.015328
[21:28:21.690] iteration 2283 : loss: 0.049130, loss_a: 0.028900
[21:28:22.424] iteration 2284 : loss: 0.027649, loss_a: 0.016264
[21:28:23.770] iteration 2285 : loss: 0.032238, loss_a: 0.018964
[21:28:24.514] iteration 2286 : loss: 0.062112, loss_a: 0.036536
[21:28:25.854] iteration 2287 : loss: 0.040003, loss_a: 0.023531
[21:28:26.597] iteration 2288 : loss: 0.029389, loss_a: 0.017287
[21:28:27.973] iteration 2289 : loss: 0.043526, loss_a: 0.025604
[21:28:28.705] iteration 2290 : loss: 0.043552, loss_a: 0.025619
[21:28:30.056] iteration 2291 : loss: 0.042956, loss_a: 0.025269
[21:28:30.802] iteration 2292 : loss: 0.061757, loss_a: 0.036328
[21:28:32.153] iteration 2293 : loss: 0.036844, loss_a: 0.021673
[21:28:32.881] iteration 2294 : loss: 0.051187, loss_a: 0.030110
[21:28:34.191] iteration 2295 : loss: 0.028970, loss_a: 0.017041
[21:28:34.934] iteration 2296 : loss: 0.021475, loss_a: 0.012633
[21:28:36.281] iteration 2297 : loss: 0.047129, loss_a: 0.027723
[21:28:37.018] iteration 2298 : loss: 0.056546, loss_a: 0.033262
[21:28:38.367] iteration 2299 : loss: 0.040743, loss_a: 0.023967
[21:28:39.106] iteration 2300 : loss: 0.034284, loss_a: 0.020167
[21:28:40.459] iteration 2301 : loss: 0.045727, loss_a: 0.026899
[21:28:41.191] iteration 2302 : loss: 0.034639, loss_a: 0.020376
[21:28:42.516] iteration 2303 : loss: 0.049885, loss_a: 0.029344
[21:28:43.245] iteration 2304 : loss: 0.022110, loss_a: 0.013006
[21:28:44.566] iteration 2305 : loss: 0.065687, loss_a: 0.038639
[21:28:45.302] iteration 2306 : loss: 0.023341, loss_a: 0.013730
[21:28:46.655] iteration 2307 : loss: 0.034013, loss_a: 0.020008
[21:28:47.404] iteration 2308 : loss: 0.077941, loss_a: 0.045848
[21:28:48.755] iteration 2309 : loss: 0.054571, loss_a: 0.032100
[21:28:49.485] iteration 2310 : loss: 0.029187, loss_a: 0.017169
[21:28:50.856] iteration 2311 : loss: 0.027290, loss_a: 0.016053
[21:28:51.596] iteration 2312 : loss: 0.038518, loss_a: 0.022658
[21:28:52.950] iteration 2313 : loss: 0.028527, loss_a: 0.016781
[21:28:53.691] iteration 2314 : loss: 0.032041, loss_a: 0.018848
[21:28:55.048] iteration 2315 : loss: 0.037142, loss_a: 0.021848
[21:28:55.790] iteration 2316 : loss: 0.098412, loss_a: 0.057890
[21:28:57.142] iteration 2317 : loss: 0.046396, loss_a: 0.027292
[21:28:57.874] iteration 2318 : loss: 0.032510, loss_a: 0.019124
[21:28:59.206] iteration 2319 : loss: 0.024455, loss_a: 0.014386
[21:28:59.934] iteration 2320 : loss: 0.034958, loss_a: 0.020563
[21:29:01.300] iteration 2321 : loss: 0.118684, loss_a: 0.069814
[21:29:02.037] iteration 2322 : loss: 0.050111, loss_a: 0.029477
[21:29:03.370] iteration 2323 : loss: 0.043995, loss_a: 0.025880
[21:29:04.117] iteration 2324 : loss: 0.043460, loss_a: 0.025565
[21:29:05.460] iteration 2325 : loss: 0.033509, loss_a: 0.019711
[21:29:06.202] iteration 2326 : loss: 0.041704, loss_a: 0.024532
[21:29:07.539] iteration 2327 : loss: 0.032231, loss_a: 0.018960
[21:29:08.278] iteration 2328 : loss: 0.030115, loss_a: 0.017714
[21:29:09.622] iteration 2329 : loss: 0.057277, loss_a: 0.033692
[21:29:10.372] iteration 2330 : loss: 0.025612, loss_a: 0.015066
[21:29:11.723] iteration 2331 : loss: 0.045069, loss_a: 0.026511
[21:29:12.463] iteration 2332 : loss: 0.039942, loss_a: 0.023495
[21:29:13.816] iteration 2333 : loss: 0.037981, loss_a: 0.022342
[21:29:14.548] iteration 2334 : loss: 0.070127, loss_a: 0.041251
[21:29:15.851] iteration 2335 : loss: 0.041569, loss_a: 0.024452
[21:29:16.597] iteration 2336 : loss: 0.040089, loss_a: 0.023582
[21:29:17.959] iteration 2337 : loss: 0.043809, loss_a: 0.025770
[21:29:18.691] iteration 2338 : loss: 0.028200, loss_a: 0.016588
[21:29:20.003] iteration 2339 : loss: 0.040265, loss_a: 0.023685
[21:29:20.735] iteration 2340 : loss: 0.063511, loss_a: 0.037360
[21:29:22.067] iteration 2341 : loss: 0.034169, loss_a: 0.020100
[21:29:22.808] iteration 2342 : loss: 0.112390, loss_a: 0.066112
[21:29:24.158] iteration 2343 : loss: 0.050729, loss_a: 0.029841
[21:29:24.899] iteration 2344 : loss: 0.047581, loss_a: 0.027989
[21:29:26.223] iteration 2345 : loss: 0.055726, loss_a: 0.032780
[21:29:26.965] iteration 2346 : loss: 0.071022, loss_a: 0.041778
[21:29:28.293] iteration 2347 : loss: 0.044887, loss_a: 0.026404
[21:29:29.078] iteration 2348 : loss: 0.035965, loss_a: 0.021156
[21:29:30.429] iteration 2349 : loss: 0.036234, loss_a: 0.021314
[21:29:31.174] iteration 2350 : loss: 0.085756, loss_a: 0.050445
[21:29:32.530] iteration 2351 : loss: 0.057747, loss_a: 0.033969
[21:29:33.259] iteration 2352 : loss: 0.038187, loss_a: 0.022463
[21:29:34.604] iteration 2353 : loss: 0.047784, loss_a: 0.028108
[21:29:35.340] iteration 2354 : loss: 0.099618, loss_a: 0.058599
[21:29:36.660] iteration 2355 : loss: 0.036561, loss_a: 0.021507
[21:29:37.400] iteration 2356 : loss: 0.043196, loss_a: 0.025409
[21:29:38.769] iteration 2357 : loss: 0.038396, loss_a: 0.022586
[21:29:39.506] iteration 2358 : loss: 0.032339, loss_a: 0.019023
[21:29:40.822] iteration 2359 : loss: 0.078495, loss_a: 0.046174
[21:29:41.570] iteration 2360 : loss: 0.036494, loss_a: 0.021467
[21:29:42.891] iteration 2361 : loss: 0.054437, loss_a: 0.032022
[21:29:43.631] iteration 2362 : loss: 0.073365, loss_a: 0.043156
[21:29:44.995] iteration 2363 : loss: 0.055030, loss_a: 0.032371
[21:29:45.747] iteration 2364 : loss: 0.061767, loss_a: 0.036333
[21:29:47.075] iteration 2365 : loss: 0.050441, loss_a: 0.029671
[21:29:47.818] iteration 2366 : loss: 0.090199, loss_a: 0.053058
[21:29:49.139] iteration 2367 : loss: 0.035136, loss_a: 0.020668
[21:29:49.878] iteration 2368 : loss: 0.064982, loss_a: 0.038225
[21:29:51.190] iteration 2369 : loss: 0.035597, loss_a: 0.020940
[21:29:51.926] iteration 2370 : loss: 0.033629, loss_a: 0.019782
[21:29:53.262] iteration 2371 : loss: 0.085940, loss_a: 0.050553
[21:29:54.012] iteration 2372 : loss: 0.071087, loss_a: 0.041816
[21:29:55.393] iteration 2373 : loss: 0.068614, loss_a: 0.040361
[21:29:56.128] iteration 2374 : loss: 0.031823, loss_a: 0.018719
[21:29:57.445] iteration 2375 : loss: 0.053440, loss_a: 0.031435
[21:29:58.188] iteration 2376 : loss: 0.037021, loss_a: 0.021777
[21:29:59.519] iteration 2377 : loss: 0.028206, loss_a: 0.016592
[21:30:00.264] iteration 2378 : loss: 0.055588, loss_a: 0.032699
[21:30:01.622] iteration 2379 : loss: 0.082761, loss_a: 0.048683
[21:30:02.381] iteration 2380 : loss: 0.031537, loss_a: 0.018551
[21:30:03.713] iteration 2381 : loss: 0.156011, loss_a: 0.091771
[21:30:04.450] iteration 2382 : loss: 0.033471, loss_a: 0.019689
[21:30:05.806] iteration 2383 : loss: 0.051014, loss_a: 0.030008
[21:30:06.550] iteration 2384 : loss: 0.039431, loss_a: 0.023195
[21:30:07.857] iteration 2385 : loss: 0.066472, loss_a: 0.039101
[21:30:08.592] iteration 2386 : loss: 0.022228, loss_a: 0.013075
[21:30:09.904] iteration 2387 : loss: 0.065922, loss_a: 0.038778
[21:30:10.661] iteration 2388 : loss: 0.079935, loss_a: 0.047021
[21:30:11.981] iteration 2389 : loss: 0.053067, loss_a: 0.031216
[21:30:12.717] iteration 2390 : loss: 0.030641, loss_a: 0.018024
[21:30:14.094] iteration 2391 : loss: 0.027807, loss_a: 0.016357
[21:30:14.841] iteration 2392 : loss: 0.068977, loss_a: 0.040575
[21:30:16.160] iteration 2393 : loss: 0.048612, loss_a: 0.028596
[21:30:16.898] iteration 2394 : loss: 0.040657, loss_a: 0.023916
[21:30:18.214] iteration 2395 : loss: 0.052280, loss_a: 0.030753
[21:30:18.949] iteration 2396 : loss: 0.025179, loss_a: 0.014811
[21:30:20.270] iteration 2397 : loss: 0.037932, loss_a: 0.022313
[21:30:20.998] iteration 2398 : loss: 0.027510, loss_a: 0.016182
[21:30:22.326] iteration 2399 : loss: 0.019811, loss_a: 0.011654
[21:30:23.059] iteration 2400 : loss: 0.021110, loss_a: 0.012417
[21:30:46.576] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_2400_dice_0.8852.pth
[21:30:47.919] iteration 2401 : loss: 0.051042, loss_a: 0.030025
[21:30:50.186] iteration 2402 : loss: 0.033855, loss_a: 0.019915
[21:30:51.513] iteration 2403 : loss: 0.038673, loss_a: 0.022749
[21:30:52.242] iteration 2404 : loss: 0.056556, loss_a: 0.033268
[21:30:53.574] iteration 2405 : loss: 0.042422, loss_a: 0.024954
[21:30:54.311] iteration 2406 : loss: 0.039934, loss_a: 0.023491
[21:30:55.666] iteration 2407 : loss: 0.025732, loss_a: 0.015137
[21:30:56.398] iteration 2408 : loss: 0.044239, loss_a: 0.026023
[21:30:57.744] iteration 2409 : loss: 0.040343, loss_a: 0.023731
[21:30:58.487] iteration 2410 : loss: 0.024916, loss_a: 0.014656
[21:30:59.842] iteration 2411 : loss: 0.051506, loss_a: 0.030298
[21:31:00.585] iteration 2412 : loss: 0.030012, loss_a: 0.017654
[21:31:01.904] iteration 2413 : loss: 0.037905, loss_a: 0.022297
[21:31:02.644] iteration 2414 : loss: 0.040264, loss_a: 0.023685
[21:31:04.004] iteration 2415 : loss: 0.059100, loss_a: 0.034765
[21:31:04.758] iteration 2416 : loss: 0.039664, loss_a: 0.023332
[21:31:06.089] iteration 2417 : loss: 0.045583, loss_a: 0.026814
[21:31:06.826] iteration 2418 : loss: 0.026396, loss_a: 0.015527
[21:31:08.139] iteration 2419 : loss: 0.066962, loss_a: 0.039390
[21:31:08.882] iteration 2420 : loss: 0.028471, loss_a: 0.016747
[21:31:10.197] iteration 2421 : loss: 0.023038, loss_a: 0.013552
[21:31:10.940] iteration 2422 : loss: 0.034368, loss_a: 0.020217
[21:31:12.257] iteration 2423 : loss: 0.018082, loss_a: 0.010637
[21:31:13.000] iteration 2424 : loss: 0.024188, loss_a: 0.014228
[21:31:14.346] iteration 2425 : loss: 0.033297, loss_a: 0.019587
[21:31:15.084] iteration 2426 : loss: 0.017707, loss_a: 0.010416
[21:31:16.406] iteration 2427 : loss: 0.021366, loss_a: 0.012568
[21:31:17.145] iteration 2428 : loss: 0.027686, loss_a: 0.016286
[21:31:18.485] iteration 2429 : loss: 0.057648, loss_a: 0.033910
[21:31:19.223] iteration 2430 : loss: 0.025277, loss_a: 0.014869
[21:31:20.546] iteration 2431 : loss: 0.022079, loss_a: 0.012988
[21:31:21.296] iteration 2432 : loss: 0.034324, loss_a: 0.020190
[21:31:22.626] iteration 2433 : loss: 0.027922, loss_a: 0.016425
[21:31:23.367] iteration 2434 : loss: 0.052793, loss_a: 0.031055
[21:31:24.704] iteration 2435 : loss: 0.031010, loss_a: 0.018241
[21:31:25.440] iteration 2436 : loss: 0.021083, loss_a: 0.012402
[21:31:26.826] iteration 2437 : loss: 0.038550, loss_a: 0.022676
[21:31:27.568] iteration 2438 : loss: 0.060728, loss_a: 0.035722
[21:31:28.901] iteration 2439 : loss: 0.025572, loss_a: 0.015042
[21:31:29.636] iteration 2440 : loss: 0.045996, loss_a: 0.027057
[21:31:30.934] iteration 2441 : loss: 0.044341, loss_a: 0.026083
[21:31:31.680] iteration 2442 : loss: 0.043017, loss_a: 0.025304
[21:31:32.992] iteration 2443 : loss: 0.029822, loss_a: 0.017542
[21:31:33.736] iteration 2444 : loss: 0.037383, loss_a: 0.021990
[21:31:35.072] iteration 2445 : loss: 0.024898, loss_a: 0.014646
[21:31:35.810] iteration 2446 : loss: 0.030104, loss_a: 0.017708
[21:31:37.133] iteration 2447 : loss: 0.036525, loss_a: 0.021485
[21:31:37.868] iteration 2448 : loss: 0.025165, loss_a: 0.014803
[21:31:39.211] iteration 2449 : loss: 0.068893, loss_a: 0.040525
[21:31:39.958] iteration 2450 : loss: 0.114855, loss_a: 0.067562
[21:31:41.279] iteration 2451 : loss: 0.038959, loss_a: 0.022917
[21:31:42.032] iteration 2452 : loss: 0.057147, loss_a: 0.033616
[21:31:43.351] iteration 2453 : loss: 0.030747, loss_a: 0.018086
[21:31:44.094] iteration 2454 : loss: 0.038743, loss_a: 0.022790
[21:31:45.431] iteration 2455 : loss: 0.058435, loss_a: 0.034374
[21:31:46.171] iteration 2456 : loss: 0.034015, loss_a: 0.020009
[21:31:47.485] iteration 2457 : loss: 0.024070, loss_a: 0.014159
[21:31:48.227] iteration 2458 : loss: 0.024960, loss_a: 0.014683
[21:31:49.550] iteration 2459 : loss: 0.041771, loss_a: 0.024571
[21:31:50.289] iteration 2460 : loss: 0.040652, loss_a: 0.023913
[21:31:51.606] iteration 2461 : loss: 0.044348, loss_a: 0.026087
[21:31:52.342] iteration 2462 : loss: 0.046881, loss_a: 0.027577
[21:31:53.687] iteration 2463 : loss: 0.062269, loss_a: 0.036629
[21:31:54.426] iteration 2464 : loss: 0.032232, loss_a: 0.018960
[21:31:55.747] iteration 2465 : loss: 0.048408, loss_a: 0.028475
[21:31:56.479] iteration 2466 : loss: 0.021564, loss_a: 0.012685
[21:31:57.785] iteration 2467 : loss: 0.016396, loss_a: 0.009645
[21:31:58.532] iteration 2468 : loss: 0.051981, loss_a: 0.030577
[21:31:59.889] iteration 2469 : loss: 0.028186, loss_a: 0.016580
[21:32:00.635] iteration 2470 : loss: 0.032416, loss_a: 0.019068
[21:32:01.955] iteration 2471 : loss: 0.038753, loss_a: 0.022796
[21:32:02.695] iteration 2472 : loss: 0.022506, loss_a: 0.013239
[21:32:04.032] iteration 2473 : loss: 0.016145, loss_a: 0.009497
[21:32:04.767] iteration 2474 : loss: 0.025125, loss_a: 0.014779
[21:32:06.121] iteration 2475 : loss: 0.026006, loss_a: 0.015298
[21:32:06.858] iteration 2476 : loss: 0.034204, loss_a: 0.020120
[21:32:08.174] iteration 2477 : loss: 0.036945, loss_a: 0.021732
[21:32:08.926] iteration 2478 : loss: 0.053988, loss_a: 0.031758
[21:32:10.228] iteration 2479 : loss: 0.039115, loss_a: 0.023009
[21:32:10.966] iteration 2480 : loss: 0.037664, loss_a: 0.022155
[21:32:12.327] iteration 2481 : loss: 0.060743, loss_a: 0.035731
[21:32:13.061] iteration 2482 : loss: 0.015998, loss_a: 0.009410
[21:32:14.392] iteration 2483 : loss: 0.043142, loss_a: 0.025378
[21:32:15.139] iteration 2484 : loss: 0.040187, loss_a: 0.023640
[21:32:16.475] iteration 2485 : loss: 0.049145, loss_a: 0.028909
[21:32:17.221] iteration 2486 : loss: 0.040535, loss_a: 0.023844
[21:32:18.566] iteration 2487 : loss: 0.031366, loss_a: 0.018451
[21:32:19.302] iteration 2488 : loss: 0.025649, loss_a: 0.015088
[21:32:20.628] iteration 2489 : loss: 0.049213, loss_a: 0.028949
[21:32:21.365] iteration 2490 : loss: 0.058058, loss_a: 0.034152
[21:32:22.685] iteration 2491 : loss: 0.040371, loss_a: 0.023748
[21:32:23.427] iteration 2492 : loss: 0.050792, loss_a: 0.029878
[21:32:24.784] iteration 2493 : loss: 0.034295, loss_a: 0.020173
[21:32:25.526] iteration 2494 : loss: 0.026381, loss_a: 0.015518
[21:32:26.866] iteration 2495 : loss: 0.030147, loss_a: 0.017733
[21:32:27.620] iteration 2496 : loss: 0.075473, loss_a: 0.044396
[21:32:28.942] iteration 2497 : loss: 0.055710, loss_a: 0.032771
[21:32:29.673] iteration 2498 : loss: 0.024914, loss_a: 0.014656
[21:32:30.992] iteration 2499 : loss: 0.026603, loss_a: 0.015649
[21:32:31.735] iteration 2500 : loss: 0.026392, loss_a: 0.015525
[21:32:33.078] iteration 2501 : loss: 0.065983, loss_a: 0.038813
[21:32:33.821] iteration 2502 : loss: 0.054265, loss_a: 0.031921
[21:32:35.131] iteration 2503 : loss: 0.025417, loss_a: 0.014951
[21:32:35.867] iteration 2504 : loss: 0.039769, loss_a: 0.023394
[21:32:37.244] iteration 2505 : loss: 0.028582, loss_a: 0.016813
[21:32:37.983] iteration 2506 : loss: 0.033255, loss_a: 0.019561
[21:32:39.300] iteration 2507 : loss: 0.031909, loss_a: 0.018770
[21:32:40.039] iteration 2508 : loss: 0.048542, loss_a: 0.028554
[21:32:41.369] iteration 2509 : loss: 0.030754, loss_a: 0.018091
[21:32:42.114] iteration 2510 : loss: 0.066444, loss_a: 0.039085
[21:32:43.409] iteration 2511 : loss: 0.054430, loss_a: 0.032018
[21:32:44.145] iteration 2512 : loss: 0.018419, loss_a: 0.010834
[21:32:45.493] iteration 2513 : loss: 0.027470, loss_a: 0.016159
[21:32:46.233] iteration 2514 : loss: 0.025597, loss_a: 0.015057
[21:32:47.575] iteration 2515 : loss: 0.034855, loss_a: 0.020503
[21:32:48.323] iteration 2516 : loss: 0.077291, loss_a: 0.045466
[21:32:49.639] iteration 2517 : loss: 0.027877, loss_a: 0.016398
[21:32:50.372] iteration 2518 : loss: 0.057884, loss_a: 0.034049
[21:32:51.689] iteration 2519 : loss: 0.037789, loss_a: 0.022229
[21:32:52.434] iteration 2520 : loss: 0.023929, loss_a: 0.014076
[21:32:53.796] iteration 2521 : loss: 0.024166, loss_a: 0.014215
[21:32:54.543] iteration 2522 : loss: 0.047439, loss_a: 0.027905
[21:32:55.902] iteration 2523 : loss: 0.036467, loss_a: 0.021451
[21:32:56.644] iteration 2524 : loss: 0.025848, loss_a: 0.015205
[21:32:58.009] iteration 2525 : loss: 0.051914, loss_a: 0.030538
[21:32:58.749] iteration 2526 : loss: 0.068468, loss_a: 0.040275
[21:33:00.075] iteration 2527 : loss: 0.023231, loss_a: 0.013665
[21:33:00.824] iteration 2528 : loss: 0.066563, loss_a: 0.039155
[21:33:02.152] iteration 2529 : loss: 0.034453, loss_a: 0.020266
[21:33:02.887] iteration 2530 : loss: 0.018048, loss_a: 0.010616
[21:33:04.194] iteration 2531 : loss: 0.030434, loss_a: 0.017902
[21:33:04.927] iteration 2532 : loss: 0.029944, loss_a: 0.017614
[21:33:06.254] iteration 2533 : loss: 0.033867, loss_a: 0.019922
[21:33:06.994] iteration 2534 : loss: 0.029050, loss_a: 0.017088
[21:33:08.350] iteration 2535 : loss: 0.018707, loss_a: 0.011004
[21:33:09.092] iteration 2536 : loss: 0.038759, loss_a: 0.022800
[21:33:10.452] iteration 2537 : loss: 0.031754, loss_a: 0.018679
[21:33:11.192] iteration 2538 : loss: 0.053092, loss_a: 0.031230
[21:33:12.524] iteration 2539 : loss: 0.060837, loss_a: 0.035787
[21:33:13.263] iteration 2540 : loss: 0.045392, loss_a: 0.026701
[21:33:14.589] iteration 2541 : loss: 0.027616, loss_a: 0.016244
[21:33:15.334] iteration 2542 : loss: 0.029089, loss_a: 0.017111
[21:33:16.655] iteration 2543 : loss: 0.034165, loss_a: 0.020097
[21:33:17.393] iteration 2544 : loss: 0.025482, loss_a: 0.014989
[21:33:18.755] iteration 2545 : loss: 0.028448, loss_a: 0.016734
[21:33:19.504] iteration 2546 : loss: 0.045740, loss_a: 0.026906
[21:33:20.839] iteration 2547 : loss: 0.026626, loss_a: 0.015662
[21:33:21.574] iteration 2548 : loss: 0.028261, loss_a: 0.016624
[21:33:22.923] iteration 2549 : loss: 0.027807, loss_a: 0.016357
[21:33:23.658] iteration 2550 : loss: 0.030759, loss_a: 0.018094
[21:33:24.973] iteration 2551 : loss: 0.051955, loss_a: 0.030562
[21:33:25.718] iteration 2552 : loss: 0.044278, loss_a: 0.026046
[21:33:27.049] iteration 2553 : loss: 0.032450, loss_a: 0.019088
[21:33:27.794] iteration 2554 : loss: 0.033765, loss_a: 0.019862
[21:33:29.149] iteration 2555 : loss: 0.018065, loss_a: 0.010626
[21:33:29.904] iteration 2556 : loss: 0.047937, loss_a: 0.028198
[21:33:31.210] iteration 2557 : loss: 0.045408, loss_a: 0.026711
[21:33:31.947] iteration 2558 : loss: 0.033963, loss_a: 0.019978
[21:33:33.277] iteration 2559 : loss: 0.060177, loss_a: 0.035398
[21:33:34.010] iteration 2560 : loss: 0.018077, loss_a: 0.010633
[21:33:35.341] iteration 2561 : loss: 0.043003, loss_a: 0.025296
[21:33:36.084] iteration 2562 : loss: 0.025984, loss_a: 0.015285
[21:33:37.427] iteration 2563 : loss: 0.035747, loss_a: 0.021028
[21:33:38.161] iteration 2564 : loss: 0.022390, loss_a: 0.013170
[21:33:39.478] iteration 2565 : loss: 0.016246, loss_a: 0.009557
[21:33:40.215] iteration 2566 : loss: 0.029097, loss_a: 0.017116
[21:33:41.541] iteration 2567 : loss: 0.050219, loss_a: 0.029541
[21:33:42.286] iteration 2568 : loss: 0.051755, loss_a: 0.030444
[21:33:43.624] iteration 2569 : loss: 0.077679, loss_a: 0.045694
[21:33:44.365] iteration 2570 : loss: 0.040078, loss_a: 0.023575
[21:33:45.707] iteration 2571 : loss: 0.060147, loss_a: 0.035380
[21:33:46.439] iteration 2572 : loss: 0.062658, loss_a: 0.036858
[21:33:47.755] iteration 2573 : loss: 0.027470, loss_a: 0.016159
[21:33:48.502] iteration 2574 : loss: 0.048147, loss_a: 0.028322
[21:33:49.845] iteration 2575 : loss: 0.044264, loss_a: 0.026037
[21:33:50.583] iteration 2576 : loss: 0.043328, loss_a: 0.025487
[21:33:51.941] iteration 2577 : loss: 0.038919, loss_a: 0.022894
[21:33:52.680] iteration 2578 : loss: 0.019758, loss_a: 0.011622
[21:33:54.033] iteration 2579 : loss: 0.040813, loss_a: 0.024008
[21:33:54.767] iteration 2580 : loss: 0.049212, loss_a: 0.028948
[21:33:56.087] iteration 2581 : loss: 0.028259, loss_a: 0.016623
[21:33:56.822] iteration 2582 : loss: 0.015350, loss_a: 0.009030
[21:33:58.147] iteration 2583 : loss: 0.025390, loss_a: 0.014935
[21:33:58.882] iteration 2584 : loss: 0.017884, loss_a: 0.010520
[21:34:00.241] iteration 2585 : loss: 0.032323, loss_a: 0.019014
[21:34:00.990] iteration 2586 : loss: 0.047907, loss_a: 0.028181
[21:34:02.336] iteration 2587 : loss: 0.041716, loss_a: 0.024539
[21:34:03.079] iteration 2588 : loss: 0.021538, loss_a: 0.012670
[21:34:04.422] iteration 2589 : loss: 0.042399, loss_a: 0.024941
[21:34:05.168] iteration 2590 : loss: 0.044714, loss_a: 0.026302
[21:34:06.529] iteration 2591 : loss: 0.027735, loss_a: 0.016315
[21:34:07.280] iteration 2592 : loss: 0.050349, loss_a: 0.029617
[21:34:08.610] iteration 2593 : loss: 0.039696, loss_a: 0.023351
[21:34:09.351] iteration 2594 : loss: 0.063836, loss_a: 0.037550
[21:34:10.696] iteration 2595 : loss: 0.020686, loss_a: 0.012168
[21:34:11.432] iteration 2596 : loss: 0.032032, loss_a: 0.018843
[21:34:12.792] iteration 2597 : loss: 0.077462, loss_a: 0.045566
[21:34:13.541] iteration 2598 : loss: 0.065380, loss_a: 0.038459
[21:34:14.862] iteration 2599 : loss: 0.049582, loss_a: 0.029166
[21:34:15.603] iteration 2600 : loss: 0.051791, loss_a: 0.030465
[21:34:40.276] iteration 2601 : loss: 0.036077, loss_a: 0.021222
[21:34:42.402] iteration 2602 : loss: 0.023437, loss_a: 0.013787
[21:34:43.761] iteration 2603 : loss: 0.112335, loss_a: 0.066079
[21:34:44.503] iteration 2604 : loss: 0.143792, loss_a: 0.084584
[21:34:45.828] iteration 2605 : loss: 0.052580, loss_a: 0.030929
[21:34:46.569] iteration 2606 : loss: 0.070774, loss_a: 0.041632
[21:34:47.884] iteration 2607 : loss: 0.038151, loss_a: 0.022442
[21:34:48.618] iteration 2608 : loss: 0.044563, loss_a: 0.026213
[21:34:49.962] iteration 2609 : loss: 0.064240, loss_a: 0.037788
[21:34:50.703] iteration 2610 : loss: 0.028598, loss_a: 0.016822
[21:34:52.063] iteration 2611 : loss: 0.052287, loss_a: 0.030757
[21:34:52.809] iteration 2612 : loss: 0.050043, loss_a: 0.029437
[21:34:54.158] iteration 2613 : loss: 0.048966, loss_a: 0.028803
[21:34:54.901] iteration 2614 : loss: 0.033823, loss_a: 0.019896
[21:34:56.257] iteration 2615 : loss: 0.028858, loss_a: 0.016975
[21:34:57.006] iteration 2616 : loss: 0.040276, loss_a: 0.023692
[21:34:58.324] iteration 2617 : loss: 0.031465, loss_a: 0.018509
[21:34:59.062] iteration 2618 : loss: 0.039443, loss_a: 0.023202
[21:35:00.403] iteration 2619 : loss: 0.034793, loss_a: 0.020466
[21:35:01.151] iteration 2620 : loss: 0.034442, loss_a: 0.020260
[21:35:02.477] iteration 2621 : loss: 0.036063, loss_a: 0.021213
[21:35:03.209] iteration 2622 : loss: 0.021887, loss_a: 0.012875
[21:35:04.560] iteration 2623 : loss: 0.285086, loss_a: 0.167698
[21:35:05.316] iteration 2624 : loss: 0.101353, loss_a: 0.059619
[21:35:06.647] iteration 2625 : loss: 0.032632, loss_a: 0.019195
[21:35:07.393] iteration 2626 : loss: 0.035366, loss_a: 0.020804
[21:35:08.741] iteration 2627 : loss: 0.037160, loss_a: 0.021859
[21:35:09.487] iteration 2628 : loss: 0.046986, loss_a: 0.027639
[21:35:10.819] iteration 2629 : loss: 0.061681, loss_a: 0.036283
[21:35:11.564] iteration 2630 : loss: 0.042025, loss_a: 0.024721
[21:35:12.918] iteration 2631 : loss: 0.039852, loss_a: 0.023442
[21:35:13.671] iteration 2632 : loss: 0.113083, loss_a: 0.066519
[21:35:15.003] iteration 2633 : loss: 0.040744, loss_a: 0.023967
[21:35:15.754] iteration 2634 : loss: 0.044767, loss_a: 0.026334
[21:35:17.076] iteration 2635 : loss: 0.049984, loss_a: 0.029402
[21:35:17.822] iteration 2636 : loss: 0.076769, loss_a: 0.045158
[21:35:19.139] iteration 2637 : loss: 0.049710, loss_a: 0.029241
[21:35:19.878] iteration 2638 : loss: 0.031618, loss_a: 0.018599
[21:35:21.235] iteration 2639 : loss: 0.025603, loss_a: 0.015061
[21:35:21.974] iteration 2640 : loss: 0.032904, loss_a: 0.019356
[21:35:23.338] iteration 2641 : loss: 0.126102, loss_a: 0.074178
[21:35:24.071] iteration 2642 : loss: 0.024266, loss_a: 0.014274
[21:35:25.409] iteration 2643 : loss: 0.032045, loss_a: 0.018850
[21:35:26.145] iteration 2644 : loss: 0.033746, loss_a: 0.019851
[21:35:27.495] iteration 2645 : loss: 0.058553, loss_a: 0.034443
[21:35:28.231] iteration 2646 : loss: 0.036818, loss_a: 0.021658
[21:35:29.585] iteration 2647 : loss: 0.034666, loss_a: 0.020392
[21:35:30.341] iteration 2648 : loss: 0.032034, loss_a: 0.018844
[21:35:31.692] iteration 2649 : loss: 0.079412, loss_a: 0.046713
[21:35:32.437] iteration 2650 : loss: 0.070940, loss_a: 0.041730
[21:35:33.782] iteration 2651 : loss: 0.036201, loss_a: 0.021295
[21:35:34.520] iteration 2652 : loss: 0.020676, loss_a: 0.012162
[21:35:35.887] iteration 2653 : loss: 0.041708, loss_a: 0.024534
[21:35:36.623] iteration 2654 : loss: 0.041550, loss_a: 0.024441
[21:35:37.983] iteration 2655 : loss: 0.046111, loss_a: 0.027124
[21:35:38.722] iteration 2656 : loss: 0.030419, loss_a: 0.017893
[21:35:40.044] iteration 2657 : loss: 0.022532, loss_a: 0.013254
[21:35:40.793] iteration 2658 : loss: 0.038379, loss_a: 0.022576
[21:35:42.129] iteration 2659 : loss: 0.031485, loss_a: 0.018521
[21:35:42.870] iteration 2660 : loss: 0.047860, loss_a: 0.028153
[21:35:44.227] iteration 2661 : loss: 0.025282, loss_a: 0.014872
[21:35:44.970] iteration 2662 : loss: 0.022795, loss_a: 0.013409
[21:35:46.335] iteration 2663 : loss: 0.053218, loss_a: 0.031305
[21:35:47.076] iteration 2664 : loss: 0.044751, loss_a: 0.026324
[21:35:48.401] iteration 2665 : loss: 0.028004, loss_a: 0.016473
[21:35:49.149] iteration 2666 : loss: 0.030741, loss_a: 0.018083
[21:35:50.500] iteration 2667 : loss: 0.038017, loss_a: 0.022363
[21:35:51.246] iteration 2668 : loss: 0.033130, loss_a: 0.019488
[21:35:52.585] iteration 2669 : loss: 0.030614, loss_a: 0.018008
[21:35:53.329] iteration 2670 : loss: 0.046145, loss_a: 0.027144
[21:35:54.674] iteration 2671 : loss: 0.022307, loss_a: 0.013122
[21:35:55.410] iteration 2672 : loss: 0.027139, loss_a: 0.015964
[21:35:56.743] iteration 2673 : loss: 0.050664, loss_a: 0.029803
[21:35:57.476] iteration 2674 : loss: 0.018147, loss_a: 0.010675
[21:35:58.789] iteration 2675 : loss: 0.050517, loss_a: 0.029716
[21:35:59.526] iteration 2676 : loss: 0.080923, loss_a: 0.047602
[21:36:00.855] iteration 2677 : loss: 0.016020, loss_a: 0.009423
[21:36:01.597] iteration 2678 : loss: 0.064061, loss_a: 0.037683
[21:36:02.948] iteration 2679 : loss: 0.038424, loss_a: 0.022603
[21:36:03.691] iteration 2680 : loss: 0.061430, loss_a: 0.036136
[21:36:05.012] iteration 2681 : loss: 0.018213, loss_a: 0.010714
[21:36:05.754] iteration 2682 : loss: 0.115967, loss_a: 0.068216
[21:36:07.111] iteration 2683 : loss: 0.055382, loss_a: 0.032578
[21:36:07.855] iteration 2684 : loss: 0.040991, loss_a: 0.024112
[21:36:09.201] iteration 2685 : loss: 0.029252, loss_a: 0.017207
[21:36:09.936] iteration 2686 : loss: 0.022734, loss_a: 0.013373
[21:36:11.297] iteration 2687 : loss: 0.030366, loss_a: 0.017862
[21:36:12.048] iteration 2688 : loss: 0.052739, loss_a: 0.031023
[21:36:13.395] iteration 2689 : loss: 0.043998, loss_a: 0.025881
[21:36:14.127] iteration 2690 : loss: 0.024029, loss_a: 0.014135
[21:36:15.441] iteration 2691 : loss: 0.029886, loss_a: 0.017580
[21:36:16.187] iteration 2692 : loss: 0.044619, loss_a: 0.026246
[21:36:17.503] iteration 2693 : loss: 0.050237, loss_a: 0.029551
[21:36:18.251] iteration 2694 : loss: 0.033723, loss_a: 0.019837
[21:36:19.582] iteration 2695 : loss: 0.026589, loss_a: 0.015640
[21:36:20.321] iteration 2696 : loss: 0.073374, loss_a: 0.043161
[21:36:21.669] iteration 2697 : loss: 0.031281, loss_a: 0.018400
[21:36:22.405] iteration 2698 : loss: 0.017695, loss_a: 0.010409
[21:36:23.756] iteration 2699 : loss: 0.038168, loss_a: 0.022452
[21:36:24.501] iteration 2700 : loss: 0.041077, loss_a: 0.024163
[21:36:25.849] iteration 2701 : loss: 0.062573, loss_a: 0.036808
[21:36:26.591] iteration 2702 : loss: 0.063546, loss_a: 0.037380
[21:36:27.934] iteration 2703 : loss: 0.092161, loss_a: 0.054212
[21:36:28.671] iteration 2704 : loss: 0.019646, loss_a: 0.011557
[21:36:30.014] iteration 2705 : loss: 0.026602, loss_a: 0.015648
[21:36:30.762] iteration 2706 : loss: 0.045898, loss_a: 0.026999
[21:36:32.094] iteration 2707 : loss: 0.053308, loss_a: 0.031358
[21:36:32.841] iteration 2708 : loss: 0.023350, loss_a: 0.013736
[21:36:34.171] iteration 2709 : loss: 0.062061, loss_a: 0.036506
[21:36:34.908] iteration 2710 : loss: 0.027923, loss_a: 0.016426
[21:36:36.243] iteration 2711 : loss: 0.052342, loss_a: 0.030789
[21:36:36.984] iteration 2712 : loss: 0.035984, loss_a: 0.021167
[21:36:38.354] iteration 2713 : loss: 0.045821, loss_a: 0.026954
[21:36:39.091] iteration 2714 : loss: 0.023505, loss_a: 0.013827
[21:36:40.452] iteration 2715 : loss: 0.087354, loss_a: 0.051384
[21:36:41.189] iteration 2716 : loss: 0.017811, loss_a: 0.010477
[21:36:42.515] iteration 2717 : loss: 0.071291, loss_a: 0.041936
[21:36:43.259] iteration 2718 : loss: 0.043721, loss_a: 0.025718
[21:36:44.582] iteration 2719 : loss: 0.027175, loss_a: 0.015985
[21:36:45.328] iteration 2720 : loss: 0.052393, loss_a: 0.030820
[21:36:46.640] iteration 2721 : loss: 0.030521, loss_a: 0.017953
[21:36:47.379] iteration 2722 : loss: 0.028696, loss_a: 0.016880
[21:36:48.733] iteration 2723 : loss: 0.033566, loss_a: 0.019745
[21:36:49.474] iteration 2724 : loss: 0.038578, loss_a: 0.022693
[21:36:50.815] iteration 2725 : loss: 0.029665, loss_a: 0.017450
[21:36:51.550] iteration 2726 : loss: 0.063714, loss_a: 0.037479
[21:36:52.888] iteration 2727 : loss: 0.023518, loss_a: 0.013834
[21:36:53.631] iteration 2728 : loss: 0.058475, loss_a: 0.034397
[21:36:54.954] iteration 2729 : loss: 0.026301, loss_a: 0.015471
[21:36:55.693] iteration 2730 : loss: 0.030059, loss_a: 0.017682
[21:36:57.020] iteration 2731 : loss: 0.032215, loss_a: 0.018950
[21:36:57.767] iteration 2732 : loss: 0.031597, loss_a: 0.018587
[21:36:59.089] iteration 2733 : loss: 0.028184, loss_a: 0.016579
[21:36:59.837] iteration 2734 : loss: 0.023811, loss_a: 0.014007
[21:37:01.171] iteration 2735 : loss: 0.034379, loss_a: 0.020223
[21:37:01.908] iteration 2736 : loss: 0.038914, loss_a: 0.022891
[21:37:03.222] iteration 2737 : loss: 0.056299, loss_a: 0.033117
[21:37:03.961] iteration 2738 : loss: 0.030133, loss_a: 0.017725
[21:37:05.321] iteration 2739 : loss: 0.063457, loss_a: 0.037327
[21:37:06.062] iteration 2740 : loss: 0.058846, loss_a: 0.034615
[21:37:07.385] iteration 2741 : loss: 0.030604, loss_a: 0.018002
[21:37:08.127] iteration 2742 : loss: 0.059104, loss_a: 0.034767
[21:37:09.458] iteration 2743 : loss: 0.043985, loss_a: 0.025874
[21:37:10.196] iteration 2744 : loss: 0.051103, loss_a: 0.030060
[21:37:11.542] iteration 2745 : loss: 0.067148, loss_a: 0.039499
[21:37:12.290] iteration 2746 : loss: 0.023134, loss_a: 0.013608
[21:37:13.620] iteration 2747 : loss: 0.071221, loss_a: 0.041895
[21:37:14.371] iteration 2748 : loss: 0.057098, loss_a: 0.033587
[21:37:15.722] iteration 2749 : loss: 0.020537, loss_a: 0.012081
[21:37:16.467] iteration 2750 : loss: 0.022023, loss_a: 0.012955
[21:37:17.812] iteration 2751 : loss: 0.029297, loss_a: 0.017233
[21:37:18.558] iteration 2752 : loss: 0.021786, loss_a: 0.012815
[21:37:19.894] iteration 2753 : loss: 0.086757, loss_a: 0.051034
[21:37:20.643] iteration 2754 : loss: 0.029486, loss_a: 0.017344
[21:37:21.953] iteration 2755 : loss: 0.024561, loss_a: 0.014447
[21:37:22.702] iteration 2756 : loss: 0.033775, loss_a: 0.019868
[21:37:24.023] iteration 2757 : loss: 0.057965, loss_a: 0.034097
[21:37:24.773] iteration 2758 : loss: 0.039375, loss_a: 0.023161
[21:37:26.096] iteration 2759 : loss: 0.049189, loss_a: 0.028935
[21:37:26.842] iteration 2760 : loss: 0.049764, loss_a: 0.029273
[21:37:28.173] iteration 2761 : loss: 0.018500, loss_a: 0.010882
[21:37:28.914] iteration 2762 : loss: 0.024250, loss_a: 0.014264
[21:37:30.236] iteration 2763 : loss: 0.032785, loss_a: 0.019285
[21:37:30.976] iteration 2764 : loss: 0.030792, loss_a: 0.018113
[21:37:32.290] iteration 2765 : loss: 0.039895, loss_a: 0.023468
[21:37:33.032] iteration 2766 : loss: 0.058619, loss_a: 0.034482
[21:37:34.377] iteration 2767 : loss: 0.031313, loss_a: 0.018419
[21:37:35.113] iteration 2768 : loss: 0.033947, loss_a: 0.019969
[21:37:36.436] iteration 2769 : loss: 0.024154, loss_a: 0.014208
[21:37:37.162] iteration 2770 : loss: 0.010873, loss_a: 0.006396
[21:37:38.506] iteration 2771 : loss: 0.055644, loss_a: 0.032732
[21:37:39.239] iteration 2772 : loss: 0.030092, loss_a: 0.017701
[21:37:40.571] iteration 2773 : loss: 0.018914, loss_a: 0.011126
[21:37:41.306] iteration 2774 : loss: 0.022801, loss_a: 0.013413
[21:37:42.679] iteration 2775 : loss: 0.053566, loss_a: 0.031509
[21:37:43.439] iteration 2776 : loss: 0.044950, loss_a: 0.026441
[21:37:44.762] iteration 2777 : loss: 0.049507, loss_a: 0.029122
[21:37:45.499] iteration 2778 : loss: 0.021599, loss_a: 0.012706
[21:37:46.814] iteration 2779 : loss: 0.038957, loss_a: 0.022916
[21:37:47.565] iteration 2780 : loss: 0.036470, loss_a: 0.021453
[21:37:48.924] iteration 2781 : loss: 0.043142, loss_a: 0.025377
[21:37:49.663] iteration 2782 : loss: 0.029921, loss_a: 0.017601
[21:37:50.973] iteration 2783 : loss: 0.043398, loss_a: 0.025528
[21:37:51.710] iteration 2784 : loss: 0.082879, loss_a: 0.048753
[21:37:53.030] iteration 2785 : loss: 0.048891, loss_a: 0.028760
[21:37:53.793] iteration 2786 : loss: 0.045321, loss_a: 0.026659
[21:37:55.141] iteration 2787 : loss: 0.032527, loss_a: 0.019133
[21:37:55.892] iteration 2788 : loss: 0.036381, loss_a: 0.021400
[21:37:57.230] iteration 2789 : loss: 0.026007, loss_a: 0.015298
[21:37:57.963] iteration 2790 : loss: 0.023538, loss_a: 0.013846
[21:37:59.302] iteration 2791 : loss: 0.021226, loss_a: 0.012486
[21:38:00.036] iteration 2792 : loss: 0.027078, loss_a: 0.015928
[21:38:01.383] iteration 2793 : loss: 0.027966, loss_a: 0.016451
[21:38:02.120] iteration 2794 : loss: 0.030245, loss_a: 0.017791
[21:38:03.439] iteration 2795 : loss: 0.065064, loss_a: 0.038273
[21:38:04.171] iteration 2796 : loss: 0.017550, loss_a: 0.010323
[21:38:05.530] iteration 2797 : loss: 0.031893, loss_a: 0.018761
[21:38:06.262] iteration 2798 : loss: 0.025142, loss_a: 0.014790
[21:38:07.612] iteration 2799 : loss: 0.047810, loss_a: 0.028123
[21:38:08.357] iteration 2800 : loss: 0.053879, loss_a: 0.031694
[21:38:31.951] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_2800_dice_0.8888.pth
[21:38:33.301] iteration 2801 : loss: 0.051572, loss_a: 0.030336
[21:38:35.603] iteration 2802 : loss: 0.023990, loss_a: 0.014112
[21:38:36.928] iteration 2803 : loss: 0.032827, loss_a: 0.019310
[21:38:37.667] iteration 2804 : loss: 0.043973, loss_a: 0.025867
[21:38:39.044] iteration 2805 : loss: 0.202451, loss_a: 0.119089
[21:38:39.801] iteration 2806 : loss: 0.071752, loss_a: 0.042207
[21:38:41.108] iteration 2807 : loss: 0.028833, loss_a: 0.016961
[21:38:41.850] iteration 2808 : loss: 0.037011, loss_a: 0.021771
[21:38:43.181] iteration 2809 : loss: 0.016847, loss_a: 0.009910
[21:38:43.918] iteration 2810 : loss: 0.031562, loss_a: 0.018566
[21:38:45.257] iteration 2811 : loss: 0.051919, loss_a: 0.030540
[21:38:45.997] iteration 2812 : loss: 0.026054, loss_a: 0.015326
[21:38:47.333] iteration 2813 : loss: 0.036242, loss_a: 0.021319
[21:38:48.088] iteration 2814 : loss: 0.037266, loss_a: 0.021921
[21:38:49.419] iteration 2815 : loss: 0.027735, loss_a: 0.016314
[21:38:50.172] iteration 2816 : loss: 0.070846, loss_a: 0.041674
[21:38:51.496] iteration 2817 : loss: 0.024518, loss_a: 0.014422
[21:38:52.233] iteration 2818 : loss: 0.055466, loss_a: 0.032627
[21:38:53.553] iteration 2819 : loss: 0.072819, loss_a: 0.042835
[21:38:54.284] iteration 2820 : loss: 0.045268, loss_a: 0.026628
[21:38:55.619] iteration 2821 : loss: 0.044167, loss_a: 0.025980
[21:38:56.358] iteration 2822 : loss: 0.050154, loss_a: 0.029503
[21:38:57.764] iteration 2823 : loss: 0.081299, loss_a: 0.047823
[21:38:58.514] iteration 2824 : loss: 0.043824, loss_a: 0.025779
[21:38:59.874] iteration 2825 : loss: 0.035342, loss_a: 0.020790
[21:39:00.603] iteration 2826 : loss: 0.031957, loss_a: 0.018798
[21:39:01.953] iteration 2827 : loss: 0.032677, loss_a: 0.019222
[21:39:02.706] iteration 2828 : loss: 0.035165, loss_a: 0.020685
[21:39:04.051] iteration 2829 : loss: 0.018364, loss_a: 0.010803
[21:39:04.802] iteration 2830 : loss: 0.030485, loss_a: 0.017932
[21:39:06.128] iteration 2831 : loss: 0.030608, loss_a: 0.018005
[21:39:06.878] iteration 2832 : loss: 0.069945, loss_a: 0.041144
[21:39:08.188] iteration 2833 : loss: 0.032028, loss_a: 0.018840
[21:39:08.924] iteration 2834 : loss: 0.031071, loss_a: 0.018277
[21:39:10.253] iteration 2835 : loss: 0.037098, loss_a: 0.021822
[21:39:11.004] iteration 2836 : loss: 0.051255, loss_a: 0.030150
[21:39:12.351] iteration 2837 : loss: 0.038617, loss_a: 0.022716
[21:39:13.084] iteration 2838 : loss: 0.060957, loss_a: 0.035857
[21:39:14.408] iteration 2839 : loss: 0.029214, loss_a: 0.017185
[21:39:15.146] iteration 2840 : loss: 0.022427, loss_a: 0.013192
[21:39:16.510] iteration 2841 : loss: 0.021078, loss_a: 0.012399
[21:39:17.253] iteration 2842 : loss: 0.028369, loss_a: 0.016688
[21:39:18.573] iteration 2843 : loss: 0.040062, loss_a: 0.023566
[21:39:19.306] iteration 2844 : loss: 0.019985, loss_a: 0.011756
[21:39:20.623] iteration 2845 : loss: 0.054927, loss_a: 0.032310
[21:39:21.361] iteration 2846 : loss: 0.040143, loss_a: 0.023613
[21:39:22.730] iteration 2847 : loss: 0.047849, loss_a: 0.028147
[21:39:23.479] iteration 2848 : loss: 0.021229, loss_a: 0.012488
[21:39:24.832] iteration 2849 : loss: 0.036092, loss_a: 0.021230
[21:39:25.573] iteration 2850 : loss: 0.026737, loss_a: 0.015728
[21:39:26.888] iteration 2851 : loss: 0.037231, loss_a: 0.021900
[21:39:27.621] iteration 2852 : loss: 0.045507, loss_a: 0.026769
[21:39:28.975] iteration 2853 : loss: 0.023082, loss_a: 0.013577
[21:39:29.721] iteration 2854 : loss: 0.027331, loss_a: 0.016077
[21:39:31.056] iteration 2855 : loss: 0.029979, loss_a: 0.017635
[21:39:31.798] iteration 2856 : loss: 0.026857, loss_a: 0.015798
[21:39:33.145] iteration 2857 : loss: 0.034471, loss_a: 0.020277
[21:39:33.897] iteration 2858 : loss: 0.036651, loss_a: 0.021559
[21:39:35.219] iteration 2859 : loss: 0.032834, loss_a: 0.019314
[21:39:35.960] iteration 2860 : loss: 0.046755, loss_a: 0.027503
[21:39:37.272] iteration 2861 : loss: 0.023611, loss_a: 0.013889
[21:39:38.013] iteration 2862 : loss: 0.058280, loss_a: 0.034283
[21:39:39.368] iteration 2863 : loss: 0.027659, loss_a: 0.016270
[21:39:40.113] iteration 2864 : loss: 0.034394, loss_a: 0.020232
[21:39:41.481] iteration 2865 : loss: 0.039473, loss_a: 0.023220
[21:39:42.222] iteration 2866 : loss: 0.031094, loss_a: 0.018291
[21:39:43.551] iteration 2867 : loss: 0.020850, loss_a: 0.012265
[21:39:44.299] iteration 2868 : loss: 0.042296, loss_a: 0.024880
[21:39:45.656] iteration 2869 : loss: 0.045473, loss_a: 0.026749
[21:39:46.387] iteration 2870 : loss: 0.023222, loss_a: 0.013660
[21:39:47.748] iteration 2871 : loss: 0.051615, loss_a: 0.030362
[21:39:48.492] iteration 2872 : loss: 0.022826, loss_a: 0.013427
[21:39:49.816] iteration 2873 : loss: 0.029636, loss_a: 0.017433
[21:39:50.565] iteration 2874 : loss: 0.084643, loss_a: 0.049790
[21:39:51.875] iteration 2875 : loss: 0.019582, loss_a: 0.011519
[21:39:52.619] iteration 2876 : loss: 0.035597, loss_a: 0.020940
[21:39:53.937] iteration 2877 : loss: 0.014981, loss_a: 0.008813
[21:39:54.684] iteration 2878 : loss: 0.065749, loss_a: 0.038676
[21:39:56.058] iteration 2879 : loss: 0.042986, loss_a: 0.025286
[21:39:56.800] iteration 2880 : loss: 0.065257, loss_a: 0.038386
[21:39:58.113] iteration 2881 : loss: 0.029048, loss_a: 0.017087
[21:39:58.847] iteration 2882 : loss: 0.032459, loss_a: 0.019094
[21:40:00.188] iteration 2883 : loss: 0.023906, loss_a: 0.014063
[21:40:00.922] iteration 2884 : loss: 0.085829, loss_a: 0.050488
[21:40:02.248] iteration 2885 : loss: 0.027723, loss_a: 0.016308
[21:40:02.995] iteration 2886 : loss: 0.039124, loss_a: 0.023014
[21:40:04.353] iteration 2887 : loss: 0.026914, loss_a: 0.015832
[21:40:05.096] iteration 2888 : loss: 0.042623, loss_a: 0.025072
[21:40:06.404] iteration 2889 : loss: 0.057353, loss_a: 0.033737
[21:40:07.136] iteration 2890 : loss: 0.032910, loss_a: 0.019359
[21:40:08.482] iteration 2891 : loss: 0.030029, loss_a: 0.017664
[21:40:09.219] iteration 2892 : loss: 0.022383, loss_a: 0.013167
[21:40:10.560] iteration 2893 : loss: 0.061763, loss_a: 0.036331
[21:40:11.304] iteration 2894 : loss: 0.028444, loss_a: 0.016732
[21:40:12.639] iteration 2895 : loss: 0.028684, loss_a: 0.016873
[21:40:13.376] iteration 2896 : loss: 0.024253, loss_a: 0.014266
[21:40:14.707] iteration 2897 : loss: 0.023965, loss_a: 0.014097
[21:40:15.446] iteration 2898 : loss: 0.049738, loss_a: 0.029258
[21:40:16.786] iteration 2899 : loss: 0.054705, loss_a: 0.032180
[21:40:17.524] iteration 2900 : loss: 0.030237, loss_a: 0.017786
[21:40:18.893] iteration 2901 : loss: 0.035029, loss_a: 0.020606
[21:40:19.638] iteration 2902 : loss: 0.034805, loss_a: 0.020473
[21:40:20.978] iteration 2903 : loss: 0.063006, loss_a: 0.037062
[21:40:21.722] iteration 2904 : loss: 0.023037, loss_a: 0.013551
[21:40:23.060] iteration 2905 : loss: 0.032459, loss_a: 0.019094
[21:40:23.804] iteration 2906 : loss: 0.050742, loss_a: 0.029848
[21:40:25.153] iteration 2907 : loss: 0.051363, loss_a: 0.030213
[21:40:25.901] iteration 2908 : loss: 0.023096, loss_a: 0.013586
[21:40:27.263] iteration 2909 : loss: 0.060372, loss_a: 0.035513
[21:40:28.003] iteration 2910 : loss: 0.037548, loss_a: 0.022087
[21:40:29.335] iteration 2911 : loss: 0.023224, loss_a: 0.013661
[21:40:30.076] iteration 2912 : loss: 0.029877, loss_a: 0.017575
[21:40:31.435] iteration 2913 : loss: 0.037325, loss_a: 0.021956
[21:40:32.179] iteration 2914 : loss: 0.050437, loss_a: 0.029669
[21:40:33.545] iteration 2915 : loss: 0.070330, loss_a: 0.041371
[21:40:34.296] iteration 2916 : loss: 0.045178, loss_a: 0.026576
[21:40:35.650] iteration 2917 : loss: 0.022288, loss_a: 0.013111
[21:40:36.386] iteration 2918 : loss: 0.022710, loss_a: 0.013359
[21:40:37.723] iteration 2919 : loss: 0.037341, loss_a: 0.021965
[21:40:38.465] iteration 2920 : loss: 0.026627, loss_a: 0.015663
[21:40:39.788] iteration 2921 : loss: 0.018722, loss_a: 0.011013
[21:40:40.529] iteration 2922 : loss: 0.034710, loss_a: 0.020418
[21:40:41.854] iteration 2923 : loss: 0.012488, loss_a: 0.007346
[21:40:42.592] iteration 2924 : loss: 0.061346, loss_a: 0.036086
[21:40:43.915] iteration 2925 : loss: 0.022311, loss_a: 0.013124
[21:40:44.656] iteration 2926 : loss: 0.037646, loss_a: 0.022145
[21:40:45.981] iteration 2927 : loss: 0.045026, loss_a: 0.026486
[21:40:46.721] iteration 2928 : loss: 0.024010, loss_a: 0.014123
[21:40:48.064] iteration 2929 : loss: 0.019005, loss_a: 0.011179
[21:40:48.802] iteration 2930 : loss: 0.035844, loss_a: 0.021085
[21:40:50.170] iteration 2931 : loss: 0.081679, loss_a: 0.048046
[21:40:50.918] iteration 2932 : loss: 0.055875, loss_a: 0.032868
[21:40:52.281] iteration 2933 : loss: 0.035178, loss_a: 0.020693
[21:40:53.019] iteration 2934 : loss: 0.020921, loss_a: 0.012307
[21:40:54.378] iteration 2935 : loss: 0.037882, loss_a: 0.022284
[21:40:55.124] iteration 2936 : loss: 0.021375, loss_a: 0.012574
[21:40:56.509] iteration 2937 : loss: 0.067673, loss_a: 0.039808
[21:40:57.249] iteration 2938 : loss: 0.039820, loss_a: 0.023423
[21:40:58.588] iteration 2939 : loss: 0.059999, loss_a: 0.035294
[21:40:59.321] iteration 2940 : loss: 0.027910, loss_a: 0.016418
[21:41:00.678] iteration 2941 : loss: 0.066872, loss_a: 0.039337
[21:41:01.415] iteration 2942 : loss: 0.055511, loss_a: 0.032654
[21:41:02.778] iteration 2943 : loss: 0.042346, loss_a: 0.024910
[21:41:03.526] iteration 2944 : loss: 0.074191, loss_a: 0.043642
[21:41:04.897] iteration 2945 : loss: 0.046613, loss_a: 0.027419
[21:41:05.630] iteration 2946 : loss: 0.062021, loss_a: 0.036483
[21:41:07.006] iteration 2947 : loss: 0.033507, loss_a: 0.019710
[21:41:07.758] iteration 2948 : loss: 0.079295, loss_a: 0.046644
[21:41:09.085] iteration 2949 : loss: 0.029351, loss_a: 0.017265
[21:41:09.825] iteration 2950 : loss: 0.031681, loss_a: 0.018636
[21:41:11.163] iteration 2951 : loss: 0.034886, loss_a: 0.020521
[21:41:11.916] iteration 2952 : loss: 0.038247, loss_a: 0.022498
[21:41:13.237] iteration 2953 : loss: 0.027994, loss_a: 0.016467
[21:41:13.990] iteration 2954 : loss: 0.048613, loss_a: 0.028596
[21:41:15.305] iteration 2955 : loss: 0.050962, loss_a: 0.029977
[21:41:16.049] iteration 2956 : loss: 0.043746, loss_a: 0.025733
[21:41:17.387] iteration 2957 : loss: 0.029622, loss_a: 0.017425
[21:41:18.124] iteration 2958 : loss: 0.025335, loss_a: 0.014903
[21:41:19.470] iteration 2959 : loss: 0.033504, loss_a: 0.019708
[21:41:20.216] iteration 2960 : loss: 0.028268, loss_a: 0.016628
[21:41:21.573] iteration 2961 : loss: 0.026900, loss_a: 0.015824
[21:41:22.308] iteration 2962 : loss: 0.049170, loss_a: 0.028924
[21:41:23.621] iteration 2963 : loss: 0.036200, loss_a: 0.021294
[21:41:24.361] iteration 2964 : loss: 0.027827, loss_a: 0.016369
[21:41:25.723] iteration 2965 : loss: 0.055194, loss_a: 0.032467
[21:41:26.463] iteration 2966 : loss: 0.041372, loss_a: 0.024336
[21:41:27.829] iteration 2967 : loss: 0.066048, loss_a: 0.038852
[21:41:28.567] iteration 2968 : loss: 0.042620, loss_a: 0.025071
[21:41:29.904] iteration 2969 : loss: 0.037949, loss_a: 0.022323
[21:41:30.647] iteration 2970 : loss: 0.039601, loss_a: 0.023295
[21:41:31.997] iteration 2971 : loss: 0.028145, loss_a: 0.016556
[21:41:32.740] iteration 2972 : loss: 0.051539, loss_a: 0.030317
[21:41:34.047] iteration 2973 : loss: 0.035205, loss_a: 0.020709
[21:41:34.788] iteration 2974 : loss: 0.048393, loss_a: 0.028467
[21:41:36.113] iteration 2975 : loss: 0.073497, loss_a: 0.043234
[21:41:36.858] iteration 2976 : loss: 0.048278, loss_a: 0.028399
[21:41:38.182] iteration 2977 : loss: 0.020014, loss_a: 0.011773
[21:41:38.915] iteration 2978 : loss: 0.027772, loss_a: 0.016336
[21:41:40.290] iteration 2979 : loss: 0.130490, loss_a: 0.076759
[21:41:41.019] iteration 2980 : loss: 0.023496, loss_a: 0.013821
[21:41:42.370] iteration 2981 : loss: 0.023316, loss_a: 0.013715
[21:41:43.111] iteration 2982 : loss: 0.059465, loss_a: 0.034979
[21:41:44.451] iteration 2983 : loss: 0.048669, loss_a: 0.028629
[21:41:45.193] iteration 2984 : loss: 0.033118, loss_a: 0.019481
[21:41:46.529] iteration 2985 : loss: 0.071146, loss_a: 0.041850
[21:41:47.271] iteration 2986 : loss: 0.020883, loss_a: 0.012284
[21:41:48.608] iteration 2987 : loss: 0.030831, loss_a: 0.018136
[21:41:49.352] iteration 2988 : loss: 0.030301, loss_a: 0.017824
[21:41:50.704] iteration 2989 : loss: 0.041887, loss_a: 0.024639
[21:41:51.437] iteration 2990 : loss: 0.026450, loss_a: 0.015559
[21:41:52.754] iteration 2991 : loss: 0.016281, loss_a: 0.009577
[21:41:53.485] iteration 2992 : loss: 0.026372, loss_a: 0.015513
[21:41:54.811] iteration 2993 : loss: 0.036146, loss_a: 0.021262
[21:41:55.557] iteration 2994 : loss: 0.040035, loss_a: 0.023550
[21:41:56.865] iteration 2995 : loss: 0.016171, loss_a: 0.009512
[21:41:57.596] iteration 2996 : loss: 0.023933, loss_a: 0.014078
[21:41:58.911] iteration 2997 : loss: 0.030444, loss_a: 0.017908
[21:41:59.647] iteration 2998 : loss: 0.031334, loss_a: 0.018432
[21:42:00.991] iteration 2999 : loss: 0.032016, loss_a: 0.018833
[21:42:01.735] iteration 3000 : loss: 0.019953, loss_a: 0.011737
[21:42:26.388] iteration 3001 : loss: 0.055656, loss_a: 0.032739
[21:42:28.585] iteration 3002 : loss: 0.024214, loss_a: 0.014243
[21:42:29.946] iteration 3003 : loss: 0.032204, loss_a: 0.018944
[21:42:30.680] iteration 3004 : loss: 0.048251, loss_a: 0.028383
[21:42:31.985] iteration 3005 : loss: 0.027371, loss_a: 0.016101
[21:42:32.729] iteration 3006 : loss: 0.022427, loss_a: 0.013192
[21:42:34.054] iteration 3007 : loss: 0.048307, loss_a: 0.028416
[21:42:34.797] iteration 3008 : loss: 0.026646, loss_a: 0.015674
[21:42:36.146] iteration 3009 : loss: 0.037967, loss_a: 0.022334
[21:42:36.895] iteration 3010 : loss: 0.043669, loss_a: 0.025687
[21:42:38.210] iteration 3011 : loss: 0.024991, loss_a: 0.014701
[21:42:38.955] iteration 3012 : loss: 0.045735, loss_a: 0.026903
[21:42:40.304] iteration 3013 : loss: 0.057436, loss_a: 0.033786
[21:42:41.061] iteration 3014 : loss: 0.039860, loss_a: 0.023447
[21:42:42.368] iteration 3015 : loss: 0.026418, loss_a: 0.015540
[21:42:43.112] iteration 3016 : loss: 0.048163, loss_a: 0.028331
[21:42:44.425] iteration 3017 : loss: 0.055263, loss_a: 0.032508
[21:42:45.155] iteration 3018 : loss: 0.026225, loss_a: 0.015426
[21:42:46.496] iteration 3019 : loss: 0.023868, loss_a: 0.014040
[21:42:47.241] iteration 3020 : loss: 0.061370, loss_a: 0.036100
[21:42:48.578] iteration 3021 : loss: 0.031060, loss_a: 0.018270
[21:42:49.331] iteration 3022 : loss: 0.049963, loss_a: 0.029390
[21:42:50.689] iteration 3023 : loss: 0.044716, loss_a: 0.026304
[21:42:51.423] iteration 3024 : loss: 0.028873, loss_a: 0.016984
[21:42:52.738] iteration 3025 : loss: 0.025556, loss_a: 0.015033
[21:42:53.493] iteration 3026 : loss: 0.060391, loss_a: 0.035524
[21:42:54.835] iteration 3027 : loss: 0.066383, loss_a: 0.039049
[21:42:55.575] iteration 3028 : loss: 0.026443, loss_a: 0.015555
[21:42:56.941] iteration 3029 : loss: 0.026230, loss_a: 0.015430
[21:42:57.676] iteration 3030 : loss: 0.035406, loss_a: 0.020827
[21:42:58.999] iteration 3031 : loss: 0.031271, loss_a: 0.018395
[21:42:59.739] iteration 3032 : loss: 0.044850, loss_a: 0.026382
[21:43:01.106] iteration 3033 : loss: 0.027887, loss_a: 0.016404
[21:43:01.848] iteration 3034 : loss: 0.071896, loss_a: 0.042292
[21:43:03.207] iteration 3035 : loss: 0.023792, loss_a: 0.013995
[21:43:03.953] iteration 3036 : loss: 0.068745, loss_a: 0.040439
[21:43:05.301] iteration 3037 : loss: 0.023524, loss_a: 0.013837
[21:43:06.057] iteration 3038 : loss: 0.081749, loss_a: 0.048087
[21:43:07.390] iteration 3039 : loss: 0.029857, loss_a: 0.017563
[21:43:08.144] iteration 3040 : loss: 0.039122, loss_a: 0.023013
[21:43:09.473] iteration 3041 : loss: 0.027872, loss_a: 0.016395
[21:43:10.204] iteration 3042 : loss: 0.016817, loss_a: 0.009892
[21:43:11.544] iteration 3043 : loss: 0.022747, loss_a: 0.013380
[21:43:12.283] iteration 3044 : loss: 0.026268, loss_a: 0.015452
[21:43:13.631] iteration 3045 : loss: 0.035065, loss_a: 0.020626
[21:43:14.379] iteration 3046 : loss: 0.028946, loss_a: 0.017027
[21:43:15.714] iteration 3047 : loss: 0.058616, loss_a: 0.034480
[21:43:16.456] iteration 3048 : loss: 0.026364, loss_a: 0.015508
[21:43:17.792] iteration 3049 : loss: 0.043075, loss_a: 0.025338
[21:43:18.536] iteration 3050 : loss: 0.027711, loss_a: 0.016301
[21:43:19.870] iteration 3051 : loss: 0.029449, loss_a: 0.017323
[21:43:20.613] iteration 3052 : loss: 0.046890, loss_a: 0.027582
[21:43:21.936] iteration 3053 : loss: 0.023578, loss_a: 0.013869
[21:43:22.672] iteration 3054 : loss: 0.019494, loss_a: 0.011467
[21:43:23.981] iteration 3055 : loss: 0.024022, loss_a: 0.014131
[21:43:24.724] iteration 3056 : loss: 0.027472, loss_a: 0.016160
[21:43:26.045] iteration 3057 : loss: 0.028584, loss_a: 0.016814
[21:43:26.783] iteration 3058 : loss: 0.032538, loss_a: 0.019140
[21:43:28.099] iteration 3059 : loss: 0.010988, loss_a: 0.006463
[21:43:28.839] iteration 3060 : loss: 0.027446, loss_a: 0.016145
[21:43:30.172] iteration 3061 : loss: 0.043727, loss_a: 0.025722
[21:43:30.921] iteration 3062 : loss: 0.050245, loss_a: 0.029556
[21:43:32.268] iteration 3063 : loss: 0.027964, loss_a: 0.016450
[21:43:33.008] iteration 3064 : loss: 0.024323, loss_a: 0.014308
[21:43:34.321] iteration 3065 : loss: 0.015610, loss_a: 0.009182
[21:43:35.069] iteration 3066 : loss: 0.036322, loss_a: 0.021366
[21:43:36.393] iteration 3067 : loss: 0.044447, loss_a: 0.026145
[21:43:37.133] iteration 3068 : loss: 0.026763, loss_a: 0.015743
[21:43:38.483] iteration 3069 : loss: 0.032499, loss_a: 0.019117
[21:43:39.223] iteration 3070 : loss: 0.045522, loss_a: 0.026778
[21:43:40.570] iteration 3071 : loss: 0.048892, loss_a: 0.028760
[21:43:41.311] iteration 3072 : loss: 0.033643, loss_a: 0.019790
[21:43:42.678] iteration 3073 : loss: 0.050494, loss_a: 0.029702
[21:43:43.414] iteration 3074 : loss: 0.038485, loss_a: 0.022638
[21:43:44.760] iteration 3075 : loss: 0.034636, loss_a: 0.020374
[21:43:45.512] iteration 3076 : loss: 0.041046, loss_a: 0.024145
[21:43:46.828] iteration 3077 : loss: 0.039844, loss_a: 0.023437
[21:43:47.570] iteration 3078 : loss: 0.020231, loss_a: 0.011901
[21:43:48.897] iteration 3079 : loss: 0.075588, loss_a: 0.044463
[21:43:49.646] iteration 3080 : loss: 0.031910, loss_a: 0.018771
[21:43:50.959] iteration 3081 : loss: 0.019416, loss_a: 0.011421
[21:43:51.714] iteration 3082 : loss: 0.031724, loss_a: 0.018661
[21:43:53.055] iteration 3083 : loss: 0.017482, loss_a: 0.010283
[21:43:53.806] iteration 3084 : loss: 0.023629, loss_a: 0.013899
[21:43:55.139] iteration 3085 : loss: 0.038962, loss_a: 0.022919
[21:43:55.868] iteration 3086 : loss: 0.042946, loss_a: 0.025262
[21:43:57.193] iteration 3087 : loss: 0.025154, loss_a: 0.014797
[21:43:57.943] iteration 3088 : loss: 0.034099, loss_a: 0.020058
[21:43:59.281] iteration 3089 : loss: 0.023079, loss_a: 0.013576
[21:44:00.030] iteration 3090 : loss: 0.038138, loss_a: 0.022434
[21:44:01.394] iteration 3091 : loss: 0.033932, loss_a: 0.019960
[21:44:02.139] iteration 3092 : loss: 0.028598, loss_a: 0.016822
[21:44:03.499] iteration 3093 : loss: 0.027571, loss_a: 0.016218
[21:44:04.253] iteration 3094 : loss: 0.118476, loss_a: 0.069692
[21:44:05.599] iteration 3095 : loss: 0.060667, loss_a: 0.035686
[21:44:06.340] iteration 3096 : loss: 0.034888, loss_a: 0.020522
[21:44:07.696] iteration 3097 : loss: 0.026210, loss_a: 0.015418
[21:44:08.432] iteration 3098 : loss: 0.017580, loss_a: 0.010341
[21:44:09.756] iteration 3099 : loss: 0.023593, loss_a: 0.013878
[21:44:10.495] iteration 3100 : loss: 0.034709, loss_a: 0.020417
[21:44:11.810] iteration 3101 : loss: 0.028078, loss_a: 0.016517
[21:44:12.554] iteration 3102 : loss: 0.020621, loss_a: 0.012130
[21:44:13.905] iteration 3103 : loss: 0.025556, loss_a: 0.015033
[21:44:14.690] iteration 3104 : loss: 0.039251, loss_a: 0.023089
[21:44:16.051] iteration 3105 : loss: 0.061785, loss_a: 0.036344
[21:44:16.795] iteration 3106 : loss: 0.029634, loss_a: 0.017431
[21:44:18.151] iteration 3107 : loss: 0.050819, loss_a: 0.029893
[21:44:18.901] iteration 3108 : loss: 0.023767, loss_a: 0.013980
[21:44:20.237] iteration 3109 : loss: 0.030029, loss_a: 0.017664
[21:44:20.989] iteration 3110 : loss: 0.066322, loss_a: 0.039013
[21:44:22.340] iteration 3111 : loss: 0.021926, loss_a: 0.012898
[21:44:23.091] iteration 3112 : loss: 0.030709, loss_a: 0.018064
[21:44:24.429] iteration 3113 : loss: 0.031452, loss_a: 0.018501
[21:44:25.168] iteration 3114 : loss: 0.029909, loss_a: 0.017594
[21:44:26.511] iteration 3115 : loss: 0.019222, loss_a: 0.011307
[21:44:27.254] iteration 3116 : loss: 0.027703, loss_a: 0.016296
[21:44:28.575] iteration 3117 : loss: 0.047921, loss_a: 0.028189
[21:44:29.309] iteration 3118 : loss: 0.019870, loss_a: 0.011688
[21:44:30.670] iteration 3119 : loss: 0.032257, loss_a: 0.018975
[21:44:31.417] iteration 3120 : loss: 0.022050, loss_a: 0.012971
[21:44:32.762] iteration 3121 : loss: 0.045484, loss_a: 0.026755
[21:44:33.504] iteration 3122 : loss: 0.036290, loss_a: 0.021347
[21:44:34.840] iteration 3123 : loss: 0.022601, loss_a: 0.013295
[21:44:35.592] iteration 3124 : loss: 0.060643, loss_a: 0.035672
[21:44:36.926] iteration 3125 : loss: 0.043325, loss_a: 0.025485
[21:44:37.665] iteration 3126 : loss: 0.047397, loss_a: 0.027881
[21:44:39.024] iteration 3127 : loss: 0.047024, loss_a: 0.027661
[21:44:39.771] iteration 3128 : loss: 0.039326, loss_a: 0.023133
[21:44:41.110] iteration 3129 : loss: 0.029112, loss_a: 0.017125
[21:44:41.860] iteration 3130 : loss: 0.022055, loss_a: 0.012974
[21:44:43.181] iteration 3131 : loss: 0.029876, loss_a: 0.017574
[21:44:43.931] iteration 3132 : loss: 0.064130, loss_a: 0.037724
[21:44:45.244] iteration 3133 : loss: 0.028888, loss_a: 0.016993
[21:44:45.992] iteration 3134 : loss: 0.054252, loss_a: 0.031913
[21:44:47.325] iteration 3135 : loss: 0.031726, loss_a: 0.018662
[21:44:48.058] iteration 3136 : loss: 0.044754, loss_a: 0.026326
[21:44:49.399] iteration 3137 : loss: 0.085117, loss_a: 0.050069
[21:44:50.141] iteration 3138 : loss: 0.036523, loss_a: 0.021484
[21:44:51.519] iteration 3139 : loss: 0.029251, loss_a: 0.017206
[21:44:52.267] iteration 3140 : loss: 0.049113, loss_a: 0.028890
[21:44:53.610] iteration 3141 : loss: 0.019504, loss_a: 0.011473
[21:44:54.355] iteration 3142 : loss: 0.032672, loss_a: 0.019219
[21:44:55.731] iteration 3143 : loss: 0.042891, loss_a: 0.025230
[21:44:56.471] iteration 3144 : loss: 0.054011, loss_a: 0.031771
[21:44:57.812] iteration 3145 : loss: 0.022573, loss_a: 0.013278
[21:44:58.544] iteration 3146 : loss: 0.020395, loss_a: 0.011997
[21:44:59.886] iteration 3147 : loss: 0.034478, loss_a: 0.020281
[21:45:00.635] iteration 3148 : loss: 0.054970, loss_a: 0.032335
[21:45:01.974] iteration 3149 : loss: 0.034816, loss_a: 0.020480
[21:45:02.717] iteration 3150 : loss: 0.051154, loss_a: 0.030091
[21:45:04.042] iteration 3151 : loss: 0.024782, loss_a: 0.014577
[21:45:04.790] iteration 3152 : loss: 0.035873, loss_a: 0.021102
[21:45:06.135] iteration 3153 : loss: 0.031795, loss_a: 0.018703
[21:45:06.876] iteration 3154 : loss: 0.018106, loss_a: 0.010650
[21:45:08.246] iteration 3155 : loss: 0.046063, loss_a: 0.027096
[21:45:08.988] iteration 3156 : loss: 0.028546, loss_a: 0.016792
[21:45:10.307] iteration 3157 : loss: 0.060185, loss_a: 0.035403
[21:45:11.040] iteration 3158 : loss: 0.025583, loss_a: 0.015049
[21:45:12.370] iteration 3159 : loss: 0.041208, loss_a: 0.024240
[21:45:13.113] iteration 3160 : loss: 0.021757, loss_a: 0.012798
[21:45:14.468] iteration 3161 : loss: 0.065401, loss_a: 0.038471
[21:45:15.201] iteration 3162 : loss: 0.016864, loss_a: 0.009920
[21:45:16.556] iteration 3163 : loss: 0.021168, loss_a: 0.012452
[21:45:17.298] iteration 3164 : loss: 0.037959, loss_a: 0.022329
[21:45:18.641] iteration 3165 : loss: 0.035560, loss_a: 0.020918
[21:45:19.395] iteration 3166 : loss: 0.066755, loss_a: 0.039267
[21:45:20.758] iteration 3167 : loss: 0.029865, loss_a: 0.017568
[21:45:21.508] iteration 3168 : loss: 0.056862, loss_a: 0.033448
[21:45:22.863] iteration 3169 : loss: 0.041818, loss_a: 0.024599
[21:45:23.595] iteration 3170 : loss: 0.038272, loss_a: 0.022513
[21:45:24.947] iteration 3171 : loss: 0.031668, loss_a: 0.018628
[21:45:25.683] iteration 3172 : loss: 0.028161, loss_a: 0.016565
[21:45:27.036] iteration 3173 : loss: 0.047269, loss_a: 0.027805
[21:45:27.776] iteration 3174 : loss: 0.029449, loss_a: 0.017323
[21:45:29.089] iteration 3175 : loss: 0.024555, loss_a: 0.014444
[21:45:29.823] iteration 3176 : loss: 0.046410, loss_a: 0.027300
[21:45:31.153] iteration 3177 : loss: 0.039411, loss_a: 0.023183
[21:45:31.898] iteration 3178 : loss: 0.042728, loss_a: 0.025134
[21:45:33.238] iteration 3179 : loss: 0.063955, loss_a: 0.037620
[21:45:33.978] iteration 3180 : loss: 0.025811, loss_a: 0.015183
[21:45:35.299] iteration 3181 : loss: 0.022048, loss_a: 0.012969
[21:45:36.044] iteration 3182 : loss: 0.022939, loss_a: 0.013494
[21:45:37.380] iteration 3183 : loss: 0.045136, loss_a: 0.026551
[21:45:38.131] iteration 3184 : loss: 0.055967, loss_a: 0.032922
[21:45:39.449] iteration 3185 : loss: 0.026562, loss_a: 0.015625
[21:45:40.185] iteration 3186 : loss: 0.029611, loss_a: 0.017418
[21:45:41.505] iteration 3187 : loss: 0.058439, loss_a: 0.034376
[21:45:42.238] iteration 3188 : loss: 0.031273, loss_a: 0.018396
[21:45:43.578] iteration 3189 : loss: 0.026232, loss_a: 0.015431
[21:45:44.326] iteration 3190 : loss: 0.063225, loss_a: 0.037191
[21:45:45.667] iteration 3191 : loss: 0.039347, loss_a: 0.023145
[21:45:46.401] iteration 3192 : loss: 0.035731, loss_a: 0.021018
[21:45:47.770] iteration 3193 : loss: 0.103065, loss_a: 0.060627
[21:45:48.512] iteration 3194 : loss: 0.052327, loss_a: 0.030781
[21:45:49.829] iteration 3195 : loss: 0.031145, loss_a: 0.018321
[21:45:50.573] iteration 3196 : loss: 0.044608, loss_a: 0.026240
[21:45:51.922] iteration 3197 : loss: 0.031490, loss_a: 0.018524
[21:45:52.671] iteration 3198 : loss: 0.050317, loss_a: 0.029598
[21:45:54.016] iteration 3199 : loss: 0.085129, loss_a: 0.050076
[21:45:54.752] iteration 3200 : loss: 0.035206, loss_a: 0.020709
[21:46:18.307] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_3200_dice_0.8926.pth
[21:46:19.652] iteration 3201 : loss: 0.017067, loss_a: 0.010039
[21:46:21.922] iteration 3202 : loss: 0.034014, loss_a: 0.020008
[21:46:23.256] iteration 3203 : loss: 0.054183, loss_a: 0.031872
[21:46:23.993] iteration 3204 : loss: 0.030111, loss_a: 0.017712
[21:46:25.325] iteration 3205 : loss: 0.100343, loss_a: 0.059025
[21:46:26.069] iteration 3206 : loss: 0.036174, loss_a: 0.021279
[21:46:27.408] iteration 3207 : loss: 0.028574, loss_a: 0.016808
[21:46:28.144] iteration 3208 : loss: 0.021654, loss_a: 0.012738
[21:46:29.461] iteration 3209 : loss: 0.029436, loss_a: 0.017315
[21:46:30.212] iteration 3210 : loss: 0.039650, loss_a: 0.023324
[21:46:31.575] iteration 3211 : loss: 0.040675, loss_a: 0.023926
[21:46:32.323] iteration 3212 : loss: 0.043895, loss_a: 0.025820
[21:46:33.684] iteration 3213 : loss: 0.024408, loss_a: 0.014358
[21:46:34.439] iteration 3214 : loss: 0.033704, loss_a: 0.019826
[21:46:35.755] iteration 3215 : loss: 0.037224, loss_a: 0.021897
[21:46:36.502] iteration 3216 : loss: 0.040107, loss_a: 0.023592
[21:46:37.819] iteration 3217 : loss: 0.059379, loss_a: 0.034929
[21:46:38.564] iteration 3218 : loss: 0.017060, loss_a: 0.010035
[21:46:39.920] iteration 3219 : loss: 0.050148, loss_a: 0.029499
[21:46:40.667] iteration 3220 : loss: 0.034199, loss_a: 0.020117
[21:46:42.002] iteration 3221 : loss: 0.043883, loss_a: 0.025814
[21:46:42.746] iteration 3222 : loss: 0.028448, loss_a: 0.016734
[21:46:44.091] iteration 3223 : loss: 0.016459, loss_a: 0.009682
[21:46:44.836] iteration 3224 : loss: 0.022190, loss_a: 0.013053
[21:46:46.197] iteration 3225 : loss: 0.056772, loss_a: 0.033395
[21:46:46.952] iteration 3226 : loss: 0.025203, loss_a: 0.014825
[21:46:48.281] iteration 3227 : loss: 0.043560, loss_a: 0.025624
[21:46:49.027] iteration 3228 : loss: 0.029021, loss_a: 0.017071
[21:46:50.331] iteration 3229 : loss: 0.014386, loss_a: 0.008462
[21:46:51.070] iteration 3230 : loss: 0.036716, loss_a: 0.021598
[21:46:52.403] iteration 3231 : loss: 0.072223, loss_a: 0.042484
[21:46:53.141] iteration 3232 : loss: 0.027057, loss_a: 0.015916
[21:46:54.461] iteration 3233 : loss: 0.034519, loss_a: 0.020305
[21:46:55.195] iteration 3234 : loss: 0.023066, loss_a: 0.013568
[21:46:56.515] iteration 3235 : loss: 0.031923, loss_a: 0.018778
[21:46:57.260] iteration 3236 : loss: 0.031555, loss_a: 0.018562
[21:46:58.587] iteration 3237 : loss: 0.023511, loss_a: 0.013830
[21:46:59.314] iteration 3238 : loss: 0.025026, loss_a: 0.014721
[21:47:00.644] iteration 3239 : loss: 0.018254, loss_a: 0.010737
[21:47:01.383] iteration 3240 : loss: 0.022488, loss_a: 0.013228
[21:47:02.751] iteration 3241 : loss: 0.044730, loss_a: 0.026312
[21:47:03.487] iteration 3242 : loss: 0.019217, loss_a: 0.011304
[21:47:04.806] iteration 3243 : loss: 0.027584, loss_a: 0.016226
[21:47:05.547] iteration 3244 : loss: 0.020988, loss_a: 0.012346
[21:47:06.872] iteration 3245 : loss: 0.019012, loss_a: 0.011184
[21:47:07.622] iteration 3246 : loss: 0.027649, loss_a: 0.016264
[21:47:08.970] iteration 3247 : loss: 0.031277, loss_a: 0.018398
[21:47:09.720] iteration 3248 : loss: 0.037433, loss_a: 0.022019
[21:47:11.047] iteration 3249 : loss: 0.042696, loss_a: 0.025116
[21:47:11.786] iteration 3250 : loss: 0.021973, loss_a: 0.012925
[21:47:13.108] iteration 3251 : loss: 0.050732, loss_a: 0.029842
[21:47:13.855] iteration 3252 : loss: 0.041197, loss_a: 0.024234
[21:47:15.227] iteration 3253 : loss: 0.048740, loss_a: 0.028671
[21:47:15.959] iteration 3254 : loss: 0.023239, loss_a: 0.013670
[21:47:17.279] iteration 3255 : loss: 0.034554, loss_a: 0.020326
[21:47:18.032] iteration 3256 : loss: 0.016601, loss_a: 0.009765
[21:47:19.392] iteration 3257 : loss: 0.021666, loss_a: 0.012745
[21:47:20.140] iteration 3258 : loss: 0.037578, loss_a: 0.022105
[21:47:21.481] iteration 3259 : loss: 0.016601, loss_a: 0.009766
[21:47:22.227] iteration 3260 : loss: 0.036122, loss_a: 0.021248
[21:47:23.598] iteration 3261 : loss: 0.024948, loss_a: 0.014675
[21:47:24.333] iteration 3262 : loss: 0.017791, loss_a: 0.010465
[21:47:25.670] iteration 3263 : loss: 0.056727, loss_a: 0.033369
[21:47:26.408] iteration 3264 : loss: 0.030528, loss_a: 0.017958
[21:47:27.756] iteration 3265 : loss: 0.016024, loss_a: 0.009426
[21:47:28.499] iteration 3266 : loss: 0.016570, loss_a: 0.009747
[21:47:29.839] iteration 3267 : loss: 0.023936, loss_a: 0.014080
[21:47:30.583] iteration 3268 : loss: 0.030585, loss_a: 0.017991
[21:47:31.942] iteration 3269 : loss: 0.049443, loss_a: 0.029084
[21:47:32.675] iteration 3270 : loss: 0.024476, loss_a: 0.014398
[21:47:34.020] iteration 3271 : loss: 0.015431, loss_a: 0.009077
[21:47:34.763] iteration 3272 : loss: 0.023401, loss_a: 0.013765
[21:47:36.093] iteration 3273 : loss: 0.048932, loss_a: 0.028784
[21:47:36.835] iteration 3274 : loss: 0.035291, loss_a: 0.020760
[21:47:38.193] iteration 3275 : loss: 0.024410, loss_a: 0.014359
[21:47:38.933] iteration 3276 : loss: 0.028994, loss_a: 0.017055
[21:47:40.282] iteration 3277 : loss: 0.018249, loss_a: 0.010735
[21:47:41.012] iteration 3278 : loss: 0.022843, loss_a: 0.013437
[21:47:42.374] iteration 3279 : loss: 0.036786, loss_a: 0.021639
[21:47:43.129] iteration 3280 : loss: 0.026133, loss_a: 0.015372
[21:47:44.490] iteration 3281 : loss: 0.028307, loss_a: 0.016651
[21:47:45.237] iteration 3282 : loss: 0.026188, loss_a: 0.015405
[21:47:46.557] iteration 3283 : loss: 0.036383, loss_a: 0.021402
[21:47:47.305] iteration 3284 : loss: 0.026555, loss_a: 0.015621
[21:47:48.683] iteration 3285 : loss: 0.049146, loss_a: 0.028909
[21:47:49.425] iteration 3286 : loss: 0.025511, loss_a: 0.015006
[21:47:50.745] iteration 3287 : loss: 0.050402, loss_a: 0.029648
[21:47:51.484] iteration 3288 : loss: 0.032699, loss_a: 0.019235
[21:47:52.838] iteration 3289 : loss: 0.019214, loss_a: 0.011302
[21:47:53.581] iteration 3290 : loss: 0.038793, loss_a: 0.022819
[21:47:54.922] iteration 3291 : loss: 0.040458, loss_a: 0.023799
[21:47:55.672] iteration 3292 : loss: 0.065317, loss_a: 0.038422
[21:47:57.035] iteration 3293 : loss: 0.032611, loss_a: 0.019183
[21:47:57.765] iteration 3294 : loss: 0.040495, loss_a: 0.023821
[21:47:59.089] iteration 3295 : loss: 0.028427, loss_a: 0.016722
[21:47:59.827] iteration 3296 : loss: 0.015491, loss_a: 0.009112
[21:48:01.177] iteration 3297 : loss: 0.044601, loss_a: 0.026236
[21:48:01.904] iteration 3298 : loss: 0.035919, loss_a: 0.021129
[21:48:03.261] iteration 3299 : loss: 0.057355, loss_a: 0.033738
[21:48:04.007] iteration 3300 : loss: 0.053156, loss_a: 0.031268
[21:48:05.327] iteration 3301 : loss: 0.026724, loss_a: 0.015720
[21:48:06.081] iteration 3302 : loss: 0.065379, loss_a: 0.038458
[21:48:07.446] iteration 3303 : loss: 0.055761, loss_a: 0.032800
[21:48:08.194] iteration 3304 : loss: 0.028742, loss_a: 0.016907
[21:48:09.536] iteration 3305 : loss: 0.027483, loss_a: 0.016167
[21:48:10.276] iteration 3306 : loss: 0.027729, loss_a: 0.016311
[21:48:11.612] iteration 3307 : loss: 0.067482, loss_a: 0.039695
[21:48:12.354] iteration 3308 : loss: 0.020532, loss_a: 0.012078
[21:48:13.709] iteration 3309 : loss: 0.027506, loss_a: 0.016180
[21:48:14.444] iteration 3310 : loss: 0.057149, loss_a: 0.033617
[21:48:15.806] iteration 3311 : loss: 0.036415, loss_a: 0.021421
[21:48:16.546] iteration 3312 : loss: 0.052111, loss_a: 0.030654
[21:48:17.878] iteration 3313 : loss: 0.015138, loss_a: 0.008905
[21:48:18.627] iteration 3314 : loss: 0.045643, loss_a: 0.026849
[21:48:19.972] iteration 3315 : loss: 0.061538, loss_a: 0.036199
[21:48:20.716] iteration 3316 : loss: 0.038241, loss_a: 0.022495
[21:48:22.071] iteration 3317 : loss: 0.032386, loss_a: 0.019051
[21:48:22.805] iteration 3318 : loss: 0.033229, loss_a: 0.019547
[21:48:24.122] iteration 3319 : loss: 0.025561, loss_a: 0.015036
[21:48:24.862] iteration 3320 : loss: 0.029298, loss_a: 0.017234
[21:48:26.206] iteration 3321 : loss: 0.049777, loss_a: 0.029281
[21:48:26.943] iteration 3322 : loss: 0.020701, loss_a: 0.012177
[21:48:28.267] iteration 3323 : loss: 0.026962, loss_a: 0.015860
[21:48:28.999] iteration 3324 : loss: 0.034679, loss_a: 0.020400
[21:48:30.323] iteration 3325 : loss: 0.045390, loss_a: 0.026700
[21:48:31.068] iteration 3326 : loss: 0.025419, loss_a: 0.014953
[21:48:32.390] iteration 3327 : loss: 0.017827, loss_a: 0.010486
[21:48:33.129] iteration 3328 : loss: 0.037414, loss_a: 0.022008
[21:48:34.471] iteration 3329 : loss: 0.029675, loss_a: 0.017456
[21:48:35.214] iteration 3330 : loss: 0.020160, loss_a: 0.011859
[21:48:36.548] iteration 3331 : loss: 0.035525, loss_a: 0.020897
[21:48:37.278] iteration 3332 : loss: 0.070819, loss_a: 0.041658
[21:48:38.603] iteration 3333 : loss: 0.062054, loss_a: 0.036502
[21:48:39.348] iteration 3334 : loss: 0.017937, loss_a: 0.010551
[21:48:40.663] iteration 3335 : loss: 0.063449, loss_a: 0.037323
[21:48:41.406] iteration 3336 : loss: 0.081522, loss_a: 0.047954
[21:48:42.715] iteration 3337 : loss: 0.036551, loss_a: 0.021500
[21:48:43.458] iteration 3338 : loss: 0.036116, loss_a: 0.021245
[21:48:44.810] iteration 3339 : loss: 0.045364, loss_a: 0.026685
[21:48:45.552] iteration 3340 : loss: 0.049357, loss_a: 0.029034
[21:48:46.904] iteration 3341 : loss: 0.031216, loss_a: 0.018362
[21:48:47.646] iteration 3342 : loss: 0.024004, loss_a: 0.014120
[21:48:48.975] iteration 3343 : loss: 0.025212, loss_a: 0.014831
[21:48:49.734] iteration 3344 : loss: 0.038705, loss_a: 0.022768
[21:48:51.078] iteration 3345 : loss: 0.028818, loss_a: 0.016952
[21:48:51.834] iteration 3346 : loss: 0.052201, loss_a: 0.030706
[21:48:53.167] iteration 3347 : loss: 0.029159, loss_a: 0.017152
[21:48:53.900] iteration 3348 : loss: 0.020317, loss_a: 0.011951
[21:48:55.241] iteration 3349 : loss: 0.048638, loss_a: 0.028611
[21:48:55.976] iteration 3350 : loss: 0.021118, loss_a: 0.012423
[21:48:57.299] iteration 3351 : loss: 0.075512, loss_a: 0.044419
[21:48:58.035] iteration 3352 : loss: 0.063128, loss_a: 0.037134
[21:48:59.382] iteration 3353 : loss: 0.036838, loss_a: 0.021670
[21:49:00.123] iteration 3354 : loss: 0.094834, loss_a: 0.055784
[21:49:01.486] iteration 3355 : loss: 0.046858, loss_a: 0.027564
[21:49:02.239] iteration 3356 : loss: 0.047483, loss_a: 0.027931
[21:49:03.608] iteration 3357 : loss: 0.045951, loss_a: 0.027030
[21:49:04.343] iteration 3358 : loss: 0.040864, loss_a: 0.024038
[21:49:05.724] iteration 3359 : loss: 0.034816, loss_a: 0.020480
[21:49:06.474] iteration 3360 : loss: 0.052359, loss_a: 0.030799
[21:49:07.839] iteration 3361 : loss: 0.032570, loss_a: 0.019159
[21:49:08.590] iteration 3362 : loss: 0.085191, loss_a: 0.050112
[21:49:09.915] iteration 3363 : loss: 0.034019, loss_a: 0.020011
[21:49:10.656] iteration 3364 : loss: 0.054925, loss_a: 0.032309
[21:49:12.002] iteration 3365 : loss: 0.051806, loss_a: 0.030474
[21:49:12.731] iteration 3366 : loss: 0.026234, loss_a: 0.015432
[21:49:14.071] iteration 3367 : loss: 0.018895, loss_a: 0.011115
[21:49:14.809] iteration 3368 : loss: 0.024133, loss_a: 0.014196
[21:49:16.177] iteration 3369 : loss: 0.024614, loss_a: 0.014479
[21:49:16.908] iteration 3370 : loss: 0.051264, loss_a: 0.030155
[21:49:18.254] iteration 3371 : loss: 0.052743, loss_a: 0.031025
[21:49:18.985] iteration 3372 : loss: 0.017104, loss_a: 0.010061
[21:49:20.336] iteration 3373 : loss: 0.033576, loss_a: 0.019751
[21:49:21.073] iteration 3374 : loss: 0.030856, loss_a: 0.018151
[21:49:22.421] iteration 3375 : loss: 0.040077, loss_a: 0.023575
[21:49:23.171] iteration 3376 : loss: 0.041428, loss_a: 0.024369
[21:49:24.489] iteration 3377 : loss: 0.026578, loss_a: 0.015634
[21:49:25.231] iteration 3378 : loss: 0.018323, loss_a: 0.010778
[21:49:26.548] iteration 3379 : loss: 0.044361, loss_a: 0.026095
[21:49:27.288] iteration 3380 : loss: 0.081052, loss_a: 0.047678
[21:49:28.618] iteration 3381 : loss: 0.039040, loss_a: 0.022965
[21:49:29.363] iteration 3382 : loss: 0.011909, loss_a: 0.007005
[21:49:30.702] iteration 3383 : loss: 0.047199, loss_a: 0.027764
[21:49:31.444] iteration 3384 : loss: 0.050683, loss_a: 0.029814
[21:49:32.798] iteration 3385 : loss: 0.017632, loss_a: 0.010372
[21:49:33.548] iteration 3386 : loss: 0.056290, loss_a: 0.033112
[21:49:34.888] iteration 3387 : loss: 0.033266, loss_a: 0.019568
[21:49:35.640] iteration 3388 : loss: 0.048599, loss_a: 0.028588
[21:49:36.982] iteration 3389 : loss: 0.040528, loss_a: 0.023840
[21:49:37.729] iteration 3390 : loss: 0.039056, loss_a: 0.022974
[21:49:39.091] iteration 3391 : loss: 0.025790, loss_a: 0.015170
[21:49:39.847] iteration 3392 : loss: 0.044173, loss_a: 0.025984
[21:49:41.177] iteration 3393 : loss: 0.029209, loss_a: 0.017182
[21:49:41.917] iteration 3394 : loss: 0.019899, loss_a: 0.011705
[21:49:43.268] iteration 3395 : loss: 0.035325, loss_a: 0.020779
[21:49:44.016] iteration 3396 : loss: 0.042484, loss_a: 0.024991
[21:49:45.340] iteration 3397 : loss: 0.023280, loss_a: 0.013694
[21:49:46.080] iteration 3398 : loss: 0.024193, loss_a: 0.014231
[21:49:47.456] iteration 3399 : loss: 0.025245, loss_a: 0.014850
[21:49:48.208] iteration 3400 : loss: 0.043198, loss_a: 0.025410
[21:50:11.719] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_3400_dice_0.8956.pth
[21:50:13.048] iteration 3401 : loss: 0.032304, loss_a: 0.019002
[21:50:15.223] iteration 3402 : loss: 0.049936, loss_a: 0.029374
[21:50:16.575] iteration 3403 : loss: 0.032172, loss_a: 0.018925
[21:50:17.314] iteration 3404 : loss: 0.023230, loss_a: 0.013665
[21:50:18.667] iteration 3405 : loss: 0.027112, loss_a: 0.015948
[21:50:19.412] iteration 3406 : loss: 0.042003, loss_a: 0.024708
[21:50:20.755] iteration 3407 : loss: 0.022143, loss_a: 0.013025
[21:50:21.498] iteration 3408 : loss: 0.036352, loss_a: 0.021384
[21:50:22.858] iteration 3409 : loss: 0.033640, loss_a: 0.019788
[21:50:23.594] iteration 3410 : loss: 0.040217, loss_a: 0.023657
[21:50:24.911] iteration 3411 : loss: 0.024373, loss_a: 0.014337
[21:50:25.654] iteration 3412 : loss: 0.027311, loss_a: 0.016066
[21:50:26.968] iteration 3413 : loss: 0.032332, loss_a: 0.019019
[21:50:27.719] iteration 3414 : loss: 0.053583, loss_a: 0.031519
[21:50:29.073] iteration 3415 : loss: 0.029652, loss_a: 0.017442
[21:50:29.808] iteration 3416 : loss: 0.025350, loss_a: 0.014912
[21:50:31.166] iteration 3417 : loss: 0.025404, loss_a: 0.014943
[21:50:31.917] iteration 3418 : loss: 0.027567, loss_a: 0.016216
[21:50:33.237] iteration 3419 : loss: 0.067029, loss_a: 0.039429
[21:50:33.981] iteration 3420 : loss: 0.037288, loss_a: 0.021934
[21:50:35.303] iteration 3421 : loss: 0.030110, loss_a: 0.017712
[21:50:36.035] iteration 3422 : loss: 0.021372, loss_a: 0.012572
[21:50:37.376] iteration 3423 : loss: 0.028236, loss_a: 0.016609
[21:50:38.114] iteration 3424 : loss: 0.041115, loss_a: 0.024186
[21:50:39.436] iteration 3425 : loss: 0.022597, loss_a: 0.013293
[21:50:40.170] iteration 3426 : loss: 0.037168, loss_a: 0.021864
[21:50:41.515] iteration 3427 : loss: 0.031817, loss_a: 0.018716
[21:50:42.254] iteration 3428 : loss: 0.028566, loss_a: 0.016803
[21:50:43.606] iteration 3429 : loss: 0.032179, loss_a: 0.018929
[21:50:44.353] iteration 3430 : loss: 0.029433, loss_a: 0.017314
[21:50:45.714] iteration 3431 : loss: 0.158511, loss_a: 0.093242
[21:50:46.457] iteration 3432 : loss: 0.026956, loss_a: 0.015856
[21:50:47.778] iteration 3433 : loss: 0.025918, loss_a: 0.015246
[21:50:48.531] iteration 3434 : loss: 0.021433, loss_a: 0.012608
[21:50:49.867] iteration 3435 : loss: 0.026430, loss_a: 0.015547
[21:50:50.598] iteration 3436 : loss: 0.033804, loss_a: 0.019885
[21:50:51.923] iteration 3437 : loss: 0.023699, loss_a: 0.013941
[21:50:52.661] iteration 3438 : loss: 0.017905, loss_a: 0.010532
[21:50:53.993] iteration 3439 : loss: 0.044313, loss_a: 0.026066
[21:50:54.728] iteration 3440 : loss: 0.019890, loss_a: 0.011700
[21:50:56.087] iteration 3441 : loss: 0.040427, loss_a: 0.023780
[21:50:56.826] iteration 3442 : loss: 0.025223, loss_a: 0.014837
[21:50:58.181] iteration 3443 : loss: 0.022795, loss_a: 0.013409
[21:50:58.923] iteration 3444 : loss: 0.016562, loss_a: 0.009742
[21:51:00.271] iteration 3445 : loss: 0.032497, loss_a: 0.019116
[21:51:01.005] iteration 3446 : loss: 0.015824, loss_a: 0.009309
[21:51:02.357] iteration 3447 : loss: 0.033790, loss_a: 0.019876
[21:51:03.093] iteration 3448 : loss: 0.036049, loss_a: 0.021205
[21:51:04.427] iteration 3449 : loss: 0.020821, loss_a: 0.012248
[21:51:05.164] iteration 3450 : loss: 0.025831, loss_a: 0.015194
[21:51:06.519] iteration 3451 : loss: 0.032868, loss_a: 0.019334
[21:51:07.257] iteration 3452 : loss: 0.025389, loss_a: 0.014935
[21:51:08.584] iteration 3453 : loss: 0.035576, loss_a: 0.020927
[21:51:09.317] iteration 3454 : loss: 0.034475, loss_a: 0.020280
[21:51:10.671] iteration 3455 : loss: 0.018126, loss_a: 0.010662
[21:51:11.407] iteration 3456 : loss: 0.021942, loss_a: 0.012907
[21:51:12.729] iteration 3457 : loss: 0.042690, loss_a: 0.025112
[21:51:13.470] iteration 3458 : loss: 0.331312, loss_a: 0.194889
[21:51:14.827] iteration 3459 : loss: 0.020843, loss_a: 0.012260
[21:51:15.567] iteration 3460 : loss: 0.049019, loss_a: 0.028835
[21:51:16.879] iteration 3461 : loss: 0.021621, loss_a: 0.012718
[21:51:17.626] iteration 3462 : loss: 0.057074, loss_a: 0.033573
[21:51:18.973] iteration 3463 : loss: 0.022329, loss_a: 0.013135
[21:51:19.704] iteration 3464 : loss: 0.021507, loss_a: 0.012651
[21:51:21.031] iteration 3465 : loss: 0.026359, loss_a: 0.015505
[21:51:21.775] iteration 3466 : loss: 0.023055, loss_a: 0.013562
[21:51:23.119] iteration 3467 : loss: 0.025186, loss_a: 0.014815
[21:51:23.861] iteration 3468 : loss: 0.031114, loss_a: 0.018302
[21:51:25.184] iteration 3469 : loss: 0.045380, loss_a: 0.026694
[21:51:25.922] iteration 3470 : loss: 0.021780, loss_a: 0.012812
[21:51:27.249] iteration 3471 : loss: 0.018047, loss_a: 0.010616
[21:51:27.987] iteration 3472 : loss: 0.021610, loss_a: 0.012712
[21:51:29.328] iteration 3473 : loss: 0.017302, loss_a: 0.010177
[21:51:30.067] iteration 3474 : loss: 0.057171, loss_a: 0.033630
[21:51:31.436] iteration 3475 : loss: 0.033012, loss_a: 0.019419
[21:51:32.178] iteration 3476 : loss: 0.012815, loss_a: 0.007538
[21:51:33.496] iteration 3477 : loss: 0.058242, loss_a: 0.034260
[21:51:34.231] iteration 3478 : loss: 0.035960, loss_a: 0.021153
[21:51:35.548] iteration 3479 : loss: 0.024452, loss_a: 0.014384
[21:51:36.297] iteration 3480 : loss: 0.030168, loss_a: 0.017746
[21:51:37.625] iteration 3481 : loss: 0.036965, loss_a: 0.021744
[21:51:38.366] iteration 3482 : loss: 0.030330, loss_a: 0.017841
[21:51:39.734] iteration 3483 : loss: 0.038946, loss_a: 0.022910
[21:51:40.482] iteration 3484 : loss: 0.052388, loss_a: 0.030817
[21:51:41.832] iteration 3485 : loss: 0.038691, loss_a: 0.022760
[21:51:42.579] iteration 3486 : loss: 0.054465, loss_a: 0.032038
[21:51:43.954] iteration 3487 : loss: 0.029251, loss_a: 0.017206
[21:51:44.681] iteration 3488 : loss: 0.016545, loss_a: 0.009732
[21:51:45.999] iteration 3489 : loss: 0.017677, loss_a: 0.010398
[21:51:46.746] iteration 3490 : loss: 0.028132, loss_a: 0.016548
[21:51:48.067] iteration 3491 : loss: 0.014277, loss_a: 0.008398
[21:51:48.811] iteration 3492 : loss: 0.040151, loss_a: 0.023618
[21:51:50.137] iteration 3493 : loss: 0.034856, loss_a: 0.020504
[21:51:50.891] iteration 3494 : loss: 0.046384, loss_a: 0.027285
[21:51:52.256] iteration 3495 : loss: 0.054943, loss_a: 0.032320
[21:51:52.996] iteration 3496 : loss: 0.019852, loss_a: 0.011678
[21:51:54.331] iteration 3497 : loss: 0.031507, loss_a: 0.018533
[21:51:55.079] iteration 3498 : loss: 0.064669, loss_a: 0.038041
[21:51:56.399] iteration 3499 : loss: 0.033478, loss_a: 0.019693
[21:51:57.134] iteration 3500 : loss: 0.021859, loss_a: 0.012858
[21:51:58.449] iteration 3501 : loss: 0.026406, loss_a: 0.015533
[21:51:59.201] iteration 3502 : loss: 0.044325, loss_a: 0.026074
[21:52:00.562] iteration 3503 : loss: 0.039997, loss_a: 0.023528
[21:52:01.307] iteration 3504 : loss: 0.040163, loss_a: 0.023625
[21:52:02.626] iteration 3505 : loss: 0.019602, loss_a: 0.011531
[21:52:03.373] iteration 3506 : loss: 0.020233, loss_a: 0.011902
[21:52:04.695] iteration 3507 : loss: 0.045162, loss_a: 0.026566
[21:52:05.438] iteration 3508 : loss: 0.046907, loss_a: 0.027592
[21:52:06.787] iteration 3509 : loss: 0.021919, loss_a: 0.012893
[21:52:07.520] iteration 3510 : loss: 0.024147, loss_a: 0.014204
[21:52:08.885] iteration 3511 : loss: 0.058768, loss_a: 0.034569
[21:52:09.616] iteration 3512 : loss: 0.012464, loss_a: 0.007332
[21:52:10.946] iteration 3513 : loss: 0.036037, loss_a: 0.021198
[21:52:11.691] iteration 3514 : loss: 0.016411, loss_a: 0.009653
[21:52:13.027] iteration 3515 : loss: 0.075952, loss_a: 0.044678
[21:52:13.779] iteration 3516 : loss: 0.048847, loss_a: 0.028734
[21:52:15.119] iteration 3517 : loss: 0.026108, loss_a: 0.015358
[21:52:15.869] iteration 3518 : loss: 0.094869, loss_a: 0.055805
[21:52:17.293] iteration 3519 : loss: 0.029785, loss_a: 0.017521
[21:52:18.033] iteration 3520 : loss: 0.031781, loss_a: 0.018695
[21:52:19.382] iteration 3521 : loss: 0.041729, loss_a: 0.024547
[21:52:20.120] iteration 3522 : loss: 0.036774, loss_a: 0.021632
[21:52:21.451] iteration 3523 : loss: 0.033480, loss_a: 0.019694
[21:52:22.194] iteration 3524 : loss: 0.040963, loss_a: 0.024096
[21:52:23.540] iteration 3525 : loss: 0.030903, loss_a: 0.018178
[21:52:24.274] iteration 3526 : loss: 0.026630, loss_a: 0.015665
[21:52:25.583] iteration 3527 : loss: 0.268645, loss_a: 0.158026
[21:52:26.315] iteration 3528 : loss: 0.023218, loss_a: 0.013658
[21:52:27.634] iteration 3529 : loss: 0.068347, loss_a: 0.040204
[21:52:28.389] iteration 3530 : loss: 0.066671, loss_a: 0.039218
[21:52:29.703] iteration 3531 : loss: 0.028653, loss_a: 0.016855
[21:52:30.441] iteration 3532 : loss: 0.029009, loss_a: 0.017064
[21:52:31.766] iteration 3533 : loss: 0.025880, loss_a: 0.015223
[21:52:32.503] iteration 3534 : loss: 0.038016, loss_a: 0.022362
[21:52:33.845] iteration 3535 : loss: 0.086711, loss_a: 0.051007
[21:52:34.590] iteration 3536 : loss: 0.068352, loss_a: 0.040207
[21:52:35.954] iteration 3537 : loss: 0.028019, loss_a: 0.016482
[21:52:36.693] iteration 3538 : loss: 0.043384, loss_a: 0.025520
[21:52:38.012] iteration 3539 : loss: 0.042153, loss_a: 0.024796
[21:52:38.769] iteration 3540 : loss: 0.058095, loss_a: 0.034174
[21:52:40.084] iteration 3541 : loss: 0.067726, loss_a: 0.039839
[21:52:40.832] iteration 3542 : loss: 0.053518, loss_a: 0.031481
[21:52:42.182] iteration 3543 : loss: 0.031173, loss_a: 0.018337
[21:52:42.924] iteration 3544 : loss: 0.036856, loss_a: 0.021680
[21:52:44.293] iteration 3545 : loss: 0.045408, loss_a: 0.026711
[21:52:45.035] iteration 3546 : loss: 0.061013, loss_a: 0.035890
[21:52:46.364] iteration 3547 : loss: 0.069200, loss_a: 0.040706
[21:52:47.107] iteration 3548 : loss: 0.019246, loss_a: 0.011321
[21:52:48.410] iteration 3549 : loss: 0.044600, loss_a: 0.026235
[21:52:49.145] iteration 3550 : loss: 0.022510, loss_a: 0.013241
[21:52:50.511] iteration 3551 : loss: 0.162182, loss_a: 0.095401
[21:52:51.244] iteration 3552 : loss: 0.023423, loss_a: 0.013778
[21:52:52.613] iteration 3553 : loss: 0.045448, loss_a: 0.026734
[21:52:53.383] iteration 3554 : loss: 0.032108, loss_a: 0.018887
[21:52:54.725] iteration 3555 : loss: 0.054576, loss_a: 0.032104
[21:52:55.462] iteration 3556 : loss: 0.028560, loss_a: 0.016800
[21:52:56.816] iteration 3557 : loss: 0.018725, loss_a: 0.011015
[21:52:57.570] iteration 3558 : loss: 0.047488, loss_a: 0.027934
[21:52:58.924] iteration 3559 : loss: 0.027318, loss_a: 0.016070
[21:52:59.665] iteration 3560 : loss: 0.038850, loss_a: 0.022853
[21:53:00.986] iteration 3561 : loss: 0.019536, loss_a: 0.011492
[21:53:01.729] iteration 3562 : loss: 0.024727, loss_a: 0.014545
[21:53:03.067] iteration 3563 : loss: 0.029299, loss_a: 0.017235
[21:53:03.803] iteration 3564 : loss: 0.026566, loss_a: 0.015627
[21:53:05.167] iteration 3565 : loss: 0.051694, loss_a: 0.030408
[21:53:05.902] iteration 3566 : loss: 0.051612, loss_a: 0.030360
[21:53:07.232] iteration 3567 : loss: 0.025617, loss_a: 0.015069
[21:53:07.965] iteration 3568 : loss: 0.014947, loss_a: 0.008792
[21:53:09.281] iteration 3569 : loss: 0.028991, loss_a: 0.017053
[21:53:10.013] iteration 3570 : loss: 0.017030, loss_a: 0.010018
[21:53:11.373] iteration 3571 : loss: 0.025479, loss_a: 0.014987
[21:53:12.107] iteration 3572 : loss: 0.019005, loss_a: 0.011179
[21:53:13.451] iteration 3573 : loss: 0.022191, loss_a: 0.013054
[21:53:14.190] iteration 3574 : loss: 0.029519, loss_a: 0.017364
[21:53:15.543] iteration 3575 : loss: 0.042902, loss_a: 0.025237
[21:53:16.276] iteration 3576 : loss: 0.020462, loss_a: 0.012036
[21:53:17.641] iteration 3577 : loss: 0.030208, loss_a: 0.017769
[21:53:18.377] iteration 3578 : loss: 0.026742, loss_a: 0.015731
[21:53:19.723] iteration 3579 : loss: 0.050818, loss_a: 0.029893
[21:53:20.465] iteration 3580 : loss: 0.038454, loss_a: 0.022620
[21:53:21.807] iteration 3581 : loss: 0.025005, loss_a: 0.014709
[21:53:22.549] iteration 3582 : loss: 0.034912, loss_a: 0.020536
[21:53:23.880] iteration 3583 : loss: 0.036647, loss_a: 0.021557
[21:53:24.620] iteration 3584 : loss: 0.049124, loss_a: 0.028896
[21:53:25.944] iteration 3585 : loss: 0.026821, loss_a: 0.015777
[21:53:26.678] iteration 3586 : loss: 0.022796, loss_a: 0.013410
[21:53:28.067] iteration 3587 : loss: 0.125825, loss_a: 0.074014
[21:53:28.805] iteration 3588 : loss: 0.043005, loss_a: 0.025297
[21:53:30.158] iteration 3589 : loss: 0.035620, loss_a: 0.020953
[21:53:30.897] iteration 3590 : loss: 0.060086, loss_a: 0.035345
[21:53:32.227] iteration 3591 : loss: 0.023451, loss_a: 0.013795
[21:53:32.978] iteration 3592 : loss: 0.045389, loss_a: 0.026700
[21:53:34.282] iteration 3593 : loss: 0.055392, loss_a: 0.032584
[21:53:35.030] iteration 3594 : loss: 0.034976, loss_a: 0.020574
[21:53:36.383] iteration 3595 : loss: 0.034121, loss_a: 0.020071
[21:53:37.132] iteration 3596 : loss: 0.027563, loss_a: 0.016214
[21:53:38.459] iteration 3597 : loss: 0.124955, loss_a: 0.073503
[21:53:39.207] iteration 3598 : loss: 0.027887, loss_a: 0.016404
[21:53:40.541] iteration 3599 : loss: 0.021887, loss_a: 0.012875
[21:53:41.275] iteration 3600 : loss: 0.020875, loss_a: 0.012279
[21:54:05.858] iteration 3601 : loss: 0.034277, loss_a: 0.020163
[21:54:07.986] iteration 3602 : loss: 0.032951, loss_a: 0.019383
[21:54:09.340] iteration 3603 : loss: 0.016222, loss_a: 0.009543
[21:54:10.069] iteration 3604 : loss: 0.024662, loss_a: 0.014507
[21:54:11.389] iteration 3605 : loss: 0.035181, loss_a: 0.020695
[21:54:12.128] iteration 3606 : loss: 0.021950, loss_a: 0.012912
[21:54:13.475] iteration 3607 : loss: 0.028040, loss_a: 0.016494
[21:54:14.219] iteration 3608 : loss: 0.054856, loss_a: 0.032268
[21:54:15.551] iteration 3609 : loss: 0.021920, loss_a: 0.012894
[21:54:16.293] iteration 3610 : loss: 0.045793, loss_a: 0.026937
[21:54:17.645] iteration 3611 : loss: 0.023597, loss_a: 0.013881
[21:54:18.385] iteration 3612 : loss: 0.036512, loss_a: 0.021477
[21:54:19.767] iteration 3613 : loss: 0.053493, loss_a: 0.031467
[21:54:20.508] iteration 3614 : loss: 0.039226, loss_a: 0.023074
[21:54:21.835] iteration 3615 : loss: 0.038001, loss_a: 0.022353
[21:54:22.582] iteration 3616 : loss: 0.043769, loss_a: 0.025746
[21:54:23.905] iteration 3617 : loss: 0.017247, loss_a: 0.010145
[21:54:24.651] iteration 3618 : loss: 0.029909, loss_a: 0.017594
[21:54:25.995] iteration 3619 : loss: 0.050007, loss_a: 0.029416
[21:54:26.742] iteration 3620 : loss: 0.021294, loss_a: 0.012526
[21:54:28.069] iteration 3621 : loss: 0.036007, loss_a: 0.021181
[21:54:28.813] iteration 3622 : loss: 0.039545, loss_a: 0.023262
[21:54:30.154] iteration 3623 : loss: 0.035884, loss_a: 0.021108
[21:54:30.884] iteration 3624 : loss: 0.036780, loss_a: 0.021636
[21:54:32.216] iteration 3625 : loss: 0.063256, loss_a: 0.037209
[21:54:32.962] iteration 3626 : loss: 0.025561, loss_a: 0.015036
[21:54:34.287] iteration 3627 : loss: 0.025250, loss_a: 0.014853
[21:54:35.024] iteration 3628 : loss: 0.033148, loss_a: 0.019499
[21:54:36.368] iteration 3629 : loss: 0.055638, loss_a: 0.032728
[21:54:37.105] iteration 3630 : loss: 0.059640, loss_a: 0.035082
[21:54:38.412] iteration 3631 : loss: 0.022492, loss_a: 0.013230
[21:54:39.149] iteration 3632 : loss: 0.026875, loss_a: 0.015809
[21:54:40.470] iteration 3633 : loss: 0.028876, loss_a: 0.016986
[21:54:41.205] iteration 3634 : loss: 0.020180, loss_a: 0.011870
[21:54:42.556] iteration 3635 : loss: 0.030533, loss_a: 0.017960
[21:54:43.294] iteration 3636 : loss: 0.041175, loss_a: 0.024221
[21:54:44.611] iteration 3637 : loss: 0.015734, loss_a: 0.009255
[21:54:45.351] iteration 3638 : loss: 0.058305, loss_a: 0.034297
[21:54:46.681] iteration 3639 : loss: 0.105937, loss_a: 0.062316
[21:54:47.425] iteration 3640 : loss: 0.022282, loss_a: 0.013107
[21:54:48.764] iteration 3641 : loss: 0.049417, loss_a: 0.029069
[21:54:49.515] iteration 3642 : loss: 0.039485, loss_a: 0.023226
[21:54:50.835] iteration 3643 : loss: 0.027921, loss_a: 0.016424
[21:54:51.575] iteration 3644 : loss: 0.022666, loss_a: 0.013333
[21:54:52.932] iteration 3645 : loss: 0.035938, loss_a: 0.021140
[21:54:53.676] iteration 3646 : loss: 0.017520, loss_a: 0.010306
[21:54:55.023] iteration 3647 : loss: 0.035305, loss_a: 0.020767
[21:54:55.766] iteration 3648 : loss: 0.032521, loss_a: 0.019130
[21:54:57.095] iteration 3649 : loss: 0.024731, loss_a: 0.014548
[21:54:57.838] iteration 3650 : loss: 0.029305, loss_a: 0.017238
[21:54:59.186] iteration 3651 : loss: 0.056048, loss_a: 0.032969
[21:54:59.927] iteration 3652 : loss: 0.017534, loss_a: 0.010314
[21:55:01.272] iteration 3653 : loss: 0.026794, loss_a: 0.015761
[21:55:02.014] iteration 3654 : loss: 0.036739, loss_a: 0.021611
[21:55:03.357] iteration 3655 : loss: 0.033720, loss_a: 0.019835
[21:55:04.092] iteration 3656 : loss: 0.031228, loss_a: 0.018369
[21:55:05.438] iteration 3657 : loss: 0.096101, loss_a: 0.056530
[21:55:06.177] iteration 3658 : loss: 0.047904, loss_a: 0.028179
[21:55:07.510] iteration 3659 : loss: 0.021724, loss_a: 0.012779
[21:55:08.249] iteration 3660 : loss: 0.040078, loss_a: 0.023575
[21:55:09.566] iteration 3661 : loss: 0.012357, loss_a: 0.007269
[21:55:10.316] iteration 3662 : loss: 0.021796, loss_a: 0.012821
[21:55:11.651] iteration 3663 : loss: 0.028121, loss_a: 0.016542
[21:55:12.397] iteration 3664 : loss: 0.040331, loss_a: 0.023724
[21:55:13.737] iteration 3665 : loss: 0.081765, loss_a: 0.048097
[21:55:14.486] iteration 3666 : loss: 0.057764, loss_a: 0.033979
[21:55:15.838] iteration 3667 : loss: 0.040970, loss_a: 0.024100
[21:55:16.572] iteration 3668 : loss: 0.036173, loss_a: 0.021278
[21:55:17.913] iteration 3669 : loss: 0.032338, loss_a: 0.019022
[21:55:18.659] iteration 3670 : loss: 0.038257, loss_a: 0.022504
[21:55:20.019] iteration 3671 : loss: 0.075300, loss_a: 0.044294
[21:55:20.765] iteration 3672 : loss: 0.046163, loss_a: 0.027154
[21:55:22.130] iteration 3673 : loss: 0.109635, loss_a: 0.064491
[21:55:22.874] iteration 3674 : loss: 0.059061, loss_a: 0.034742
[21:55:24.230] iteration 3675 : loss: 0.036437, loss_a: 0.021434
[21:55:24.963] iteration 3676 : loss: 0.021989, loss_a: 0.012935
[21:55:26.299] iteration 3677 : loss: 0.017518, loss_a: 0.010305
[21:55:27.043] iteration 3678 : loss: 0.051415, loss_a: 0.030244
[21:55:28.356] iteration 3679 : loss: 0.018286, loss_a: 0.010756
[21:55:29.122] iteration 3680 : loss: 0.033366, loss_a: 0.019627
[21:55:30.438] iteration 3681 : loss: 0.064374, loss_a: 0.037867
[21:55:31.166] iteration 3682 : loss: 0.023355, loss_a: 0.013738
[21:55:32.529] iteration 3683 : loss: 0.034699, loss_a: 0.020411
[21:55:33.273] iteration 3684 : loss: 0.047005, loss_a: 0.027650
[21:55:34.644] iteration 3685 : loss: 0.020681, loss_a: 0.012165
[21:55:35.395] iteration 3686 : loss: 0.039304, loss_a: 0.023120
[21:55:36.730] iteration 3687 : loss: 0.052237, loss_a: 0.030728
[21:55:37.469] iteration 3688 : loss: 0.017933, loss_a: 0.010549
[21:55:38.787] iteration 3689 : loss: 0.031033, loss_a: 0.018255
[21:55:39.528] iteration 3690 : loss: 0.057137, loss_a: 0.033610
[21:55:40.855] iteration 3691 : loss: 0.037786, loss_a: 0.022227
[21:55:41.595] iteration 3692 : loss: 0.026714, loss_a: 0.015714
[21:55:42.912] iteration 3693 : loss: 0.030254, loss_a: 0.017796
[21:55:43.646] iteration 3694 : loss: 0.038756, loss_a: 0.022797
[21:55:44.984] iteration 3695 : loss: 0.041475, loss_a: 0.024397
[21:55:45.732] iteration 3696 : loss: 0.077735, loss_a: 0.045727
[21:55:47.059] iteration 3697 : loss: 0.029784, loss_a: 0.017520
[21:55:47.800] iteration 3698 : loss: 0.043001, loss_a: 0.025295
[21:55:49.151] iteration 3699 : loss: 0.061617, loss_a: 0.036245
[21:55:49.893] iteration 3700 : loss: 0.022040, loss_a: 0.012965
[21:55:51.258] iteration 3701 : loss: 0.033761, loss_a: 0.019859
[21:55:51.992] iteration 3702 : loss: 0.050011, loss_a: 0.029418
[21:55:53.341] iteration 3703 : loss: 0.064517, loss_a: 0.037951
[21:55:54.084] iteration 3704 : loss: 0.030566, loss_a: 0.017980
[21:55:55.458] iteration 3705 : loss: 0.078274, loss_a: 0.046043
[21:55:56.200] iteration 3706 : loss: 0.040446, loss_a: 0.023792
[21:55:57.547] iteration 3707 : loss: 0.048961, loss_a: 0.028801
[21:55:58.291] iteration 3708 : loss: 0.032664, loss_a: 0.019214
[21:55:59.606] iteration 3709 : loss: 0.042823, loss_a: 0.025190
[21:56:00.345] iteration 3710 : loss: 0.047593, loss_a: 0.027996
[21:56:01.710] iteration 3711 : loss: 0.042890, loss_a: 0.025229
[21:56:02.447] iteration 3712 : loss: 0.023906, loss_a: 0.014062
[21:56:03.803] iteration 3713 : loss: 0.036442, loss_a: 0.021436
[21:56:04.551] iteration 3714 : loss: 0.028932, loss_a: 0.017019
[21:56:05.867] iteration 3715 : loss: 0.018945, loss_a: 0.011144
[21:56:06.609] iteration 3716 : loss: 0.024314, loss_a: 0.014302
[21:56:07.948] iteration 3717 : loss: 0.017064, loss_a: 0.010038
[21:56:08.696] iteration 3718 : loss: 0.037399, loss_a: 0.021999
[21:56:10.054] iteration 3719 : loss: 0.030414, loss_a: 0.017891
[21:56:10.800] iteration 3720 : loss: 0.052662, loss_a: 0.030977
[21:56:12.126] iteration 3721 : loss: 0.023266, loss_a: 0.013686
[21:56:12.863] iteration 3722 : loss: 0.092552, loss_a: 0.054443
[21:56:14.231] iteration 3723 : loss: 0.037752, loss_a: 0.022207
[21:56:14.980] iteration 3724 : loss: 0.020199, loss_a: 0.011882
[21:56:16.332] iteration 3725 : loss: 0.069178, loss_a: 0.040693
[21:56:17.070] iteration 3726 : loss: 0.034258, loss_a: 0.020152
[21:56:18.436] iteration 3727 : loss: 0.029027, loss_a: 0.017075
[21:56:19.171] iteration 3728 : loss: 0.041669, loss_a: 0.024511
[21:56:20.509] iteration 3729 : loss: 0.038689, loss_a: 0.022758
[21:56:21.241] iteration 3730 : loss: 0.040494, loss_a: 0.023820
[21:56:22.576] iteration 3731 : loss: 0.040399, loss_a: 0.023764
[21:56:23.323] iteration 3732 : loss: 0.032958, loss_a: 0.019387
[21:56:24.661] iteration 3733 : loss: 0.029047, loss_a: 0.017086
[21:56:25.395] iteration 3734 : loss: 0.025225, loss_a: 0.014838
[21:56:26.737] iteration 3735 : loss: 0.040011, loss_a: 0.023536
[21:56:27.484] iteration 3736 : loss: 0.046553, loss_a: 0.027384
[21:56:28.801] iteration 3737 : loss: 0.037986, loss_a: 0.022345
[21:56:29.547] iteration 3738 : loss: 0.028971, loss_a: 0.017041
[21:56:30.883] iteration 3739 : loss: 0.033665, loss_a: 0.019803
[21:56:31.628] iteration 3740 : loss: 0.036783, loss_a: 0.021637
[21:56:32.970] iteration 3741 : loss: 0.041543, loss_a: 0.024437
[21:56:33.714] iteration 3742 : loss: 0.051233, loss_a: 0.030137
[21:56:35.064] iteration 3743 : loss: 0.022359, loss_a: 0.013152
[21:56:35.805] iteration 3744 : loss: 0.026650, loss_a: 0.015676
[21:56:37.149] iteration 3745 : loss: 0.022634, loss_a: 0.013314
[21:56:37.893] iteration 3746 : loss: 0.024988, loss_a: 0.014699
[21:56:39.238] iteration 3747 : loss: 0.020696, loss_a: 0.012174
[21:56:39.982] iteration 3748 : loss: 0.039565, loss_a: 0.023273
[21:56:41.293] iteration 3749 : loss: 0.025573, loss_a: 0.015043
[21:56:42.048] iteration 3750 : loss: 0.054732, loss_a: 0.032195
[21:56:43.427] iteration 3751 : loss: 0.020773, loss_a: 0.012219
[21:56:44.171] iteration 3752 : loss: 0.027706, loss_a: 0.016298
[21:56:45.523] iteration 3753 : loss: 0.017769, loss_a: 0.010452
[21:56:46.259] iteration 3754 : loss: 0.046067, loss_a: 0.027098
[21:56:47.612] iteration 3755 : loss: 0.019289, loss_a: 0.011347
[21:56:48.354] iteration 3756 : loss: 0.027279, loss_a: 0.016047
[21:56:49.672] iteration 3757 : loss: 0.027011, loss_a: 0.015889
[21:56:50.417] iteration 3758 : loss: 0.030133, loss_a: 0.017725
[21:56:51.755] iteration 3759 : loss: 0.030637, loss_a: 0.018022
[21:56:52.537] iteration 3760 : loss: 0.504158, loss_a: 0.296563
[21:56:53.882] iteration 3761 : loss: 0.033667, loss_a: 0.019804
[21:56:54.628] iteration 3762 : loss: 0.028929, loss_a: 0.017017
[21:56:55.968] iteration 3763 : loss: 0.028948, loss_a: 0.017028
[21:56:56.705] iteration 3764 : loss: 0.023139, loss_a: 0.013611
[21:56:58.019] iteration 3765 : loss: 0.028872, loss_a: 0.016984
[21:56:58.768] iteration 3766 : loss: 0.056239, loss_a: 0.033082
[21:57:00.128] iteration 3767 : loss: 0.030957, loss_a: 0.018210
[21:57:00.874] iteration 3768 : loss: 0.020674, loss_a: 0.012161
[21:57:02.233] iteration 3769 : loss: 0.022195, loss_a: 0.013056
[21:57:02.972] iteration 3770 : loss: 0.036380, loss_a: 0.021400
[21:57:04.279] iteration 3771 : loss: 0.024636, loss_a: 0.014492
[21:57:05.020] iteration 3772 : loss: 0.022782, loss_a: 0.013401
[21:57:06.340] iteration 3773 : loss: 0.028269, loss_a: 0.016629
[21:57:07.089] iteration 3774 : loss: 0.023760, loss_a: 0.013976
[21:57:08.416] iteration 3775 : loss: 0.117418, loss_a: 0.069070
[21:57:09.164] iteration 3776 : loss: 0.026675, loss_a: 0.015691
[21:57:10.475] iteration 3777 : loss: 0.026787, loss_a: 0.015757
[21:57:11.221] iteration 3778 : loss: 0.061339, loss_a: 0.036082
[21:57:12.603] iteration 3779 : loss: 0.046612, loss_a: 0.027419
[21:57:13.336] iteration 3780 : loss: 0.028037, loss_a: 0.016492
[21:57:14.701] iteration 3781 : loss: 0.076214, loss_a: 0.044832
[21:57:15.446] iteration 3782 : loss: 0.051117, loss_a: 0.030069
[21:57:16.804] iteration 3783 : loss: 0.044131, loss_a: 0.025959
[21:57:17.534] iteration 3784 : loss: 0.018111, loss_a: 0.010653
[21:57:18.862] iteration 3785 : loss: 0.041343, loss_a: 0.024320
[21:57:19.597] iteration 3786 : loss: 0.021687, loss_a: 0.012757
[21:57:20.952] iteration 3787 : loss: 0.032067, loss_a: 0.018863
[21:57:21.701] iteration 3788 : loss: 0.057859, loss_a: 0.034035
[21:57:23.048] iteration 3789 : loss: 0.042518, loss_a: 0.025011
[21:57:23.786] iteration 3790 : loss: 0.022955, loss_a: 0.013503
[21:57:25.112] iteration 3791 : loss: 0.047364, loss_a: 0.027861
[21:57:25.841] iteration 3792 : loss: 0.016229, loss_a: 0.009546
[21:57:27.194] iteration 3793 : loss: 0.064531, loss_a: 0.037959
[21:57:27.933] iteration 3794 : loss: 0.038144, loss_a: 0.022437
[21:57:29.291] iteration 3795 : loss: 0.035692, loss_a: 0.020995
[21:57:30.029] iteration 3796 : loss: 0.041578, loss_a: 0.024458
[21:57:31.352] iteration 3797 : loss: 0.015750, loss_a: 0.009265
[21:57:32.104] iteration 3798 : loss: 0.039893, loss_a: 0.023466
[21:57:33.434] iteration 3799 : loss: 0.038236, loss_a: 0.022492
[21:57:34.168] iteration 3800 : loss: 0.029868, loss_a: 0.017569
[21:57:58.841] iteration 3801 : loss: 0.034897, loss_a: 0.020528
[21:58:01.001] iteration 3802 : loss: 0.021951, loss_a: 0.012912
[21:58:02.342] iteration 3803 : loss: 0.037763, loss_a: 0.022214
[21:58:03.083] iteration 3804 : loss: 0.023932, loss_a: 0.014078
[21:58:04.424] iteration 3805 : loss: 0.025909, loss_a: 0.015241
[21:58:05.164] iteration 3806 : loss: 0.027310, loss_a: 0.016065
[21:58:06.530] iteration 3807 : loss: 0.035832, loss_a: 0.021078
[21:58:07.282] iteration 3808 : loss: 0.041889, loss_a: 0.024640
[21:58:08.641] iteration 3809 : loss: 0.063550, loss_a: 0.037382
[21:58:09.382] iteration 3810 : loss: 0.038443, loss_a: 0.022614
[21:58:10.714] iteration 3811 : loss: 0.058636, loss_a: 0.034492
[21:58:11.463] iteration 3812 : loss: 0.036129, loss_a: 0.021252
[21:58:12.786] iteration 3813 : loss: 0.045757, loss_a: 0.026916
[21:58:13.533] iteration 3814 : loss: 0.050056, loss_a: 0.029445
[21:58:14.901] iteration 3815 : loss: 0.052533, loss_a: 0.030902
[21:58:15.641] iteration 3816 : loss: 0.067379, loss_a: 0.039635
[21:58:16.952] iteration 3817 : loss: 0.035289, loss_a: 0.020758
[21:58:17.690] iteration 3818 : loss: 0.033045, loss_a: 0.019438
[21:58:18.995] iteration 3819 : loss: 0.015482, loss_a: 0.009107
[21:58:19.733] iteration 3820 : loss: 0.024773, loss_a: 0.014572
[21:58:21.110] iteration 3821 : loss: 0.035472, loss_a: 0.020866
[21:58:21.865] iteration 3822 : loss: 0.035244, loss_a: 0.020732
[21:58:23.245] iteration 3823 : loss: 0.022321, loss_a: 0.013130
[21:58:23.986] iteration 3824 : loss: 0.053546, loss_a: 0.031498
[21:58:25.316] iteration 3825 : loss: 0.033085, loss_a: 0.019462
[21:58:26.062] iteration 3826 : loss: 0.336786, loss_a: 0.198109
[21:58:27.397] iteration 3827 : loss: 0.029235, loss_a: 0.017197
[21:58:28.132] iteration 3828 : loss: 0.026228, loss_a: 0.015428
[21:58:29.469] iteration 3829 : loss: 0.036667, loss_a: 0.021569
[21:58:30.212] iteration 3830 : loss: 0.025776, loss_a: 0.015163
[21:58:31.542] iteration 3831 : loss: 0.021964, loss_a: 0.012920
[21:58:32.281] iteration 3832 : loss: 0.039401, loss_a: 0.023177
[21:58:33.641] iteration 3833 : loss: 0.033766, loss_a: 0.019862
[21:58:34.379] iteration 3834 : loss: 0.037372, loss_a: 0.021984
[21:58:35.714] iteration 3835 : loss: 0.070719, loss_a: 0.041600
[21:58:36.464] iteration 3836 : loss: 0.019780, loss_a: 0.011635
[21:58:37.792] iteration 3837 : loss: 0.034080, loss_a: 0.020047
[21:58:38.527] iteration 3838 : loss: 0.071659, loss_a: 0.042152
[21:58:39.855] iteration 3839 : loss: 0.025298, loss_a: 0.014881
[21:58:40.587] iteration 3840 : loss: 0.018966, loss_a: 0.011156
[21:58:41.957] iteration 3841 : loss: 0.041671, loss_a: 0.024512
[21:58:42.698] iteration 3842 : loss: 0.026286, loss_a: 0.015462
[21:58:44.029] iteration 3843 : loss: 0.021464, loss_a: 0.012626
[21:58:44.770] iteration 3844 : loss: 0.020064, loss_a: 0.011802
[21:58:46.101] iteration 3845 : loss: 0.039242, loss_a: 0.023084
[21:58:46.849] iteration 3846 : loss: 0.036433, loss_a: 0.021431
[21:58:48.205] iteration 3847 : loss: 0.045141, loss_a: 0.026553
[21:58:48.949] iteration 3848 : loss: 0.032534, loss_a: 0.019138
[21:58:50.273] iteration 3849 : loss: 0.020885, loss_a: 0.012285
[21:58:51.013] iteration 3850 : loss: 0.096599, loss_a: 0.056823
[21:58:52.374] iteration 3851 : loss: 0.080646, loss_a: 0.047439
[21:58:53.116] iteration 3852 : loss: 0.056380, loss_a: 0.033165
[21:58:54.459] iteration 3853 : loss: 0.051880, loss_a: 0.030518
[21:58:55.196] iteration 3854 : loss: 0.024033, loss_a: 0.014137
[21:58:56.550] iteration 3855 : loss: 0.030366, loss_a: 0.017862
[21:58:57.298] iteration 3856 : loss: 0.078751, loss_a: 0.046324
[21:58:58.612] iteration 3857 : loss: 0.016106, loss_a: 0.009474
[21:58:59.357] iteration 3858 : loss: 0.038633, loss_a: 0.022725
[21:59:00.705] iteration 3859 : loss: 0.023176, loss_a: 0.013633
[21:59:01.451] iteration 3860 : loss: 0.019453, loss_a: 0.011443
[21:59:02.805] iteration 3861 : loss: 0.031660, loss_a: 0.018623
[21:59:03.557] iteration 3862 : loss: 0.015666, loss_a: 0.009215
[21:59:04.910] iteration 3863 : loss: 0.037167, loss_a: 0.021863
[21:59:05.664] iteration 3864 : loss: 0.029547, loss_a: 0.017381
[21:59:07.023] iteration 3865 : loss: 0.068207, loss_a: 0.040122
[21:59:07.772] iteration 3866 : loss: 0.046109, loss_a: 0.027123
[21:59:09.108] iteration 3867 : loss: 0.046813, loss_a: 0.027537
[21:59:09.873] iteration 3868 : loss: 0.043962, loss_a: 0.025860
[21:59:11.183] iteration 3869 : loss: 0.023968, loss_a: 0.014099
[21:59:11.930] iteration 3870 : loss: 0.026308, loss_a: 0.015475
[21:59:13.244] iteration 3871 : loss: 0.037014, loss_a: 0.021773
[21:59:13.982] iteration 3872 : loss: 0.024805, loss_a: 0.014591
[21:59:15.335] iteration 3873 : loss: 0.039049, loss_a: 0.022970
[21:59:16.073] iteration 3874 : loss: 0.034116, loss_a: 0.020068
[21:59:17.449] iteration 3875 : loss: 0.051483, loss_a: 0.030284
[21:59:18.188] iteration 3876 : loss: 0.038546, loss_a: 0.022674
[21:59:19.499] iteration 3877 : loss: 0.019326, loss_a: 0.011368
[21:59:20.255] iteration 3878 : loss: 0.067917, loss_a: 0.039951
[21:59:21.567] iteration 3879 : loss: 0.021161, loss_a: 0.012448
[21:59:22.301] iteration 3880 : loss: 0.028615, loss_a: 0.016832
[21:59:23.642] iteration 3881 : loss: 0.047863, loss_a: 0.028155
[21:59:24.382] iteration 3882 : loss: 0.038509, loss_a: 0.022652
[21:59:25.698] iteration 3883 : loss: 0.015097, loss_a: 0.008880
[21:59:26.478] iteration 3884 : loss: 0.527389, loss_a: 0.310229
[21:59:27.819] iteration 3885 : loss: 0.024044, loss_a: 0.014144
[21:59:28.552] iteration 3886 : loss: 0.018543, loss_a: 0.010908
[21:59:29.895] iteration 3887 : loss: 0.057170, loss_a: 0.033629
[21:59:30.652] iteration 3888 : loss: 0.087655, loss_a: 0.051562
[21:59:31.993] iteration 3889 : loss: 0.059054, loss_a: 0.034738
[21:59:32.726] iteration 3890 : loss: 0.019857, loss_a: 0.011681
[21:59:34.036] iteration 3891 : loss: 0.034069, loss_a: 0.020040
[21:59:34.775] iteration 3892 : loss: 0.043900, loss_a: 0.025824
[21:59:36.107] iteration 3893 : loss: 0.029670, loss_a: 0.017453
[21:59:36.851] iteration 3894 : loss: 0.057432, loss_a: 0.033784
[21:59:38.208] iteration 3895 : loss: 0.073236, loss_a: 0.043080
[21:59:38.950] iteration 3896 : loss: 0.021135, loss_a: 0.012432
[21:59:40.329] iteration 3897 : loss: 0.030823, loss_a: 0.018131
[21:59:41.068] iteration 3898 : loss: 0.029985, loss_a: 0.017638
[21:59:42.424] iteration 3899 : loss: 0.041169, loss_a: 0.024217
[21:59:43.161] iteration 3900 : loss: 0.029086, loss_a: 0.017109
[21:59:44.495] iteration 3901 : loss: 0.018046, loss_a: 0.010615
[21:59:45.238] iteration 3902 : loss: 0.026587, loss_a: 0.015639
[21:59:46.597] iteration 3903 : loss: 0.015655, loss_a: 0.009209
[21:59:47.336] iteration 3904 : loss: 0.021670, loss_a: 0.012747
[21:59:48.707] iteration 3905 : loss: 0.050968, loss_a: 0.029981
[21:59:49.459] iteration 3906 : loss: 0.029268, loss_a: 0.017216
[21:59:50.806] iteration 3907 : loss: 0.049949, loss_a: 0.029382
[21:59:51.551] iteration 3908 : loss: 0.030941, loss_a: 0.018201
[21:59:52.876] iteration 3909 : loss: 0.055315, loss_a: 0.032538
[21:59:53.614] iteration 3910 : loss: 0.027549, loss_a: 0.016205
[21:59:54.924] iteration 3911 : loss: 0.057306, loss_a: 0.033709
[21:59:55.663] iteration 3912 : loss: 0.310324, loss_a: 0.182544
[21:59:57.027] iteration 3913 : loss: 0.055637, loss_a: 0.032727
[21:59:57.767] iteration 3914 : loss: 0.035755, loss_a: 0.021032
[21:59:59.128] iteration 3915 : loss: 0.078997, loss_a: 0.046469
[21:59:59.863] iteration 3916 : loss: 0.041758, loss_a: 0.024564
[22:00:01.224] iteration 3917 : loss: 0.037903, loss_a: 0.022296
[22:00:01.971] iteration 3918 : loss: 0.038833, loss_a: 0.022843
[22:00:03.293] iteration 3919 : loss: 0.027810, loss_a: 0.016359
[22:00:04.056] iteration 3920 : loss: 0.054486, loss_a: 0.032050
[22:00:05.378] iteration 3921 : loss: 0.023244, loss_a: 0.013673
[22:00:06.118] iteration 3922 : loss: 0.034928, loss_a: 0.020546
[22:00:07.454] iteration 3923 : loss: 0.060122, loss_a: 0.035366
[22:00:08.194] iteration 3924 : loss: 0.044423, loss_a: 0.026131
[22:00:09.518] iteration 3925 : loss: 0.019935, loss_a: 0.011727
[22:00:10.260] iteration 3926 : loss: 0.038821, loss_a: 0.022836
[22:00:11.594] iteration 3927 : loss: 0.038417, loss_a: 0.022598
[22:00:12.326] iteration 3928 : loss: 0.027514, loss_a: 0.016185
[22:00:13.640] iteration 3929 : loss: 0.033160, loss_a: 0.019506
[22:00:14.384] iteration 3930 : loss: 0.029473, loss_a: 0.017337
[22:00:15.735] iteration 3931 : loss: 0.019097, loss_a: 0.011234
[22:00:16.486] iteration 3932 : loss: 0.032428, loss_a: 0.019075
[22:00:17.827] iteration 3933 : loss: 0.032560, loss_a: 0.019153
[22:00:18.563] iteration 3934 : loss: 0.036654, loss_a: 0.021561
[22:00:19.895] iteration 3935 : loss: 0.025212, loss_a: 0.014830
[22:00:20.633] iteration 3936 : loss: 0.024374, loss_a: 0.014338
[22:00:21.975] iteration 3937 : loss: 0.034972, loss_a: 0.020572
[22:00:22.706] iteration 3938 : loss: 0.042536, loss_a: 0.025021
[22:00:24.064] iteration 3939 : loss: 0.042138, loss_a: 0.024787
[22:00:24.803] iteration 3940 : loss: 0.032558, loss_a: 0.019152
[22:00:26.138] iteration 3941 : loss: 0.052685, loss_a: 0.030991
[22:00:26.880] iteration 3942 : loss: 0.060430, loss_a: 0.035547
[22:00:28.232] iteration 3943 : loss: 0.035121, loss_a: 0.020660
[22:00:28.966] iteration 3944 : loss: 0.054497, loss_a: 0.032057
[22:00:30.311] iteration 3945 : loss: 0.047311, loss_a: 0.027830
[22:00:31.051] iteration 3946 : loss: 0.059285, loss_a: 0.034874
[22:00:32.389] iteration 3947 : loss: 0.053417, loss_a: 0.031422
[22:00:33.134] iteration 3948 : loss: 0.069758, loss_a: 0.041034
[22:00:34.498] iteration 3949 : loss: 0.044788, loss_a: 0.026346
[22:00:35.235] iteration 3950 : loss: 0.066337, loss_a: 0.039022
[22:00:36.554] iteration 3951 : loss: 0.019289, loss_a: 0.011346
[22:00:37.291] iteration 3952 : loss: 0.039599, loss_a: 0.023293
[22:00:38.645] iteration 3953 : loss: 0.044597, loss_a: 0.026234
[22:00:39.395] iteration 3954 : loss: 0.041578, loss_a: 0.024458
[22:00:40.747] iteration 3955 : loss: 0.035304, loss_a: 0.020767
[22:00:41.500] iteration 3956 : loss: 0.031568, loss_a: 0.018570
[22:00:42.832] iteration 3957 : loss: 0.015627, loss_a: 0.009192
[22:00:43.562] iteration 3958 : loss: 0.016553, loss_a: 0.009737
[22:00:44.875] iteration 3959 : loss: 0.035259, loss_a: 0.020741
[22:00:45.610] iteration 3960 : loss: 0.051044, loss_a: 0.030026
[22:00:46.969] iteration 3961 : loss: 0.052051, loss_a: 0.030618
[22:00:47.717] iteration 3962 : loss: 0.106389, loss_a: 0.062582
[22:00:49.033] iteration 3963 : loss: 0.092333, loss_a: 0.054314
[22:00:49.768] iteration 3964 : loss: 0.041534, loss_a: 0.024432
[22:00:51.122] iteration 3965 : loss: 0.028896, loss_a: 0.016998
[22:00:51.867] iteration 3966 : loss: 0.086801, loss_a: 0.051059
[22:00:53.188] iteration 3967 : loss: 0.048244, loss_a: 0.028379
[22:00:53.925] iteration 3968 : loss: 0.061680, loss_a: 0.036283
[22:00:55.276] iteration 3969 : loss: 0.036410, loss_a: 0.021418
[22:00:56.020] iteration 3970 : loss: 0.022560, loss_a: 0.013271
[22:00:57.338] iteration 3971 : loss: 0.021317, loss_a: 0.012539
[22:00:58.094] iteration 3972 : loss: 0.039379, loss_a: 0.023164
[22:00:59.411] iteration 3973 : loss: 0.059028, loss_a: 0.034722
[22:01:00.145] iteration 3974 : loss: 0.019126, loss_a: 0.011251
[22:01:01.470] iteration 3975 : loss: 0.019608, loss_a: 0.011534
[22:01:02.220] iteration 3976 : loss: 0.043426, loss_a: 0.025544
[22:01:03.545] iteration 3977 : loss: 0.058289, loss_a: 0.034288
[22:01:04.287] iteration 3978 : loss: 0.042656, loss_a: 0.025092
[22:01:05.644] iteration 3979 : loss: 0.037406, loss_a: 0.022004
[22:01:06.373] iteration 3980 : loss: 0.034129, loss_a: 0.020076
[22:01:07.748] iteration 3981 : loss: 0.047683, loss_a: 0.028049
[22:01:08.481] iteration 3982 : loss: 0.036288, loss_a: 0.021346
[22:01:09.848] iteration 3983 : loss: 0.038199, loss_a: 0.022470
[22:01:10.581] iteration 3984 : loss: 0.054038, loss_a: 0.031787
[22:01:11.893] iteration 3985 : loss: 0.025316, loss_a: 0.014892
[22:01:12.649] iteration 3986 : loss: 0.045432, loss_a: 0.026725
[22:01:14.007] iteration 3987 : loss: 0.029654, loss_a: 0.017444
[22:01:14.739] iteration 3988 : loss: 0.022673, loss_a: 0.013337
[22:01:16.093] iteration 3989 : loss: 0.030928, loss_a: 0.018193
[22:01:16.846] iteration 3990 : loss: 0.036263, loss_a: 0.021331
[22:01:18.188] iteration 3991 : loss: 0.036813, loss_a: 0.021655
[22:01:18.925] iteration 3992 : loss: 0.040842, loss_a: 0.024025
[22:01:20.256] iteration 3993 : loss: 0.017288, loss_a: 0.010170
[22:01:21.006] iteration 3994 : loss: 0.037598, loss_a: 0.022116
[22:01:22.332] iteration 3995 : loss: 0.024976, loss_a: 0.014692
[22:01:23.074] iteration 3996 : loss: 0.042723, loss_a: 0.025131
[22:01:24.415] iteration 3997 : loss: 0.030468, loss_a: 0.017922
[22:01:25.157] iteration 3998 : loss: 0.057116, loss_a: 0.033598
[22:01:26.473] iteration 3999 : loss: 0.052873, loss_a: 0.031102
[22:01:27.219] iteration 4000 : loss: 0.034627, loss_a: 0.020369
[22:01:51.932] iteration 4001 : loss: 0.066376, loss_a: 0.039045
[22:01:54.114] iteration 4002 : loss: 0.033847, loss_a: 0.019910
[22:01:55.440] iteration 4003 : loss: 0.026205, loss_a: 0.015415
[22:01:56.195] iteration 4004 : loss: 0.035297, loss_a: 0.020763
[22:01:57.526] iteration 4005 : loss: 0.019839, loss_a: 0.011670
[22:01:58.289] iteration 4006 : loss: 0.080105, loss_a: 0.047120
[22:01:59.604] iteration 4007 : loss: 0.051897, loss_a: 0.030528
[22:02:00.353] iteration 4008 : loss: 0.057081, loss_a: 0.033577
[22:02:01.675] iteration 4009 : loss: 0.022415, loss_a: 0.013185
[22:02:02.420] iteration 4010 : loss: 0.021840, loss_a: 0.012847
[22:02:03.726] iteration 4011 : loss: 0.025972, loss_a: 0.015278
[22:02:04.465] iteration 4012 : loss: 0.017300, loss_a: 0.010176
[22:02:05.789] iteration 4013 : loss: 0.053796, loss_a: 0.031645
[22:02:06.529] iteration 4014 : loss: 0.026608, loss_a: 0.015652
[22:02:07.870] iteration 4015 : loss: 0.031926, loss_a: 0.018780
[22:02:08.616] iteration 4016 : loss: 0.054708, loss_a: 0.032181
[22:02:09.964] iteration 4017 : loss: 0.025938, loss_a: 0.015258
[22:02:10.707] iteration 4018 : loss: 0.021730, loss_a: 0.012782
[22:02:12.073] iteration 4019 : loss: 0.036793, loss_a: 0.021643
[22:02:12.817] iteration 4020 : loss: 0.032947, loss_a: 0.019381
[22:02:14.170] iteration 4021 : loss: 0.024705, loss_a: 0.014533
[22:02:14.925] iteration 4022 : loss: 0.067229, loss_a: 0.039546
[22:02:16.308] iteration 4023 : loss: 0.037574, loss_a: 0.022102
[22:02:17.045] iteration 4024 : loss: 0.022838, loss_a: 0.013434
[22:02:18.368] iteration 4025 : loss: 0.045277, loss_a: 0.026634
[22:02:19.116] iteration 4026 : loss: 0.025158, loss_a: 0.014799
[22:02:20.444] iteration 4027 : loss: 0.056159, loss_a: 0.033035
[22:02:21.196] iteration 4028 : loss: 0.080193, loss_a: 0.047173
[22:02:22.525] iteration 4029 : loss: 0.021739, loss_a: 0.012788
[22:02:23.269] iteration 4030 : loss: 0.035871, loss_a: 0.021101
[22:02:24.636] iteration 4031 : loss: 0.020942, loss_a: 0.012319
[22:02:25.386] iteration 4032 : loss: 0.026973, loss_a: 0.015866
[22:02:26.742] iteration 4033 : loss: 0.020127, loss_a: 0.011839
[22:02:27.470] iteration 4034 : loss: 0.023709, loss_a: 0.013947
[22:02:28.787] iteration 4035 : loss: 0.018380, loss_a: 0.010812
[22:02:29.525] iteration 4036 : loss: 0.022136, loss_a: 0.013021
[22:02:30.882] iteration 4037 : loss: 0.045538, loss_a: 0.026787
[22:02:31.614] iteration 4038 : loss: 0.026391, loss_a: 0.015524
[22:02:32.965] iteration 4039 : loss: 0.017171, loss_a: 0.010101
[22:02:33.707] iteration 4040 : loss: 0.038173, loss_a: 0.022455
[22:02:35.021] iteration 4041 : loss: 0.023044, loss_a: 0.013555
[22:02:35.756] iteration 4042 : loss: 0.038237, loss_a: 0.022493
[22:02:37.092] iteration 4043 : loss: 0.023735, loss_a: 0.013962
[22:02:37.826] iteration 4044 : loss: 0.022783, loss_a: 0.013402
[22:02:39.130] iteration 4045 : loss: 0.030307, loss_a: 0.017828
[22:02:39.882] iteration 4046 : loss: 0.024778, loss_a: 0.014576
[22:02:41.191] iteration 4047 : loss: 0.038381, loss_a: 0.022577
[22:02:41.950] iteration 4048 : loss: 0.046303, loss_a: 0.027237
[22:02:43.287] iteration 4049 : loss: 0.025666, loss_a: 0.015098
[22:02:44.030] iteration 4050 : loss: 0.036275, loss_a: 0.021338
[22:02:45.346] iteration 4051 : loss: 0.027219, loss_a: 0.016011
[22:02:46.087] iteration 4052 : loss: 0.050800, loss_a: 0.029882
[22:02:47.422] iteration 4053 : loss: 0.011775, loss_a: 0.006927
[22:02:48.168] iteration 4054 : loss: 0.078041, loss_a: 0.045906
[22:02:49.551] iteration 4055 : loss: 0.040546, loss_a: 0.023851
[22:02:50.284] iteration 4056 : loss: 0.049702, loss_a: 0.029236
[22:02:51.623] iteration 4057 : loss: 0.018576, loss_a: 0.010927
[22:02:52.367] iteration 4058 : loss: 0.025399, loss_a: 0.014941
[22:02:53.707] iteration 4059 : loss: 0.047273, loss_a: 0.027807
[22:02:54.442] iteration 4060 : loss: 0.022766, loss_a: 0.013392
[22:02:55.785] iteration 4061 : loss: 0.034987, loss_a: 0.020581
[22:02:56.521] iteration 4062 : loss: 0.026517, loss_a: 0.015598
[22:02:57.856] iteration 4063 : loss: 0.034833, loss_a: 0.020490
[22:02:58.592] iteration 4064 : loss: 0.049091, loss_a: 0.028877
[22:02:59.907] iteration 4065 : loss: 0.013847, loss_a: 0.008145
[22:03:00.657] iteration 4066 : loss: 0.029386, loss_a: 0.017286
[22:03:02.010] iteration 4067 : loss: 0.036358, loss_a: 0.021387
[22:03:02.736] iteration 4068 : loss: 0.022803, loss_a: 0.013413
[22:03:04.055] iteration 4069 : loss: 0.030091, loss_a: 0.017700
[22:03:04.795] iteration 4070 : loss: 0.026560, loss_a: 0.015623
[22:03:06.123] iteration 4071 : loss: 0.034759, loss_a: 0.020446
[22:03:06.864] iteration 4072 : loss: 0.025532, loss_a: 0.015019
[22:03:08.212] iteration 4073 : loss: 0.044434, loss_a: 0.026138
[22:03:08.951] iteration 4074 : loss: 0.054743, loss_a: 0.032202
[22:03:10.290] iteration 4075 : loss: 0.063488, loss_a: 0.037346
[22:03:11.029] iteration 4076 : loss: 0.034736, loss_a: 0.020433
[22:03:12.344] iteration 4077 : loss: 0.019342, loss_a: 0.011378
[22:03:13.082] iteration 4078 : loss: 0.017566, loss_a: 0.010333
[22:03:14.407] iteration 4079 : loss: 0.028082, loss_a: 0.016519
[22:03:15.143] iteration 4080 : loss: 0.021206, loss_a: 0.012474
[22:03:16.485] iteration 4081 : loss: 0.026885, loss_a: 0.015815
[22:03:17.218] iteration 4082 : loss: 0.027358, loss_a: 0.016093
[22:03:18.572] iteration 4083 : loss: 0.032748, loss_a: 0.019264
[22:03:19.310] iteration 4084 : loss: 0.033027, loss_a: 0.019427
[22:03:20.637] iteration 4085 : loss: 0.024191, loss_a: 0.014230
[22:03:21.397] iteration 4086 : loss: 0.071760, loss_a: 0.042212
[22:03:22.738] iteration 4087 : loss: 0.068509, loss_a: 0.040299
[22:03:23.474] iteration 4088 : loss: 0.031014, loss_a: 0.018244
[22:03:24.793] iteration 4089 : loss: 0.044437, loss_a: 0.026139
[22:03:25.523] iteration 4090 : loss: 0.017478, loss_a: 0.010281
[22:03:26.886] iteration 4091 : loss: 0.042353, loss_a: 0.024914
[22:03:27.638] iteration 4092 : loss: 0.037641, loss_a: 0.022142
[22:03:28.994] iteration 4093 : loss: 0.033745, loss_a: 0.019850
[22:03:29.727] iteration 4094 : loss: 0.020984, loss_a: 0.012343
[22:03:31.065] iteration 4095 : loss: 0.032858, loss_a: 0.019328
[22:03:31.828] iteration 4096 : loss: 0.028234, loss_a: 0.016608
[22:03:33.122] iteration 4097 : loss: 0.024494, loss_a: 0.014408
[22:03:33.887] iteration 4098 : loss: 0.024571, loss_a: 0.014454
[22:03:35.211] iteration 4099 : loss: 0.030986, loss_a: 0.018227
[22:03:35.956] iteration 4100 : loss: 0.038145, loss_a: 0.022438
[22:03:37.310] iteration 4101 : loss: 0.041806, loss_a: 0.024592
[22:03:38.044] iteration 4102 : loss: 0.033611, loss_a: 0.019771
[22:03:39.377] iteration 4103 : loss: 0.030851, loss_a: 0.018148
[22:03:40.118] iteration 4104 : loss: 0.047159, loss_a: 0.027741
[22:03:41.452] iteration 4105 : loss: 0.019120, loss_a: 0.011247
[22:03:42.231] iteration 4106 : loss: 0.021118, loss_a: 0.012423
[22:03:43.574] iteration 4107 : loss: 0.035612, loss_a: 0.020948
[22:03:44.309] iteration 4108 : loss: 0.034701, loss_a: 0.020413
[22:03:45.638] iteration 4109 : loss: 0.028660, loss_a: 0.016859
[22:03:46.375] iteration 4110 : loss: 0.045328, loss_a: 0.026664
[22:03:47.698] iteration 4111 : loss: 0.021135, loss_a: 0.012433
[22:03:48.441] iteration 4112 : loss: 0.030717, loss_a: 0.018069
[22:03:49.771] iteration 4113 : loss: 0.046867, loss_a: 0.027569
[22:03:50.511] iteration 4114 : loss: 0.033860, loss_a: 0.019917
[22:03:51.851] iteration 4115 : loss: 0.069783, loss_a: 0.041049
[22:03:52.587] iteration 4116 : loss: 0.025833, loss_a: 0.015196
[22:03:53.942] iteration 4117 : loss: 0.029460, loss_a: 0.017330
[22:03:54.683] iteration 4118 : loss: 0.075151, loss_a: 0.044207
[22:03:56.037] iteration 4119 : loss: 0.059443, loss_a: 0.034966
[22:03:56.781] iteration 4120 : loss: 0.060163, loss_a: 0.035390
[22:03:58.126] iteration 4121 : loss: 0.043491, loss_a: 0.025583
[22:03:58.863] iteration 4122 : loss: 0.033498, loss_a: 0.019705
[22:04:00.201] iteration 4123 : loss: 0.020030, loss_a: 0.011782
[22:04:00.953] iteration 4124 : loss: 0.060554, loss_a: 0.035620
[22:04:02.300] iteration 4125 : loss: 0.051792, loss_a: 0.030466
[22:04:03.030] iteration 4126 : loss: 0.035249, loss_a: 0.020735
[22:04:04.354] iteration 4127 : loss: 0.028067, loss_a: 0.016510
[22:04:05.094] iteration 4128 : loss: 0.025093, loss_a: 0.014761
[22:04:06.448] iteration 4129 : loss: 0.026868, loss_a: 0.015805
[22:04:07.198] iteration 4130 : loss: 0.034086, loss_a: 0.020051
[22:04:08.510] iteration 4131 : loss: 0.016958, loss_a: 0.009975
[22:04:09.248] iteration 4132 : loss: 0.023124, loss_a: 0.013603
[22:04:10.584] iteration 4133 : loss: 0.062736, loss_a: 0.036903
[22:04:11.336] iteration 4134 : loss: 0.053352, loss_a: 0.031384
[22:04:12.680] iteration 4135 : loss: 0.036684, loss_a: 0.021579
[22:04:13.417] iteration 4136 : loss: 0.059633, loss_a: 0.035078
[22:04:14.748] iteration 4137 : loss: 0.026245, loss_a: 0.015438
[22:04:15.499] iteration 4138 : loss: 0.049141, loss_a: 0.028907
[22:04:16.828] iteration 4139 : loss: 0.029134, loss_a: 0.017138
[22:04:17.589] iteration 4140 : loss: 0.046341, loss_a: 0.027260
[22:04:18.943] iteration 4141 : loss: 0.057074, loss_a: 0.033573
[22:04:19.688] iteration 4142 : loss: 0.052525, loss_a: 0.030897
[22:04:21.054] iteration 4143 : loss: 0.057101, loss_a: 0.033589
[22:04:21.789] iteration 4144 : loss: 0.025097, loss_a: 0.014763
[22:04:23.127] iteration 4145 : loss: 0.046638, loss_a: 0.027434
[22:04:23.870] iteration 4146 : loss: 0.036612, loss_a: 0.021536
[22:04:25.208] iteration 4147 : loss: 0.050275, loss_a: 0.029574
[22:04:25.960] iteration 4148 : loss: 0.032670, loss_a: 0.019217
[22:04:27.293] iteration 4149 : loss: 0.032598, loss_a: 0.019175
[22:04:28.045] iteration 4150 : loss: 0.080718, loss_a: 0.047481
[22:04:29.403] iteration 4151 : loss: 0.041774, loss_a: 0.024573
[22:04:30.145] iteration 4152 : loss: 0.065828, loss_a: 0.038722
[22:04:31.475] iteration 4153 : loss: 0.034594, loss_a: 0.020350
[22:04:32.211] iteration 4154 : loss: 0.037582, loss_a: 0.022107
[22:04:33.561] iteration 4155 : loss: 0.039860, loss_a: 0.023447
[22:04:34.298] iteration 4156 : loss: 0.075049, loss_a: 0.044146
[22:04:35.627] iteration 4157 : loss: 0.033977, loss_a: 0.019986
[22:04:36.369] iteration 4158 : loss: 0.093790, loss_a: 0.055170
[22:04:37.721] iteration 4159 : loss: 0.030907, loss_a: 0.018181
[22:04:38.460] iteration 4160 : loss: 0.055029, loss_a: 0.032370
[22:04:39.821] iteration 4161 : loss: 0.035244, loss_a: 0.020732
[22:04:40.558] iteration 4162 : loss: 0.082323, loss_a: 0.048425
[22:04:41.880] iteration 4163 : loss: 0.036765, loss_a: 0.021627
[22:04:42.614] iteration 4164 : loss: 0.080371, loss_a: 0.047277
[22:04:43.998] iteration 4165 : loss: 0.063783, loss_a: 0.037519
[22:04:44.741] iteration 4166 : loss: 0.036506, loss_a: 0.021474
[22:04:46.082] iteration 4167 : loss: 0.026601, loss_a: 0.015648
[22:04:46.823] iteration 4168 : loss: 0.043443, loss_a: 0.025554
[22:04:48.192] iteration 4169 : loss: 0.080622, loss_a: 0.047425
[22:04:48.957] iteration 4170 : loss: 0.049023, loss_a: 0.028837
[22:04:50.303] iteration 4171 : loss: 0.038418, loss_a: 0.022599
[22:04:51.044] iteration 4172 : loss: 0.023363, loss_a: 0.013743
[22:04:52.413] iteration 4173 : loss: 0.041902, loss_a: 0.024648
[22:04:53.152] iteration 4174 : loss: 0.055320, loss_a: 0.032541
[22:04:54.518] iteration 4175 : loss: 0.054898, loss_a: 0.032293
[22:04:55.264] iteration 4176 : loss: 0.019825, loss_a: 0.011662
[22:04:56.614] iteration 4177 : loss: 0.041727, loss_a: 0.024545
[22:04:57.350] iteration 4178 : loss: 0.062959, loss_a: 0.037035
[22:04:58.692] iteration 4179 : loss: 0.033296, loss_a: 0.019586
[22:04:59.434] iteration 4180 : loss: 0.047469, loss_a: 0.027923
[22:05:00.762] iteration 4181 : loss: 0.016535, loss_a: 0.009727
[22:05:01.498] iteration 4182 : loss: 0.046139, loss_a: 0.027140
[22:05:02.823] iteration 4183 : loss: 0.042512, loss_a: 0.025007
[22:05:03.567] iteration 4184 : loss: 0.038638, loss_a: 0.022728
[22:05:04.891] iteration 4185 : loss: 0.023735, loss_a: 0.013962
[22:05:05.635] iteration 4186 : loss: 0.044029, loss_a: 0.025899
[22:05:06.997] iteration 4187 : loss: 0.021499, loss_a: 0.012646
[22:05:07.743] iteration 4188 : loss: 0.018322, loss_a: 0.010778
[22:05:09.055] iteration 4189 : loss: 0.049050, loss_a: 0.028853
[22:05:09.800] iteration 4190 : loss: 0.035434, loss_a: 0.020844
[22:05:11.140] iteration 4191 : loss: 0.023699, loss_a: 0.013940
[22:05:11.877] iteration 4192 : loss: 0.021353, loss_a: 0.012560
[22:05:13.224] iteration 4193 : loss: 0.071499, loss_a: 0.042058
[22:05:13.975] iteration 4194 : loss: 0.015807, loss_a: 0.009298
[22:05:15.321] iteration 4195 : loss: 0.038886, loss_a: 0.022874
[22:05:16.063] iteration 4196 : loss: 0.052094, loss_a: 0.030643
[22:05:17.408] iteration 4197 : loss: 0.044150, loss_a: 0.025970
[22:05:18.147] iteration 4198 : loss: 0.054910, loss_a: 0.032300
[22:05:19.446] iteration 4199 : loss: 0.024650, loss_a: 0.014500
[22:05:20.191] iteration 4200 : loss: 0.035240, loss_a: 0.020729
[22:05:44.850] iteration 4201 : loss: 0.036619, loss_a: 0.021541
[22:05:47.074] iteration 4202 : loss: 0.030413, loss_a: 0.017890
[22:05:48.424] iteration 4203 : loss: 0.043097, loss_a: 0.025351
[22:05:49.179] iteration 4204 : loss: 0.028066, loss_a: 0.016509
[22:05:50.506] iteration 4205 : loss: 0.039050, loss_a: 0.022971
[22:05:51.260] iteration 4206 : loss: 0.046071, loss_a: 0.027101
[22:05:52.604] iteration 4207 : loss: 0.033116, loss_a: 0.019480
[22:05:53.344] iteration 4208 : loss: 0.028453, loss_a: 0.016737
[22:05:54.656] iteration 4209 : loss: 0.041637, loss_a: 0.024492
[22:05:55.400] iteration 4210 : loss: 0.026682, loss_a: 0.015695
[22:05:56.744] iteration 4211 : loss: 0.051805, loss_a: 0.030474
[22:05:57.483] iteration 4212 : loss: 0.020740, loss_a: 0.012200
[22:05:58.839] iteration 4213 : loss: 0.046950, loss_a: 0.027618
[22:05:59.579] iteration 4214 : loss: 0.036883, loss_a: 0.021696
[22:06:00.923] iteration 4215 : loss: 0.044793, loss_a: 0.026349
[22:06:01.666] iteration 4216 : loss: 0.065838, loss_a: 0.038728
[22:06:03.028] iteration 4217 : loss: 0.034778, loss_a: 0.020458
[22:06:03.768] iteration 4218 : loss: 0.046236, loss_a: 0.027197
[22:06:05.145] iteration 4219 : loss: 0.056349, loss_a: 0.033147
[22:06:05.900] iteration 4220 : loss: 0.051256, loss_a: 0.030151
[22:06:07.244] iteration 4221 : loss: 0.039642, loss_a: 0.023319
[22:06:07.989] iteration 4222 : loss: 0.018060, loss_a: 0.010623
[22:06:09.324] iteration 4223 : loss: 0.058217, loss_a: 0.034245
[22:06:10.069] iteration 4224 : loss: 0.025872, loss_a: 0.015219
[22:06:11.374] iteration 4225 : loss: 0.039733, loss_a: 0.023372
[22:06:12.132] iteration 4226 : loss: 0.058990, loss_a: 0.034700
[22:06:13.487] iteration 4227 : loss: 0.049449, loss_a: 0.029087
[22:06:14.225] iteration 4228 : loss: 0.020961, loss_a: 0.012330
[22:06:15.541] iteration 4229 : loss: 0.064095, loss_a: 0.037703
[22:06:16.286] iteration 4230 : loss: 0.023116, loss_a: 0.013598
[22:06:17.631] iteration 4231 : loss: 0.067554, loss_a: 0.039737
[22:06:18.380] iteration 4232 : loss: 0.031079, loss_a: 0.018282
[22:06:19.730] iteration 4233 : loss: 0.027102, loss_a: 0.015942
[22:06:20.475] iteration 4234 : loss: 0.028935, loss_a: 0.017020
[22:06:21.828] iteration 4235 : loss: 0.037793, loss_a: 0.022231
[22:06:22.574] iteration 4236 : loss: 0.045608, loss_a: 0.026829
[22:06:23.930] iteration 4237 : loss: 0.056777, loss_a: 0.033398
[22:06:24.670] iteration 4238 : loss: 0.027530, loss_a: 0.016194
[22:06:25.995] iteration 4239 : loss: 0.072410, loss_a: 0.042594
[22:06:26.748] iteration 4240 : loss: 0.066171, loss_a: 0.038924
[22:06:28.077] iteration 4241 : loss: 0.024730, loss_a: 0.014547
[22:06:28.825] iteration 4242 : loss: 0.032245, loss_a: 0.018968
[22:06:30.146] iteration 4243 : loss: 0.053581, loss_a: 0.031518
[22:06:30.880] iteration 4244 : loss: 0.020074, loss_a: 0.011808
[22:06:32.230] iteration 4245 : loss: 0.022295, loss_a: 0.013115
[22:06:32.965] iteration 4246 : loss: 0.021454, loss_a: 0.012620
[22:06:34.321] iteration 4247 : loss: 0.039864, loss_a: 0.023449
[22:06:35.055] iteration 4248 : loss: 0.037078, loss_a: 0.021811
[22:06:36.395] iteration 4249 : loss: 0.053586, loss_a: 0.031521
[22:06:37.135] iteration 4250 : loss: 0.013469, loss_a: 0.007923
[22:06:38.480] iteration 4251 : loss: 0.129403, loss_a: 0.076119
[22:06:39.227] iteration 4252 : loss: 0.030158, loss_a: 0.017740
[22:06:40.571] iteration 4253 : loss: 0.030239, loss_a: 0.017788
[22:06:41.312] iteration 4254 : loss: 0.062214, loss_a: 0.036596
[22:06:42.662] iteration 4255 : loss: 0.047778, loss_a: 0.028105
[22:06:43.415] iteration 4256 : loss: 0.099589, loss_a: 0.058582
[22:06:44.772] iteration 4257 : loss: 0.062742, loss_a: 0.036907
[22:06:45.514] iteration 4258 : loss: 0.049261, loss_a: 0.028977
[22:06:46.884] iteration 4259 : loss: 0.080665, loss_a: 0.047450
[22:06:47.632] iteration 4260 : loss: 0.050218, loss_a: 0.029540
[22:06:48.967] iteration 4261 : loss: 0.047494, loss_a: 0.027938
[22:06:49.721] iteration 4262 : loss: 0.038729, loss_a: 0.022782
[22:06:51.058] iteration 4263 : loss: 0.032693, loss_a: 0.019231
[22:06:51.808] iteration 4264 : loss: 0.047100, loss_a: 0.027706
[22:06:53.125] iteration 4265 : loss: 0.027477, loss_a: 0.016163
[22:06:53.872] iteration 4266 : loss: 0.065322, loss_a: 0.038425
[22:06:55.199] iteration 4267 : loss: 0.055396, loss_a: 0.032586
[22:06:55.943] iteration 4268 : loss: 0.039366, loss_a: 0.023156
[22:06:57.271] iteration 4269 : loss: 0.038804, loss_a: 0.022826
[22:06:58.018] iteration 4270 : loss: 0.039242, loss_a: 0.023083
[22:06:59.347] iteration 4271 : loss: 0.075016, loss_a: 0.044127
[22:07:00.090] iteration 4272 : loss: 0.039559, loss_a: 0.023270
[22:07:01.448] iteration 4273 : loss: 0.034798, loss_a: 0.020470
[22:07:02.191] iteration 4274 : loss: 0.028598, loss_a: 0.016822
[22:07:03.515] iteration 4275 : loss: 0.059683, loss_a: 0.035108
[22:07:04.259] iteration 4276 : loss: 0.044466, loss_a: 0.026156
[22:07:05.589] iteration 4277 : loss: 0.062610, loss_a: 0.036829
[22:07:06.325] iteration 4278 : loss: 0.034531, loss_a: 0.020312
[22:07:07.674] iteration 4279 : loss: 0.071842, loss_a: 0.042260
[22:07:08.417] iteration 4280 : loss: 0.071312, loss_a: 0.041948
[22:07:09.741] iteration 4281 : loss: 0.059269, loss_a: 0.034864
[22:07:10.478] iteration 4282 : loss: 0.074641, loss_a: 0.043906
[22:07:11.833] iteration 4283 : loss: 0.025138, loss_a: 0.014787
[22:07:12.583] iteration 4284 : loss: 0.036811, loss_a: 0.021654
[22:07:13.942] iteration 4285 : loss: 0.082993, loss_a: 0.048819
[22:07:14.699] iteration 4286 : loss: 0.029375, loss_a: 0.017280
[22:07:16.038] iteration 4287 : loss: 0.047637, loss_a: 0.028022
[22:07:16.774] iteration 4288 : loss: 0.016946, loss_a: 0.009968
[22:07:18.125] iteration 4289 : loss: 0.071700, loss_a: 0.042176
[22:07:18.865] iteration 4290 : loss: 0.025935, loss_a: 0.015256
[22:07:20.189] iteration 4291 : loss: 0.037885, loss_a: 0.022286
[22:07:20.928] iteration 4292 : loss: 0.062929, loss_a: 0.037017
[22:07:22.288] iteration 4293 : loss: 0.147417, loss_a: 0.086716
[22:07:23.040] iteration 4294 : loss: 0.037897, loss_a: 0.022292
[22:07:24.399] iteration 4295 : loss: 0.037335, loss_a: 0.021962
[22:07:25.140] iteration 4296 : loss: 0.029577, loss_a: 0.017398
[22:07:26.478] iteration 4297 : loss: 0.028701, loss_a: 0.016883
[22:07:27.214] iteration 4298 : loss: 0.026009, loss_a: 0.015299
[22:07:28.575] iteration 4299 : loss: 0.042419, loss_a: 0.024952
[22:07:29.307] iteration 4300 : loss: 0.053551, loss_a: 0.031501
[22:07:30.626] iteration 4301 : loss: 0.048330, loss_a: 0.028430
[22:07:31.359] iteration 4302 : loss: 0.022179, loss_a: 0.013046
[22:07:32.720] iteration 4303 : loss: 0.036047, loss_a: 0.021204
[22:07:33.456] iteration 4304 : loss: 0.050810, loss_a: 0.029888
[22:07:34.812] iteration 4305 : loss: 0.032537, loss_a: 0.019139
[22:07:35.553] iteration 4306 : loss: 0.030572, loss_a: 0.017984
[22:07:36.897] iteration 4307 : loss: 0.024277, loss_a: 0.014280
[22:07:37.630] iteration 4308 : loss: 0.041360, loss_a: 0.024330
[22:07:38.956] iteration 4309 : loss: 0.044236, loss_a: 0.026021
[22:07:39.700] iteration 4310 : loss: 0.022750, loss_a: 0.013382
[22:07:41.050] iteration 4311 : loss: 0.050963, loss_a: 0.029978
[22:07:41.781] iteration 4312 : loss: 0.030148, loss_a: 0.017734
[22:07:43.113] iteration 4313 : loss: 0.032603, loss_a: 0.019178
[22:07:43.856] iteration 4314 : loss: 0.040551, loss_a: 0.023854
[22:07:45.195] iteration 4315 : loss: 0.016845, loss_a: 0.009909
[22:07:45.935] iteration 4316 : loss: 0.042173, loss_a: 0.024807
[22:07:47.294] iteration 4317 : loss: 0.023547, loss_a: 0.013851
[22:07:48.030] iteration 4318 : loss: 0.017572, loss_a: 0.010337
[22:07:49.380] iteration 4319 : loss: 0.027108, loss_a: 0.015946
[22:07:50.130] iteration 4320 : loss: 0.068316, loss_a: 0.040186
[22:07:51.525] iteration 4321 : loss: 0.034936, loss_a: 0.020550
[22:07:52.271] iteration 4322 : loss: 0.018439, loss_a: 0.010847
[22:07:53.612] iteration 4323 : loss: 0.039278, loss_a: 0.023105
[22:07:54.372] iteration 4324 : loss: 0.033961, loss_a: 0.019977
[22:07:55.723] iteration 4325 : loss: 0.026597, loss_a: 0.015645
[22:07:56.459] iteration 4326 : loss: 0.033244, loss_a: 0.019556
[22:07:57.804] iteration 4327 : loss: 0.036153, loss_a: 0.021266
[22:07:58.540] iteration 4328 : loss: 0.020660, loss_a: 0.012153
[22:07:59.893] iteration 4329 : loss: 0.048658, loss_a: 0.028622
[22:08:00.633] iteration 4330 : loss: 0.034725, loss_a: 0.020426
[22:08:01.992] iteration 4331 : loss: 0.026693, loss_a: 0.015702
[22:08:02.739] iteration 4332 : loss: 0.046319, loss_a: 0.027247
[22:08:04.106] iteration 4333 : loss: 0.033547, loss_a: 0.019734
[22:08:04.839] iteration 4334 : loss: 0.031149, loss_a: 0.018323
[22:08:06.161] iteration 4335 : loss: 0.031709, loss_a: 0.018652
[22:08:06.902] iteration 4336 : loss: 0.031127, loss_a: 0.018310
[22:08:08.232] iteration 4337 : loss: 0.051746, loss_a: 0.030439
[22:08:08.962] iteration 4338 : loss: 0.024834, loss_a: 0.014608
[22:08:10.328] iteration 4339 : loss: 0.037953, loss_a: 0.022325
[22:08:11.071] iteration 4340 : loss: 0.040217, loss_a: 0.023657
[22:08:12.400] iteration 4341 : loss: 0.041957, loss_a: 0.024681
[22:08:13.182] iteration 4342 : loss: 0.462124, loss_a: 0.271838
[22:08:14.512] iteration 4343 : loss: 0.030011, loss_a: 0.017653
[22:08:15.254] iteration 4344 : loss: 0.040392, loss_a: 0.023760
[22:08:16.597] iteration 4345 : loss: 0.043162, loss_a: 0.025389
[22:08:17.330] iteration 4346 : loss: 0.025140, loss_a: 0.014788
[22:08:18.659] iteration 4347 : loss: 0.021142, loss_a: 0.012436
[22:08:19.399] iteration 4348 : loss: 0.063221, loss_a: 0.037189
[22:08:20.747] iteration 4349 : loss: 0.038090, loss_a: 0.022406
[22:08:21.494] iteration 4350 : loss: 0.044396, loss_a: 0.026115
[22:08:22.808] iteration 4351 : loss: 0.018266, loss_a: 0.010745
[22:08:23.547] iteration 4352 : loss: 0.028274, loss_a: 0.016632
[22:08:24.858] iteration 4353 : loss: 0.029566, loss_a: 0.017392
[22:08:25.598] iteration 4354 : loss: 0.026545, loss_a: 0.015614
[22:08:26.919] iteration 4355 : loss: 0.019898, loss_a: 0.011705
[22:08:27.657] iteration 4356 : loss: 0.043604, loss_a: 0.025649
[22:08:29.009] iteration 4357 : loss: 0.030327, loss_a: 0.017840
[22:08:29.743] iteration 4358 : loss: 0.040052, loss_a: 0.023560
[22:08:31.084] iteration 4359 : loss: 0.024859, loss_a: 0.014623
[22:08:31.829] iteration 4360 : loss: 0.039429, loss_a: 0.023193
[22:08:33.142] iteration 4361 : loss: 0.019899, loss_a: 0.011705
[22:08:33.888] iteration 4362 : loss: 0.059244, loss_a: 0.034849
[22:08:35.233] iteration 4363 : loss: 0.025134, loss_a: 0.014785
[22:08:35.976] iteration 4364 : loss: 0.027570, loss_a: 0.016218
[22:08:37.304] iteration 4365 : loss: 0.016586, loss_a: 0.009757
[22:08:38.037] iteration 4366 : loss: 0.054156, loss_a: 0.031856
[22:08:39.368] iteration 4367 : loss: 0.025908, loss_a: 0.015240
[22:08:40.107] iteration 4368 : loss: 0.046835, loss_a: 0.027550
[22:08:41.467] iteration 4369 : loss: 0.033753, loss_a: 0.019854
[22:08:42.213] iteration 4370 : loss: 0.033122, loss_a: 0.019483
[22:08:43.535] iteration 4371 : loss: 0.031326, loss_a: 0.018427
[22:08:44.285] iteration 4372 : loss: 0.029377, loss_a: 0.017281
[22:08:45.604] iteration 4373 : loss: 0.039157, loss_a: 0.023033
[22:08:46.346] iteration 4374 : loss: 0.051183, loss_a: 0.030108
[22:08:47.699] iteration 4375 : loss: 0.024538, loss_a: 0.014434
[22:08:48.435] iteration 4376 : loss: 0.033448, loss_a: 0.019675
[22:08:49.749] iteration 4377 : loss: 0.035172, loss_a: 0.020689
[22:08:50.494] iteration 4378 : loss: 0.029821, loss_a: 0.017542
[22:08:51.819] iteration 4379 : loss: 0.050698, loss_a: 0.029822
[22:08:52.557] iteration 4380 : loss: 0.021327, loss_a: 0.012545
[22:08:53.899] iteration 4381 : loss: 0.022367, loss_a: 0.013157
[22:08:54.637] iteration 4382 : loss: 0.044553, loss_a: 0.026208
[22:08:55.998] iteration 4383 : loss: 0.044305, loss_a: 0.026062
[22:08:56.745] iteration 4384 : loss: 0.087477, loss_a: 0.051457
[22:08:58.074] iteration 4385 : loss: 0.045283, loss_a: 0.026637
[22:08:58.815] iteration 4386 : loss: 0.017824, loss_a: 0.010485
[22:09:00.144] iteration 4387 : loss: 0.028583, loss_a: 0.016813
[22:09:00.888] iteration 4388 : loss: 0.021778, loss_a: 0.012811
[22:09:02.211] iteration 4389 : loss: 0.029899, loss_a: 0.017588
[22:09:02.964] iteration 4390 : loss: 0.031261, loss_a: 0.018389
[22:09:04.328] iteration 4391 : loss: 0.051780, loss_a: 0.030459
[22:09:05.079] iteration 4392 : loss: 0.024997, loss_a: 0.014704
[22:09:06.402] iteration 4393 : loss: 0.026437, loss_a: 0.015551
[22:09:07.144] iteration 4394 : loss: 0.028030, loss_a: 0.016488
[22:09:08.470] iteration 4395 : loss: 0.018072, loss_a: 0.010631
[22:09:09.225] iteration 4396 : loss: 0.087085, loss_a: 0.051226
[22:09:10.568] iteration 4397 : loss: 0.021798, loss_a: 0.012822
[22:09:11.307] iteration 4398 : loss: 0.023797, loss_a: 0.013998
[22:09:12.646] iteration 4399 : loss: 0.033789, loss_a: 0.019876
[22:09:13.389] iteration 4400 : loss: 0.042068, loss_a: 0.024746
[22:09:38.080] iteration 4401 : loss: 0.026129, loss_a: 0.015370
[22:09:40.423] iteration 4402 : loss: 0.016270, loss_a: 0.009571
[22:09:41.738] iteration 4403 : loss: 0.083997, loss_a: 0.049410
[22:09:42.471] iteration 4404 : loss: 0.023993, loss_a: 0.014114
[22:09:43.810] iteration 4405 : loss: 0.025433, loss_a: 0.014961
[22:09:44.556] iteration 4406 : loss: 0.057722, loss_a: 0.033954
[22:09:45.909] iteration 4407 : loss: 0.056936, loss_a: 0.033492
[22:09:46.657] iteration 4408 : loss: 0.031000, loss_a: 0.018235
[22:09:48.004] iteration 4409 : loss: 0.061010, loss_a: 0.035888
[22:09:48.737] iteration 4410 : loss: 0.017779, loss_a: 0.010459
[22:09:50.086] iteration 4411 : loss: 0.052854, loss_a: 0.031090
[22:09:50.827] iteration 4412 : loss: 0.037359, loss_a: 0.021976
[22:09:52.162] iteration 4413 : loss: 0.041252, loss_a: 0.024266
[22:09:52.896] iteration 4414 : loss: 0.037689, loss_a: 0.022170
[22:09:54.227] iteration 4415 : loss: 0.049151, loss_a: 0.028913
[22:09:54.967] iteration 4416 : loss: 0.043861, loss_a: 0.025800
[22:09:56.293] iteration 4417 : loss: 0.031473, loss_a: 0.018513
[22:09:57.031] iteration 4418 : loss: 0.024465, loss_a: 0.014391
[22:09:58.378] iteration 4419 : loss: 0.030352, loss_a: 0.017854
[22:09:59.113] iteration 4420 : loss: 0.296342, loss_a: 0.174319
[22:10:00.472] iteration 4421 : loss: 0.046404, loss_a: 0.027296
[22:10:01.217] iteration 4422 : loss: 0.035695, loss_a: 0.020997
[22:10:02.522] iteration 4423 : loss: 0.012379, loss_a: 0.007282
[22:10:03.268] iteration 4424 : loss: 0.039749, loss_a: 0.023382
[22:10:04.603] iteration 4425 : loss: 0.047792, loss_a: 0.028113
[22:10:05.343] iteration 4426 : loss: 0.030091, loss_a: 0.017700
[22:10:06.688] iteration 4427 : loss: 0.019904, loss_a: 0.011708
[22:10:07.429] iteration 4428 : loss: 0.032133, loss_a: 0.018902
[22:10:08.753] iteration 4429 : loss: 0.019414, loss_a: 0.011420
[22:10:09.494] iteration 4430 : loss: 0.028182, loss_a: 0.016578
[22:10:10.842] iteration 4431 : loss: 0.021962, loss_a: 0.012919
[22:10:11.580] iteration 4432 : loss: 0.051693, loss_a: 0.030407
[22:10:12.952] iteration 4433 : loss: 0.027302, loss_a: 0.016060
[22:10:13.696] iteration 4434 : loss: 0.038889, loss_a: 0.022876
[22:10:15.034] iteration 4435 : loss: 0.051104, loss_a: 0.030061
[22:10:15.775] iteration 4436 : loss: 0.059251, loss_a: 0.034854
[22:10:17.090] iteration 4437 : loss: 0.048229, loss_a: 0.028370
[22:10:17.837] iteration 4438 : loss: 0.022688, loss_a: 0.013346
[22:10:19.164] iteration 4439 : loss: 0.038083, loss_a: 0.022402
[22:10:19.915] iteration 4440 : loss: 0.027747, loss_a: 0.016321
[22:10:21.260] iteration 4441 : loss: 0.029334, loss_a: 0.017255
[22:10:21.997] iteration 4442 : loss: 0.028193, loss_a: 0.016584
[22:10:23.326] iteration 4443 : loss: 0.025114, loss_a: 0.014773
[22:10:24.060] iteration 4444 : loss: 0.049555, loss_a: 0.029150
[22:10:25.376] iteration 4445 : loss: 0.028441, loss_a: 0.016730
[22:10:26.117] iteration 4446 : loss: 0.042850, loss_a: 0.025206
[22:10:27.430] iteration 4447 : loss: 0.045534, loss_a: 0.026785
[22:10:28.164] iteration 4448 : loss: 0.033790, loss_a: 0.019876
[22:10:29.480] iteration 4449 : loss: 0.022467, loss_a: 0.013216
[22:10:30.221] iteration 4450 : loss: 0.035643, loss_a: 0.020966
[22:10:31.567] iteration 4451 : loss: 0.055721, loss_a: 0.032777
[22:10:32.303] iteration 4452 : loss: 0.021928, loss_a: 0.012899
[22:10:33.660] iteration 4453 : loss: 0.062481, loss_a: 0.036754
[22:10:34.402] iteration 4454 : loss: 0.034611, loss_a: 0.020359
[22:10:35.728] iteration 4455 : loss: 0.049135, loss_a: 0.028903
[22:10:36.464] iteration 4456 : loss: 0.037977, loss_a: 0.022339
[22:10:37.798] iteration 4457 : loss: 0.027662, loss_a: 0.016272
[22:10:38.540] iteration 4458 : loss: 0.033173, loss_a: 0.019514
[22:10:39.904] iteration 4459 : loss: 0.091961, loss_a: 0.054095
[22:10:40.655] iteration 4460 : loss: 0.049056, loss_a: 0.028856
[22:10:41.960] iteration 4461 : loss: 0.047370, loss_a: 0.027865
[22:10:42.710] iteration 4462 : loss: 0.043678, loss_a: 0.025693
[22:10:44.041] iteration 4463 : loss: 0.046791, loss_a: 0.027524
[22:10:44.786] iteration 4464 : loss: 0.030328, loss_a: 0.017840
[22:10:46.125] iteration 4465 : loss: 0.044530, loss_a: 0.026194
[22:10:46.869] iteration 4466 : loss: 0.036539, loss_a: 0.021493
[22:10:48.230] iteration 4467 : loss: 0.058013, loss_a: 0.034125
[22:10:48.971] iteration 4468 : loss: 0.036759, loss_a: 0.021623
[22:10:50.308] iteration 4469 : loss: 0.033463, loss_a: 0.019684
[22:10:51.059] iteration 4470 : loss: 0.049131, loss_a: 0.028901
[22:10:52.399] iteration 4471 : loss: 0.087318, loss_a: 0.051363
[22:10:53.145] iteration 4472 : loss: 0.026688, loss_a: 0.015699
[22:10:54.489] iteration 4473 : loss: 0.048458, loss_a: 0.028505
[22:10:55.234] iteration 4474 : loss: 0.023327, loss_a: 0.013722
[22:10:56.540] iteration 4475 : loss: 0.051551, loss_a: 0.030324
[22:10:57.284] iteration 4476 : loss: 0.035850, loss_a: 0.021088
[22:10:58.637] iteration 4477 : loss: 0.042453, loss_a: 0.024972
[22:10:59.377] iteration 4478 : loss: 0.035641, loss_a: 0.020965
[22:11:00.702] iteration 4479 : loss: 0.038785, loss_a: 0.022815
[22:11:01.444] iteration 4480 : loss: 0.037503, loss_a: 0.022061
[22:11:02.766] iteration 4481 : loss: 0.033857, loss_a: 0.019916
[22:11:03.502] iteration 4482 : loss: 0.024346, loss_a: 0.014321
[22:11:04.859] iteration 4483 : loss: 0.034135, loss_a: 0.020079
[22:11:05.605] iteration 4484 : loss: 0.031033, loss_a: 0.018255
[22:11:06.915] iteration 4485 : loss: 0.023885, loss_a: 0.014050
[22:11:07.663] iteration 4486 : loss: 0.037234, loss_a: 0.021902
[22:11:08.999] iteration 4487 : loss: 0.029783, loss_a: 0.017519
[22:11:09.732] iteration 4488 : loss: 0.012376, loss_a: 0.007280
[22:11:11.101] iteration 4489 : loss: 0.040139, loss_a: 0.023611
[22:11:11.851] iteration 4490 : loss: 0.036130, loss_a: 0.021253
[22:11:13.181] iteration 4491 : loss: 0.026299, loss_a: 0.015470
[22:11:13.910] iteration 4492 : loss: 0.016828, loss_a: 0.009899
[22:11:15.295] iteration 4493 : loss: 0.035365, loss_a: 0.020803
[22:11:16.037] iteration 4494 : loss: 0.049520, loss_a: 0.029130
[22:11:17.380] iteration 4495 : loss: 0.060146, loss_a: 0.035380
[22:11:18.104] iteration 4496 : loss: 0.014825, loss_a: 0.008720
[22:11:19.445] iteration 4497 : loss: 0.021207, loss_a: 0.012475
[22:11:20.173] iteration 4498 : loss: 0.014049, loss_a: 0.008264
[22:11:21.558] iteration 4499 : loss: 0.021625, loss_a: 0.012721
[22:11:22.303] iteration 4500 : loss: 0.036111, loss_a: 0.021242
[22:11:23.695] iteration 4501 : loss: 0.037062, loss_a: 0.021801
[22:11:24.436] iteration 4502 : loss: 0.023556, loss_a: 0.013856
[22:11:25.785] iteration 4503 : loss: 0.049223, loss_a: 0.028955
[22:11:26.525] iteration 4504 : loss: 0.037513, loss_a: 0.022066
[22:11:27.867] iteration 4505 : loss: 0.051538, loss_a: 0.030317
[22:11:28.609] iteration 4506 : loss: 0.038687, loss_a: 0.022757
[22:11:29.971] iteration 4507 : loss: 0.041436, loss_a: 0.024374
[22:11:30.706] iteration 4508 : loss: 0.017808, loss_a: 0.010475
[22:11:32.039] iteration 4509 : loss: 0.076527, loss_a: 0.045016
[22:11:32.779] iteration 4510 : loss: 0.017885, loss_a: 0.010521
[22:11:34.141] iteration 4511 : loss: 0.054978, loss_a: 0.032340
[22:11:34.877] iteration 4512 : loss: 0.023218, loss_a: 0.013657
[22:11:36.202] iteration 4513 : loss: 0.022562, loss_a: 0.013272
[22:11:36.943] iteration 4514 : loss: 0.030908, loss_a: 0.018181
[22:11:38.304] iteration 4515 : loss: 0.097788, loss_a: 0.057523
[22:11:39.033] iteration 4516 : loss: 0.016260, loss_a: 0.009565
[22:11:40.392] iteration 4517 : loss: 0.057865, loss_a: 0.034038
[22:11:41.124] iteration 4518 : loss: 0.017437, loss_a: 0.010257
[22:11:42.442] iteration 4519 : loss: 0.023951, loss_a: 0.014089
[22:11:43.176] iteration 4520 : loss: 0.028346, loss_a: 0.016674
[22:11:44.536] iteration 4521 : loss: 0.066151, loss_a: 0.038912
[22:11:45.275] iteration 4522 : loss: 0.030775, loss_a: 0.018103
[22:11:46.617] iteration 4523 : loss: 0.035016, loss_a: 0.020597
[22:11:47.357] iteration 4524 : loss: 0.017299, loss_a: 0.010176
[22:11:48.671] iteration 4525 : loss: 0.024195, loss_a: 0.014233
[22:11:49.416] iteration 4526 : loss: 0.030520, loss_a: 0.017953
[22:11:50.734] iteration 4527 : loss: 0.034535, loss_a: 0.020315
[22:11:51.472] iteration 4528 : loss: 0.031151, loss_a: 0.018324
[22:11:52.836] iteration 4529 : loss: 0.036670, loss_a: 0.021570
[22:11:53.567] iteration 4530 : loss: 0.025907, loss_a: 0.015239
[22:11:54.926] iteration 4531 : loss: 0.023661, loss_a: 0.013918
[22:11:55.667] iteration 4532 : loss: 0.030562, loss_a: 0.017978
[22:11:57.016] iteration 4533 : loss: 0.069264, loss_a: 0.040744
[22:11:57.752] iteration 4534 : loss: 0.026451, loss_a: 0.015559
[22:11:59.101] iteration 4535 : loss: 0.055665, loss_a: 0.032744
[22:11:59.837] iteration 4536 : loss: 0.025930, loss_a: 0.015253
[22:12:01.173] iteration 4537 : loss: 0.024931, loss_a: 0.014665
[22:12:01.911] iteration 4538 : loss: 0.021700, loss_a: 0.012764
[22:12:03.264] iteration 4539 : loss: 0.017146, loss_a: 0.010086
[22:12:04.019] iteration 4540 : loss: 0.042631, loss_a: 0.025077
[22:12:05.382] iteration 4541 : loss: 0.041167, loss_a: 0.024216
[22:12:06.127] iteration 4542 : loss: 0.020644, loss_a: 0.012144
[22:12:07.498] iteration 4543 : loss: 0.059842, loss_a: 0.035201
[22:12:08.241] iteration 4544 : loss: 0.027117, loss_a: 0.015951
[22:12:09.585] iteration 4545 : loss: 0.023551, loss_a: 0.013854
[22:12:10.326] iteration 4546 : loss: 0.083175, loss_a: 0.048927
[22:12:11.653] iteration 4547 : loss: 0.026561, loss_a: 0.015624
[22:12:12.401] iteration 4548 : loss: 0.060709, loss_a: 0.035711
[22:12:13.719] iteration 4549 : loss: 0.037887, loss_a: 0.022286
[22:12:14.470] iteration 4550 : loss: 0.055678, loss_a: 0.032752
[22:12:15.804] iteration 4551 : loss: 0.079759, loss_a: 0.046917
[22:12:16.538] iteration 4552 : loss: 0.029294, loss_a: 0.017232
[22:12:17.859] iteration 4553 : loss: 0.037367, loss_a: 0.021980
[22:12:18.606] iteration 4554 : loss: 0.036231, loss_a: 0.021312
[22:12:19.969] iteration 4555 : loss: 0.031366, loss_a: 0.018451
[22:12:20.709] iteration 4556 : loss: 0.020509, loss_a: 0.012064
[22:12:22.051] iteration 4557 : loss: 0.036605, loss_a: 0.021532
[22:12:22.806] iteration 4558 : loss: 0.092484, loss_a: 0.054402
[22:12:24.135] iteration 4559 : loss: 0.024812, loss_a: 0.014595
[22:12:24.877] iteration 4560 : loss: 0.029511, loss_a: 0.017359
[22:12:26.205] iteration 4561 : loss: 0.021282, loss_a: 0.012519
[22:12:26.944] iteration 4562 : loss: 0.037265, loss_a: 0.021921
[22:12:28.260] iteration 4563 : loss: 0.037649, loss_a: 0.022146
[22:12:29.003] iteration 4564 : loss: 0.025542, loss_a: 0.015025
[22:12:30.316] iteration 4565 : loss: 0.084017, loss_a: 0.049422
[22:12:31.059] iteration 4566 : loss: 0.033556, loss_a: 0.019739
[22:12:32.374] iteration 4567 : loss: 0.032289, loss_a: 0.018993
[22:12:33.113] iteration 4568 : loss: 0.052132, loss_a: 0.030666
[22:12:34.468] iteration 4569 : loss: 0.073389, loss_a: 0.043170
[22:12:35.201] iteration 4570 : loss: 0.024334, loss_a: 0.014314
[22:12:36.537] iteration 4571 : loss: 0.022822, loss_a: 0.013425
[22:12:37.276] iteration 4572 : loss: 0.037661, loss_a: 0.022153
[22:12:38.625] iteration 4573 : loss: 0.028542, loss_a: 0.016789
[22:12:39.364] iteration 4574 : loss: 0.089382, loss_a: 0.052577
[22:12:40.716] iteration 4575 : loss: 0.090997, loss_a: 0.053527
[22:12:41.464] iteration 4576 : loss: 0.050371, loss_a: 0.029630
[22:12:42.788] iteration 4577 : loss: 0.085743, loss_a: 0.050437
[22:12:43.539] iteration 4578 : loss: 0.044684, loss_a: 0.026285
[22:12:44.896] iteration 4579 : loss: 0.047666, loss_a: 0.028039
[22:12:45.644] iteration 4580 : loss: 0.066608, loss_a: 0.039181
[22:12:46.990] iteration 4581 : loss: 0.038654, loss_a: 0.022738
[22:12:47.723] iteration 4582 : loss: 0.037203, loss_a: 0.021884
[22:12:49.059] iteration 4583 : loss: 0.085468, loss_a: 0.050275
[22:12:49.809] iteration 4584 : loss: 0.032358, loss_a: 0.019034
[22:12:51.128] iteration 4585 : loss: 0.031813, loss_a: 0.018713
[22:12:51.864] iteration 4586 : loss: 0.064321, loss_a: 0.037836
[22:12:53.198] iteration 4587 : loss: 0.047181, loss_a: 0.027753
[22:12:53.956] iteration 4588 : loss: 0.035220, loss_a: 0.020718
[22:12:55.301] iteration 4589 : loss: 0.029876, loss_a: 0.017574
[22:12:56.046] iteration 4590 : loss: 0.026454, loss_a: 0.015561
[22:12:57.367] iteration 4591 : loss: 0.026123, loss_a: 0.015366
[22:12:58.103] iteration 4592 : loss: 0.039435, loss_a: 0.023197
[22:12:59.429] iteration 4593 : loss: 0.037033, loss_a: 0.021784
[22:13:00.164] iteration 4594 : loss: 0.039611, loss_a: 0.023301
[22:13:01.497] iteration 4595 : loss: 0.038316, loss_a: 0.022539
[22:13:02.238] iteration 4596 : loss: 0.055294, loss_a: 0.032526
[22:13:03.559] iteration 4597 : loss: 0.059131, loss_a: 0.034783
[22:13:04.301] iteration 4598 : loss: 0.037516, loss_a: 0.022068
[22:13:05.672] iteration 4599 : loss: 0.107917, loss_a: 0.063481
[22:13:06.411] iteration 4600 : loss: 0.057640, loss_a: 0.033906
[22:13:30.962] iteration 4601 : loss: 0.088750, loss_a: 0.052206
[22:13:33.132] iteration 4602 : loss: 0.054546, loss_a: 0.032086
[22:13:34.484] iteration 4603 : loss: 0.035460, loss_a: 0.020859
[22:13:35.223] iteration 4604 : loss: 0.029493, loss_a: 0.017349
[22:13:36.535] iteration 4605 : loss: 0.023549, loss_a: 0.013853
[22:13:37.280] iteration 4606 : loss: 0.050288, loss_a: 0.029581
[22:13:38.642] iteration 4607 : loss: 0.079057, loss_a: 0.046504
[22:13:39.389] iteration 4608 : loss: 0.049540, loss_a: 0.029141
[22:13:40.744] iteration 4609 : loss: 0.040456, loss_a: 0.023798
[22:13:41.485] iteration 4610 : loss: 0.028117, loss_a: 0.016539
[22:13:42.806] iteration 4611 : loss: 0.046490, loss_a: 0.027347
[22:13:43.551] iteration 4612 : loss: 0.047505, loss_a: 0.027944
[22:13:44.903] iteration 4613 : loss: 0.063572, loss_a: 0.037395
[22:13:45.648] iteration 4614 : loss: 0.030286, loss_a: 0.017815
[22:13:46.983] iteration 4615 : loss: 0.024893, loss_a: 0.014643
[22:13:47.732] iteration 4616 : loss: 0.040503, loss_a: 0.023825
[22:13:49.082] iteration 4617 : loss: 0.034854, loss_a: 0.020502
[22:13:49.822] iteration 4618 : loss: 0.042454, loss_a: 0.024973
[22:13:51.169] iteration 4619 : loss: 0.044579, loss_a: 0.026223
[22:13:51.899] iteration 4620 : loss: 0.018082, loss_a: 0.010637
[22:13:53.254] iteration 4621 : loss: 0.018605, loss_a: 0.010944
[22:13:53.996] iteration 4622 : loss: 0.049478, loss_a: 0.029105
[22:13:55.362] iteration 4623 : loss: 0.055134, loss_a: 0.032432
[22:13:56.097] iteration 4624 : loss: 0.030601, loss_a: 0.018001
[22:13:57.468] iteration 4625 : loss: 0.053148, loss_a: 0.031263
[22:13:58.207] iteration 4626 : loss: 0.023040, loss_a: 0.013553
[22:13:59.542] iteration 4627 : loss: 0.043358, loss_a: 0.025505
[22:14:00.288] iteration 4628 : loss: 0.058341, loss_a: 0.034318
[22:14:01.621] iteration 4629 : loss: 0.046774, loss_a: 0.027514
[22:14:02.398] iteration 4630 : loss: 0.062917, loss_a: 0.037010
[22:14:03.739] iteration 4631 : loss: 0.044855, loss_a: 0.026385
[22:14:04.471] iteration 4632 : loss: 0.020346, loss_a: 0.011968
[22:14:05.833] iteration 4633 : loss: 0.046215, loss_a: 0.027186
[22:14:06.583] iteration 4634 : loss: 0.016730, loss_a: 0.009841
[22:14:07.931] iteration 4635 : loss: 0.028562, loss_a: 0.016801
[22:14:08.666] iteration 4636 : loss: 0.029046, loss_a: 0.017086
[22:14:10.023] iteration 4637 : loss: 0.053312, loss_a: 0.031360
[22:14:10.782] iteration 4638 : loss: 0.027986, loss_a: 0.016462
[22:14:12.130] iteration 4639 : loss: 0.039221, loss_a: 0.023071
[22:14:12.869] iteration 4640 : loss: 0.029180, loss_a: 0.017165
[22:14:14.222] iteration 4641 : loss: 0.049227, loss_a: 0.028957
[22:14:14.973] iteration 4642 : loss: 0.044009, loss_a: 0.025888
[22:14:16.322] iteration 4643 : loss: 0.031985, loss_a: 0.018815
[22:14:17.056] iteration 4644 : loss: 0.027654, loss_a: 0.016267
[22:14:18.400] iteration 4645 : loss: 0.023280, loss_a: 0.013694
[22:14:19.132] iteration 4646 : loss: 0.031787, loss_a: 0.018698
[22:14:20.473] iteration 4647 : loss: 0.058208, loss_a: 0.034240
[22:14:21.214] iteration 4648 : loss: 0.026426, loss_a: 0.015545
[22:14:22.555] iteration 4649 : loss: 0.029980, loss_a: 0.017635
[22:14:23.291] iteration 4650 : loss: 0.042582, loss_a: 0.025048
[22:14:24.656] iteration 4651 : loss: 0.067685, loss_a: 0.039815
[22:14:25.399] iteration 4652 : loss: 0.020083, loss_a: 0.011814
[22:14:26.741] iteration 4653 : loss: 0.025091, loss_a: 0.014759
[22:14:27.495] iteration 4654 : loss: 0.048329, loss_a: 0.028429
[22:14:28.833] iteration 4655 : loss: 0.023818, loss_a: 0.014011
[22:14:29.577] iteration 4656 : loss: 0.024253, loss_a: 0.014266
[22:14:30.962] iteration 4657 : loss: 0.066278, loss_a: 0.038987
[22:14:31.710] iteration 4658 : loss: 0.051286, loss_a: 0.030168
[22:14:33.029] iteration 4659 : loss: 0.034887, loss_a: 0.020522
[22:14:33.787] iteration 4660 : loss: 0.057067, loss_a: 0.033569
[22:14:35.129] iteration 4661 : loss: 0.056058, loss_a: 0.032975
[22:14:35.868] iteration 4662 : loss: 0.069534, loss_a: 0.040903
[22:14:37.203] iteration 4663 : loss: 0.032290, loss_a: 0.018994
[22:14:37.950] iteration 4664 : loss: 0.020486, loss_a: 0.012051
[22:14:39.267] iteration 4665 : loss: 0.032769, loss_a: 0.019276
[22:14:40.017] iteration 4666 : loss: 0.040004, loss_a: 0.023532
[22:14:41.376] iteration 4667 : loss: 0.024977, loss_a: 0.014693
[22:14:42.113] iteration 4668 : loss: 0.020450, loss_a: 0.012029
[22:14:43.465] iteration 4669 : loss: 0.067406, loss_a: 0.039651
[22:14:44.209] iteration 4670 : loss: 0.038988, loss_a: 0.022934
[22:14:45.523] iteration 4671 : loss: 0.029352, loss_a: 0.017266
[22:14:46.274] iteration 4672 : loss: 0.040761, loss_a: 0.023977
[22:14:47.612] iteration 4673 : loss: 0.027493, loss_a: 0.016172
[22:14:48.365] iteration 4674 : loss: 0.053494, loss_a: 0.031467
[22:14:49.677] iteration 4675 : loss: 0.017563, loss_a: 0.010331
[22:14:50.416] iteration 4676 : loss: 0.032900, loss_a: 0.019353
[22:14:51.761] iteration 4677 : loss: 0.015204, loss_a: 0.008944
[22:14:52.505] iteration 4678 : loss: 0.038838, loss_a: 0.022846
[22:14:53.838] iteration 4679 : loss: 0.029009, loss_a: 0.017064
[22:14:54.596] iteration 4680 : loss: 0.031954, loss_a: 0.018796
[22:14:55.918] iteration 4681 : loss: 0.029308, loss_a: 0.017240
[22:14:56.660] iteration 4682 : loss: 0.041980, loss_a: 0.024694
[22:14:58.012] iteration 4683 : loss: 0.066157, loss_a: 0.038916
[22:14:58.751] iteration 4684 : loss: 0.027494, loss_a: 0.016173
[22:15:00.101] iteration 4685 : loss: 0.033774, loss_a: 0.019867
[22:15:00.847] iteration 4686 : loss: 0.031463, loss_a: 0.018508
[22:15:02.213] iteration 4687 : loss: 0.037101, loss_a: 0.021824
[22:15:02.966] iteration 4688 : loss: 0.030391, loss_a: 0.017877
[22:15:04.323] iteration 4689 : loss: 0.060589, loss_a: 0.035641
[22:15:05.067] iteration 4690 : loss: 0.024231, loss_a: 0.014253
[22:15:06.387] iteration 4691 : loss: 0.017851, loss_a: 0.010501
[22:15:07.124] iteration 4692 : loss: 0.024707, loss_a: 0.014534
[22:15:08.476] iteration 4693 : loss: 0.019483, loss_a: 0.011460
[22:15:09.214] iteration 4694 : loss: 0.026345, loss_a: 0.015497
[22:15:10.588] iteration 4695 : loss: 0.034993, loss_a: 0.020584
[22:15:11.321] iteration 4696 : loss: 0.056360, loss_a: 0.033153
[22:15:12.673] iteration 4697 : loss: 0.017740, loss_a: 0.010435
[22:15:13.412] iteration 4698 : loss: 0.041431, loss_a: 0.024371
[22:15:14.738] iteration 4699 : loss: 0.017963, loss_a: 0.010566
[22:15:15.491] iteration 4700 : loss: 0.090791, loss_a: 0.053406
[22:15:16.821] iteration 4701 : loss: 0.023338, loss_a: 0.013728
[22:15:17.561] iteration 4702 : loss: 0.042092, loss_a: 0.024760
[22:15:18.866] iteration 4703 : loss: 0.023704, loss_a: 0.013944
[22:15:19.595] iteration 4704 : loss: 0.026528, loss_a: 0.015605
[22:15:20.953] iteration 4705 : loss: 0.056623, loss_a: 0.033307
[22:15:21.691] iteration 4706 : loss: 0.031940, loss_a: 0.018788
[22:15:23.020] iteration 4707 : loss: 0.041040, loss_a: 0.024141
[22:15:23.759] iteration 4708 : loss: 0.030183, loss_a: 0.017755
[22:15:25.089] iteration 4709 : loss: 0.036360, loss_a: 0.021388
[22:15:25.826] iteration 4710 : loss: 0.028855, loss_a: 0.016973
[22:15:27.171] iteration 4711 : loss: 0.019412, loss_a: 0.011419
[22:15:27.916] iteration 4712 : loss: 0.096929, loss_a: 0.057017
[22:15:29.262] iteration 4713 : loss: 0.037536, loss_a: 0.022080
[22:15:30.004] iteration 4714 : loss: 0.037506, loss_a: 0.022063
[22:15:31.339] iteration 4715 : loss: 0.043224, loss_a: 0.025426
[22:15:32.076] iteration 4716 : loss: 0.028750, loss_a: 0.016912
[22:15:33.422] iteration 4717 : loss: 0.027843, loss_a: 0.016379
[22:15:34.164] iteration 4718 : loss: 0.045849, loss_a: 0.026970
[22:15:35.510] iteration 4719 : loss: 0.033010, loss_a: 0.019417
[22:15:36.258] iteration 4720 : loss: 0.019726, loss_a: 0.011603
[22:15:37.613] iteration 4721 : loss: 0.022396, loss_a: 0.013174
[22:15:38.387] iteration 4722 : loss: 0.024654, loss_a: 0.014502
[22:15:39.744] iteration 4723 : loss: 0.026800, loss_a: 0.015765
[22:15:40.484] iteration 4724 : loss: 0.033925, loss_a: 0.019956
[22:15:41.837] iteration 4725 : loss: 0.017846, loss_a: 0.010498
[22:15:42.576] iteration 4726 : loss: 0.043155, loss_a: 0.025385
[22:15:43.902] iteration 4727 : loss: 0.015798, loss_a: 0.009293
[22:15:44.642] iteration 4728 : loss: 0.018486, loss_a: 0.010874
[22:15:45.965] iteration 4729 : loss: 0.047830, loss_a: 0.028135
[22:15:46.705] iteration 4730 : loss: 0.027773, loss_a: 0.016337
[22:15:48.018] iteration 4731 : loss: 0.019336, loss_a: 0.011374
[22:15:48.764] iteration 4732 : loss: 0.027667, loss_a: 0.016275
[22:15:50.097] iteration 4733 : loss: 0.026933, loss_a: 0.015843
[22:15:50.846] iteration 4734 : loss: 0.026594, loss_a: 0.015643
[22:15:52.210] iteration 4735 : loss: 0.042386, loss_a: 0.024933
[22:15:52.950] iteration 4736 : loss: 0.029744, loss_a: 0.017497
[22:15:54.273] iteration 4737 : loss: 0.038747, loss_a: 0.022792
[22:15:55.019] iteration 4738 : loss: 0.024048, loss_a: 0.014146
[22:15:56.332] iteration 4739 : loss: 0.043404, loss_a: 0.025531
[22:15:57.075] iteration 4740 : loss: 0.038135, loss_a: 0.022432
[22:15:58.431] iteration 4741 : loss: 0.045103, loss_a: 0.026531
[22:15:59.175] iteration 4742 : loss: 0.026351, loss_a: 0.015501
[22:16:00.506] iteration 4743 : loss: 0.028620, loss_a: 0.016835
[22:16:01.244] iteration 4744 : loss: 0.027604, loss_a: 0.016237
[22:16:02.602] iteration 4745 : loss: 0.060140, loss_a: 0.035376
[22:16:03.342] iteration 4746 : loss: 0.032800, loss_a: 0.019294
[22:16:04.655] iteration 4747 : loss: 0.022919, loss_a: 0.013482
[22:16:05.387] iteration 4748 : loss: 0.027449, loss_a: 0.016146
[22:16:06.732] iteration 4749 : loss: 0.016940, loss_a: 0.009965
[22:16:07.462] iteration 4750 : loss: 0.029370, loss_a: 0.017276
[22:16:08.836] iteration 4751 : loss: 0.037067, loss_a: 0.021804
[22:16:09.579] iteration 4752 : loss: 0.023727, loss_a: 0.013957
[22:16:10.922] iteration 4753 : loss: 0.026881, loss_a: 0.015812
[22:16:11.664] iteration 4754 : loss: 0.024092, loss_a: 0.014172
[22:16:12.992] iteration 4755 : loss: 0.041184, loss_a: 0.024226
[22:16:13.735] iteration 4756 : loss: 0.020523, loss_a: 0.012073
[22:16:15.074] iteration 4757 : loss: 0.020555, loss_a: 0.012091
[22:16:15.810] iteration 4758 : loss: 0.020092, loss_a: 0.011819
[22:16:17.160] iteration 4759 : loss: 0.026052, loss_a: 0.015324
[22:16:17.901] iteration 4760 : loss: 0.020048, loss_a: 0.011793
[22:16:19.222] iteration 4761 : loss: 0.021960, loss_a: 0.012918
[22:16:19.956] iteration 4762 : loss: 0.021447, loss_a: 0.012616
[22:16:21.305] iteration 4763 : loss: 0.022146, loss_a: 0.013027
[22:16:22.043] iteration 4764 : loss: 0.027744, loss_a: 0.016320
[22:16:23.383] iteration 4765 : loss: 0.033643, loss_a: 0.019790
[22:16:24.128] iteration 4766 : loss: 0.026156, loss_a: 0.015386
[22:16:25.473] iteration 4767 : loss: 0.021559, loss_a: 0.012682
[22:16:26.208] iteration 4768 : loss: 0.016504, loss_a: 0.009708
[22:16:27.532] iteration 4769 : loss: 0.021573, loss_a: 0.012690
[22:16:28.282] iteration 4770 : loss: 0.021188, loss_a: 0.012463
[22:16:29.619] iteration 4771 : loss: 0.069498, loss_a: 0.040881
[22:16:30.355] iteration 4772 : loss: 0.039080, loss_a: 0.022988
[22:16:31.705] iteration 4773 : loss: 0.015205, loss_a: 0.008944
[22:16:32.440] iteration 4774 : loss: 0.015575, loss_a: 0.009162
[22:16:33.794] iteration 4775 : loss: 0.038930, loss_a: 0.022900
[22:16:34.548] iteration 4776 : loss: 0.027765, loss_a: 0.016332
[22:16:35.911] iteration 4777 : loss: 0.036544, loss_a: 0.021497
[22:16:36.657] iteration 4778 : loss: 0.031310, loss_a: 0.018418
[22:16:37.958] iteration 4779 : loss: 0.024171, loss_a: 0.014218
[22:16:38.705] iteration 4780 : loss: 0.038569, loss_a: 0.022688
[22:16:40.039] iteration 4781 : loss: 0.064527, loss_a: 0.037957
[22:16:40.784] iteration 4782 : loss: 0.030744, loss_a: 0.018085
[22:16:42.120] iteration 4783 : loss: 0.048742, loss_a: 0.028672
[22:16:42.872] iteration 4784 : loss: 0.021803, loss_a: 0.012825
[22:16:44.228] iteration 4785 : loss: 0.016523, loss_a: 0.009719
[22:16:44.979] iteration 4786 : loss: 0.049642, loss_a: 0.029201
[22:16:46.334] iteration 4787 : loss: 0.028216, loss_a: 0.016597
[22:16:47.069] iteration 4788 : loss: 0.019641, loss_a: 0.011554
[22:16:48.444] iteration 4789 : loss: 0.037008, loss_a: 0.021770
[22:16:49.191] iteration 4790 : loss: 0.028444, loss_a: 0.016732
[22:16:50.526] iteration 4791 : loss: 0.042243, loss_a: 0.024849
[22:16:51.272] iteration 4792 : loss: 0.041807, loss_a: 0.024593
[22:16:52.609] iteration 4793 : loss: 0.026877, loss_a: 0.015810
[22:16:53.345] iteration 4794 : loss: 0.024137, loss_a: 0.014198
[22:16:54.688] iteration 4795 : loss: 0.073348, loss_a: 0.043146
[22:16:55.439] iteration 4796 : loss: 0.014467, loss_a: 0.008510
[22:16:56.756] iteration 4797 : loss: 0.030932, loss_a: 0.018195
[22:16:57.492] iteration 4798 : loss: 0.037079, loss_a: 0.021811
[22:16:58.833] iteration 4799 : loss: 0.037874, loss_a: 0.022279
[22:16:59.573] iteration 4800 : loss: 0.035131, loss_a: 0.020665
[22:17:24.213] iteration 4801 : loss: 0.052580, loss_a: 0.030929
[22:17:26.380] iteration 4802 : loss: 0.036536, loss_a: 0.021492
[22:17:27.701] iteration 4803 : loss: 0.074882, loss_a: 0.044048
[22:17:28.438] iteration 4804 : loss: 0.015415, loss_a: 0.009068
[22:17:29.787] iteration 4805 : loss: 0.025003, loss_a: 0.014707
[22:17:30.531] iteration 4806 : loss: 0.025621, loss_a: 0.015071
[22:17:31.887] iteration 4807 : loss: 0.017549, loss_a: 0.010323
[22:17:32.637] iteration 4808 : loss: 0.035182, loss_a: 0.020695
[22:17:33.980] iteration 4809 : loss: 0.034335, loss_a: 0.020197
[22:17:34.715] iteration 4810 : loss: 0.035842, loss_a: 0.021084
[22:17:36.061] iteration 4811 : loss: 0.045048, loss_a: 0.026499
[22:17:36.808] iteration 4812 : loss: 0.032359, loss_a: 0.019035
[22:17:38.126] iteration 4813 : loss: 0.050010, loss_a: 0.029418
[22:17:38.862] iteration 4814 : loss: 0.047774, loss_a: 0.028102
[22:17:40.221] iteration 4815 : loss: 0.025531, loss_a: 0.015018
[22:17:40.957] iteration 4816 : loss: 0.030356, loss_a: 0.017856
[22:17:42.308] iteration 4817 : loss: 0.080665, loss_a: 0.047450
[22:17:43.043] iteration 4818 : loss: 0.025873, loss_a: 0.015219
[22:17:44.357] iteration 4819 : loss: 0.047978, loss_a: 0.028222
[22:17:45.104] iteration 4820 : loss: 0.043056, loss_a: 0.025327
[22:17:46.466] iteration 4821 : loss: 0.037530, loss_a: 0.022077
[22:17:47.208] iteration 4822 : loss: 0.035458, loss_a: 0.020857
[22:17:48.557] iteration 4823 : loss: 0.049970, loss_a: 0.029394
[22:17:49.301] iteration 4824 : loss: 0.018755, loss_a: 0.011032
[22:17:50.616] iteration 4825 : loss: 0.044801, loss_a: 0.026354
[22:17:51.352] iteration 4826 : loss: 0.029896, loss_a: 0.017586
[22:17:52.658] iteration 4827 : loss: 0.018529, loss_a: 0.010899
[22:17:53.387] iteration 4828 : loss: 0.022057, loss_a: 0.012975
[22:17:54.723] iteration 4829 : loss: 0.032096, loss_a: 0.018880
[22:17:55.455] iteration 4830 : loss: 0.030050, loss_a: 0.017676
[22:17:56.783] iteration 4831 : loss: 0.025947, loss_a: 0.015263
[22:17:57.521] iteration 4832 : loss: 0.041480, loss_a: 0.024400
[22:17:58.857] iteration 4833 : loss: 0.035957, loss_a: 0.021151
[22:17:59.602] iteration 4834 : loss: 0.034689, loss_a: 0.020405
[22:18:00.914] iteration 4835 : loss: 0.013125, loss_a: 0.007720
[22:18:01.651] iteration 4836 : loss: 0.058891, loss_a: 0.034642
[22:18:02.960] iteration 4837 : loss: 0.026544, loss_a: 0.015614
[22:18:03.707] iteration 4838 : loss: 0.028345, loss_a: 0.016674
[22:18:05.073] iteration 4839 : loss: 0.044122, loss_a: 0.025954
[22:18:05.822] iteration 4840 : loss: 0.026926, loss_a: 0.015839
[22:18:07.134] iteration 4841 : loss: 0.030454, loss_a: 0.017914
[22:18:07.864] iteration 4842 : loss: 0.013275, loss_a: 0.007809
[22:18:09.186] iteration 4843 : loss: 0.026395, loss_a: 0.015527
[22:18:09.916] iteration 4844 : loss: 0.019450, loss_a: 0.011441
[22:18:11.236] iteration 4845 : loss: 0.046309, loss_a: 0.027240
[22:18:11.981] iteration 4846 : loss: 0.047440, loss_a: 0.027906
[22:18:13.323] iteration 4847 : loss: 0.034775, loss_a: 0.020456
[22:18:14.061] iteration 4848 : loss: 0.023371, loss_a: 0.013748
[22:18:15.377] iteration 4849 : loss: 0.030549, loss_a: 0.017970
[22:18:16.122] iteration 4850 : loss: 0.027829, loss_a: 0.016370
[22:18:17.474] iteration 4851 : loss: 0.056412, loss_a: 0.033184
[22:18:18.217] iteration 4852 : loss: 0.048692, loss_a: 0.028642
[22:18:19.530] iteration 4853 : loss: 0.065869, loss_a: 0.038747
[22:18:20.280] iteration 4854 : loss: 0.036932, loss_a: 0.021725
[22:18:21.609] iteration 4855 : loss: 0.043957, loss_a: 0.025857
[22:18:22.346] iteration 4856 : loss: 0.257964, loss_a: 0.151744
[22:18:23.683] iteration 4857 : loss: 0.029879, loss_a: 0.017576
[22:18:24.423] iteration 4858 : loss: 0.026462, loss_a: 0.015566
[22:18:25.749] iteration 4859 : loss: 0.039532, loss_a: 0.023254
[22:18:26.498] iteration 4860 : loss: 0.022897, loss_a: 0.013469
[22:18:27.836] iteration 4861 : loss: 0.034342, loss_a: 0.020201
[22:18:28.599] iteration 4862 : loss: 0.026906, loss_a: 0.015827
[22:18:29.964] iteration 4863 : loss: 0.036265, loss_a: 0.021332
[22:18:30.751] iteration 4864 : loss: 0.036326, loss_a: 0.021368
[22:18:32.139] iteration 4865 : loss: 0.039440, loss_a: 0.023200
[22:18:32.942] iteration 4866 : loss: 0.067523, loss_a: 0.039719
[22:18:34.296] iteration 4867 : loss: 0.013157, loss_a: 0.007739
[22:18:35.156] iteration 4868 : loss: 0.062947, loss_a: 0.037027
[22:18:36.710] iteration 4869 : loss: 0.039395, loss_a: 0.023174
[22:18:37.533] iteration 4870 : loss: 0.044979, loss_a: 0.026459
[22:18:38.900] iteration 4871 : loss: 0.021040, loss_a: 0.012376
[22:18:39.627] iteration 4872 : loss: 0.014680, loss_a: 0.008635
[22:18:40.984] iteration 4873 : loss: 0.030585, loss_a: 0.017991
[22:18:41.746] iteration 4874 : loss: 0.020433, loss_a: 0.012019
[22:18:43.096] iteration 4875 : loss: 0.024392, loss_a: 0.014348
[22:18:43.836] iteration 4876 : loss: 0.036370, loss_a: 0.021394
[22:18:45.158] iteration 4877 : loss: 0.032220, loss_a: 0.018953
[22:18:45.898] iteration 4878 : loss: 0.040982, loss_a: 0.024107
[22:18:47.237] iteration 4879 : loss: 0.036034, loss_a: 0.021197
[22:18:47.970] iteration 4880 : loss: 0.028082, loss_a: 0.016519
[22:18:49.319] iteration 4881 : loss: 0.028478, loss_a: 0.016751
[22:18:50.057] iteration 4882 : loss: 0.030804, loss_a: 0.018120
[22:18:51.386] iteration 4883 : loss: 0.039894, loss_a: 0.023467
[22:18:52.130] iteration 4884 : loss: 0.031576, loss_a: 0.018574
[22:18:53.467] iteration 4885 : loss: 0.056650, loss_a: 0.033324
[22:18:54.222] iteration 4886 : loss: 0.040928, loss_a: 0.024075
[22:18:55.539] iteration 4887 : loss: 0.035996, loss_a: 0.021174
[22:18:56.282] iteration 4888 : loss: 0.027012, loss_a: 0.015890
[22:18:57.603] iteration 4889 : loss: 0.038840, loss_a: 0.022847
[22:18:58.341] iteration 4890 : loss: 0.041618, loss_a: 0.024481
[22:18:59.691] iteration 4891 : loss: 0.031509, loss_a: 0.018535
[22:19:00.438] iteration 4892 : loss: 0.038042, loss_a: 0.022377
[22:19:01.751] iteration 4893 : loss: 0.053971, loss_a: 0.031748
[22:19:02.484] iteration 4894 : loss: 0.022161, loss_a: 0.013036
[22:19:03.842] iteration 4895 : loss: 0.027674, loss_a: 0.016279
[22:19:04.578] iteration 4896 : loss: 0.031055, loss_a: 0.018267
[22:19:05.901] iteration 4897 : loss: 0.026450, loss_a: 0.015559
[22:19:06.634] iteration 4898 : loss: 0.044007, loss_a: 0.025887
[22:19:07.967] iteration 4899 : loss: 0.043643, loss_a: 0.025672
[22:19:08.711] iteration 4900 : loss: 0.022782, loss_a: 0.013401
[22:19:10.032] iteration 4901 : loss: 0.062237, loss_a: 0.036610
[22:19:10.771] iteration 4902 : loss: 0.028020, loss_a: 0.016482
[22:19:12.126] iteration 4903 : loss: 0.025847, loss_a: 0.015204
[22:19:12.880] iteration 4904 : loss: 0.052076, loss_a: 0.030633
[22:19:14.230] iteration 4905 : loss: 0.029206, loss_a: 0.017180
[22:19:14.965] iteration 4906 : loss: 0.028176, loss_a: 0.016574
[22:19:16.304] iteration 4907 : loss: 0.023238, loss_a: 0.013670
[22:19:17.042] iteration 4908 : loss: 0.022623, loss_a: 0.013308
[22:19:18.378] iteration 4909 : loss: 0.011636, loss_a: 0.006845
[22:19:19.111] iteration 4910 : loss: 0.033458, loss_a: 0.019681
[22:19:20.427] iteration 4911 : loss: 0.023424, loss_a: 0.013779
[22:19:21.172] iteration 4912 : loss: 0.034720, loss_a: 0.020423
[22:19:22.532] iteration 4913 : loss: 0.020060, loss_a: 0.011800
[22:19:23.269] iteration 4914 : loss: 0.025812, loss_a: 0.015184
[22:19:24.592] iteration 4915 : loss: 0.027919, loss_a: 0.016423
[22:19:25.329] iteration 4916 : loss: 0.022352, loss_a: 0.013148
[22:19:26.662] iteration 4917 : loss: 0.021944, loss_a: 0.012908
[22:19:27.394] iteration 4918 : loss: 0.019691, loss_a: 0.011583
[22:19:28.734] iteration 4919 : loss: 0.024751, loss_a: 0.014559
[22:19:29.475] iteration 4920 : loss: 0.014347, loss_a: 0.008439
[22:19:30.808] iteration 4921 : loss: 0.024640, loss_a: 0.014494
[22:19:31.544] iteration 4922 : loss: 0.028006, loss_a: 0.016474
[22:19:32.884] iteration 4923 : loss: 0.044312, loss_a: 0.026066
[22:19:33.615] iteration 4924 : loss: 0.020519, loss_a: 0.012070
[22:19:34.935] iteration 4925 : loss: 0.028766, loss_a: 0.016921
[22:19:35.674] iteration 4926 : loss: 0.031224, loss_a: 0.018367
[22:19:37.023] iteration 4927 : loss: 0.027514, loss_a: 0.016185
[22:19:37.760] iteration 4928 : loss: 0.019755, loss_a: 0.011621
[22:19:39.087] iteration 4929 : loss: 0.023769, loss_a: 0.013982
[22:19:39.826] iteration 4930 : loss: 0.024405, loss_a: 0.014356
[22:19:41.147] iteration 4931 : loss: 0.026655, loss_a: 0.015680
[22:19:41.886] iteration 4932 : loss: 0.020691, loss_a: 0.012171
[22:19:43.237] iteration 4933 : loss: 0.034358, loss_a: 0.020211
[22:19:43.975] iteration 4934 : loss: 0.021478, loss_a: 0.012634
[22:19:45.330] iteration 4935 : loss: 0.039381, loss_a: 0.023165
[22:19:46.080] iteration 4936 : loss: 0.039099, loss_a: 0.022999
[22:19:47.411] iteration 4937 : loss: 0.022643, loss_a: 0.013319
[22:19:48.149] iteration 4938 : loss: 0.029711, loss_a: 0.017477
[22:19:49.494] iteration 4939 : loss: 0.023465, loss_a: 0.013803
[22:19:50.224] iteration 4940 : loss: 0.040614, loss_a: 0.023890
[22:19:51.536] iteration 4941 : loss: 0.013567, loss_a: 0.007981
[22:19:52.326] iteration 4942 : loss: 0.052223, loss_a: 0.030720
[22:19:53.699] iteration 4943 : loss: 0.021916, loss_a: 0.012892
[22:19:54.445] iteration 4944 : loss: 0.050315, loss_a: 0.029597
[22:19:55.785] iteration 4945 : loss: 0.026045, loss_a: 0.015321
[22:19:56.517] iteration 4946 : loss: 0.041011, loss_a: 0.024124
[22:19:57.830] iteration 4947 : loss: 0.018302, loss_a: 0.010766
[22:19:58.574] iteration 4948 : loss: 0.052833, loss_a: 0.031078
[22:19:59.928] iteration 4949 : loss: 0.020988, loss_a: 0.012346
[22:20:00.668] iteration 4950 : loss: 0.026404, loss_a: 0.015532
[22:20:01.980] iteration 4951 : loss: 0.039725, loss_a: 0.023367
[22:20:02.713] iteration 4952 : loss: 0.024520, loss_a: 0.014424
[22:20:04.070] iteration 4953 : loss: 0.035477, loss_a: 0.020869
[22:20:04.814] iteration 4954 : loss: 0.039787, loss_a: 0.023404
[22:20:06.142] iteration 4955 : loss: 0.048887, loss_a: 0.028757
[22:20:06.883] iteration 4956 : loss: 0.016735, loss_a: 0.009844
[22:20:08.193] iteration 4957 : loss: 0.013387, loss_a: 0.007875
[22:20:08.933] iteration 4958 : loss: 0.025301, loss_a: 0.014883
[22:20:10.245] iteration 4959 : loss: 0.022653, loss_a: 0.013325
[22:20:10.989] iteration 4960 : loss: 0.054324, loss_a: 0.031955
[22:20:12.339] iteration 4961 : loss: 0.025456, loss_a: 0.014974
[22:20:13.078] iteration 4962 : loss: 0.029828, loss_a: 0.017546
[22:20:14.415] iteration 4963 : loss: 0.023032, loss_a: 0.013548
[22:20:15.154] iteration 4964 : loss: 0.019396, loss_a: 0.011409
[22:20:16.458] iteration 4965 : loss: 0.020738, loss_a: 0.012199
[22:20:17.210] iteration 4966 : loss: 0.023399, loss_a: 0.013764
[22:20:18.525] iteration 4967 : loss: 0.021904, loss_a: 0.012885
[22:20:19.261] iteration 4968 : loss: 0.042670, loss_a: 0.025100
[22:20:20.611] iteration 4969 : loss: 0.024385, loss_a: 0.014344
[22:20:21.354] iteration 4970 : loss: 0.023558, loss_a: 0.013858
[22:20:22.676] iteration 4971 : loss: 0.035685, loss_a: 0.020991
[22:20:23.415] iteration 4972 : loss: 0.040338, loss_a: 0.023728
[22:20:24.763] iteration 4973 : loss: 0.047240, loss_a: 0.027788
[22:20:25.494] iteration 4974 : loss: 0.032050, loss_a: 0.018853
[22:20:26.807] iteration 4975 : loss: 0.047835, loss_a: 0.028138
[22:20:27.572] iteration 4976 : loss: 0.020611, loss_a: 0.012124
[22:20:28.928] iteration 4977 : loss: 0.029472, loss_a: 0.017337
[22:20:29.665] iteration 4978 : loss: 0.028282, loss_a: 0.016637
[22:20:31.018] iteration 4979 : loss: 0.023795, loss_a: 0.013997
[22:20:31.757] iteration 4980 : loss: 0.041063, loss_a: 0.024155
[22:20:33.099] iteration 4981 : loss: 0.069865, loss_a: 0.041097
[22:20:33.845] iteration 4982 : loss: 0.042195, loss_a: 0.024820
[22:20:35.175] iteration 4983 : loss: 0.034658, loss_a: 0.020387
[22:20:35.916] iteration 4984 : loss: 0.026792, loss_a: 0.015760
[22:20:37.253] iteration 4985 : loss: 0.018303, loss_a: 0.010766
[22:20:37.996] iteration 4986 : loss: 0.038851, loss_a: 0.022853
[22:20:39.321] iteration 4987 : loss: 0.024392, loss_a: 0.014348
[22:20:40.059] iteration 4988 : loss: 0.024963, loss_a: 0.014684
[22:20:41.412] iteration 4989 : loss: 0.040415, loss_a: 0.023773
[22:20:42.162] iteration 4990 : loss: 0.023879, loss_a: 0.014047
[22:20:43.504] iteration 4991 : loss: 0.032804, loss_a: 0.019296
[22:20:44.256] iteration 4992 : loss: 0.028642, loss_a: 0.016848
[22:20:45.605] iteration 4993 : loss: 0.081784, loss_a: 0.048108
[22:20:46.348] iteration 4994 : loss: 0.025260, loss_a: 0.014859
[22:20:47.695] iteration 4995 : loss: 0.032029, loss_a: 0.018841
[22:20:48.444] iteration 4996 : loss: 0.035307, loss_a: 0.020769
[22:20:49.760] iteration 4997 : loss: 0.027735, loss_a: 0.016315
[22:20:50.495] iteration 4998 : loss: 0.026383, loss_a: 0.015519
[22:20:51.852] iteration 4999 : loss: 0.025880, loss_a: 0.015223
[22:20:52.582] iteration 5000 : loss: 0.030514, loss_a: 0.017949
[22:21:17.195] iteration 5001 : loss: 0.025833, loss_a: 0.015196
[22:21:19.385] iteration 5002 : loss: 0.024405, loss_a: 0.014356
[22:21:20.734] iteration 5003 : loss: 0.039105, loss_a: 0.023003
[22:21:21.471] iteration 5004 : loss: 0.030203, loss_a: 0.017766
[22:21:22.816] iteration 5005 : loss: 0.018673, loss_a: 0.010984
[22:21:23.553] iteration 5006 : loss: 0.031161, loss_a: 0.018330
[22:21:24.873] iteration 5007 : loss: 0.025340, loss_a: 0.014906
[22:21:25.610] iteration 5008 : loss: 0.020916, loss_a: 0.012304
[22:21:26.960] iteration 5009 : loss: 0.027690, loss_a: 0.016288
[22:21:27.689] iteration 5010 : loss: 0.019429, loss_a: 0.011429
[22:21:29.049] iteration 5011 : loss: 0.066236, loss_a: 0.038962
[22:21:29.784] iteration 5012 : loss: 0.034777, loss_a: 0.020457
[22:21:31.150] iteration 5013 : loss: 0.043623, loss_a: 0.025661
[22:21:31.893] iteration 5014 : loss: 0.021518, loss_a: 0.012658
[22:21:33.225] iteration 5015 : loss: 0.033599, loss_a: 0.019764
[22:21:33.964] iteration 5016 : loss: 0.058334, loss_a: 0.034314
[22:21:35.319] iteration 5017 : loss: 0.027425, loss_a: 0.016133
[22:21:36.065] iteration 5018 : loss: 0.033243, loss_a: 0.019555
[22:21:37.437] iteration 5019 : loss: 0.030369, loss_a: 0.017864
[22:21:38.174] iteration 5020 : loss: 0.037650, loss_a: 0.022147
[22:21:39.526] iteration 5021 : loss: 0.056490, loss_a: 0.033230
[22:21:40.268] iteration 5022 : loss: 0.017966, loss_a: 0.010568
[22:21:41.650] iteration 5023 : loss: 0.071468, loss_a: 0.042040
[22:21:42.388] iteration 5024 : loss: 0.017385, loss_a: 0.010226
[22:21:43.741] iteration 5025 : loss: 0.020714, loss_a: 0.012185
[22:21:44.481] iteration 5026 : loss: 0.022965, loss_a: 0.013509
[22:21:45.891] iteration 5027 : loss: 0.042962, loss_a: 0.025272
[22:21:46.632] iteration 5028 : loss: 0.018650, loss_a: 0.010971
[22:21:47.989] iteration 5029 : loss: 0.016695, loss_a: 0.009821
[22:21:48.725] iteration 5030 : loss: 0.015918, loss_a: 0.009363
[22:21:50.085] iteration 5031 : loss: 0.042011, loss_a: 0.024712
[22:21:50.828] iteration 5032 : loss: 0.033896, loss_a: 0.019939
[22:21:52.186] iteration 5033 : loss: 0.030987, loss_a: 0.018228
[22:21:52.926] iteration 5034 : loss: 0.042382, loss_a: 0.024931
[22:21:54.271] iteration 5035 : loss: 0.031826, loss_a: 0.018721
[22:21:55.018] iteration 5036 : loss: 0.022371, loss_a: 0.013160
[22:21:56.367] iteration 5037 : loss: 0.067723, loss_a: 0.039837
[22:21:57.105] iteration 5038 : loss: 0.018467, loss_a: 0.010863
[22:21:58.446] iteration 5039 : loss: 0.068324, loss_a: 0.040190
[22:21:59.202] iteration 5040 : loss: 0.036842, loss_a: 0.021671
[22:22:00.549] iteration 5041 : loss: 0.022154, loss_a: 0.013032
[22:22:01.293] iteration 5042 : loss: 0.073590, loss_a: 0.043288
[22:22:02.636] iteration 5043 : loss: 0.045843, loss_a: 0.026966
[22:22:03.414] iteration 5044 : loss: 0.513596, loss_a: 0.302115
[22:22:04.767] iteration 5045 : loss: 0.036379, loss_a: 0.021400
[22:22:05.506] iteration 5046 : loss: 0.019552, loss_a: 0.011501
[22:22:06.863] iteration 5047 : loss: 0.057697, loss_a: 0.033940
[22:22:07.606] iteration 5048 : loss: 0.048339, loss_a: 0.028435
[22:22:08.931] iteration 5049 : loss: 0.048002, loss_a: 0.028236
[22:22:09.669] iteration 5050 : loss: 0.059262, loss_a: 0.034860
[22:22:11.032] iteration 5051 : loss: 0.025168, loss_a: 0.014804
[22:22:11.766] iteration 5052 : loss: 0.030180, loss_a: 0.017753
[22:22:13.106] iteration 5053 : loss: 0.017770, loss_a: 0.010453
[22:22:13.854] iteration 5054 : loss: 0.023669, loss_a: 0.013923
[22:22:15.201] iteration 5055 : loss: 0.021568, loss_a: 0.012687
[22:22:15.945] iteration 5056 : loss: 0.016943, loss_a: 0.009966
[22:22:17.295] iteration 5057 : loss: 0.027216, loss_a: 0.016010
[22:22:18.040] iteration 5058 : loss: 0.029150, loss_a: 0.017147
[22:22:19.354] iteration 5059 : loss: 0.031367, loss_a: 0.018451
[22:22:20.122] iteration 5060 : loss: 0.049251, loss_a: 0.028971
[22:22:21.489] iteration 5061 : loss: 0.037025, loss_a: 0.021779
[22:22:22.231] iteration 5062 : loss: 0.022852, loss_a: 0.013443
[22:22:23.553] iteration 5063 : loss: 0.016148, loss_a: 0.009499
[22:22:24.295] iteration 5064 : loss: 0.036433, loss_a: 0.021431
[22:22:25.648] iteration 5065 : loss: 0.020997, loss_a: 0.012351
[22:22:26.379] iteration 5066 : loss: 0.019786, loss_a: 0.011639
[22:22:27.711] iteration 5067 : loss: 0.023774, loss_a: 0.013985
[22:22:28.452] iteration 5068 : loss: 0.029813, loss_a: 0.017537
[22:22:29.794] iteration 5069 : loss: 0.051547, loss_a: 0.030321
[22:22:30.533] iteration 5070 : loss: 0.025681, loss_a: 0.015106
[22:22:31.895] iteration 5071 : loss: 0.021206, loss_a: 0.012474
[22:22:32.641] iteration 5072 : loss: 0.025127, loss_a: 0.014781
[22:22:33.983] iteration 5073 : loss: 0.027746, loss_a: 0.016321
[22:22:34.716] iteration 5074 : loss: 0.024363, loss_a: 0.014331
[22:22:36.058] iteration 5075 : loss: 0.020255, loss_a: 0.011915
[22:22:36.804] iteration 5076 : loss: 0.028880, loss_a: 0.016988
[22:22:38.183] iteration 5077 : loss: 0.028577, loss_a: 0.016810
[22:22:38.934] iteration 5078 : loss: 0.035759, loss_a: 0.021034
[22:22:40.260] iteration 5079 : loss: 0.029287, loss_a: 0.017227
[22:22:41.024] iteration 5080 : loss: 0.045568, loss_a: 0.026805
[22:22:42.336] iteration 5081 : loss: 0.024196, loss_a: 0.014233
[22:22:43.083] iteration 5082 : loss: 0.032010, loss_a: 0.018830
[22:22:44.458] iteration 5083 : loss: 0.057960, loss_a: 0.034094
[22:22:45.208] iteration 5084 : loss: 0.033364, loss_a: 0.019626
[22:22:46.544] iteration 5085 : loss: 0.033465, loss_a: 0.019685
[22:22:47.283] iteration 5086 : loss: 0.035505, loss_a: 0.020886
[22:22:48.600] iteration 5087 : loss: 0.020003, loss_a: 0.011766
[22:22:49.341] iteration 5088 : loss: 0.039705, loss_a: 0.023356
[22:22:50.654] iteration 5089 : loss: 0.048003, loss_a: 0.028237
[22:22:51.396] iteration 5090 : loss: 0.023470, loss_a: 0.013806
[22:22:52.724] iteration 5091 : loss: 0.021047, loss_a: 0.012381
[22:22:53.469] iteration 5092 : loss: 0.027695, loss_a: 0.016291
[22:22:54.803] iteration 5093 : loss: 0.021035, loss_a: 0.012374
[22:22:55.537] iteration 5094 : loss: 0.037966, loss_a: 0.022333
[22:22:56.888] iteration 5095 : loss: 0.016044, loss_a: 0.009438
[22:22:57.626] iteration 5096 : loss: 0.046867, loss_a: 0.027569
[22:22:58.966] iteration 5097 : loss: 0.017961, loss_a: 0.010566
[22:22:59.706] iteration 5098 : loss: 0.027289, loss_a: 0.016052
[22:23:01.023] iteration 5099 : loss: 0.024882, loss_a: 0.014637
[22:23:01.756] iteration 5100 : loss: 0.028955, loss_a: 0.017032
[22:23:03.078] iteration 5101 : loss: 0.028333, loss_a: 0.016666
[22:23:03.820] iteration 5102 : loss: 0.017199, loss_a: 0.010117
[22:23:05.157] iteration 5103 : loss: 0.053281, loss_a: 0.031342
[22:23:05.892] iteration 5104 : loss: 0.035747, loss_a: 0.021028
[22:23:07.223] iteration 5105 : loss: 0.014695, loss_a: 0.008644
[22:23:07.956] iteration 5106 : loss: 0.055502, loss_a: 0.032648
[22:23:09.326] iteration 5107 : loss: 0.048583, loss_a: 0.028578
[22:23:10.065] iteration 5108 : loss: 0.038381, loss_a: 0.022577
[22:23:11.406] iteration 5109 : loss: 0.011610, loss_a: 0.006829
[22:23:12.153] iteration 5110 : loss: 0.057575, loss_a: 0.033867
[22:23:13.508] iteration 5111 : loss: 0.032947, loss_a: 0.019381
[22:23:14.257] iteration 5112 : loss: 0.041660, loss_a: 0.024506
[22:23:15.619] iteration 5113 : loss: 0.012081, loss_a: 0.007107
[22:23:16.360] iteration 5114 : loss: 0.035954, loss_a: 0.021150
[22:23:17.691] iteration 5115 : loss: 0.019126, loss_a: 0.011250
[22:23:18.439] iteration 5116 : loss: 0.028327, loss_a: 0.016663
[22:23:19.808] iteration 5117 : loss: 0.018406, loss_a: 0.010827
[22:23:20.543] iteration 5118 : loss: 0.014419, loss_a: 0.008482
[22:23:21.890] iteration 5119 : loss: 0.026701, loss_a: 0.015706
[22:23:22.640] iteration 5120 : loss: 0.024459, loss_a: 0.014387
[22:23:24.003] iteration 5121 : loss: 0.017752, loss_a: 0.010442
[22:23:24.733] iteration 5122 : loss: 0.023692, loss_a: 0.013936
[22:23:26.089] iteration 5123 : loss: 0.038184, loss_a: 0.022461
[22:23:26.834] iteration 5124 : loss: 0.071063, loss_a: 0.041802
[22:23:28.191] iteration 5125 : loss: 0.028861, loss_a: 0.016977
[22:23:28.929] iteration 5126 : loss: 0.076383, loss_a: 0.044931
[22:23:30.257] iteration 5127 : loss: 0.030638, loss_a: 0.018023
[22:23:31.005] iteration 5128 : loss: 0.033102, loss_a: 0.019472
[22:23:32.345] iteration 5129 : loss: 0.016796, loss_a: 0.009880
[22:23:33.095] iteration 5130 : loss: 0.049039, loss_a: 0.028846
[22:23:34.426] iteration 5131 : loss: 0.034286, loss_a: 0.020168
[22:23:35.158] iteration 5132 : loss: 0.038973, loss_a: 0.022925
[22:23:36.475] iteration 5133 : loss: 0.015296, loss_a: 0.008997
[22:23:37.222] iteration 5134 : loss: 0.050091, loss_a: 0.029465
[22:23:38.540] iteration 5135 : loss: 0.022707, loss_a: 0.013357
[22:23:39.289] iteration 5136 : loss: 0.039512, loss_a: 0.023242
[22:23:40.617] iteration 5137 : loss: 0.037523, loss_a: 0.022072
[22:23:41.351] iteration 5138 : loss: 0.020937, loss_a: 0.012316
[22:23:42.682] iteration 5139 : loss: 0.022751, loss_a: 0.013383
[22:23:43.418] iteration 5140 : loss: 0.031934, loss_a: 0.018785
[22:23:44.732] iteration 5141 : loss: 0.015854, loss_a: 0.009326
[22:23:45.483] iteration 5142 : loss: 0.050483, loss_a: 0.029696
[22:23:46.800] iteration 5143 : loss: 0.033554, loss_a: 0.019738
[22:23:47.545] iteration 5144 : loss: 0.045183, loss_a: 0.026578
[22:23:48.863] iteration 5145 : loss: 0.025052, loss_a: 0.014736
[22:23:49.604] iteration 5146 : loss: 0.046947, loss_a: 0.027616
[22:23:50.946] iteration 5147 : loss: 0.041186, loss_a: 0.024227
[22:23:51.681] iteration 5148 : loss: 0.020853, loss_a: 0.012266
[22:23:53.030] iteration 5149 : loss: 0.016026, loss_a: 0.009427
[22:23:53.765] iteration 5150 : loss: 0.025261, loss_a: 0.014859
[22:23:55.102] iteration 5151 : loss: 0.062991, loss_a: 0.037054
[22:23:55.834] iteration 5152 : loss: 0.045869, loss_a: 0.026982
[22:23:57.157] iteration 5153 : loss: 0.046469, loss_a: 0.027335
[22:23:57.889] iteration 5154 : loss: 0.018561, loss_a: 0.010919
[22:23:59.203] iteration 5155 : loss: 0.030966, loss_a: 0.018215
[22:23:59.936] iteration 5156 : loss: 0.015749, loss_a: 0.009264
[22:24:01.257] iteration 5157 : loss: 0.024620, loss_a: 0.014482
[22:24:01.999] iteration 5158 : loss: 0.026751, loss_a: 0.015736
[22:24:03.395] iteration 5159 : loss: 0.038672, loss_a: 0.022748
[22:24:04.138] iteration 5160 : loss: 0.035271, loss_a: 0.020747
[22:24:05.485] iteration 5161 : loss: 0.024312, loss_a: 0.014301
[22:24:06.233] iteration 5162 : loss: 0.030689, loss_a: 0.018052
[22:24:07.551] iteration 5163 : loss: 0.017861, loss_a: 0.010506
[22:24:08.294] iteration 5164 : loss: 0.030384, loss_a: 0.017873
[22:24:09.639] iteration 5165 : loss: 0.021771, loss_a: 0.012806
[22:24:10.391] iteration 5166 : loss: 0.024979, loss_a: 0.014694
[22:24:11.722] iteration 5167 : loss: 0.036227, loss_a: 0.021310
[22:24:12.462] iteration 5168 : loss: 0.026190, loss_a: 0.015406
[22:24:13.824] iteration 5169 : loss: 0.061554, loss_a: 0.036208
[22:24:14.572] iteration 5170 : loss: 0.027128, loss_a: 0.015958
[22:24:15.905] iteration 5171 : loss: 0.022641, loss_a: 0.013318
[22:24:16.648] iteration 5172 : loss: 0.030861, loss_a: 0.018153
[22:24:17.976] iteration 5173 : loss: 0.014714, loss_a: 0.008655
[22:24:18.720] iteration 5174 : loss: 0.030063, loss_a: 0.017684
[22:24:20.084] iteration 5175 : loss: 0.045152, loss_a: 0.026560
[22:24:20.828] iteration 5176 : loss: 0.022515, loss_a: 0.013244
[22:24:22.165] iteration 5177 : loss: 0.016901, loss_a: 0.009942
[22:24:22.901] iteration 5178 : loss: 0.021511, loss_a: 0.012654
[22:24:24.251] iteration 5179 : loss: 0.046819, loss_a: 0.027540
[22:24:24.988] iteration 5180 : loss: 0.046217, loss_a: 0.027186
[22:24:26.322] iteration 5181 : loss: 0.018039, loss_a: 0.010611
[22:24:27.072] iteration 5182 : loss: 0.041613, loss_a: 0.024478
[22:24:28.411] iteration 5183 : loss: 0.057081, loss_a: 0.033577
[22:24:29.151] iteration 5184 : loss: 0.023522, loss_a: 0.013836
[22:24:30.466] iteration 5185 : loss: 0.014779, loss_a: 0.008694
[22:24:31.205] iteration 5186 : loss: 0.029673, loss_a: 0.017454
[22:24:32.557] iteration 5187 : loss: 0.033451, loss_a: 0.019677
[22:24:33.296] iteration 5188 : loss: 0.024969, loss_a: 0.014688
[22:24:34.650] iteration 5189 : loss: 0.030605, loss_a: 0.018003
[22:24:35.392] iteration 5190 : loss: 0.031058, loss_a: 0.018270
[22:24:36.711] iteration 5191 : loss: 0.036872, loss_a: 0.021690
[22:24:37.453] iteration 5192 : loss: 0.035841, loss_a: 0.021083
[22:24:38.799] iteration 5193 : loss: 0.044850, loss_a: 0.026382
[22:24:39.536] iteration 5194 : loss: 0.030509, loss_a: 0.017946
[22:24:40.863] iteration 5195 : loss: 0.058708, loss_a: 0.034534
[22:24:41.607] iteration 5196 : loss: 0.043841, loss_a: 0.025789
[22:24:42.953] iteration 5197 : loss: 0.014676, loss_a: 0.008633
[22:24:43.711] iteration 5198 : loss: 0.019689, loss_a: 0.011582
[22:24:45.058] iteration 5199 : loss: 0.016882, loss_a: 0.009930
[22:24:45.799] iteration 5200 : loss: 0.047735, loss_a: 0.028080
[22:25:09.348] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_5200_dice_0.8984.pth
[22:25:10.665] iteration 5201 : loss: 0.036797, loss_a: 0.021645
[22:25:12.789] iteration 5202 : loss: 0.056449, loss_a: 0.033205
[22:25:14.130] iteration 5203 : loss: 0.023194, loss_a: 0.013644
[22:25:14.878] iteration 5204 : loss: 0.064844, loss_a: 0.038143
[22:25:16.203] iteration 5205 : loss: 0.023740, loss_a: 0.013965
[22:25:16.942] iteration 5206 : loss: 0.025095, loss_a: 0.014762
[22:25:18.258] iteration 5207 : loss: 0.022492, loss_a: 0.013231
[22:25:18.989] iteration 5208 : loss: 0.011758, loss_a: 0.006917
[22:25:20.316] iteration 5209 : loss: 0.020815, loss_a: 0.012244
[22:25:21.060] iteration 5210 : loss: 0.044965, loss_a: 0.026450
[22:25:22.420] iteration 5211 : loss: 0.052676, loss_a: 0.030986
[22:25:23.166] iteration 5212 : loss: 0.047675, loss_a: 0.028044
[22:25:24.481] iteration 5213 : loss: 0.042393, loss_a: 0.024937
[22:25:25.213] iteration 5214 : loss: 0.041662, loss_a: 0.024507
[22:25:26.565] iteration 5215 : loss: 0.048842, loss_a: 0.028730
[22:25:27.302] iteration 5216 : loss: 0.019873, loss_a: 0.011690
[22:25:28.639] iteration 5217 : loss: 0.027765, loss_a: 0.016333
[22:25:29.383] iteration 5218 : loss: 0.038895, loss_a: 0.022879
[22:25:30.700] iteration 5219 : loss: 0.060181, loss_a: 0.035401
[22:25:31.438] iteration 5220 : loss: 0.037347, loss_a: 0.021969
[22:25:32.803] iteration 5221 : loss: 0.052627, loss_a: 0.030957
[22:25:33.549] iteration 5222 : loss: 0.044998, loss_a: 0.026470
[22:25:34.881] iteration 5223 : loss: 0.045390, loss_a: 0.026700
[22:25:35.614] iteration 5224 : loss: 0.014057, loss_a: 0.008269
[22:25:36.975] iteration 5225 : loss: 0.024756, loss_a: 0.014562
[22:25:37.704] iteration 5226 : loss: 0.017579, loss_a: 0.010341
[22:25:39.066] iteration 5227 : loss: 0.028505, loss_a: 0.016768
[22:25:39.802] iteration 5228 : loss: 0.038180, loss_a: 0.022459
[22:25:41.144] iteration 5229 : loss: 0.035019, loss_a: 0.020599
[22:25:41.885] iteration 5230 : loss: 0.016985, loss_a: 0.009991
[22:25:43.203] iteration 5231 : loss: 0.016830, loss_a: 0.009900
[22:25:43.958] iteration 5232 : loss: 0.029066, loss_a: 0.017098
[22:25:45.305] iteration 5233 : loss: 0.049252, loss_a: 0.028971
[22:25:46.042] iteration 5234 : loss: 0.022806, loss_a: 0.013415
[22:25:47.358] iteration 5235 : loss: 0.028543, loss_a: 0.016790
[22:25:48.108] iteration 5236 : loss: 0.056480, loss_a: 0.033223
[22:25:49.455] iteration 5237 : loss: 0.009445, loss_a: 0.005556
[22:25:50.199] iteration 5238 : loss: 0.043073, loss_a: 0.025337
[22:25:51.507] iteration 5239 : loss: 0.029839, loss_a: 0.017552
[22:25:52.252] iteration 5240 : loss: 0.027063, loss_a: 0.015919
[22:25:53.566] iteration 5241 : loss: 0.030667, loss_a: 0.018039
[22:25:54.302] iteration 5242 : loss: 0.028688, loss_a: 0.016876
[22:25:55.652] iteration 5243 : loss: 0.018214, loss_a: 0.010714
[22:25:56.388] iteration 5244 : loss: 0.034983, loss_a: 0.020578
[22:25:57.748] iteration 5245 : loss: 0.020899, loss_a: 0.012294
[22:25:58.488] iteration 5246 : loss: 0.020313, loss_a: 0.011949
[22:25:59.835] iteration 5247 : loss: 0.051246, loss_a: 0.030145
[22:26:00.571] iteration 5248 : loss: 0.040837, loss_a: 0.024022
[22:26:01.935] iteration 5249 : loss: 0.026289, loss_a: 0.015464
[22:26:02.668] iteration 5250 : loss: 0.015892, loss_a: 0.009348
[22:26:04.011] iteration 5251 : loss: 0.024440, loss_a: 0.014377
[22:26:04.755] iteration 5252 : loss: 0.036429, loss_a: 0.021429
[22:26:06.075] iteration 5253 : loss: 0.021808, loss_a: 0.012828
[22:26:06.821] iteration 5254 : loss: 0.037519, loss_a: 0.022070
[22:26:08.190] iteration 5255 : loss: 0.067201, loss_a: 0.039530
[22:26:08.939] iteration 5256 : loss: 0.028943, loss_a: 0.017025
[22:26:10.287] iteration 5257 : loss: 0.038897, loss_a: 0.022880
[22:26:11.028] iteration 5258 : loss: 0.023214, loss_a: 0.013655
[22:26:12.371] iteration 5259 : loss: 0.030400, loss_a: 0.017882
[22:26:13.124] iteration 5260 : loss: 0.036280, loss_a: 0.021341
[22:26:14.449] iteration 5261 : loss: 0.027291, loss_a: 0.016053
[22:26:15.187] iteration 5262 : loss: 0.025017, loss_a: 0.014716
[22:26:16.500] iteration 5263 : loss: 0.032463, loss_a: 0.019096
[22:26:17.240] iteration 5264 : loss: 0.028508, loss_a: 0.016769
[22:26:18.560] iteration 5265 : loss: 0.022782, loss_a: 0.013401
[22:26:19.305] iteration 5266 : loss: 0.031972, loss_a: 0.018807
[22:26:20.633] iteration 5267 : loss: 0.031827, loss_a: 0.018722
[22:26:21.375] iteration 5268 : loss: 0.038163, loss_a: 0.022449
[22:26:22.729] iteration 5269 : loss: 0.016719, loss_a: 0.009835
[22:26:23.467] iteration 5270 : loss: 0.016855, loss_a: 0.009915
[22:26:24.795] iteration 5271 : loss: 0.046025, loss_a: 0.027073
[22:26:25.555] iteration 5272 : loss: 0.059656, loss_a: 0.035092
[22:26:26.898] iteration 5273 : loss: 0.025157, loss_a: 0.014798
[22:26:27.634] iteration 5274 : loss: 0.046901, loss_a: 0.027589
[22:26:28.986] iteration 5275 : loss: 0.078613, loss_a: 0.046243
[22:26:29.749] iteration 5276 : loss: 0.031464, loss_a: 0.018508
[22:26:31.113] iteration 5277 : loss: 0.054788, loss_a: 0.032228
[22:26:31.847] iteration 5278 : loss: 0.016877, loss_a: 0.009928
[22:26:33.205] iteration 5279 : loss: 0.065903, loss_a: 0.038767
[22:26:33.939] iteration 5280 : loss: 0.023846, loss_a: 0.014027
[22:26:35.245] iteration 5281 : loss: 0.021320, loss_a: 0.012541
[22:26:35.988] iteration 5282 : loss: 0.079824, loss_a: 0.046955
[22:26:37.322] iteration 5283 : loss: 0.025778, loss_a: 0.015163
[22:26:38.073] iteration 5284 : loss: 0.039193, loss_a: 0.023055
[22:26:39.418] iteration 5285 : loss: 0.063827, loss_a: 0.037545
[22:26:40.161] iteration 5286 : loss: 0.024740, loss_a: 0.014553
[22:26:41.505] iteration 5287 : loss: 0.016039, loss_a: 0.009435
[22:26:42.245] iteration 5288 : loss: 0.031070, loss_a: 0.018277
[22:26:43.575] iteration 5289 : loss: 0.026773, loss_a: 0.015749
[22:26:44.331] iteration 5290 : loss: 0.035144, loss_a: 0.020673
[22:26:45.648] iteration 5291 : loss: 0.030037, loss_a: 0.017669
[22:26:46.384] iteration 5292 : loss: 0.031209, loss_a: 0.018358
[22:26:47.702] iteration 5293 : loss: 0.014470, loss_a: 0.008512
[22:26:48.438] iteration 5294 : loss: 0.042520, loss_a: 0.025012
[22:26:49.770] iteration 5295 : loss: 0.035469, loss_a: 0.020864
[22:26:50.509] iteration 5296 : loss: 0.038264, loss_a: 0.022508
[22:26:51.848] iteration 5297 : loss: 0.038037, loss_a: 0.022375
[22:26:52.591] iteration 5298 : loss: 0.034393, loss_a: 0.020231
[22:26:53.921] iteration 5299 : loss: 0.022336, loss_a: 0.013139
[22:26:54.655] iteration 5300 : loss: 0.066968, loss_a: 0.039393
[22:26:55.974] iteration 5301 : loss: 0.025146, loss_a: 0.014792
[22:26:56.719] iteration 5302 : loss: 0.024120, loss_a: 0.014188
[22:26:58.046] iteration 5303 : loss: 0.027064, loss_a: 0.015920
[22:26:58.789] iteration 5304 : loss: 0.048370, loss_a: 0.028453
[22:27:00.107] iteration 5305 : loss: 0.023517, loss_a: 0.013834
[22:27:00.847] iteration 5306 : loss: 0.032711, loss_a: 0.019242
[22:27:02.176] iteration 5307 : loss: 0.026489, loss_a: 0.015582
[22:27:02.929] iteration 5308 : loss: 0.030612, loss_a: 0.018007
[22:27:04.245] iteration 5309 : loss: 0.013807, loss_a: 0.008122
[22:27:04.985] iteration 5310 : loss: 0.028008, loss_a: 0.016476
[22:27:06.347] iteration 5311 : loss: 0.055476, loss_a: 0.032633
[22:27:07.093] iteration 5312 : loss: 0.041343, loss_a: 0.024319
[22:27:08.435] iteration 5313 : loss: 0.019645, loss_a: 0.011556
[22:27:09.194] iteration 5314 : loss: 0.026923, loss_a: 0.015837
[22:27:10.509] iteration 5315 : loss: 0.027351, loss_a: 0.016089
[22:27:11.264] iteration 5316 : loss: 0.044282, loss_a: 0.026048
[22:27:12.593] iteration 5317 : loss: 0.015628, loss_a: 0.009193
[22:27:13.348] iteration 5318 : loss: 0.022433, loss_a: 0.013196
[22:27:14.706] iteration 5319 : loss: 0.032226, loss_a: 0.018957
[22:27:15.440] iteration 5320 : loss: 0.031768, loss_a: 0.018687
[22:27:16.797] iteration 5321 : loss: 0.033860, loss_a: 0.019918
[22:27:17.550] iteration 5322 : loss: 0.024782, loss_a: 0.014577
[22:27:18.911] iteration 5323 : loss: 0.029040, loss_a: 0.017083
[22:27:19.645] iteration 5324 : loss: 0.019897, loss_a: 0.011704
[22:27:20.999] iteration 5325 : loss: 0.017884, loss_a: 0.010520
[22:27:21.738] iteration 5326 : loss: 0.042305, loss_a: 0.024885
[22:27:23.065] iteration 5327 : loss: 0.036861, loss_a: 0.021683
[22:27:23.820] iteration 5328 : loss: 0.032389, loss_a: 0.019052
[22:27:25.206] iteration 5329 : loss: 0.031310, loss_a: 0.018418
[22:27:25.939] iteration 5330 : loss: 0.025434, loss_a: 0.014961
[22:27:27.273] iteration 5331 : loss: 0.027999, loss_a: 0.016470
[22:27:28.018] iteration 5332 : loss: 0.053981, loss_a: 0.031754
[22:27:29.373] iteration 5333 : loss: 0.063877, loss_a: 0.037575
[22:27:30.125] iteration 5334 : loss: 0.075942, loss_a: 0.044672
[22:27:31.475] iteration 5335 : loss: 0.031282, loss_a: 0.018401
[22:27:32.221] iteration 5336 : loss: 0.058421, loss_a: 0.034365
[22:27:33.542] iteration 5337 : loss: 0.068140, loss_a: 0.040082
[22:27:34.288] iteration 5338 : loss: 0.026946, loss_a: 0.015851
[22:27:35.620] iteration 5339 : loss: 0.032772, loss_a: 0.019278
[22:27:36.360] iteration 5340 : loss: 0.027688, loss_a: 0.016287
[22:27:37.713] iteration 5341 : loss: 0.019767, loss_a: 0.011628
[22:27:38.453] iteration 5342 : loss: 0.025382, loss_a: 0.014931
[22:27:39.777] iteration 5343 : loss: 0.022732, loss_a: 0.013372
[22:27:40.527] iteration 5344 : loss: 0.033488, loss_a: 0.019699
[22:27:41.870] iteration 5345 : loss: 0.045827, loss_a: 0.026957
[22:27:42.628] iteration 5346 : loss: 0.039458, loss_a: 0.023211
[22:27:43.988] iteration 5347 : loss: 0.032966, loss_a: 0.019392
[22:27:44.734] iteration 5348 : loss: 0.061932, loss_a: 0.036431
[22:27:46.099] iteration 5349 : loss: 0.025473, loss_a: 0.014984
[22:27:46.838] iteration 5350 : loss: 0.047426, loss_a: 0.027898
[22:27:48.204] iteration 5351 : loss: 0.025821, loss_a: 0.015189
[22:27:48.936] iteration 5352 : loss: 0.022668, loss_a: 0.013334
[22:27:50.277] iteration 5353 : loss: 0.024877, loss_a: 0.014634
[22:27:51.006] iteration 5354 : loss: 0.017002, loss_a: 0.010001
[22:27:52.363] iteration 5355 : loss: 0.029072, loss_a: 0.017101
[22:27:53.114] iteration 5356 : loss: 0.022144, loss_a: 0.013026
[22:27:54.436] iteration 5357 : loss: 0.030829, loss_a: 0.018134
[22:27:55.173] iteration 5358 : loss: 0.015729, loss_a: 0.009252
[22:27:56.504] iteration 5359 : loss: 0.014382, loss_a: 0.008460
[22:27:57.241] iteration 5360 : loss: 0.040045, loss_a: 0.023556
[22:27:58.580] iteration 5361 : loss: 0.032442, loss_a: 0.019084
[22:27:59.324] iteration 5362 : loss: 0.028777, loss_a: 0.016928
[22:28:00.648] iteration 5363 : loss: 0.019675, loss_a: 0.011574
[22:28:01.384] iteration 5364 : loss: 0.015642, loss_a: 0.009201
[22:28:02.722] iteration 5365 : loss: 0.028986, loss_a: 0.017050
[22:28:03.461] iteration 5366 : loss: 0.017036, loss_a: 0.010021
[22:28:04.768] iteration 5367 : loss: 0.037405, loss_a: 0.022003
[22:28:05.515] iteration 5368 : loss: 0.022701, loss_a: 0.013354
[22:28:06.868] iteration 5369 : loss: 0.051330, loss_a: 0.030194
[22:28:07.606] iteration 5370 : loss: 0.052141, loss_a: 0.030671
[22:28:08.912] iteration 5371 : loss: 0.014357, loss_a: 0.008445
[22:28:09.640] iteration 5372 : loss: 0.034518, loss_a: 0.020305
[22:28:10.976] iteration 5373 : loss: 0.038908, loss_a: 0.022887
[22:28:11.713] iteration 5374 : loss: 0.021564, loss_a: 0.012685
[22:28:13.037] iteration 5375 : loss: 0.063047, loss_a: 0.037087
[22:28:13.782] iteration 5376 : loss: 0.037456, loss_a: 0.022033
[22:28:15.104] iteration 5377 : loss: 0.023338, loss_a: 0.013728
[22:28:15.846] iteration 5378 : loss: 0.048911, loss_a: 0.028771
[22:28:17.174] iteration 5379 : loss: 0.035450, loss_a: 0.020853
[22:28:17.911] iteration 5380 : loss: 0.029267, loss_a: 0.017216
[22:28:19.225] iteration 5381 : loss: 0.017941, loss_a: 0.010553
[22:28:19.979] iteration 5382 : loss: 0.072003, loss_a: 0.042354
[22:28:21.330] iteration 5383 : loss: 0.024924, loss_a: 0.014661
[22:28:22.064] iteration 5384 : loss: 0.017617, loss_a: 0.010363
[22:28:23.391] iteration 5385 : loss: 0.011309, loss_a: 0.006652
[22:28:24.135] iteration 5386 : loss: 0.027251, loss_a: 0.016030
[22:28:25.447] iteration 5387 : loss: 0.015641, loss_a: 0.009201
[22:28:26.186] iteration 5388 : loss: 0.037415, loss_a: 0.022009
[22:28:27.528] iteration 5389 : loss: 0.021734, loss_a: 0.012785
[22:28:28.267] iteration 5390 : loss: 0.040861, loss_a: 0.024036
[22:28:29.585] iteration 5391 : loss: 0.028067, loss_a: 0.016510
[22:28:30.324] iteration 5392 : loss: 0.018421, loss_a: 0.010836
[22:28:31.678] iteration 5393 : loss: 0.033147, loss_a: 0.019498
[22:28:32.418] iteration 5394 : loss: 0.040514, loss_a: 0.023832
[22:28:33.716] iteration 5395 : loss: 0.011153, loss_a: 0.006561
[22:28:34.467] iteration 5396 : loss: 0.035666, loss_a: 0.020980
[22:28:35.833] iteration 5397 : loss: 0.051341, loss_a: 0.030201
[22:28:36.576] iteration 5398 : loss: 0.044663, loss_a: 0.026272
[22:28:37.926] iteration 5399 : loss: 0.050421, loss_a: 0.029660
[22:28:38.659] iteration 5400 : loss: 0.023723, loss_a: 0.013954
[22:29:03.291] iteration 5401 : loss: 0.019652, loss_a: 0.011560
[22:29:05.422] iteration 5402 : loss: 0.019093, loss_a: 0.011231
[22:29:06.759] iteration 5403 : loss: 0.037199, loss_a: 0.021882
[22:29:07.522] iteration 5404 : loss: 0.041346, loss_a: 0.024321
[22:29:08.835] iteration 5405 : loss: 0.035299, loss_a: 0.020764
[22:29:09.583] iteration 5406 : loss: 0.024385, loss_a: 0.014344
[22:29:10.953] iteration 5407 : loss: 0.019384, loss_a: 0.011402
[22:29:11.705] iteration 5408 : loss: 0.026816, loss_a: 0.015774
[22:29:13.039] iteration 5409 : loss: 0.039321, loss_a: 0.023130
[22:29:13.794] iteration 5410 : loss: 0.046720, loss_a: 0.027482
[22:29:15.134] iteration 5411 : loss: 0.026161, loss_a: 0.015389
[22:29:15.887] iteration 5412 : loss: 0.045485, loss_a: 0.026756
[22:29:17.218] iteration 5413 : loss: 0.024525, loss_a: 0.014426
[22:29:17.956] iteration 5414 : loss: 0.018663, loss_a: 0.010978
[22:29:19.277] iteration 5415 : loss: 0.024098, loss_a: 0.014175
[22:29:20.025] iteration 5416 : loss: 0.053398, loss_a: 0.031411
[22:29:21.356] iteration 5417 : loss: 0.022761, loss_a: 0.013389
[22:29:22.096] iteration 5418 : loss: 0.053547, loss_a: 0.031498
[22:29:23.470] iteration 5419 : loss: 0.048636, loss_a: 0.028610
[22:29:24.222] iteration 5420 : loss: 0.047028, loss_a: 0.027663
[22:29:25.563] iteration 5421 : loss: 0.039747, loss_a: 0.023381
[22:29:26.301] iteration 5422 : loss: 0.018371, loss_a: 0.010806
[22:29:27.660] iteration 5423 : loss: 0.038372, loss_a: 0.022571
[22:29:28.399] iteration 5424 : loss: 0.031635, loss_a: 0.018609
[22:29:29.756] iteration 5425 : loss: 0.014674, loss_a: 0.008632
[22:29:30.496] iteration 5426 : loss: 0.020842, loss_a: 0.012260
[22:29:31.854] iteration 5427 : loss: 0.041652, loss_a: 0.024501
[22:29:32.600] iteration 5428 : loss: 0.023800, loss_a: 0.014000
[22:29:33.929] iteration 5429 : loss: 0.038528, loss_a: 0.022663
[22:29:34.686] iteration 5430 : loss: 0.022859, loss_a: 0.013446
[22:29:36.029] iteration 5431 : loss: 0.011803, loss_a: 0.006943
[22:29:36.771] iteration 5432 : loss: 0.026292, loss_a: 0.015466
[22:29:38.161] iteration 5433 : loss: 0.024103, loss_a: 0.014178
[22:29:38.911] iteration 5434 : loss: 0.051389, loss_a: 0.030229
[22:29:40.249] iteration 5435 : loss: 0.051478, loss_a: 0.030281
[22:29:40.987] iteration 5436 : loss: 0.019964, loss_a: 0.011744
[22:29:42.302] iteration 5437 : loss: 0.037138, loss_a: 0.021846
[22:29:43.040] iteration 5438 : loss: 0.013943, loss_a: 0.008202
[22:29:44.397] iteration 5439 : loss: 0.035590, loss_a: 0.020935
[22:29:45.135] iteration 5440 : loss: 0.020392, loss_a: 0.011996
[22:29:46.482] iteration 5441 : loss: 0.021495, loss_a: 0.012644
[22:29:47.234] iteration 5442 : loss: 0.028281, loss_a: 0.016636
[22:29:48.625] iteration 5443 : loss: 0.061499, loss_a: 0.036176
[22:29:49.363] iteration 5444 : loss: 0.045407, loss_a: 0.026710
[22:29:50.737] iteration 5445 : loss: 0.039933, loss_a: 0.023490
[22:29:51.477] iteration 5446 : loss: 0.018405, loss_a: 0.010827
[22:29:52.840] iteration 5447 : loss: 0.055027, loss_a: 0.032369
[22:29:53.581] iteration 5448 : loss: 0.032545, loss_a: 0.019144
[22:29:54.935] iteration 5449 : loss: 0.020424, loss_a: 0.012014
[22:29:55.694] iteration 5450 : loss: 0.060718, loss_a: 0.035716
[22:29:57.028] iteration 5451 : loss: 0.034705, loss_a: 0.020415
[22:29:57.765] iteration 5452 : loss: 0.017260, loss_a: 0.010153
[22:29:59.086] iteration 5453 : loss: 0.030452, loss_a: 0.017913
[22:29:59.821] iteration 5454 : loss: 0.028502, loss_a: 0.016766
[22:30:01.188] iteration 5455 : loss: 0.080008, loss_a: 0.047064
[22:30:01.932] iteration 5456 : loss: 0.029872, loss_a: 0.017572
[22:30:03.261] iteration 5457 : loss: 0.016796, loss_a: 0.009880
[22:30:04.010] iteration 5458 : loss: 0.039985, loss_a: 0.023520
[22:30:05.327] iteration 5459 : loss: 0.033736, loss_a: 0.019845
[22:30:06.076] iteration 5460 : loss: 0.035056, loss_a: 0.020621
[22:30:07.419] iteration 5461 : loss: 0.067685, loss_a: 0.039814
[22:30:08.159] iteration 5462 : loss: 0.015250, loss_a: 0.008971
[22:30:09.499] iteration 5463 : loss: 0.045704, loss_a: 0.026885
[22:30:10.238] iteration 5464 : loss: 0.024556, loss_a: 0.014445
[22:30:11.572] iteration 5465 : loss: 0.043826, loss_a: 0.025780
[22:30:12.311] iteration 5466 : loss: 0.024423, loss_a: 0.014366
[22:30:13.620] iteration 5467 : loss: 0.040143, loss_a: 0.023614
[22:30:14.364] iteration 5468 : loss: 0.030980, loss_a: 0.018224
[22:30:15.688] iteration 5469 : loss: 0.024563, loss_a: 0.014449
[22:30:16.435] iteration 5470 : loss: 0.022956, loss_a: 0.013504
[22:30:17.786] iteration 5471 : loss: 0.038497, loss_a: 0.022645
[22:30:18.531] iteration 5472 : loss: 0.031475, loss_a: 0.018515
[22:30:19.848] iteration 5473 : loss: 0.028480, loss_a: 0.016753
[22:30:20.582] iteration 5474 : loss: 0.041877, loss_a: 0.024634
[22:30:21.936] iteration 5475 : loss: 0.071695, loss_a: 0.042173
[22:30:22.676] iteration 5476 : loss: 0.022321, loss_a: 0.013130
[22:30:24.046] iteration 5477 : loss: 0.017123, loss_a: 0.010073
[22:30:24.780] iteration 5478 : loss: 0.027685, loss_a: 0.016285
[22:30:26.120] iteration 5479 : loss: 0.066464, loss_a: 0.039096
[22:30:26.865] iteration 5480 : loss: 0.032686, loss_a: 0.019227
[22:30:28.224] iteration 5481 : loss: 0.023533, loss_a: 0.013843
[22:30:28.965] iteration 5482 : loss: 0.024154, loss_a: 0.014208
[22:30:30.308] iteration 5483 : loss: 0.024220, loss_a: 0.014247
[22:30:31.059] iteration 5484 : loss: 0.017659, loss_a: 0.010388
[22:30:32.384] iteration 5485 : loss: 0.026913, loss_a: 0.015831
[22:30:33.133] iteration 5486 : loss: 0.063330, loss_a: 0.037253
[22:30:34.494] iteration 5487 : loss: 0.032512, loss_a: 0.019124
[22:30:35.235] iteration 5488 : loss: 0.031220, loss_a: 0.018364
[22:30:36.590] iteration 5489 : loss: 0.033976, loss_a: 0.019986
[22:30:37.326] iteration 5490 : loss: 0.041431, loss_a: 0.024371
[22:30:38.663] iteration 5491 : loss: 0.033269, loss_a: 0.019570
[22:30:39.399] iteration 5492 : loss: 0.020171, loss_a: 0.011865
[22:30:40.742] iteration 5493 : loss: 0.048690, loss_a: 0.028641
[22:30:41.480] iteration 5494 : loss: 0.032310, loss_a: 0.019006
[22:30:42.799] iteration 5495 : loss: 0.021843, loss_a: 0.012849
[22:30:43.548] iteration 5496 : loss: 0.042346, loss_a: 0.024910
[22:30:44.881] iteration 5497 : loss: 0.034203, loss_a: 0.020120
[22:30:45.621] iteration 5498 : loss: 0.037773, loss_a: 0.022219
[22:30:46.969] iteration 5499 : loss: 0.054527, loss_a: 0.032075
[22:30:47.716] iteration 5500 : loss: 0.033662, loss_a: 0.019801
[22:30:49.049] iteration 5501 : loss: 0.063691, loss_a: 0.037465
[22:30:49.791] iteration 5502 : loss: 0.023722, loss_a: 0.013954
[22:30:51.112] iteration 5503 : loss: 0.018403, loss_a: 0.010825
[22:30:51.862] iteration 5504 : loss: 0.033498, loss_a: 0.019705
[22:30:53.210] iteration 5505 : loss: 0.049916, loss_a: 0.029362
[22:30:53.953] iteration 5506 : loss: 0.047151, loss_a: 0.027736
[22:30:55.287] iteration 5507 : loss: 0.054180, loss_a: 0.031871
[22:30:56.023] iteration 5508 : loss: 0.030611, loss_a: 0.018006
[22:30:57.381] iteration 5509 : loss: 0.015721, loss_a: 0.009248
[22:30:58.122] iteration 5510 : loss: 0.039625, loss_a: 0.023309
[22:30:59.457] iteration 5511 : loss: 0.021027, loss_a: 0.012369
[22:31:00.197] iteration 5512 : loss: 0.023341, loss_a: 0.013730
[22:31:01.533] iteration 5513 : loss: 0.034599, loss_a: 0.020352
[22:31:02.275] iteration 5514 : loss: 0.033107, loss_a: 0.019475
[22:31:03.639] iteration 5515 : loss: 0.016665, loss_a: 0.009803
[22:31:04.385] iteration 5516 : loss: 0.061785, loss_a: 0.036344
[22:31:05.729] iteration 5517 : loss: 0.043182, loss_a: 0.025401
[22:31:06.467] iteration 5518 : loss: 0.079996, loss_a: 0.047057
[22:31:07.770] iteration 5519 : loss: 0.022405, loss_a: 0.013179
[22:31:08.521] iteration 5520 : loss: 0.099183, loss_a: 0.058343
[22:31:09.882] iteration 5521 : loss: 0.009882, loss_a: 0.005813
[22:31:10.633] iteration 5522 : loss: 0.056350, loss_a: 0.033147
[22:31:11.999] iteration 5523 : loss: 0.029908, loss_a: 0.017593
[22:31:12.739] iteration 5524 : loss: 0.025210, loss_a: 0.014830
[22:31:14.089] iteration 5525 : loss: 0.019350, loss_a: 0.011383
[22:31:14.830] iteration 5526 : loss: 0.024112, loss_a: 0.014183
[22:31:16.165] iteration 5527 : loss: 0.019769, loss_a: 0.011629
[22:31:16.905] iteration 5528 : loss: 0.035724, loss_a: 0.021014
[22:31:18.227] iteration 5529 : loss: 0.019552, loss_a: 0.011501
[22:31:18.970] iteration 5530 : loss: 0.020628, loss_a: 0.012134
[22:31:20.325] iteration 5531 : loss: 0.037368, loss_a: 0.021981
[22:31:21.065] iteration 5532 : loss: 0.022894, loss_a: 0.013467
[22:31:22.429] iteration 5533 : loss: 0.055234, loss_a: 0.032491
[22:31:23.166] iteration 5534 : loss: 0.023219, loss_a: 0.013658
[22:31:24.515] iteration 5535 : loss: 0.020557, loss_a: 0.012093
[22:31:25.257] iteration 5536 : loss: 0.049810, loss_a: 0.029300
[22:31:26.610] iteration 5537 : loss: 0.030375, loss_a: 0.017868
[22:31:27.354] iteration 5538 : loss: 0.035670, loss_a: 0.020982
[22:31:28.715] iteration 5539 : loss: 0.066424, loss_a: 0.039073
[22:31:29.466] iteration 5540 : loss: 0.032554, loss_a: 0.019149
[22:31:30.802] iteration 5541 : loss: 0.019210, loss_a: 0.011300
[22:31:31.543] iteration 5542 : loss: 0.018676, loss_a: 0.010986
[22:31:32.879] iteration 5543 : loss: 0.036623, loss_a: 0.021543
[22:31:33.625] iteration 5544 : loss: 0.023477, loss_a: 0.013810
[22:31:35.016] iteration 5545 : loss: 0.162994, loss_a: 0.095879
[22:31:35.757] iteration 5546 : loss: 0.021961, loss_a: 0.012918
[22:31:37.101] iteration 5547 : loss: 0.014128, loss_a: 0.008311
[22:31:37.847] iteration 5548 : loss: 0.020820, loss_a: 0.012247
[22:31:39.207] iteration 5549 : loss: 0.023691, loss_a: 0.013936
[22:31:39.955] iteration 5550 : loss: 0.035161, loss_a: 0.020683
[22:31:41.288] iteration 5551 : loss: 0.039942, loss_a: 0.023495
[22:31:42.031] iteration 5552 : loss: 0.018843, loss_a: 0.011084
[22:31:43.351] iteration 5553 : loss: 0.031137, loss_a: 0.018316
[22:31:44.101] iteration 5554 : loss: 0.038175, loss_a: 0.022456
[22:31:45.416] iteration 5555 : loss: 0.032806, loss_a: 0.019298
[22:31:46.153] iteration 5556 : loss: 0.036216, loss_a: 0.021304
[22:31:47.481] iteration 5557 : loss: 0.070887, loss_a: 0.041698
[22:31:48.217] iteration 5558 : loss: 0.013602, loss_a: 0.008001
[22:31:49.569] iteration 5559 : loss: 0.036187, loss_a: 0.021287
[22:31:50.315] iteration 5560 : loss: 0.028773, loss_a: 0.016925
[22:31:51.646] iteration 5561 : loss: 0.017980, loss_a: 0.010577
[22:31:52.395] iteration 5562 : loss: 0.037631, loss_a: 0.022136
[22:31:53.730] iteration 5563 : loss: 0.072476, loss_a: 0.042633
[22:31:54.481] iteration 5564 : loss: 0.028921, loss_a: 0.017012
[22:31:55.825] iteration 5565 : loss: 0.029258, loss_a: 0.017211
[22:31:56.568] iteration 5566 : loss: 0.023030, loss_a: 0.013547
[22:31:57.892] iteration 5567 : loss: 0.023031, loss_a: 0.013548
[22:31:58.642] iteration 5568 : loss: 0.025766, loss_a: 0.015156
[22:31:59.964] iteration 5569 : loss: 0.030578, loss_a: 0.017987
[22:32:00.703] iteration 5570 : loss: 0.018992, loss_a: 0.011172
[22:32:02.049] iteration 5571 : loss: 0.035741, loss_a: 0.021024
[22:32:02.796] iteration 5572 : loss: 0.017016, loss_a: 0.010009
[22:32:04.099] iteration 5573 : loss: 0.015051, loss_a: 0.008853
[22:32:04.853] iteration 5574 : loss: 0.040384, loss_a: 0.023755
[22:32:06.217] iteration 5575 : loss: 0.020742, loss_a: 0.012201
[22:32:06.956] iteration 5576 : loss: 0.018803, loss_a: 0.011060
[22:32:08.324] iteration 5577 : loss: 0.037556, loss_a: 0.022092
[22:32:09.084] iteration 5578 : loss: 0.020844, loss_a: 0.012261
[22:32:10.421] iteration 5579 : loss: 0.032506, loss_a: 0.019121
[22:32:11.166] iteration 5580 : loss: 0.017614, loss_a: 0.010361
[22:32:12.516] iteration 5581 : loss: 0.032675, loss_a: 0.019220
[22:32:13.255] iteration 5582 : loss: 0.016452, loss_a: 0.009678
[22:32:14.607] iteration 5583 : loss: 0.014984, loss_a: 0.008814
[22:32:15.352] iteration 5584 : loss: 0.044593, loss_a: 0.026231
[22:32:16.703] iteration 5585 : loss: 0.022603, loss_a: 0.013296
[22:32:17.443] iteration 5586 : loss: 0.060743, loss_a: 0.035731
[22:32:18.808] iteration 5587 : loss: 0.064410, loss_a: 0.037888
[22:32:19.548] iteration 5588 : loss: 0.045850, loss_a: 0.026970
[22:32:20.899] iteration 5589 : loss: 0.015961, loss_a: 0.009389
[22:32:21.626] iteration 5590 : loss: 0.025896, loss_a: 0.015233
[22:32:22.953] iteration 5591 : loss: 0.040927, loss_a: 0.024075
[22:32:23.693] iteration 5592 : loss: 0.062301, loss_a: 0.036648
[22:32:25.059] iteration 5593 : loss: 0.024341, loss_a: 0.014318
[22:32:25.798] iteration 5594 : loss: 0.025247, loss_a: 0.014851
[22:32:27.140] iteration 5595 : loss: 0.017108, loss_a: 0.010063
[22:32:27.878] iteration 5596 : loss: 0.019044, loss_a: 0.011202
[22:32:29.216] iteration 5597 : loss: 0.013565, loss_a: 0.007980
[22:32:29.965] iteration 5598 : loss: 0.040588, loss_a: 0.023875
[22:32:31.272] iteration 5599 : loss: 0.019199, loss_a: 0.011294
[22:32:32.018] iteration 5600 : loss: 0.025920, loss_a: 0.015247
[22:32:56.662] iteration 5601 : loss: 0.049147, loss_a: 0.028910
[22:32:58.878] iteration 5602 : loss: 0.032027, loss_a: 0.018840
[22:33:00.212] iteration 5603 : loss: 0.016476, loss_a: 0.009692
[22:33:00.966] iteration 5604 : loss: 0.049576, loss_a: 0.029162
[22:33:02.285] iteration 5605 : loss: 0.015539, loss_a: 0.009141
[22:33:03.030] iteration 5606 : loss: 0.018802, loss_a: 0.011060
[22:33:04.362] iteration 5607 : loss: 0.033226, loss_a: 0.019545
[22:33:05.103] iteration 5608 : loss: 0.028755, loss_a: 0.016915
[22:33:06.471] iteration 5609 : loss: 0.045368, loss_a: 0.026687
[22:33:07.208] iteration 5610 : loss: 0.015942, loss_a: 0.009377
[22:33:08.563] iteration 5611 : loss: 0.025314, loss_a: 0.014890
[22:33:09.289] iteration 5612 : loss: 0.015605, loss_a: 0.009179
[22:33:10.609] iteration 5613 : loss: 0.045559, loss_a: 0.026799
[22:33:11.355] iteration 5614 : loss: 0.026499, loss_a: 0.015588
[22:33:12.698] iteration 5615 : loss: 0.034536, loss_a: 0.020315
[22:33:13.442] iteration 5616 : loss: 0.034278, loss_a: 0.020164
[22:33:14.784] iteration 5617 : loss: 0.019204, loss_a: 0.011297
[22:33:15.522] iteration 5618 : loss: 0.017001, loss_a: 0.010001
[22:33:16.850] iteration 5619 : loss: 0.035396, loss_a: 0.020821
[22:33:17.596] iteration 5620 : loss: 0.030601, loss_a: 0.018001
[22:33:18.938] iteration 5621 : loss: 0.015804, loss_a: 0.009297
[22:33:19.683] iteration 5622 : loss: 0.043875, loss_a: 0.025809
[22:33:21.026] iteration 5623 : loss: 0.025988, loss_a: 0.015287
[22:33:21.778] iteration 5624 : loss: 0.027752, loss_a: 0.016325
[22:33:23.091] iteration 5625 : loss: 0.023981, loss_a: 0.014106
[22:33:23.842] iteration 5626 : loss: 0.052189, loss_a: 0.030699
[22:33:25.199] iteration 5627 : loss: 0.024316, loss_a: 0.014303
[22:33:25.940] iteration 5628 : loss: 0.062614, loss_a: 0.036832
[22:33:27.261] iteration 5629 : loss: 0.014541, loss_a: 0.008554
[22:33:28.010] iteration 5630 : loss: 0.038268, loss_a: 0.022511
[22:33:29.368] iteration 5631 : loss: 0.022272, loss_a: 0.013101
[22:33:30.110] iteration 5632 : loss: 0.040295, loss_a: 0.023703
[22:33:31.468] iteration 5633 : loss: 0.031253, loss_a: 0.018384
[22:33:32.210] iteration 5634 : loss: 0.027680, loss_a: 0.016282
[22:33:33.546] iteration 5635 : loss: 0.055226, loss_a: 0.032486
[22:33:34.284] iteration 5636 : loss: 0.047361, loss_a: 0.027859
[22:33:35.643] iteration 5637 : loss: 0.025165, loss_a: 0.014803
[22:33:36.379] iteration 5638 : loss: 0.021082, loss_a: 0.012401
[22:33:37.738] iteration 5639 : loss: 0.061419, loss_a: 0.036129
[22:33:38.472] iteration 5640 : loss: 0.027723, loss_a: 0.016307
[22:33:39.828] iteration 5641 : loss: 0.026185, loss_a: 0.015403
[22:33:40.579] iteration 5642 : loss: 0.029644, loss_a: 0.017438
[22:33:41.919] iteration 5643 : loss: 0.023698, loss_a: 0.013940
[22:33:42.666] iteration 5644 : loss: 0.041675, loss_a: 0.024515
[22:33:44.017] iteration 5645 : loss: 0.024245, loss_a: 0.014262
[22:33:44.758] iteration 5646 : loss: 0.029756, loss_a: 0.017503
[22:33:46.102] iteration 5647 : loss: 0.024543, loss_a: 0.014437
[22:33:46.847] iteration 5648 : loss: 0.043675, loss_a: 0.025691
[22:33:48.191] iteration 5649 : loss: 0.032584, loss_a: 0.019167
[22:33:48.937] iteration 5650 : loss: 0.066058, loss_a: 0.038858
[22:33:50.257] iteration 5651 : loss: 0.014946, loss_a: 0.008792
[22:33:51.004] iteration 5652 : loss: 0.015279, loss_a: 0.008988
[22:33:52.331] iteration 5653 : loss: 0.024284, loss_a: 0.014285
[22:33:53.063] iteration 5654 : loss: 0.025205, loss_a: 0.014826
[22:33:54.420] iteration 5655 : loss: 0.069804, loss_a: 0.041061
[22:33:55.156] iteration 5656 : loss: 0.020890, loss_a: 0.012288
[22:33:56.521] iteration 5657 : loss: 0.030415, loss_a: 0.017891
[22:33:57.259] iteration 5658 : loss: 0.039236, loss_a: 0.023080
[22:33:58.607] iteration 5659 : loss: 0.017280, loss_a: 0.010164
[22:33:59.338] iteration 5660 : loss: 0.015227, loss_a: 0.008957
[22:34:00.644] iteration 5661 : loss: 0.018780, loss_a: 0.011047
[22:34:01.392] iteration 5662 : loss: 0.071150, loss_a: 0.041853
[22:34:02.725] iteration 5663 : loss: 0.027584, loss_a: 0.016226
[22:34:03.466] iteration 5664 : loss: 0.028336, loss_a: 0.016668
[22:34:04.816] iteration 5665 : loss: 0.017852, loss_a: 0.010501
[22:34:05.566] iteration 5666 : loss: 0.027034, loss_a: 0.015903
[22:34:06.913] iteration 5667 : loss: 0.038829, loss_a: 0.022840
[22:34:07.654] iteration 5668 : loss: 0.024903, loss_a: 0.014649
[22:34:08.975] iteration 5669 : loss: 0.042530, loss_a: 0.025018
[22:34:09.710] iteration 5670 : loss: 0.012196, loss_a: 0.007174
[22:34:11.064] iteration 5671 : loss: 0.039968, loss_a: 0.023510
[22:34:11.801] iteration 5672 : loss: 0.032012, loss_a: 0.018831
[22:34:13.120] iteration 5673 : loss: 0.045763, loss_a: 0.026919
[22:34:13.859] iteration 5674 : loss: 0.016132, loss_a: 0.009490
[22:34:15.197] iteration 5675 : loss: 0.039351, loss_a: 0.023148
[22:34:15.936] iteration 5676 : loss: 0.021155, loss_a: 0.012444
[22:34:17.258] iteration 5677 : loss: 0.041664, loss_a: 0.024508
[22:34:18.009] iteration 5678 : loss: 0.041318, loss_a: 0.024305
[22:34:19.367] iteration 5679 : loss: 0.035660, loss_a: 0.020976
[22:34:20.100] iteration 5680 : loss: 0.019542, loss_a: 0.011495
[22:34:21.425] iteration 5681 : loss: 0.026800, loss_a: 0.015765
[22:34:22.170] iteration 5682 : loss: 0.024311, loss_a: 0.014300
[22:34:23.514] iteration 5683 : loss: 0.021370, loss_a: 0.012571
[22:34:24.244] iteration 5684 : loss: 0.016378, loss_a: 0.009634
[22:34:25.605] iteration 5685 : loss: 0.039204, loss_a: 0.023061
[22:34:26.340] iteration 5686 : loss: 0.018985, loss_a: 0.011168
[22:34:27.688] iteration 5687 : loss: 0.023024, loss_a: 0.013543
[22:34:28.435] iteration 5688 : loss: 0.057026, loss_a: 0.033545
[22:34:29.769] iteration 5689 : loss: 0.034431, loss_a: 0.020254
[22:34:30.512] iteration 5690 : loss: 0.059655, loss_a: 0.035091
[22:34:31.834] iteration 5691 : loss: 0.037720, loss_a: 0.022188
[22:34:32.601] iteration 5692 : loss: 0.039777, loss_a: 0.023398
[22:34:33.920] iteration 5693 : loss: 0.023346, loss_a: 0.013733
[22:34:34.661] iteration 5694 : loss: 0.021548, loss_a: 0.012675
[22:34:36.010] iteration 5695 : loss: 0.023380, loss_a: 0.013753
[22:34:36.756] iteration 5696 : loss: 0.025532, loss_a: 0.015019
[22:34:38.118] iteration 5697 : loss: 0.039190, loss_a: 0.023053
[22:34:38.866] iteration 5698 : loss: 0.038256, loss_a: 0.022504
[22:34:40.232] iteration 5699 : loss: 0.028325, loss_a: 0.016662
[22:34:40.978] iteration 5700 : loss: 0.020909, loss_a: 0.012299
[22:34:42.302] iteration 5701 : loss: 0.028477, loss_a: 0.016751
[22:34:43.042] iteration 5702 : loss: 0.042562, loss_a: 0.025036
[22:34:44.358] iteration 5703 : loss: 0.022503, loss_a: 0.013237
[22:34:45.090] iteration 5704 : loss: 0.020430, loss_a: 0.012018
[22:34:46.420] iteration 5705 : loss: 0.025387, loss_a: 0.014933
[22:34:47.156] iteration 5706 : loss: 0.021075, loss_a: 0.012397
[22:34:48.503] iteration 5707 : loss: 0.025359, loss_a: 0.014917
[22:34:49.236] iteration 5708 : loss: 0.034534, loss_a: 0.020314
[22:34:50.568] iteration 5709 : loss: 0.056314, loss_a: 0.033126
[22:34:51.315] iteration 5710 : loss: 0.049774, loss_a: 0.029279
[22:34:52.643] iteration 5711 : loss: 0.020495, loss_a: 0.012056
[22:34:53.385] iteration 5712 : loss: 0.017304, loss_a: 0.010179
[22:34:54.741] iteration 5713 : loss: 0.030780, loss_a: 0.018106
[22:34:55.478] iteration 5714 : loss: 0.023194, loss_a: 0.013644
[22:34:56.836] iteration 5715 : loss: 0.034016, loss_a: 0.020009
[22:34:57.580] iteration 5716 : loss: 0.027533, loss_a: 0.016196
[22:34:58.937] iteration 5717 : loss: 0.088604, loss_a: 0.052120
[22:34:59.679] iteration 5718 : loss: 0.022736, loss_a: 0.013374
[22:35:00.998] iteration 5719 : loss: 0.020453, loss_a: 0.012031
[22:35:01.736] iteration 5720 : loss: 0.018121, loss_a: 0.010659
[22:35:03.056] iteration 5721 : loss: 0.053476, loss_a: 0.031457
[22:35:03.791] iteration 5722 : loss: 0.038757, loss_a: 0.022798
[22:35:05.105] iteration 5723 : loss: 0.024861, loss_a: 0.014624
[22:35:05.841] iteration 5724 : loss: 0.027655, loss_a: 0.016268
[22:35:07.170] iteration 5725 : loss: 0.042416, loss_a: 0.024951
[22:35:07.923] iteration 5726 : loss: 0.020232, loss_a: 0.011901
[22:35:09.269] iteration 5727 : loss: 0.018267, loss_a: 0.010745
[22:35:10.003] iteration 5728 : loss: 0.041601, loss_a: 0.024471
[22:35:11.339] iteration 5729 : loss: 0.034023, loss_a: 0.020013
[22:35:12.083] iteration 5730 : loss: 0.019778, loss_a: 0.011634
[22:35:13.445] iteration 5731 : loss: 0.029761, loss_a: 0.017507
[22:35:14.191] iteration 5732 : loss: 0.054965, loss_a: 0.032332
[22:35:15.530] iteration 5733 : loss: 0.036137, loss_a: 0.021257
[22:35:16.271] iteration 5734 : loss: 0.034625, loss_a: 0.020367
[22:35:17.603] iteration 5735 : loss: 0.022360, loss_a: 0.013153
[22:35:18.336] iteration 5736 : loss: 0.055190, loss_a: 0.032465
[22:35:19.697] iteration 5737 : loss: 0.058363, loss_a: 0.034331
[22:35:20.441] iteration 5738 : loss: 0.033792, loss_a: 0.019878
[22:35:21.763] iteration 5739 : loss: 0.024207, loss_a: 0.014239
[22:35:22.506] iteration 5740 : loss: 0.031072, loss_a: 0.018278
[22:35:23.828] iteration 5741 : loss: 0.021995, loss_a: 0.012938
[22:35:24.583] iteration 5742 : loss: 0.026684, loss_a: 0.015696
[22:35:25.913] iteration 5743 : loss: 0.035644, loss_a: 0.020967
[22:35:26.658] iteration 5744 : loss: 0.045501, loss_a: 0.026766
[22:35:28.003] iteration 5745 : loss: 0.033465, loss_a: 0.019686
[22:35:28.741] iteration 5746 : loss: 0.029479, loss_a: 0.017341
[22:35:30.075] iteration 5747 : loss: 0.032630, loss_a: 0.019194
[22:35:30.813] iteration 5748 : loss: 0.039747, loss_a: 0.023381
[22:35:32.147] iteration 5749 : loss: 0.022223, loss_a: 0.013072
[22:35:32.894] iteration 5750 : loss: 0.036287, loss_a: 0.021345
[22:35:34.225] iteration 5751 : loss: 0.020800, loss_a: 0.012235
[22:35:34.976] iteration 5752 : loss: 0.061332, loss_a: 0.036077
[22:35:36.308] iteration 5753 : loss: 0.025870, loss_a: 0.015218
[22:35:37.039] iteration 5754 : loss: 0.017743, loss_a: 0.010437
[22:35:38.400] iteration 5755 : loss: 0.035138, loss_a: 0.020670
[22:35:39.136] iteration 5756 : loss: 0.056224, loss_a: 0.033073
[22:35:40.488] iteration 5757 : loss: 0.027999, loss_a: 0.016470
[22:35:41.233] iteration 5758 : loss: 0.027714, loss_a: 0.016303
[22:35:42.629] iteration 5759 : loss: 0.031378, loss_a: 0.018458
[22:35:43.378] iteration 5760 : loss: 0.033942, loss_a: 0.019966
[22:35:44.707] iteration 5761 : loss: 0.033548, loss_a: 0.019734
[22:35:45.447] iteration 5762 : loss: 0.029843, loss_a: 0.017555
[22:35:46.796] iteration 5763 : loss: 0.022144, loss_a: 0.013026
[22:35:47.520] iteration 5764 : loss: 0.019671, loss_a: 0.011571
[22:35:48.885] iteration 5765 : loss: 0.054472, loss_a: 0.032042
[22:35:49.627] iteration 5766 : loss: 0.022340, loss_a: 0.013141
[22:35:50.959] iteration 5767 : loss: 0.059443, loss_a: 0.034966
[22:35:51.707] iteration 5768 : loss: 0.046077, loss_a: 0.027104
[22:35:53.055] iteration 5769 : loss: 0.015953, loss_a: 0.009384
[22:35:53.800] iteration 5770 : loss: 0.020676, loss_a: 0.012162
[22:35:55.129] iteration 5771 : loss: 0.048658, loss_a: 0.028623
[22:35:55.871] iteration 5772 : loss: 0.015768, loss_a: 0.009275
[22:35:57.251] iteration 5773 : loss: 0.031606, loss_a: 0.018592
[22:35:57.995] iteration 5774 : loss: 0.023345, loss_a: 0.013732
[22:35:59.324] iteration 5775 : loss: 0.017347, loss_a: 0.010204
[22:36:00.065] iteration 5776 : loss: 0.034551, loss_a: 0.020324
[22:36:01.389] iteration 5777 : loss: 0.025019, loss_a: 0.014717
[22:36:02.121] iteration 5778 : loss: 0.052135, loss_a: 0.030668
[22:36:03.477] iteration 5779 : loss: 0.050144, loss_a: 0.029496
[22:36:04.211] iteration 5780 : loss: 0.028435, loss_a: 0.016726
[22:36:05.540] iteration 5781 : loss: 0.011938, loss_a: 0.007023
[22:36:06.280] iteration 5782 : loss: 0.032384, loss_a: 0.019049
[22:36:07.653] iteration 5783 : loss: 0.029707, loss_a: 0.017475
[22:36:08.388] iteration 5784 : loss: 0.021452, loss_a: 0.012619
[22:36:09.755] iteration 5785 : loss: 0.041567, loss_a: 0.024451
[22:36:10.500] iteration 5786 : loss: 0.031136, loss_a: 0.018315
[22:36:11.834] iteration 5787 : loss: 0.034597, loss_a: 0.020351
[22:36:12.579] iteration 5788 : loss: 0.029807, loss_a: 0.017534
[22:36:13.932] iteration 5789 : loss: 0.025129, loss_a: 0.014782
[22:36:14.686] iteration 5790 : loss: 0.018916, loss_a: 0.011127
[22:36:16.011] iteration 5791 : loss: 0.032429, loss_a: 0.019076
[22:36:16.767] iteration 5792 : loss: 0.024839, loss_a: 0.014611
[22:36:18.112] iteration 5793 : loss: 0.025295, loss_a: 0.014880
[22:36:18.850] iteration 5794 : loss: 0.063328, loss_a: 0.037252
[22:36:20.163] iteration 5795 : loss: 0.036192, loss_a: 0.021290
[22:36:20.902] iteration 5796 : loss: 0.038358, loss_a: 0.022564
[22:36:22.217] iteration 5797 : loss: 0.028167, loss_a: 0.016569
[22:36:22.961] iteration 5798 : loss: 0.052613, loss_a: 0.030949
[22:36:24.254] iteration 5799 : loss: 0.013928, loss_a: 0.008193
[22:36:24.995] iteration 5800 : loss: 0.010982, loss_a: 0.006460
[22:36:49.664] iteration 5801 : loss: 0.014483, loss_a: 0.008519
[22:36:51.969] iteration 5802 : loss: 0.032882, loss_a: 0.019343
[22:36:53.289] iteration 5803 : loss: 0.023393, loss_a: 0.013760
[22:36:54.026] iteration 5804 : loss: 0.022088, loss_a: 0.012993
[22:36:55.350] iteration 5805 : loss: 0.024729, loss_a: 0.014546
[22:36:56.097] iteration 5806 : loss: 0.038930, loss_a: 0.022900
[22:36:57.451] iteration 5807 : loss: 0.024460, loss_a: 0.014388
[22:36:58.196] iteration 5808 : loss: 0.020752, loss_a: 0.012207
[22:36:59.551] iteration 5809 : loss: 0.035813, loss_a: 0.021067
[22:37:00.288] iteration 5810 : loss: 0.012727, loss_a: 0.007486
[22:37:01.633] iteration 5811 : loss: 0.040699, loss_a: 0.023940
[22:37:02.367] iteration 5812 : loss: 0.019734, loss_a: 0.011608
[22:37:03.738] iteration 5813 : loss: 0.040406, loss_a: 0.023768
[22:37:04.464] iteration 5814 : loss: 0.014119, loss_a: 0.008305
[22:37:05.784] iteration 5815 : loss: 0.052937, loss_a: 0.031140
[22:37:06.527] iteration 5816 : loss: 0.024033, loss_a: 0.014137
[22:37:07.860] iteration 5817 : loss: 0.027425, loss_a: 0.016133
[22:37:08.592] iteration 5818 : loss: 0.028783, loss_a: 0.016931
[22:37:09.922] iteration 5819 : loss: 0.027758, loss_a: 0.016328
[22:37:10.680] iteration 5820 : loss: 0.075960, loss_a: 0.044682
[22:37:12.018] iteration 5821 : loss: 0.030936, loss_a: 0.018198
[22:37:12.762] iteration 5822 : loss: 0.032619, loss_a: 0.019188
[22:37:14.083] iteration 5823 : loss: 0.046046, loss_a: 0.027086
[22:37:14.815] iteration 5824 : loss: 0.038764, loss_a: 0.022802
[22:37:16.154] iteration 5825 : loss: 0.025220, loss_a: 0.014835
[22:37:16.908] iteration 5826 : loss: 0.024477, loss_a: 0.014398
[22:37:18.252] iteration 5827 : loss: 0.068491, loss_a: 0.040289
[22:37:18.998] iteration 5828 : loss: 0.025166, loss_a: 0.014803
[22:37:20.344] iteration 5829 : loss: 0.047272, loss_a: 0.027807
[22:37:21.076] iteration 5830 : loss: 0.025966, loss_a: 0.015274
[22:37:22.420] iteration 5831 : loss: 0.028034, loss_a: 0.016491
[22:37:23.168] iteration 5832 : loss: 0.048574, loss_a: 0.028573
[22:37:24.529] iteration 5833 : loss: 0.015025, loss_a: 0.008838
[22:37:25.265] iteration 5834 : loss: 0.023680, loss_a: 0.013929
[22:37:26.585] iteration 5835 : loss: 0.030012, loss_a: 0.017654
[22:37:27.331] iteration 5836 : loss: 0.036951, loss_a: 0.021736
[22:37:28.670] iteration 5837 : loss: 0.038408, loss_a: 0.022593
[22:37:29.404] iteration 5838 : loss: 0.023446, loss_a: 0.013792
[22:37:30.743] iteration 5839 : loss: 0.025479, loss_a: 0.014988
[22:37:31.481] iteration 5840 : loss: 0.060221, loss_a: 0.035424
[22:37:32.806] iteration 5841 : loss: 0.016096, loss_a: 0.009468
[22:37:33.545] iteration 5842 : loss: 0.020576, loss_a: 0.012104
[22:37:34.901] iteration 5843 : loss: 0.018550, loss_a: 0.010912
[22:37:35.637] iteration 5844 : loss: 0.020226, loss_a: 0.011897
[22:37:36.984] iteration 5845 : loss: 0.020362, loss_a: 0.011978
[22:37:37.729] iteration 5846 : loss: 0.038321, loss_a: 0.022542
[22:37:39.097] iteration 5847 : loss: 0.033818, loss_a: 0.019893
[22:37:39.839] iteration 5848 : loss: 0.022890, loss_a: 0.013465
[22:37:41.162] iteration 5849 : loss: 0.022579, loss_a: 0.013282
[22:37:41.902] iteration 5850 : loss: 0.033144, loss_a: 0.019496
[22:37:43.225] iteration 5851 : loss: 0.074416, loss_a: 0.043774
[22:37:43.959] iteration 5852 : loss: 0.020906, loss_a: 0.012297
[22:37:45.318] iteration 5853 : loss: 0.030199, loss_a: 0.017764
[22:37:46.065] iteration 5854 : loss: 0.011913, loss_a: 0.007007
[22:37:47.393] iteration 5855 : loss: 0.022229, loss_a: 0.013076
[22:37:48.139] iteration 5856 : loss: 0.021763, loss_a: 0.012802
[22:37:49.480] iteration 5857 : loss: 0.037729, loss_a: 0.022194
[22:37:50.220] iteration 5858 : loss: 0.023196, loss_a: 0.013645
[22:37:51.576] iteration 5859 : loss: 0.043324, loss_a: 0.025485
[22:37:52.343] iteration 5860 : loss: 0.028143, loss_a: 0.016555
[22:37:53.680] iteration 5861 : loss: 0.014799, loss_a: 0.008705
[22:37:54.432] iteration 5862 : loss: 0.036431, loss_a: 0.021430
[22:37:55.795] iteration 5863 : loss: 0.060566, loss_a: 0.035627
[22:37:56.543] iteration 5864 : loss: 0.024390, loss_a: 0.014347
[22:37:57.884] iteration 5865 : loss: 0.017551, loss_a: 0.010324
[22:37:58.627] iteration 5866 : loss: 0.022368, loss_a: 0.013157
[22:37:59.972] iteration 5867 : loss: 0.013502, loss_a: 0.007942
[22:38:00.719] iteration 5868 : loss: 0.038582, loss_a: 0.022695
[22:38:02.074] iteration 5869 : loss: 0.035581, loss_a: 0.020930
[22:38:02.822] iteration 5870 : loss: 0.030634, loss_a: 0.018020
[22:38:04.174] iteration 5871 : loss: 0.019110, loss_a: 0.011241
[22:38:04.928] iteration 5872 : loss: 0.037073, loss_a: 0.021808
[22:38:06.247] iteration 5873 : loss: 0.016175, loss_a: 0.009514
[22:38:06.996] iteration 5874 : loss: 0.021365, loss_a: 0.012568
[22:38:08.360] iteration 5875 : loss: 0.045527, loss_a: 0.026780
[22:38:09.108] iteration 5876 : loss: 0.048640, loss_a: 0.028612
[22:38:10.421] iteration 5877 : loss: 0.046056, loss_a: 0.027092
[22:38:11.159] iteration 5878 : loss: 0.019673, loss_a: 0.011572
[22:38:12.496] iteration 5879 : loss: 0.016898, loss_a: 0.009940
[22:38:13.237] iteration 5880 : loss: 0.020399, loss_a: 0.011999
[22:38:14.560] iteration 5881 : loss: 0.016195, loss_a: 0.009527
[22:38:15.323] iteration 5882 : loss: 0.038437, loss_a: 0.022610
[22:38:16.660] iteration 5883 : loss: 0.015888, loss_a: 0.009346
[22:38:17.401] iteration 5884 : loss: 0.023510, loss_a: 0.013829
[22:38:18.729] iteration 5885 : loss: 0.020823, loss_a: 0.012249
[22:38:19.467] iteration 5886 : loss: 0.030094, loss_a: 0.017702
[22:38:20.789] iteration 5887 : loss: 0.020348, loss_a: 0.011969
[22:38:21.530] iteration 5888 : loss: 0.038569, loss_a: 0.022688
[22:38:22.848] iteration 5889 : loss: 0.019372, loss_a: 0.011395
[22:38:23.593] iteration 5890 : loss: 0.063576, loss_a: 0.037398
[22:38:24.948] iteration 5891 : loss: 0.030110, loss_a: 0.017711
[22:38:25.686] iteration 5892 : loss: 0.026571, loss_a: 0.015630
[22:38:27.028] iteration 5893 : loss: 0.024591, loss_a: 0.014465
[22:38:27.763] iteration 5894 : loss: 0.018199, loss_a: 0.010705
[22:38:29.121] iteration 5895 : loss: 0.030641, loss_a: 0.018024
[22:38:29.863] iteration 5896 : loss: 0.014154, loss_a: 0.008326
[22:38:31.229] iteration 5897 : loss: 0.027528, loss_a: 0.016193
[22:38:31.963] iteration 5898 : loss: 0.020303, loss_a: 0.011943
[22:38:33.312] iteration 5899 : loss: 0.061667, loss_a: 0.036275
[22:38:34.054] iteration 5900 : loss: 0.029078, loss_a: 0.017104
[22:38:35.399] iteration 5901 : loss: 0.054977, loss_a: 0.032339
[22:38:36.136] iteration 5902 : loss: 0.016733, loss_a: 0.009843
[22:38:37.467] iteration 5903 : loss: 0.030802, loss_a: 0.018119
[22:38:38.206] iteration 5904 : loss: 0.033370, loss_a: 0.019629
[22:38:39.523] iteration 5905 : loss: 0.017678, loss_a: 0.010399
[22:38:40.258] iteration 5906 : loss: 0.050179, loss_a: 0.029517
[22:38:41.581] iteration 5907 : loss: 0.034747, loss_a: 0.020440
[22:38:42.323] iteration 5908 : loss: 0.048209, loss_a: 0.028358
[22:38:43.678] iteration 5909 : loss: 0.027008, loss_a: 0.015887
[22:38:44.410] iteration 5910 : loss: 0.054730, loss_a: 0.032194
[22:38:45.767] iteration 5911 : loss: 0.029978, loss_a: 0.017634
[22:38:46.503] iteration 5912 : loss: 0.021066, loss_a: 0.012392
[22:38:47.841] iteration 5913 : loss: 0.020731, loss_a: 0.012195
[22:38:48.572] iteration 5914 : loss: 0.017773, loss_a: 0.010455
[22:38:49.903] iteration 5915 : loss: 0.016677, loss_a: 0.009810
[22:38:50.638] iteration 5916 : loss: 0.012204, loss_a: 0.007179
[22:38:52.003] iteration 5917 : loss: 0.031621, loss_a: 0.018601
[22:38:52.745] iteration 5918 : loss: 0.019178, loss_a: 0.011281
[22:38:54.107] iteration 5919 : loss: 0.023071, loss_a: 0.013571
[22:38:54.859] iteration 5920 : loss: 0.041655, loss_a: 0.024503
[22:38:56.206] iteration 5921 : loss: 0.021042, loss_a: 0.012378
[22:38:56.948] iteration 5922 : loss: 0.017572, loss_a: 0.010336
[22:38:58.274] iteration 5923 : loss: 0.051416, loss_a: 0.030245
[22:38:59.016] iteration 5924 : loss: 0.020729, loss_a: 0.012194
[22:39:00.363] iteration 5925 : loss: 0.017341, loss_a: 0.010201
[22:39:01.125] iteration 5926 : loss: 0.037440, loss_a: 0.022023
[22:39:02.500] iteration 5927 : loss: 0.028669, loss_a: 0.016864
[22:39:03.246] iteration 5928 : loss: 0.047963, loss_a: 0.028213
[22:39:04.600] iteration 5929 : loss: 0.015460, loss_a: 0.009094
[22:39:05.337] iteration 5930 : loss: 0.017676, loss_a: 0.010398
[22:39:06.701] iteration 5931 : loss: 0.042401, loss_a: 0.024942
[22:39:07.434] iteration 5932 : loss: 0.027660, loss_a: 0.016270
[22:39:08.761] iteration 5933 : loss: 0.025376, loss_a: 0.014927
[22:39:09.515] iteration 5934 : loss: 0.040371, loss_a: 0.023748
[22:39:10.867] iteration 5935 : loss: 0.034313, loss_a: 0.020184
[22:39:11.600] iteration 5936 : loss: 0.030150, loss_a: 0.017736
[22:39:12.922] iteration 5937 : loss: 0.013682, loss_a: 0.008048
[22:39:13.661] iteration 5938 : loss: 0.033292, loss_a: 0.019583
[22:39:15.029] iteration 5939 : loss: 0.020980, loss_a: 0.012341
[22:39:15.776] iteration 5940 : loss: 0.022689, loss_a: 0.013346
[22:39:17.126] iteration 5941 : loss: 0.043191, loss_a: 0.025407
[22:39:17.872] iteration 5942 : loss: 0.022490, loss_a: 0.013229
[22:39:19.237] iteration 5943 : loss: 0.020827, loss_a: 0.012251
[22:39:19.978] iteration 5944 : loss: 0.021317, loss_a: 0.012539
[22:39:21.322] iteration 5945 : loss: 0.044216, loss_a: 0.026010
[22:39:22.056] iteration 5946 : loss: 0.016767, loss_a: 0.009863
[22:39:23.398] iteration 5947 : loss: 0.011976, loss_a: 0.007045
[22:39:24.145] iteration 5948 : loss: 0.048509, loss_a: 0.028535
[22:39:25.501] iteration 5949 : loss: 0.039538, loss_a: 0.023258
[22:39:26.238] iteration 5950 : loss: 0.024462, loss_a: 0.014389
[22:39:27.565] iteration 5951 : loss: 0.021665, loss_a: 0.012744
[22:39:28.297] iteration 5952 : loss: 0.024354, loss_a: 0.014326
[22:39:29.639] iteration 5953 : loss: 0.025121, loss_a: 0.014777
[22:39:30.375] iteration 5954 : loss: 0.039607, loss_a: 0.023298
[22:39:31.725] iteration 5955 : loss: 0.023619, loss_a: 0.013894
[22:39:32.467] iteration 5956 : loss: 0.024892, loss_a: 0.014642
[22:39:33.819] iteration 5957 : loss: 0.024417, loss_a: 0.014363
[22:39:34.556] iteration 5958 : loss: 0.019812, loss_a: 0.011654
[22:39:35.904] iteration 5959 : loss: 0.020832, loss_a: 0.012254
[22:39:36.647] iteration 5960 : loss: 0.022104, loss_a: 0.013002
[22:39:37.969] iteration 5961 : loss: 0.035138, loss_a: 0.020670
[22:39:38.727] iteration 5962 : loss: 0.046319, loss_a: 0.027247
[22:39:40.045] iteration 5963 : loss: 0.022757, loss_a: 0.013386
[22:39:40.793] iteration 5964 : loss: 0.034408, loss_a: 0.020240
[22:39:42.132] iteration 5965 : loss: 0.024916, loss_a: 0.014657
[22:39:42.873] iteration 5966 : loss: 0.024416, loss_a: 0.014362
[22:39:44.218] iteration 5967 : loss: 0.015255, loss_a: 0.008973
[22:39:44.956] iteration 5968 : loss: 0.099951, loss_a: 0.058795
[22:39:46.299] iteration 5969 : loss: 0.017805, loss_a: 0.010473
[22:39:47.053] iteration 5970 : loss: 0.057177, loss_a: 0.033633
[22:39:48.395] iteration 5971 : loss: 0.036745, loss_a: 0.021615
[22:39:49.136] iteration 5972 : loss: 0.032099, loss_a: 0.018882
[22:39:50.470] iteration 5973 : loss: 0.043473, loss_a: 0.025572
[22:39:51.211] iteration 5974 : loss: 0.026562, loss_a: 0.015625
[22:39:52.526] iteration 5975 : loss: 0.026774, loss_a: 0.015749
[22:39:53.266] iteration 5976 : loss: 0.017691, loss_a: 0.010407
[22:39:54.597] iteration 5977 : loss: 0.018840, loss_a: 0.011083
[22:39:55.342] iteration 5978 : loss: 0.036788, loss_a: 0.021640
[22:39:56.687] iteration 5979 : loss: 0.037487, loss_a: 0.022051
[22:39:57.432] iteration 5980 : loss: 0.028172, loss_a: 0.016572
[22:39:58.747] iteration 5981 : loss: 0.024468, loss_a: 0.014393
[22:39:59.503] iteration 5982 : loss: 0.049608, loss_a: 0.029181
[22:40:00.839] iteration 5983 : loss: 0.020181, loss_a: 0.011871
[22:40:01.578] iteration 5984 : loss: 0.022070, loss_a: 0.012983
[22:40:02.933] iteration 5985 : loss: 0.036853, loss_a: 0.021678
[22:40:03.672] iteration 5986 : loss: 0.017719, loss_a: 0.010423
[22:40:05.029] iteration 5987 : loss: 0.021983, loss_a: 0.012931
[22:40:05.782] iteration 5988 : loss: 0.021103, loss_a: 0.012413
[22:40:07.137] iteration 5989 : loss: 0.031200, loss_a: 0.018353
[22:40:07.887] iteration 5990 : loss: 0.036035, loss_a: 0.021197
[22:40:09.203] iteration 5991 : loss: 0.009036, loss_a: 0.005315
[22:40:09.941] iteration 5992 : loss: 0.019793, loss_a: 0.011643
[22:40:11.270] iteration 5993 : loss: 0.068074, loss_a: 0.040043
[22:40:12.022] iteration 5994 : loss: 0.060051, loss_a: 0.035324
[22:40:13.381] iteration 5995 : loss: 0.038752, loss_a: 0.022795
[22:40:14.111] iteration 5996 : loss: 0.016388, loss_a: 0.009640
[22:40:15.453] iteration 5997 : loss: 0.027977, loss_a: 0.016457
[22:40:16.196] iteration 5998 : loss: 0.047403, loss_a: 0.027884
[22:40:17.520] iteration 5999 : loss: 0.017626, loss_a: 0.010368
[22:40:18.275] iteration 6000 : loss: 0.063419, loss_a: 0.037305
[22:40:42.974] iteration 6001 : loss: 0.019962, loss_a: 0.011743
[22:40:45.202] iteration 6002 : loss: 0.023098, loss_a: 0.013587
[22:40:46.505] iteration 6003 : loss: 0.036658, loss_a: 0.021563
[22:40:47.248] iteration 6004 : loss: 0.058894, loss_a: 0.034643
[22:40:48.586] iteration 6005 : loss: 0.029595, loss_a: 0.017409
[22:40:49.333] iteration 6006 : loss: 0.036719, loss_a: 0.021599
[22:40:50.691] iteration 6007 : loss: 0.037652, loss_a: 0.022149
[22:40:51.437] iteration 6008 : loss: 0.040647, loss_a: 0.023910
[22:40:52.788] iteration 6009 : loss: 0.020550, loss_a: 0.012088
[22:40:53.532] iteration 6010 : loss: 0.034743, loss_a: 0.020437
[22:40:54.886] iteration 6011 : loss: 0.041902, loss_a: 0.024648
[22:40:55.625] iteration 6012 : loss: 0.032006, loss_a: 0.018827
[22:40:56.988] iteration 6013 : loss: 0.073629, loss_a: 0.043311
[22:40:57.727] iteration 6014 : loss: 0.050026, loss_a: 0.029427
[22:40:59.078] iteration 6015 : loss: 0.020448, loss_a: 0.012028
[22:40:59.811] iteration 6016 : loss: 0.019479, loss_a: 0.011458
[22:41:01.179] iteration 6017 : loss: 0.046712, loss_a: 0.027477
[22:41:01.923] iteration 6018 : loss: 0.025608, loss_a: 0.015063
[22:41:03.275] iteration 6019 : loss: 0.029935, loss_a: 0.017609
[22:41:04.018] iteration 6020 : loss: 0.051674, loss_a: 0.030396
[22:41:05.357] iteration 6021 : loss: 0.055425, loss_a: 0.032603
[22:41:06.091] iteration 6022 : loss: 0.025427, loss_a: 0.014957
[22:41:07.445] iteration 6023 : loss: 0.031219, loss_a: 0.018364
[22:41:08.181] iteration 6024 : loss: 0.035354, loss_a: 0.020797
[22:41:09.509] iteration 6025 : loss: 0.030255, loss_a: 0.017797
[22:41:10.257] iteration 6026 : loss: 0.043814, loss_a: 0.025773
[22:41:11.606] iteration 6027 : loss: 0.046714, loss_a: 0.027479
[22:41:12.346] iteration 6028 : loss: 0.022385, loss_a: 0.013168
[22:41:13.672] iteration 6029 : loss: 0.028939, loss_a: 0.017023
[22:41:14.414] iteration 6030 : loss: 0.041396, loss_a: 0.024351
[22:41:15.737] iteration 6031 : loss: 0.022184, loss_a: 0.013049
[22:41:16.477] iteration 6032 : loss: 0.035689, loss_a: 0.020994
[22:41:17.837] iteration 6033 : loss: 0.021902, loss_a: 0.012884
[22:41:18.579] iteration 6034 : loss: 0.029511, loss_a: 0.017359
[22:41:19.917] iteration 6035 : loss: 0.021460, loss_a: 0.012624
[22:41:20.661] iteration 6036 : loss: 0.032856, loss_a: 0.019327
[22:41:21.992] iteration 6037 : loss: 0.062620, loss_a: 0.036835
[22:41:22.732] iteration 6038 : loss: 0.031080, loss_a: 0.018283
[22:41:24.069] iteration 6039 : loss: 0.051354, loss_a: 0.030208
[22:41:24.810] iteration 6040 : loss: 0.064687, loss_a: 0.038051
[22:41:26.156] iteration 6041 : loss: 0.018708, loss_a: 0.011004
[22:41:26.899] iteration 6042 : loss: 0.032197, loss_a: 0.018940
[22:41:28.247] iteration 6043 : loss: 0.034734, loss_a: 0.020432
[22:41:28.990] iteration 6044 : loss: 0.017316, loss_a: 0.010186
[22:41:30.345] iteration 6045 : loss: 0.018019, loss_a: 0.010599
[22:41:31.101] iteration 6046 : loss: 0.030956, loss_a: 0.018210
[22:41:32.435] iteration 6047 : loss: 0.029293, loss_a: 0.017231
[22:41:33.169] iteration 6048 : loss: 0.050907, loss_a: 0.029945
[22:41:34.510] iteration 6049 : loss: 0.026169, loss_a: 0.015393
[22:41:35.254] iteration 6050 : loss: 0.015514, loss_a: 0.009126
[22:41:36.576] iteration 6051 : loss: 0.019365, loss_a: 0.011391
[22:41:37.328] iteration 6052 : loss: 0.048706, loss_a: 0.028650
[22:41:38.640] iteration 6053 : loss: 0.024180, loss_a: 0.014223
[22:41:39.386] iteration 6054 : loss: 0.065693, loss_a: 0.038643
[22:41:40.739] iteration 6055 : loss: 0.032867, loss_a: 0.019333
[22:41:41.477] iteration 6056 : loss: 0.025477, loss_a: 0.014987
[22:41:42.790] iteration 6057 : loss: 0.022114, loss_a: 0.013008
[22:41:43.527] iteration 6058 : loss: 0.020230, loss_a: 0.011900
[22:41:44.899] iteration 6059 : loss: 0.040543, loss_a: 0.023849
[22:41:45.653] iteration 6060 : loss: 0.022618, loss_a: 0.013304
[22:41:47.004] iteration 6061 : loss: 0.024609, loss_a: 0.014476
[22:41:47.742] iteration 6062 : loss: 0.025323, loss_a: 0.014896
[22:41:49.069] iteration 6063 : loss: 0.022710, loss_a: 0.013359
[22:41:49.809] iteration 6064 : loss: 0.011964, loss_a: 0.007038
[22:41:51.132] iteration 6065 : loss: 0.034074, loss_a: 0.020044
[22:41:51.868] iteration 6066 : loss: 0.026258, loss_a: 0.015446
[22:41:53.231] iteration 6067 : loss: 0.038060, loss_a: 0.022388
[22:41:53.967] iteration 6068 : loss: 0.024598, loss_a: 0.014469
[22:41:55.326] iteration 6069 : loss: 0.028366, loss_a: 0.016686
[22:41:56.066] iteration 6070 : loss: 0.074860, loss_a: 0.044035
[22:41:57.415] iteration 6071 : loss: 0.018325, loss_a: 0.010779
[22:41:58.150] iteration 6072 : loss: 0.030446, loss_a: 0.017910
[22:41:59.508] iteration 6073 : loss: 0.032818, loss_a: 0.019304
[22:42:00.251] iteration 6074 : loss: 0.020077, loss_a: 0.011810
[22:42:01.614] iteration 6075 : loss: 0.069173, loss_a: 0.040690
[22:42:02.359] iteration 6076 : loss: 0.043178, loss_a: 0.025399
[22:42:03.709] iteration 6077 : loss: 0.048775, loss_a: 0.028691
[22:42:04.467] iteration 6078 : loss: 0.033729, loss_a: 0.019841
[22:42:05.800] iteration 6079 : loss: 0.037548, loss_a: 0.022087
[22:42:06.545] iteration 6080 : loss: 0.046251, loss_a: 0.027206
[22:42:07.866] iteration 6081 : loss: 0.028681, loss_a: 0.016871
[22:42:08.605] iteration 6082 : loss: 0.044337, loss_a: 0.026080
[22:42:09.928] iteration 6083 : loss: 0.023228, loss_a: 0.013664
[22:42:10.664] iteration 6084 : loss: 0.033814, loss_a: 0.019891
[22:42:12.011] iteration 6085 : loss: 0.030705, loss_a: 0.018062
[22:42:12.753] iteration 6086 : loss: 0.029664, loss_a: 0.017449
[22:42:14.116] iteration 6087 : loss: 0.044324, loss_a: 0.026073
[22:42:14.854] iteration 6088 : loss: 0.025754, loss_a: 0.015149
[22:42:16.190] iteration 6089 : loss: 0.050709, loss_a: 0.029829
[22:42:16.934] iteration 6090 : loss: 0.026602, loss_a: 0.015648
[22:42:18.279] iteration 6091 : loss: 0.037528, loss_a: 0.022075
[22:42:19.028] iteration 6092 : loss: 0.024689, loss_a: 0.014523
[22:42:20.389] iteration 6093 : loss: 0.031053, loss_a: 0.018266
[22:42:21.124] iteration 6094 : loss: 0.038940, loss_a: 0.022906
[22:42:22.477] iteration 6095 : loss: 0.071725, loss_a: 0.042191
[22:42:23.214] iteration 6096 : loss: 0.037249, loss_a: 0.021911
[22:42:24.550] iteration 6097 : loss: 0.042007, loss_a: 0.024710
[22:42:25.299] iteration 6098 : loss: 0.024199, loss_a: 0.014235
[22:42:26.619] iteration 6099 : loss: 0.079905, loss_a: 0.047003
[22:42:27.353] iteration 6100 : loss: 0.010158, loss_a: 0.005975
[22:42:28.717] iteration 6101 : loss: 0.512049, loss_a: 0.301205
[22:42:29.457] iteration 6102 : loss: 0.035549, loss_a: 0.020911
[22:42:30.809] iteration 6103 : loss: 0.020146, loss_a: 0.011850
[22:42:31.556] iteration 6104 : loss: 0.025705, loss_a: 0.015121
[22:42:32.873] iteration 6105 : loss: 0.048273, loss_a: 0.028396
[22:42:33.607] iteration 6106 : loss: 0.028895, loss_a: 0.016997
[22:42:34.931] iteration 6107 : loss: 0.033713, loss_a: 0.019831
[22:42:35.668] iteration 6108 : loss: 0.029816, loss_a: 0.017539
[22:42:37.001] iteration 6109 : loss: 0.061542, loss_a: 0.036201
[22:42:37.738] iteration 6110 : loss: 0.046961, loss_a: 0.027624
[22:42:39.040] iteration 6111 : loss: 0.034177, loss_a: 0.020104
[22:42:39.777] iteration 6112 : loss: 0.011758, loss_a: 0.006917
[22:42:41.120] iteration 6113 : loss: 0.044434, loss_a: 0.026138
[22:42:41.856] iteration 6114 : loss: 0.036816, loss_a: 0.021656
[22:42:43.168] iteration 6115 : loss: 0.045995, loss_a: 0.027056
[22:42:43.921] iteration 6116 : loss: 0.061291, loss_a: 0.036053
[22:42:45.271] iteration 6117 : loss: 0.025877, loss_a: 0.015222
[22:42:46.005] iteration 6118 : loss: 0.037133, loss_a: 0.021843
[22:42:47.339] iteration 6119 : loss: 0.019612, loss_a: 0.011537
[22:42:48.093] iteration 6120 : loss: 0.043976, loss_a: 0.025868
[22:42:49.437] iteration 6121 : loss: 0.026766, loss_a: 0.015745
[22:42:50.180] iteration 6122 : loss: 0.039129, loss_a: 0.023017
[22:42:51.486] iteration 6123 : loss: 0.013768, loss_a: 0.008099
[22:42:52.243] iteration 6124 : loss: 0.044278, loss_a: 0.026046
[22:42:53.600] iteration 6125 : loss: 0.036719, loss_a: 0.021600
[22:42:54.351] iteration 6126 : loss: 0.044724, loss_a: 0.026308
[22:42:55.711] iteration 6127 : loss: 0.046502, loss_a: 0.027354
[22:42:56.461] iteration 6128 : loss: 0.023893, loss_a: 0.014055
[22:42:57.812] iteration 6129 : loss: 0.029013, loss_a: 0.017066
[22:42:58.558] iteration 6130 : loss: 0.037553, loss_a: 0.022090
[22:42:59.891] iteration 6131 : loss: 0.053020, loss_a: 0.031188
[22:43:00.644] iteration 6132 : loss: 0.032139, loss_a: 0.018905
[22:43:01.989] iteration 6133 : loss: 0.026257, loss_a: 0.015445
[22:43:02.719] iteration 6134 : loss: 0.015772, loss_a: 0.009278
[22:43:04.029] iteration 6135 : loss: 0.026848, loss_a: 0.015793
[22:43:04.762] iteration 6136 : loss: 0.014992, loss_a: 0.008819
[22:43:06.082] iteration 6137 : loss: 0.052376, loss_a: 0.030809
[22:43:06.837] iteration 6138 : loss: 0.035492, loss_a: 0.020878
[22:43:08.162] iteration 6139 : loss: 0.045570, loss_a: 0.026806
[22:43:08.903] iteration 6140 : loss: 0.034558, loss_a: 0.020328
[22:43:10.242] iteration 6141 : loss: 0.048009, loss_a: 0.028240
[22:43:10.980] iteration 6142 : loss: 0.034111, loss_a: 0.020065
[22:43:12.303] iteration 6143 : loss: 0.035196, loss_a: 0.020703
[22:43:13.035] iteration 6144 : loss: 0.024749, loss_a: 0.014558
[22:43:14.366] iteration 6145 : loss: 0.031539, loss_a: 0.018552
[22:43:15.110] iteration 6146 : loss: 0.024337, loss_a: 0.014316
[22:43:16.450] iteration 6147 : loss: 0.057185, loss_a: 0.033638
[22:43:17.185] iteration 6148 : loss: 0.022822, loss_a: 0.013424
[22:43:18.532] iteration 6149 : loss: 0.025865, loss_a: 0.015215
[22:43:19.273] iteration 6150 : loss: 0.024150, loss_a: 0.014206
[22:43:20.621] iteration 6151 : loss: 0.014082, loss_a: 0.008283
[22:43:21.358] iteration 6152 : loss: 0.018559, loss_a: 0.010917
[22:43:22.675] iteration 6153 : loss: 0.024523, loss_a: 0.014425
[22:43:23.423] iteration 6154 : loss: 0.024001, loss_a: 0.014118
[22:43:24.770] iteration 6155 : loss: 0.030377, loss_a: 0.017869
[22:43:25.502] iteration 6156 : loss: 0.022977, loss_a: 0.013516
[22:43:26.833] iteration 6157 : loss: 0.023220, loss_a: 0.013659
[22:43:27.582] iteration 6158 : loss: 0.077548, loss_a: 0.045616
[22:43:28.931] iteration 6159 : loss: 0.029453, loss_a: 0.017325
[22:43:29.682] iteration 6160 : loss: 0.035715, loss_a: 0.021009
[22:43:31.037] iteration 6161 : loss: 0.025443, loss_a: 0.014966
[22:43:31.786] iteration 6162 : loss: 0.019751, loss_a: 0.011618
[22:43:33.172] iteration 6163 : loss: 0.048003, loss_a: 0.028237
[22:43:33.911] iteration 6164 : loss: 0.020279, loss_a: 0.011929
[22:43:35.232] iteration 6165 : loss: 0.024000, loss_a: 0.014118
[22:43:35.969] iteration 6166 : loss: 0.025078, loss_a: 0.014752
[22:43:37.283] iteration 6167 : loss: 0.025743, loss_a: 0.015143
[22:43:38.025] iteration 6168 : loss: 0.018630, loss_a: 0.010959
[22:43:39.329] iteration 6169 : loss: 0.325809, loss_a: 0.191653
[22:43:40.059] iteration 6170 : loss: 0.056638, loss_a: 0.033316
[22:43:41.391] iteration 6171 : loss: 0.059058, loss_a: 0.034740
[22:43:42.126] iteration 6172 : loss: 0.029453, loss_a: 0.017325
[22:43:43.472] iteration 6173 : loss: 0.034209, loss_a: 0.020123
[22:43:44.224] iteration 6174 : loss: 0.039172, loss_a: 0.023042
[22:43:45.559] iteration 6175 : loss: 0.039321, loss_a: 0.023130
[22:43:46.304] iteration 6176 : loss: 0.057485, loss_a: 0.033815
[22:43:47.639] iteration 6177 : loss: 0.030767, loss_a: 0.018098
[22:43:48.386] iteration 6178 : loss: 0.025713, loss_a: 0.015125
[22:43:49.749] iteration 6179 : loss: 0.052625, loss_a: 0.030956
[22:43:50.493] iteration 6180 : loss: 0.023979, loss_a: 0.014105
[22:43:51.820] iteration 6181 : loss: 0.033645, loss_a: 0.019791
[22:43:52.559] iteration 6182 : loss: 0.025915, loss_a: 0.015244
[22:43:53.909] iteration 6183 : loss: 0.056932, loss_a: 0.033490
[22:43:54.656] iteration 6184 : loss: 0.028958, loss_a: 0.017034
[22:43:56.003] iteration 6185 : loss: 0.050651, loss_a: 0.029795
[22:43:56.746] iteration 6186 : loss: 0.031970, loss_a: 0.018806
[22:43:58.091] iteration 6187 : loss: 0.024233, loss_a: 0.014254
[22:43:58.831] iteration 6188 : loss: 0.030821, loss_a: 0.018130
[22:44:00.179] iteration 6189 : loss: 0.073057, loss_a: 0.042975
[22:44:00.923] iteration 6190 : loss: 0.031774, loss_a: 0.018691
[22:44:02.255] iteration 6191 : loss: 0.032637, loss_a: 0.019198
[22:44:03.009] iteration 6192 : loss: 0.068574, loss_a: 0.040337
[22:44:04.363] iteration 6193 : loss: 0.096271, loss_a: 0.056630
[22:44:05.118] iteration 6194 : loss: 0.033076, loss_a: 0.019456
[22:44:06.471] iteration 6195 : loss: 0.019424, loss_a: 0.011426
[22:44:07.214] iteration 6196 : loss: 0.030167, loss_a: 0.017745
[22:44:08.548] iteration 6197 : loss: 0.022685, loss_a: 0.013344
[22:44:09.276] iteration 6198 : loss: 0.014677, loss_a: 0.008633
[22:44:10.601] iteration 6199 : loss: 0.035525, loss_a: 0.020897
[22:44:11.341] iteration 6200 : loss: 0.031254, loss_a: 0.018385
[22:44:36.011] iteration 6201 : loss: 0.014386, loss_a: 0.008463
[22:44:38.201] iteration 6202 : loss: 0.031844, loss_a: 0.018732
[22:44:39.513] iteration 6203 : loss: 0.032752, loss_a: 0.019266
[22:44:40.256] iteration 6204 : loss: 0.031437, loss_a: 0.018492
[22:44:41.627] iteration 6205 : loss: 0.019800, loss_a: 0.011647
[22:44:42.371] iteration 6206 : loss: 0.017551, loss_a: 0.010324
[22:44:43.731] iteration 6207 : loss: 0.022820, loss_a: 0.013423
[22:44:44.482] iteration 6208 : loss: 0.025755, loss_a: 0.015150
[22:44:45.831] iteration 6209 : loss: 0.027660, loss_a: 0.016271
[22:44:46.571] iteration 6210 : loss: 0.022811, loss_a: 0.013418
[22:44:47.897] iteration 6211 : loss: 0.041029, loss_a: 0.024135
[22:44:48.649] iteration 6212 : loss: 0.037089, loss_a: 0.021817
[22:44:50.016] iteration 6213 : loss: 0.036106, loss_a: 0.021239
[22:44:50.759] iteration 6214 : loss: 0.031407, loss_a: 0.018474
[22:44:52.092] iteration 6215 : loss: 0.033289, loss_a: 0.019582
[22:44:52.844] iteration 6216 : loss: 0.026889, loss_a: 0.015817
[22:44:54.196] iteration 6217 : loss: 0.029125, loss_a: 0.017133
[22:44:54.937] iteration 6218 : loss: 0.035314, loss_a: 0.020773
[22:44:56.268] iteration 6219 : loss: 0.020899, loss_a: 0.012294
[22:44:57.027] iteration 6220 : loss: 0.039740, loss_a: 0.023377
[22:44:58.384] iteration 6221 : loss: 0.013486, loss_a: 0.007933
[22:44:59.135] iteration 6222 : loss: 0.024605, loss_a: 0.014474
[22:45:00.479] iteration 6223 : loss: 0.021680, loss_a: 0.012753
[22:45:01.224] iteration 6224 : loss: 0.023406, loss_a: 0.013768
[22:45:02.529] iteration 6225 : loss: 0.028461, loss_a: 0.016742
[22:45:03.275] iteration 6226 : loss: 0.030523, loss_a: 0.017954
[22:45:04.611] iteration 6227 : loss: 0.068036, loss_a: 0.040021
[22:45:05.341] iteration 6228 : loss: 0.036930, loss_a: 0.021724
[22:45:06.658] iteration 6229 : loss: 0.018806, loss_a: 0.011062
[22:45:07.404] iteration 6230 : loss: 0.022195, loss_a: 0.013056
[22:45:08.748] iteration 6231 : loss: 0.034031, loss_a: 0.020018
[22:45:09.479] iteration 6232 : loss: 0.038631, loss_a: 0.022724
[22:45:10.808] iteration 6233 : loss: 0.033259, loss_a: 0.019564
[22:45:11.539] iteration 6234 : loss: 0.021668, loss_a: 0.012746
[22:45:12.865] iteration 6235 : loss: 0.030988, loss_a: 0.018228
[22:45:13.605] iteration 6236 : loss: 0.021821, loss_a: 0.012836
[22:45:14.944] iteration 6237 : loss: 0.029576, loss_a: 0.017398
[22:45:15.686] iteration 6238 : loss: 0.016333, loss_a: 0.009607
[22:45:17.015] iteration 6239 : loss: 0.043592, loss_a: 0.025642
[22:45:17.761] iteration 6240 : loss: 0.053463, loss_a: 0.031449
[22:45:19.087] iteration 6241 : loss: 0.039290, loss_a: 0.023112
[22:45:19.822] iteration 6242 : loss: 0.029327, loss_a: 0.017251
[22:45:21.131] iteration 6243 : loss: 0.023576, loss_a: 0.013868
[22:45:21.871] iteration 6244 : loss: 0.076342, loss_a: 0.044907
[22:45:23.219] iteration 6245 : loss: 0.029733, loss_a: 0.017490
[22:45:23.957] iteration 6246 : loss: 0.033208, loss_a: 0.019534
[22:45:25.282] iteration 6247 : loss: 0.017816, loss_a: 0.010480
[22:45:26.018] iteration 6248 : loss: 0.033096, loss_a: 0.019468
[22:45:27.357] iteration 6249 : loss: 0.034542, loss_a: 0.020319
[22:45:28.096] iteration 6250 : loss: 0.015610, loss_a: 0.009182
[22:45:29.468] iteration 6251 : loss: 0.048055, loss_a: 0.028267
[22:45:30.212] iteration 6252 : loss: 0.021577, loss_a: 0.012693
[22:45:31.562] iteration 6253 : loss: 0.047755, loss_a: 0.028091
[22:45:32.298] iteration 6254 : loss: 0.026709, loss_a: 0.015711
[22:45:33.638] iteration 6255 : loss: 0.017475, loss_a: 0.010280
[22:45:34.394] iteration 6256 : loss: 0.029944, loss_a: 0.017614
[22:45:35.778] iteration 6257 : loss: 0.031538, loss_a: 0.018552
[22:45:36.516] iteration 6258 : loss: 0.022378, loss_a: 0.013164
[22:45:37.880] iteration 6259 : loss: 0.025504, loss_a: 0.015002
[22:45:38.622] iteration 6260 : loss: 0.013621, loss_a: 0.008012
[22:45:39.943] iteration 6261 : loss: 0.022340, loss_a: 0.013141
[22:45:40.689] iteration 6262 : loss: 0.055613, loss_a: 0.032714
[22:45:42.003] iteration 6263 : loss: 0.019925, loss_a: 0.011721
[22:45:42.737] iteration 6264 : loss: 0.021822, loss_a: 0.012837
[22:45:44.052] iteration 6265 : loss: 0.011648, loss_a: 0.006852
[22:45:44.815] iteration 6266 : loss: 0.033096, loss_a: 0.019469
[22:45:46.154] iteration 6267 : loss: 0.035875, loss_a: 0.021103
[22:45:46.901] iteration 6268 : loss: 0.039016, loss_a: 0.022951
[22:45:48.255] iteration 6269 : loss: 0.033783, loss_a: 0.019872
[22:45:48.987] iteration 6270 : loss: 0.038292, loss_a: 0.022525
[22:45:50.321] iteration 6271 : loss: 0.029324, loss_a: 0.017250
[22:45:51.070] iteration 6272 : loss: 0.045879, loss_a: 0.026987
[22:45:52.380] iteration 6273 : loss: 0.017216, loss_a: 0.010127
[22:45:53.119] iteration 6274 : loss: 0.060413, loss_a: 0.035537
[22:45:54.438] iteration 6275 : loss: 0.025077, loss_a: 0.014751
[22:45:55.192] iteration 6276 : loss: 0.051894, loss_a: 0.030526
[22:45:56.535] iteration 6277 : loss: 0.035585, loss_a: 0.020932
[22:45:57.280] iteration 6278 : loss: 0.028352, loss_a: 0.016678
[22:45:58.621] iteration 6279 : loss: 0.049330, loss_a: 0.029018
[22:45:59.362] iteration 6280 : loss: 0.036100, loss_a: 0.021235
[22:46:00.677] iteration 6281 : loss: 0.038783, loss_a: 0.022814
[22:46:01.416] iteration 6282 : loss: 0.036021, loss_a: 0.021189
[22:46:02.767] iteration 6283 : loss: 0.019667, loss_a: 0.011569
[22:46:03.507] iteration 6284 : loss: 0.028742, loss_a: 0.016907
[22:46:04.825] iteration 6285 : loss: 0.050368, loss_a: 0.029628
[22:46:05.576] iteration 6286 : loss: 0.069940, loss_a: 0.041141
[22:46:06.923] iteration 6287 : loss: 0.059572, loss_a: 0.035042
[22:46:07.665] iteration 6288 : loss: 0.026999, loss_a: 0.015882
[22:46:09.003] iteration 6289 : loss: 0.036010, loss_a: 0.021182
[22:46:09.748] iteration 6290 : loss: 0.035618, loss_a: 0.020952
[22:46:11.099] iteration 6291 : loss: 0.028978, loss_a: 0.017046
[22:46:11.845] iteration 6292 : loss: 0.018360, loss_a: 0.010800
[22:46:13.178] iteration 6293 : loss: 0.028595, loss_a: 0.016821
[22:46:13.923] iteration 6294 : loss: 0.026212, loss_a: 0.015419
[22:46:15.288] iteration 6295 : loss: 0.028784, loss_a: 0.016932
[22:46:16.022] iteration 6296 : loss: 0.020997, loss_a: 0.012351
[22:46:17.352] iteration 6297 : loss: 0.025180, loss_a: 0.014812
[22:46:18.097] iteration 6298 : loss: 0.274529, loss_a: 0.161487
[22:46:19.465] iteration 6299 : loss: 0.045835, loss_a: 0.026962
[22:46:20.194] iteration 6300 : loss: 0.042455, loss_a: 0.024973
[22:46:21.531] iteration 6301 : loss: 0.075201, loss_a: 0.044236
[22:46:22.280] iteration 6302 : loss: 0.039198, loss_a: 0.023058
[22:46:23.617] iteration 6303 : loss: 0.022695, loss_a: 0.013350
[22:46:24.369] iteration 6304 : loss: 0.022994, loss_a: 0.013526
[22:46:25.707] iteration 6305 : loss: 0.017864, loss_a: 0.010508
[22:46:26.450] iteration 6306 : loss: 0.018721, loss_a: 0.011013
[22:46:27.780] iteration 6307 : loss: 0.031725, loss_a: 0.018662
[22:46:28.518] iteration 6308 : loss: 0.020174, loss_a: 0.011867
[22:46:29.878] iteration 6309 : loss: 0.036530, loss_a: 0.021488
[22:46:30.634] iteration 6310 : loss: 0.033211, loss_a: 0.019536
[22:46:31.972] iteration 6311 : loss: 0.020422, loss_a: 0.012013
[22:46:32.717] iteration 6312 : loss: 0.048613, loss_a: 0.028596
[22:46:34.063] iteration 6313 : loss: 0.023839, loss_a: 0.014023
[22:46:34.794] iteration 6314 : loss: 0.014775, loss_a: 0.008691
[22:46:36.170] iteration 6315 : loss: 0.075537, loss_a: 0.044434
[22:46:36.912] iteration 6316 : loss: 0.023186, loss_a: 0.013639
[22:46:38.226] iteration 6317 : loss: 0.039533, loss_a: 0.023255
[22:46:38.974] iteration 6318 : loss: 0.100274, loss_a: 0.058985
[22:46:40.307] iteration 6319 : loss: 0.046127, loss_a: 0.027134
[22:46:41.050] iteration 6320 : loss: 0.033912, loss_a: 0.019948
[22:46:42.408] iteration 6321 : loss: 0.015086, loss_a: 0.008874
[22:46:43.161] iteration 6322 : loss: 0.030318, loss_a: 0.017834
[22:46:44.524] iteration 6323 : loss: 0.066791, loss_a: 0.039289
[22:46:45.271] iteration 6324 : loss: 0.021757, loss_a: 0.012798
[22:46:46.634] iteration 6325 : loss: 0.052147, loss_a: 0.030675
[22:46:47.370] iteration 6326 : loss: 0.036900, loss_a: 0.021706
[22:46:48.701] iteration 6327 : loss: 0.022302, loss_a: 0.013119
[22:46:49.444] iteration 6328 : loss: 0.053470, loss_a: 0.031453
[22:46:50.795] iteration 6329 : loss: 0.039620, loss_a: 0.023306
[22:46:51.523] iteration 6330 : loss: 0.030186, loss_a: 0.017757
[22:46:52.880] iteration 6331 : loss: 0.046552, loss_a: 0.027384
[22:46:53.609] iteration 6332 : loss: 0.018593, loss_a: 0.010937
[22:46:54.931] iteration 6333 : loss: 0.018885, loss_a: 0.011109
[22:46:55.673] iteration 6334 : loss: 0.056140, loss_a: 0.033023
[22:46:57.019] iteration 6335 : loss: 0.021636, loss_a: 0.012727
[22:46:57.760] iteration 6336 : loss: 0.013314, loss_a: 0.007832
[22:46:59.090] iteration 6337 : loss: 0.102423, loss_a: 0.060249
[22:46:59.827] iteration 6338 : loss: 0.020576, loss_a: 0.012104
[22:47:01.169] iteration 6339 : loss: 0.021346, loss_a: 0.012556
[22:47:01.903] iteration 6340 : loss: 0.017862, loss_a: 0.010507
[22:47:03.279] iteration 6341 : loss: 0.071962, loss_a: 0.042330
[22:47:04.023] iteration 6342 : loss: 0.039007, loss_a: 0.022945
[22:47:05.376] iteration 6343 : loss: 0.056074, loss_a: 0.032985
[22:47:06.121] iteration 6344 : loss: 0.020981, loss_a: 0.012342
[22:47:07.482] iteration 6345 : loss: 0.043419, loss_a: 0.025540
[22:47:08.219] iteration 6346 : loss: 0.017953, loss_a: 0.010560
[22:47:09.553] iteration 6347 : loss: 0.017161, loss_a: 0.010095
[22:47:10.293] iteration 6348 : loss: 0.026833, loss_a: 0.015784
[22:47:11.625] iteration 6349 : loss: 0.031767, loss_a: 0.018686
[22:47:12.364] iteration 6350 : loss: 0.022869, loss_a: 0.013452
[22:47:13.687] iteration 6351 : loss: 0.059210, loss_a: 0.034830
[22:47:14.432] iteration 6352 : loss: 0.053734, loss_a: 0.031608
[22:47:15.767] iteration 6353 : loss: 0.039924, loss_a: 0.023485
[22:47:16.508] iteration 6354 : loss: 0.034595, loss_a: 0.020350
[22:47:17.844] iteration 6355 : loss: 0.016548, loss_a: 0.009734
[22:47:18.587] iteration 6356 : loss: 0.020931, loss_a: 0.012312
[22:47:19.946] iteration 6357 : loss: 0.032434, loss_a: 0.019079
[22:47:20.696] iteration 6358 : loss: 0.028689, loss_a: 0.016876
[22:47:22.026] iteration 6359 : loss: 0.021056, loss_a: 0.012386
[22:47:22.778] iteration 6360 : loss: 0.031404, loss_a: 0.018473
[22:47:24.120] iteration 6361 : loss: 0.025269, loss_a: 0.014864
[22:47:24.859] iteration 6362 : loss: 0.027510, loss_a: 0.016183
[22:47:26.177] iteration 6363 : loss: 0.033183, loss_a: 0.019519
[22:47:26.918] iteration 6364 : loss: 0.029686, loss_a: 0.017463
[22:47:28.244] iteration 6365 : loss: 0.046809, loss_a: 0.027535
[22:47:28.982] iteration 6366 : loss: 0.053333, loss_a: 0.031373
[22:47:30.349] iteration 6367 : loss: 0.037300, loss_a: 0.021941
[22:47:31.087] iteration 6368 : loss: 0.057372, loss_a: 0.033748
[22:47:32.443] iteration 6369 : loss: 0.021020, loss_a: 0.012365
[22:47:33.204] iteration 6370 : loss: 0.031438, loss_a: 0.018493
[22:47:34.537] iteration 6371 : loss: 0.020008, loss_a: 0.011769
[22:47:35.276] iteration 6372 : loss: 0.027612, loss_a: 0.016243
[22:47:36.623] iteration 6373 : loss: 0.018738, loss_a: 0.011022
[22:47:37.358] iteration 6374 : loss: 0.015927, loss_a: 0.009369
[22:47:38.688] iteration 6375 : loss: 0.022846, loss_a: 0.013439
[22:47:39.419] iteration 6376 : loss: 0.029602, loss_a: 0.017413
[22:47:40.732] iteration 6377 : loss: 0.033887, loss_a: 0.019934
[22:47:41.482] iteration 6378 : loss: 0.064520, loss_a: 0.037953
[22:47:42.799] iteration 6379 : loss: 0.021241, loss_a: 0.012495
[22:47:43.546] iteration 6380 : loss: 0.014750, loss_a: 0.008677
[22:47:44.901] iteration 6381 : loss: 0.016384, loss_a: 0.009638
[22:47:45.638] iteration 6382 : loss: 0.019648, loss_a: 0.011557
[22:47:46.987] iteration 6383 : loss: 0.014278, loss_a: 0.008399
[22:47:47.742] iteration 6384 : loss: 0.054466, loss_a: 0.032039
[22:47:49.089] iteration 6385 : loss: 0.021730, loss_a: 0.012783
[22:47:49.837] iteration 6386 : loss: 0.031184, loss_a: 0.018344
[22:47:51.163] iteration 6387 : loss: 0.022429, loss_a: 0.013194
[22:47:51.911] iteration 6388 : loss: 0.023316, loss_a: 0.013715
[22:47:53.239] iteration 6389 : loss: 0.067131, loss_a: 0.039489
[22:47:53.981] iteration 6390 : loss: 0.014826, loss_a: 0.008721
[22:47:55.325] iteration 6391 : loss: 0.031157, loss_a: 0.018328
[22:47:56.078] iteration 6392 : loss: 0.032779, loss_a: 0.019282
[22:47:57.423] iteration 6393 : loss: 0.017720, loss_a: 0.010424
[22:47:58.162] iteration 6394 : loss: 0.022714, loss_a: 0.013361
[22:47:59.479] iteration 6395 : loss: 0.046142, loss_a: 0.027142
[22:48:00.214] iteration 6396 : loss: 0.011022, loss_a: 0.006483
[22:48:01.552] iteration 6397 : loss: 0.019207, loss_a: 0.011299
[22:48:02.290] iteration 6398 : loss: 0.041179, loss_a: 0.024223
[22:48:03.655] iteration 6399 : loss: 0.017149, loss_a: 0.010088
[22:48:04.396] iteration 6400 : loss: 0.025580, loss_a: 0.015047
[22:48:29.057] iteration 6401 : loss: 0.019422, loss_a: 0.011425
[22:48:31.192] iteration 6402 : loss: 0.023758, loss_a: 0.013975
[22:48:32.556] iteration 6403 : loss: 0.044824, loss_a: 0.026367
[22:48:33.315] iteration 6404 : loss: 0.031243, loss_a: 0.018378
[22:48:34.645] iteration 6405 : loss: 0.022100, loss_a: 0.013000
[22:48:35.386] iteration 6406 : loss: 0.038460, loss_a: 0.022623
[22:48:36.735] iteration 6407 : loss: 0.022144, loss_a: 0.013026
[22:48:37.475] iteration 6408 : loss: 0.016841, loss_a: 0.009906
[22:48:38.829] iteration 6409 : loss: 0.018358, loss_a: 0.010799
[22:48:39.571] iteration 6410 : loss: 0.019147, loss_a: 0.011263
[22:48:40.926] iteration 6411 : loss: 0.020007, loss_a: 0.011769
[22:48:41.674] iteration 6412 : loss: 0.020935, loss_a: 0.012315
[22:48:43.003] iteration 6413 : loss: 0.025455, loss_a: 0.014973
[22:48:43.745] iteration 6414 : loss: 0.024540, loss_a: 0.014435
[22:48:45.087] iteration 6415 : loss: 0.079407, loss_a: 0.046710
[22:48:45.834] iteration 6416 : loss: 0.026523, loss_a: 0.015602
[22:48:47.177] iteration 6417 : loss: 0.045256, loss_a: 0.026621
[22:48:47.927] iteration 6418 : loss: 0.049287, loss_a: 0.028992
[22:48:49.257] iteration 6419 : loss: 0.037387, loss_a: 0.021992
[22:48:50.005] iteration 6420 : loss: 0.020882, loss_a: 0.012284
[22:48:51.360] iteration 6421 : loss: 0.054116, loss_a: 0.031833
[22:48:52.099] iteration 6422 : loss: 0.024967, loss_a: 0.014686
[22:48:53.412] iteration 6423 : loss: 0.038615, loss_a: 0.022715
[22:48:54.154] iteration 6424 : loss: 0.015459, loss_a: 0.009094
[22:48:55.498] iteration 6425 : loss: 0.024185, loss_a: 0.014227
[22:48:56.231] iteration 6426 : loss: 0.018955, loss_a: 0.011150
[22:48:57.550] iteration 6427 : loss: 0.021250, loss_a: 0.012500
[22:48:58.294] iteration 6428 : loss: 0.045739, loss_a: 0.026905
[22:48:59.650] iteration 6429 : loss: 0.028579, loss_a: 0.016811
[22:49:00.392] iteration 6430 : loss: 0.032057, loss_a: 0.018857
[22:49:01.708] iteration 6431 : loss: 0.077536, loss_a: 0.045609
[22:49:02.457] iteration 6432 : loss: 0.033887, loss_a: 0.019934
[22:49:03.807] iteration 6433 : loss: 0.034363, loss_a: 0.020214
[22:49:04.555] iteration 6434 : loss: 0.023807, loss_a: 0.014004
[22:49:05.883] iteration 6435 : loss: 0.036863, loss_a: 0.021684
[22:49:06.625] iteration 6436 : loss: 0.018199, loss_a: 0.010706
[22:49:07.958] iteration 6437 : loss: 0.011975, loss_a: 0.007044
[22:49:08.710] iteration 6438 : loss: 0.032640, loss_a: 0.019200
[22:49:10.082] iteration 6439 : loss: 0.027378, loss_a: 0.016105
[22:49:10.819] iteration 6440 : loss: 0.027516, loss_a: 0.016186
[22:49:12.168] iteration 6441 : loss: 0.035960, loss_a: 0.021153
[22:49:12.906] iteration 6442 : loss: 0.017398, loss_a: 0.010234
[22:49:14.250] iteration 6443 : loss: 0.024402, loss_a: 0.014354
[22:49:15.003] iteration 6444 : loss: 0.040541, loss_a: 0.023848
[22:49:16.312] iteration 6445 : loss: 0.024296, loss_a: 0.014292
[22:49:17.051] iteration 6446 : loss: 0.042981, loss_a: 0.025283
[22:49:18.407] iteration 6447 : loss: 0.018334, loss_a: 0.010785
[22:49:19.148] iteration 6448 : loss: 0.050172, loss_a: 0.029513
[22:49:20.496] iteration 6449 : loss: 0.027585, loss_a: 0.016226
[22:49:21.236] iteration 6450 : loss: 0.023338, loss_a: 0.013729
[22:49:22.566] iteration 6451 : loss: 0.027234, loss_a: 0.016020
[22:49:23.315] iteration 6452 : loss: 0.016773, loss_a: 0.009867
[22:49:24.668] iteration 6453 : loss: 0.032998, loss_a: 0.019411
[22:49:25.411] iteration 6454 : loss: 0.025802, loss_a: 0.015177
[22:49:26.738] iteration 6455 : loss: 0.029435, loss_a: 0.017315
[22:49:27.481] iteration 6456 : loss: 0.019673, loss_a: 0.011572
[22:49:28.838] iteration 6457 : loss: 0.030111, loss_a: 0.017712
[22:49:29.572] iteration 6458 : loss: 0.014698, loss_a: 0.008646
[22:49:30.940] iteration 6459 : loss: 0.026948, loss_a: 0.015852
[22:49:31.686] iteration 6460 : loss: 0.034149, loss_a: 0.020087
[22:49:33.013] iteration 6461 : loss: 0.025146, loss_a: 0.014792
[22:49:33.756] iteration 6462 : loss: 0.021937, loss_a: 0.012904
[22:49:35.074] iteration 6463 : loss: 0.018094, loss_a: 0.010643
[22:49:35.823] iteration 6464 : loss: 0.073021, loss_a: 0.042954
[22:49:37.147] iteration 6465 : loss: 0.021853, loss_a: 0.012855
[22:49:37.885] iteration 6466 : loss: 0.029379, loss_a: 0.017282
[22:49:39.209] iteration 6467 : loss: 0.023256, loss_a: 0.013680
[22:49:39.951] iteration 6468 : loss: 0.035177, loss_a: 0.020692
[22:49:41.272] iteration 6469 : loss: 0.027206, loss_a: 0.016003
[22:49:42.019] iteration 6470 : loss: 0.029832, loss_a: 0.017548
[22:49:43.367] iteration 6471 : loss: 0.044897, loss_a: 0.026410
[22:49:44.108] iteration 6472 : loss: 0.016261, loss_a: 0.009565
[22:49:45.452] iteration 6473 : loss: 0.041448, loss_a: 0.024381
[22:49:46.182] iteration 6474 : loss: 0.022818, loss_a: 0.013422
[22:49:47.544] iteration 6475 : loss: 0.020935, loss_a: 0.012315
[22:49:48.295] iteration 6476 : loss: 0.025544, loss_a: 0.015026
[22:49:49.652] iteration 6477 : loss: 0.021787, loss_a: 0.012816
[22:49:50.398] iteration 6478 : loss: 0.031437, loss_a: 0.018492
[22:49:51.739] iteration 6479 : loss: 0.023587, loss_a: 0.013875
[22:49:52.489] iteration 6480 : loss: 0.028169, loss_a: 0.016570
[22:49:53.812] iteration 6481 : loss: 0.030315, loss_a: 0.017832
[22:49:54.557] iteration 6482 : loss: 0.032502, loss_a: 0.019119
[22:49:55.877] iteration 6483 : loss: 0.019262, loss_a: 0.011331
[22:49:56.621] iteration 6484 : loss: 0.024833, loss_a: 0.014608
[22:49:57.952] iteration 6485 : loss: 0.049395, loss_a: 0.029056
[22:49:58.710] iteration 6486 : loss: 0.043989, loss_a: 0.025876
[22:50:00.040] iteration 6487 : loss: 0.030851, loss_a: 0.018147
[22:50:00.784] iteration 6488 : loss: 0.040425, loss_a: 0.023780
[22:50:02.119] iteration 6489 : loss: 0.020771, loss_a: 0.012218
[22:50:02.858] iteration 6490 : loss: 0.027509, loss_a: 0.016182
[22:50:04.204] iteration 6491 : loss: 0.033033, loss_a: 0.019431
[22:50:04.952] iteration 6492 : loss: 0.076608, loss_a: 0.045063
[22:50:06.277] iteration 6493 : loss: 0.032992, loss_a: 0.019407
[22:50:07.023] iteration 6494 : loss: 0.036157, loss_a: 0.021269
[22:50:08.361] iteration 6495 : loss: 0.048093, loss_a: 0.028290
[22:50:09.110] iteration 6496 : loss: 0.033764, loss_a: 0.019861
[22:50:10.484] iteration 6497 : loss: 0.036158, loss_a: 0.021270
[22:50:11.219] iteration 6498 : loss: 0.020117, loss_a: 0.011834
[22:50:12.583] iteration 6499 : loss: 0.051296, loss_a: 0.030174
[22:50:13.329] iteration 6500 : loss: 0.042485, loss_a: 0.024991
[22:50:14.672] iteration 6501 : loss: 0.029160, loss_a: 0.017153
[22:50:15.411] iteration 6502 : loss: 0.027454, loss_a: 0.016149
[22:50:16.770] iteration 6503 : loss: 0.035377, loss_a: 0.020810
[22:50:17.500] iteration 6504 : loss: 0.026107, loss_a: 0.015357
[22:50:18.816] iteration 6505 : loss: 0.017172, loss_a: 0.010101
[22:50:19.565] iteration 6506 : loss: 0.034372, loss_a: 0.020219
[22:50:20.887] iteration 6507 : loss: 0.025782, loss_a: 0.015166
[22:50:21.626] iteration 6508 : loss: 0.028090, loss_a: 0.016523
[22:50:22.992] iteration 6509 : loss: 0.057609, loss_a: 0.033888
[22:50:23.735] iteration 6510 : loss: 0.024221, loss_a: 0.014248
[22:50:25.066] iteration 6511 : loss: 0.021155, loss_a: 0.012444
[22:50:25.813] iteration 6512 : loss: 0.042437, loss_a: 0.024963
[22:50:27.178] iteration 6513 : loss: 0.026627, loss_a: 0.015663
[22:50:27.921] iteration 6514 : loss: 0.030509, loss_a: 0.017946
[22:50:29.259] iteration 6515 : loss: 0.020545, loss_a: 0.012086
[22:50:30.000] iteration 6516 : loss: 0.021016, loss_a: 0.012363
[22:50:31.333] iteration 6517 : loss: 0.036174, loss_a: 0.021279
[22:50:32.077] iteration 6518 : loss: 0.021391, loss_a: 0.012583
[22:50:33.406] iteration 6519 : loss: 0.025214, loss_a: 0.014831
[22:50:34.154] iteration 6520 : loss: 0.019148, loss_a: 0.011263
[22:50:35.485] iteration 6521 : loss: 0.021495, loss_a: 0.012644
[22:50:36.221] iteration 6522 : loss: 0.018614, loss_a: 0.010949
[22:50:37.577] iteration 6523 : loss: 0.026664, loss_a: 0.015684
[22:50:38.327] iteration 6524 : loss: 0.038962, loss_a: 0.022919
[22:50:39.683] iteration 6525 : loss: 0.035869, loss_a: 0.021099
[22:50:40.430] iteration 6526 : loss: 0.043213, loss_a: 0.025420
[22:50:41.772] iteration 6527 : loss: 0.059392, loss_a: 0.034937
[22:50:42.521] iteration 6528 : loss: 0.082006, loss_a: 0.048239
[22:50:43.877] iteration 6529 : loss: 0.026794, loss_a: 0.015761
[22:50:44.625] iteration 6530 : loss: 0.065507, loss_a: 0.038534
[22:50:45.956] iteration 6531 : loss: 0.030476, loss_a: 0.017927
[22:50:46.690] iteration 6532 : loss: 0.029242, loss_a: 0.017201
[22:50:48.048] iteration 6533 : loss: 0.029360, loss_a: 0.017271
[22:50:48.792] iteration 6534 : loss: 0.031394, loss_a: 0.018467
[22:50:50.146] iteration 6535 : loss: 0.015076, loss_a: 0.008868
[22:50:50.881] iteration 6536 : loss: 0.014468, loss_a: 0.008511
[22:50:52.198] iteration 6537 : loss: 0.027362, loss_a: 0.016095
[22:50:52.940] iteration 6538 : loss: 0.029070, loss_a: 0.017100
[22:50:54.296] iteration 6539 : loss: 0.031221, loss_a: 0.018365
[22:50:55.035] iteration 6540 : loss: 0.025604, loss_a: 0.015061
[22:50:56.358] iteration 6541 : loss: 0.036230, loss_a: 0.021312
[22:50:57.100] iteration 6542 : loss: 0.029117, loss_a: 0.017128
[22:50:58.411] iteration 6543 : loss: 0.036634, loss_a: 0.021550
[22:50:59.147] iteration 6544 : loss: 0.041690, loss_a: 0.024523
[22:51:00.478] iteration 6545 : loss: 0.033864, loss_a: 0.019920
[22:51:01.209] iteration 6546 : loss: 0.035868, loss_a: 0.021099
[22:51:02.577] iteration 6547 : loss: 0.033867, loss_a: 0.019922
[22:51:03.322] iteration 6548 : loss: 0.041490, loss_a: 0.024406
[22:51:04.648] iteration 6549 : loss: 0.033736, loss_a: 0.019845
[22:51:05.396] iteration 6550 : loss: 0.025692, loss_a: 0.015113
[22:51:06.754] iteration 6551 : loss: 0.037522, loss_a: 0.022072
[22:51:07.486] iteration 6552 : loss: 0.011118, loss_a: 0.006540
[22:51:08.840] iteration 6553 : loss: 0.022003, loss_a: 0.012943
[22:51:09.586] iteration 6554 : loss: 0.019097, loss_a: 0.011234
[22:51:10.942] iteration 6555 : loss: 0.024418, loss_a: 0.014363
[22:51:11.686] iteration 6556 : loss: 0.033592, loss_a: 0.019760
[22:51:13.030] iteration 6557 : loss: 0.021112, loss_a: 0.012419
[22:51:13.762] iteration 6558 : loss: 0.027281, loss_a: 0.016048
[22:51:15.099] iteration 6559 : loss: 0.013375, loss_a: 0.007868
[22:51:15.841] iteration 6560 : loss: 0.029451, loss_a: 0.017324
[22:51:17.155] iteration 6561 : loss: 0.016046, loss_a: 0.009439
[22:51:17.906] iteration 6562 : loss: 0.052485, loss_a: 0.030874
[22:51:19.248] iteration 6563 : loss: 0.026102, loss_a: 0.015354
[22:51:19.988] iteration 6564 : loss: 0.023721, loss_a: 0.013953
[22:51:21.342] iteration 6565 : loss: 0.059277, loss_a: 0.034869
[22:51:22.082] iteration 6566 : loss: 0.024820, loss_a: 0.014600
[22:51:23.397] iteration 6567 : loss: 0.021677, loss_a: 0.012751
[22:51:24.139] iteration 6568 : loss: 0.025328, loss_a: 0.014899
[22:51:25.496] iteration 6569 : loss: 0.050965, loss_a: 0.029979
[22:51:26.227] iteration 6570 : loss: 0.065284, loss_a: 0.038402
[22:51:27.581] iteration 6571 : loss: 0.058030, loss_a: 0.034135
[22:51:28.326] iteration 6572 : loss: 0.034437, loss_a: 0.020257
[22:51:29.691] iteration 6573 : loss: 0.028076, loss_a: 0.016515
[22:51:30.432] iteration 6574 : loss: 0.014931, loss_a: 0.008783
[22:51:31.754] iteration 6575 : loss: 0.016822, loss_a: 0.009896
[22:51:32.497] iteration 6576 : loss: 0.051599, loss_a: 0.030352
[22:51:33.857] iteration 6577 : loss: 0.021464, loss_a: 0.012626
[22:51:34.606] iteration 6578 : loss: 0.027156, loss_a: 0.015974
[22:51:35.970] iteration 6579 : loss: 0.039816, loss_a: 0.023421
[22:51:36.711] iteration 6580 : loss: 0.035517, loss_a: 0.020892
[22:51:38.048] iteration 6581 : loss: 0.027717, loss_a: 0.016304
[22:51:38.786] iteration 6582 : loss: 0.022872, loss_a: 0.013454
[22:51:40.150] iteration 6583 : loss: 0.072116, loss_a: 0.042421
[22:51:40.891] iteration 6584 : loss: 0.027844, loss_a: 0.016379
[22:51:42.251] iteration 6585 : loss: 0.022748, loss_a: 0.013381
[22:51:43.009] iteration 6586 : loss: 0.015166, loss_a: 0.008921
[22:51:44.358] iteration 6587 : loss: 0.030953, loss_a: 0.018207
[22:51:45.105] iteration 6588 : loss: 0.031820, loss_a: 0.018718
[22:51:46.472] iteration 6589 : loss: 0.042032, loss_a: 0.024724
[22:51:47.217] iteration 6590 : loss: 0.028036, loss_a: 0.016492
[22:51:48.564] iteration 6591 : loss: 0.044037, loss_a: 0.025904
[22:51:49.299] iteration 6592 : loss: 0.020522, loss_a: 0.012072
[22:51:50.644] iteration 6593 : loss: 0.035491, loss_a: 0.020877
[22:51:51.380] iteration 6594 : loss: 0.044433, loss_a: 0.026137
[22:51:52.727] iteration 6595 : loss: 0.027045, loss_a: 0.015909
[22:51:53.474] iteration 6596 : loss: 0.037354, loss_a: 0.021973
[22:51:54.806] iteration 6597 : loss: 0.038357, loss_a: 0.022563
[22:51:55.542] iteration 6598 : loss: 0.044106, loss_a: 0.025945
[22:51:56.848] iteration 6599 : loss: 0.021989, loss_a: 0.012935
[22:51:57.593] iteration 6600 : loss: 0.031518, loss_a: 0.018540
[22:52:22.246] iteration 6601 : loss: 0.032370, loss_a: 0.019041
[22:52:24.469] iteration 6602 : loss: 0.060060, loss_a: 0.035330
[22:52:25.823] iteration 6603 : loss: 0.029196, loss_a: 0.017174
[22:52:26.565] iteration 6604 : loss: 0.030149, loss_a: 0.017735
[22:52:27.887] iteration 6605 : loss: 0.023058, loss_a: 0.013564
[22:52:28.628] iteration 6606 : loss: 0.024780, loss_a: 0.014576
[22:52:29.953] iteration 6607 : loss: 0.090585, loss_a: 0.053285
[22:52:30.710] iteration 6608 : loss: 0.060234, loss_a: 0.035432
[22:52:32.053] iteration 6609 : loss: 0.059930, loss_a: 0.035253
[22:52:32.785] iteration 6610 : loss: 0.021856, loss_a: 0.012856
[22:52:34.129] iteration 6611 : loss: 0.057794, loss_a: 0.033997
[22:52:34.864] iteration 6612 : loss: 0.017965, loss_a: 0.010568
[22:52:36.206] iteration 6613 : loss: 0.023501, loss_a: 0.013824
[22:52:36.948] iteration 6614 : loss: 0.040990, loss_a: 0.024112
[22:52:38.251] iteration 6615 : loss: 0.043930, loss_a: 0.025841
[22:52:38.987] iteration 6616 : loss: 0.025820, loss_a: 0.015188
[22:52:40.354] iteration 6617 : loss: 0.025848, loss_a: 0.015204
[22:52:41.087] iteration 6618 : loss: 0.026792, loss_a: 0.015760
[22:52:42.409] iteration 6619 : loss: 0.027177, loss_a: 0.015986
[22:52:43.151] iteration 6620 : loss: 0.019036, loss_a: 0.011198
[22:52:44.497] iteration 6621 : loss: 0.031835, loss_a: 0.018726
[22:52:45.239] iteration 6622 : loss: 0.028591, loss_a: 0.016818
[22:52:46.583] iteration 6623 : loss: 0.012442, loss_a: 0.007319
[22:52:47.317] iteration 6624 : loss: 0.028140, loss_a: 0.016553
[22:52:48.670] iteration 6625 : loss: 0.038142, loss_a: 0.022437
[22:52:49.402] iteration 6626 : loss: 0.024722, loss_a: 0.014542
[22:52:50.752] iteration 6627 : loss: 0.043807, loss_a: 0.025769
[22:52:51.484] iteration 6628 : loss: 0.029189, loss_a: 0.017170
[22:52:52.806] iteration 6629 : loss: 0.016388, loss_a: 0.009640
[22:52:53.541] iteration 6630 : loss: 0.035745, loss_a: 0.021027
[22:52:54.878] iteration 6631 : loss: 0.012798, loss_a: 0.007528
[22:52:55.625] iteration 6632 : loss: 0.054154, loss_a: 0.031855
[22:52:56.972] iteration 6633 : loss: 0.023773, loss_a: 0.013984
[22:52:57.707] iteration 6634 : loss: 0.031640, loss_a: 0.018612
[22:52:59.057] iteration 6635 : loss: 0.025643, loss_a: 0.015084
[22:52:59.786] iteration 6636 : loss: 0.024072, loss_a: 0.014160
[22:53:01.155] iteration 6637 : loss: 0.027953, loss_a: 0.016443
[22:53:01.901] iteration 6638 : loss: 0.028719, loss_a: 0.016894
[22:53:03.252] iteration 6639 : loss: 0.036969, loss_a: 0.021746
[22:53:03.989] iteration 6640 : loss: 0.043081, loss_a: 0.025342
[22:53:05.310] iteration 6641 : loss: 0.020596, loss_a: 0.012116
[22:53:06.050] iteration 6642 : loss: 0.018149, loss_a: 0.010676
[22:53:07.414] iteration 6643 : loss: 0.027039, loss_a: 0.015906
[22:53:08.149] iteration 6644 : loss: 0.018710, loss_a: 0.011006
[22:53:09.510] iteration 6645 : loss: 0.036467, loss_a: 0.021451
[22:53:10.255] iteration 6646 : loss: 0.029510, loss_a: 0.017359
[22:53:11.590] iteration 6647 : loss: 0.041554, loss_a: 0.024444
[22:53:12.344] iteration 6648 : loss: 0.034556, loss_a: 0.020327
[22:53:13.690] iteration 6649 : loss: 0.024602, loss_a: 0.014472
[22:53:14.434] iteration 6650 : loss: 0.080360, loss_a: 0.047271
[22:53:15.764] iteration 6651 : loss: 0.029118, loss_a: 0.017128
[22:53:16.497] iteration 6652 : loss: 0.077626, loss_a: 0.045662
[22:53:17.815] iteration 6653 : loss: 0.012757, loss_a: 0.007504
[22:53:18.549] iteration 6654 : loss: 0.026334, loss_a: 0.015490
[22:53:19.901] iteration 6655 : loss: 0.026852, loss_a: 0.015795
[22:53:20.644] iteration 6656 : loss: 0.024488, loss_a: 0.014405
[22:53:21.980] iteration 6657 : loss: 0.014987, loss_a: 0.008816
[22:53:22.717] iteration 6658 : loss: 0.020084, loss_a: 0.011814
[22:53:24.050] iteration 6659 : loss: 0.044364, loss_a: 0.026096
[22:53:24.797] iteration 6660 : loss: 0.041086, loss_a: 0.024168
[22:53:26.140] iteration 6661 : loss: 0.043682, loss_a: 0.025695
[22:53:26.875] iteration 6662 : loss: 0.016113, loss_a: 0.009479
[22:53:28.206] iteration 6663 : loss: 0.021011, loss_a: 0.012359
[22:53:28.949] iteration 6664 : loss: 0.036574, loss_a: 0.021514
[22:53:30.271] iteration 6665 : loss: 0.031022, loss_a: 0.018248
[22:53:31.028] iteration 6666 : loss: 0.063180, loss_a: 0.037165
[22:53:32.374] iteration 6667 : loss: 0.019234, loss_a: 0.011314
[22:53:33.112] iteration 6668 : loss: 0.065859, loss_a: 0.038741
[22:53:34.428] iteration 6669 : loss: 0.045477, loss_a: 0.026751
[22:53:35.167] iteration 6670 : loss: 0.020227, loss_a: 0.011898
[22:53:36.489] iteration 6671 : loss: 0.042922, loss_a: 0.025248
[22:53:37.238] iteration 6672 : loss: 0.065609, loss_a: 0.038594
[22:53:38.560] iteration 6673 : loss: 0.019691, loss_a: 0.011583
[22:53:39.315] iteration 6674 : loss: 0.053134, loss_a: 0.031255
[22:53:40.682] iteration 6675 : loss: 0.052425, loss_a: 0.030838
[22:53:41.429] iteration 6676 : loss: 0.020432, loss_a: 0.012019
[22:53:42.749] iteration 6677 : loss: 0.033944, loss_a: 0.019967
[22:53:43.474] iteration 6678 : loss: 0.033045, loss_a: 0.019438
[22:53:44.791] iteration 6679 : loss: 0.022667, loss_a: 0.013334
[22:53:45.534] iteration 6680 : loss: 0.041830, loss_a: 0.024606
[22:53:46.883] iteration 6681 : loss: 0.030590, loss_a: 0.017994
[22:53:47.621] iteration 6682 : loss: 0.024921, loss_a: 0.014660
[22:53:48.986] iteration 6683 : loss: 0.019615, loss_a: 0.011538
[22:53:49.720] iteration 6684 : loss: 0.028554, loss_a: 0.016796
[22:53:51.045] iteration 6685 : loss: 0.022990, loss_a: 0.013523
[22:53:51.796] iteration 6686 : loss: 0.034836, loss_a: 0.020492
[22:53:53.149] iteration 6687 : loss: 0.027587, loss_a: 0.016228
[22:53:53.890] iteration 6688 : loss: 0.017146, loss_a: 0.010086
[22:53:55.222] iteration 6689 : loss: 0.023309, loss_a: 0.013711
[22:53:55.968] iteration 6690 : loss: 0.036128, loss_a: 0.021252
[22:53:57.279] iteration 6691 : loss: 0.035320, loss_a: 0.020777
[22:53:58.023] iteration 6692 : loss: 0.046611, loss_a: 0.027418
[22:53:59.326] iteration 6693 : loss: 0.034597, loss_a: 0.020351
[22:54:00.064] iteration 6694 : loss: 0.021629, loss_a: 0.012723
[22:54:01.424] iteration 6695 : loss: 0.023044, loss_a: 0.013555
[22:54:02.158] iteration 6696 : loss: 0.032422, loss_a: 0.019072
[22:54:03.497] iteration 6697 : loss: 0.054962, loss_a: 0.032330
[22:54:04.239] iteration 6698 : loss: 0.051312, loss_a: 0.030183
[22:54:05.574] iteration 6699 : loss: 0.031838, loss_a: 0.018728
[22:54:06.322] iteration 6700 : loss: 0.041970, loss_a: 0.024688
[22:54:07.632] iteration 6701 : loss: 0.031365, loss_a: 0.018450
[22:54:08.367] iteration 6702 : loss: 0.044693, loss_a: 0.026290
[22:54:09.730] iteration 6703 : loss: 0.024756, loss_a: 0.014562
[22:54:10.476] iteration 6704 : loss: 0.047005, loss_a: 0.027650
[22:54:11.823] iteration 6705 : loss: 0.016585, loss_a: 0.009756
[22:54:12.565] iteration 6706 : loss: 0.051763, loss_a: 0.030449
[22:54:13.889] iteration 6707 : loss: 0.015406, loss_a: 0.009062
[22:54:14.632] iteration 6708 : loss: 0.027561, loss_a: 0.016212
[22:54:15.944] iteration 6709 : loss: 0.024435, loss_a: 0.014373
[22:54:16.684] iteration 6710 : loss: 0.022871, loss_a: 0.013453
[22:54:18.026] iteration 6711 : loss: 0.038910, loss_a: 0.022888
[22:54:18.761] iteration 6712 : loss: 0.033964, loss_a: 0.019979
[22:54:20.132] iteration 6713 : loss: 0.043124, loss_a: 0.025367
[22:54:20.876] iteration 6714 : loss: 0.031896, loss_a: 0.018762
[22:54:22.224] iteration 6715 : loss: 0.024501, loss_a: 0.014412
[22:54:22.978] iteration 6716 : loss: 0.103360, loss_a: 0.060800
[22:54:24.322] iteration 6717 : loss: 0.024266, loss_a: 0.014274
[22:54:25.069] iteration 6718 : loss: 0.026261, loss_a: 0.015448
[22:54:26.451] iteration 6719 : loss: 0.016633, loss_a: 0.009784
[22:54:27.200] iteration 6720 : loss: 0.024388, loss_a: 0.014346
[22:54:28.534] iteration 6721 : loss: 0.038388, loss_a: 0.022581
[22:54:29.271] iteration 6722 : loss: 0.029100, loss_a: 0.017118
[22:54:30.630] iteration 6723 : loss: 0.048877, loss_a: 0.028751
[22:54:31.371] iteration 6724 : loss: 0.022698, loss_a: 0.013352
[22:54:32.726] iteration 6725 : loss: 0.026463, loss_a: 0.015567
[22:54:33.466] iteration 6726 : loss: 0.027315, loss_a: 0.016068
[22:54:34.815] iteration 6727 : loss: 0.036325, loss_a: 0.021368
[22:54:35.555] iteration 6728 : loss: 0.028950, loss_a: 0.017030
[22:54:36.890] iteration 6729 : loss: 0.039530, loss_a: 0.023253
[22:54:37.633] iteration 6730 : loss: 0.034167, loss_a: 0.020098
[22:54:38.970] iteration 6731 : loss: 0.037904, loss_a: 0.022296
[22:54:39.706] iteration 6732 : loss: 0.034772, loss_a: 0.020454
[22:54:41.032] iteration 6733 : loss: 0.032975, loss_a: 0.019397
[22:54:41.775] iteration 6734 : loss: 0.015332, loss_a: 0.009019
[22:54:43.113] iteration 6735 : loss: 0.016321, loss_a: 0.009601
[22:54:43.851] iteration 6736 : loss: 0.041281, loss_a: 0.024283
[22:54:45.200] iteration 6737 : loss: 0.020623, loss_a: 0.012131
[22:54:45.946] iteration 6738 : loss: 0.045942, loss_a: 0.027025
[22:54:47.296] iteration 6739 : loss: 0.026550, loss_a: 0.015617
[22:54:48.033] iteration 6740 : loss: 0.029898, loss_a: 0.017587
[22:54:49.354] iteration 6741 : loss: 0.046705, loss_a: 0.027474
[22:54:50.097] iteration 6742 : loss: 0.025945, loss_a: 0.015262
[22:54:51.443] iteration 6743 : loss: 0.032558, loss_a: 0.019152
[22:54:52.189] iteration 6744 : loss: 0.023627, loss_a: 0.013899
[22:54:53.536] iteration 6745 : loss: 0.042050, loss_a: 0.024735
[22:54:54.273] iteration 6746 : loss: 0.020320, loss_a: 0.011953
[22:54:55.629] iteration 6747 : loss: 0.018616, loss_a: 0.010951
[22:54:56.364] iteration 6748 : loss: 0.046692, loss_a: 0.027466
[22:54:57.742] iteration 6749 : loss: 0.037821, loss_a: 0.022248
[22:54:58.492] iteration 6750 : loss: 0.038848, loss_a: 0.022852
[22:54:59.848] iteration 6751 : loss: 0.023704, loss_a: 0.013943
[22:55:00.599] iteration 6752 : loss: 0.035093, loss_a: 0.020643
[22:55:01.951] iteration 6753 : loss: 0.045499, loss_a: 0.026764
[22:55:02.682] iteration 6754 : loss: 0.044173, loss_a: 0.025984
[22:55:04.021] iteration 6755 : loss: 0.024023, loss_a: 0.014131
[22:55:04.771] iteration 6756 : loss: 0.035462, loss_a: 0.020860
[22:55:06.125] iteration 6757 : loss: 0.036296, loss_a: 0.021350
[22:55:06.866] iteration 6758 : loss: 0.013015, loss_a: 0.007656
[22:55:08.219] iteration 6759 : loss: 0.029181, loss_a: 0.017166
[22:55:08.961] iteration 6760 : loss: 0.030172, loss_a: 0.017748
[22:55:10.321] iteration 6761 : loss: 0.028602, loss_a: 0.016825
[22:55:11.066] iteration 6762 : loss: 0.022707, loss_a: 0.013357
[22:55:12.398] iteration 6763 : loss: 0.036504, loss_a: 0.021473
[22:55:13.150] iteration 6764 : loss: 0.036233, loss_a: 0.021313
[22:55:14.492] iteration 6765 : loss: 0.051411, loss_a: 0.030241
[22:55:15.238] iteration 6766 : loss: 0.017040, loss_a: 0.010023
[22:55:16.581] iteration 6767 : loss: 0.023488, loss_a: 0.013817
[22:55:17.326] iteration 6768 : loss: 0.028713, loss_a: 0.016890
[22:55:18.670] iteration 6769 : loss: 0.034562, loss_a: 0.020331
[22:55:19.411] iteration 6770 : loss: 0.040388, loss_a: 0.023758
[22:55:20.748] iteration 6771 : loss: 0.022480, loss_a: 0.013223
[22:55:21.498] iteration 6772 : loss: 0.024186, loss_a: 0.014227
[22:55:22.866] iteration 6773 : loss: 0.080350, loss_a: 0.047265
[22:55:23.603] iteration 6774 : loss: 0.012464, loss_a: 0.007332
[22:55:24.961] iteration 6775 : loss: 0.041816, loss_a: 0.024597
[22:55:25.710] iteration 6776 : loss: 0.030916, loss_a: 0.018186
[22:55:27.048] iteration 6777 : loss: 0.024099, loss_a: 0.014176
[22:55:27.800] iteration 6778 : loss: 0.029698, loss_a: 0.017469
[22:55:29.111] iteration 6779 : loss: 0.017966, loss_a: 0.010568
[22:55:29.859] iteration 6780 : loss: 0.039781, loss_a: 0.023400
[22:55:31.189] iteration 6781 : loss: 0.030246, loss_a: 0.017792
[22:55:31.918] iteration 6782 : loss: 0.016994, loss_a: 0.009996
[22:55:33.229] iteration 6783 : loss: 0.066408, loss_a: 0.039063
[22:55:33.967] iteration 6784 : loss: 0.014644, loss_a: 0.008614
[22:55:35.327] iteration 6785 : loss: 0.037461, loss_a: 0.022036
[22:55:36.070] iteration 6786 : loss: 0.032770, loss_a: 0.019276
[22:55:37.411] iteration 6787 : loss: 0.013432, loss_a: 0.007901
[22:55:38.151] iteration 6788 : loss: 0.032747, loss_a: 0.019263
[22:55:39.474] iteration 6789 : loss: 0.026029, loss_a: 0.015311
[22:55:40.219] iteration 6790 : loss: 0.032916, loss_a: 0.019363
[22:55:41.586] iteration 6791 : loss: 0.037905, loss_a: 0.022297
[22:55:42.320] iteration 6792 : loss: 0.020532, loss_a: 0.012078
[22:55:43.691] iteration 6793 : loss: 0.035581, loss_a: 0.020930
[22:55:44.427] iteration 6794 : loss: 0.055397, loss_a: 0.032586
[22:55:45.754] iteration 6795 : loss: 0.014408, loss_a: 0.008475
[22:55:46.492] iteration 6796 : loss: 0.026562, loss_a: 0.015625
[22:55:47.843] iteration 6797 : loss: 0.063365, loss_a: 0.037274
[22:55:48.597] iteration 6798 : loss: 0.049439, loss_a: 0.029082
[22:55:49.939] iteration 6799 : loss: 0.034462, loss_a: 0.020272
[22:55:50.691] iteration 6800 : loss: 0.042450, loss_a: 0.024970
[22:56:15.369] iteration 6801 : loss: 0.020536, loss_a: 0.012080
[22:56:17.636] iteration 6802 : loss: 0.046697, loss_a: 0.027469
[22:56:19.012] iteration 6803 : loss: 0.040754, loss_a: 0.023973
[22:56:19.755] iteration 6804 : loss: 0.058383, loss_a: 0.034343
[22:56:21.101] iteration 6805 : loss: 0.030390, loss_a: 0.017876
[22:56:21.845] iteration 6806 : loss: 0.025583, loss_a: 0.015049
[22:56:23.156] iteration 6807 : loss: 0.046419, loss_a: 0.027306
[22:56:23.908] iteration 6808 : loss: 0.034554, loss_a: 0.020326
[22:56:25.260] iteration 6809 : loss: 0.026272, loss_a: 0.015454
[22:56:25.998] iteration 6810 : loss: 0.024577, loss_a: 0.014457
[22:56:27.355] iteration 6811 : loss: 0.034478, loss_a: 0.020281
[22:56:28.115] iteration 6812 : loss: 0.053292, loss_a: 0.031348
[22:56:29.510] iteration 6813 : loss: 0.071964, loss_a: 0.042332
[22:56:30.254] iteration 6814 : loss: 0.023948, loss_a: 0.014087
[22:56:31.624] iteration 6815 : loss: 0.029832, loss_a: 0.017548
[22:56:32.373] iteration 6816 : loss: 0.032246, loss_a: 0.018968
[22:56:33.723] iteration 6817 : loss: 0.027566, loss_a: 0.016215
[22:56:34.466] iteration 6818 : loss: 0.026000, loss_a: 0.015294
[22:56:35.807] iteration 6819 : loss: 0.022080, loss_a: 0.012988
[22:56:36.548] iteration 6820 : loss: 0.026392, loss_a: 0.015524
[22:56:37.856] iteration 6821 : loss: 0.016459, loss_a: 0.009682
[22:56:38.592] iteration 6822 : loss: 0.044784, loss_a: 0.026343
[22:56:39.930] iteration 6823 : loss: 0.039828, loss_a: 0.023428
[22:56:40.666] iteration 6824 : loss: 0.033440, loss_a: 0.019671
[22:56:41.975] iteration 6825 : loss: 0.054882, loss_a: 0.032284
[22:56:42.712] iteration 6826 : loss: 0.049066, loss_a: 0.028862
[22:56:44.075] iteration 6827 : loss: 0.023907, loss_a: 0.014063
[22:56:44.809] iteration 6828 : loss: 0.012303, loss_a: 0.007237
[22:56:46.138] iteration 6829 : loss: 0.021580, loss_a: 0.012694
[22:56:46.875] iteration 6830 : loss: 0.058309, loss_a: 0.034299
[22:56:48.226] iteration 6831 : loss: 0.016576, loss_a: 0.009750
[22:56:48.972] iteration 6832 : loss: 0.029908, loss_a: 0.017593
[22:56:50.318] iteration 6833 : loss: 0.029816, loss_a: 0.017539
[22:56:51.054] iteration 6834 : loss: 0.036255, loss_a: 0.021327
[22:56:52.357] iteration 6835 : loss: 0.040906, loss_a: 0.024062
[22:56:53.103] iteration 6836 : loss: 0.025815, loss_a: 0.015185
[22:56:54.427] iteration 6837 : loss: 0.066014, loss_a: 0.038832
[22:56:55.182] iteration 6838 : loss: 0.037867, loss_a: 0.022275
[22:56:56.535] iteration 6839 : loss: 0.038986, loss_a: 0.022933
[22:56:57.269] iteration 6840 : loss: 0.042960, loss_a: 0.025270
[22:56:58.578] iteration 6841 : loss: 0.049445, loss_a: 0.029085
[22:56:59.313] iteration 6842 : loss: 0.018928, loss_a: 0.011134
[22:57:00.673] iteration 6843 : loss: 0.057796, loss_a: 0.033998
[22:57:01.420] iteration 6844 : loss: 0.025436, loss_a: 0.014962
[22:57:02.742] iteration 6845 : loss: 0.039979, loss_a: 0.023517
[22:57:03.490] iteration 6846 : loss: 0.031335, loss_a: 0.018432
[22:57:04.810] iteration 6847 : loss: 0.046927, loss_a: 0.027604
[22:57:05.553] iteration 6848 : loss: 0.054882, loss_a: 0.032283
[22:57:06.913] iteration 6849 : loss: 0.023402, loss_a: 0.013766
[22:57:07.644] iteration 6850 : loss: 0.016274, loss_a: 0.009573
[22:57:08.974] iteration 6851 : loss: 0.027778, loss_a: 0.016340
[22:57:09.709] iteration 6852 : loss: 0.021523, loss_a: 0.012660
[22:57:11.067] iteration 6853 : loss: 0.039845, loss_a: 0.023438
[22:57:11.808] iteration 6854 : loss: 0.016530, loss_a: 0.009724
[22:57:13.137] iteration 6855 : loss: 0.018192, loss_a: 0.010701
[22:57:13.886] iteration 6856 : loss: 0.068419, loss_a: 0.040246
[22:57:15.214] iteration 6857 : loss: 0.073555, loss_a: 0.043268
[22:57:15.967] iteration 6858 : loss: 0.026080, loss_a: 0.015341
[22:57:17.299] iteration 6859 : loss: 0.041068, loss_a: 0.024158
[22:57:18.038] iteration 6860 : loss: 0.020737, loss_a: 0.012198
[22:57:19.350] iteration 6861 : loss: 0.015414, loss_a: 0.009067
[22:57:20.088] iteration 6862 : loss: 0.023940, loss_a: 0.014082
[22:57:21.422] iteration 6863 : loss: 0.026052, loss_a: 0.015325
[22:57:22.182] iteration 6864 : loss: 0.034907, loss_a: 0.020533
[22:57:23.514] iteration 6865 : loss: 0.022960, loss_a: 0.013506
[22:57:24.252] iteration 6866 : loss: 0.034042, loss_a: 0.020025
[22:57:25.604] iteration 6867 : loss: 0.021569, loss_a: 0.012688
[22:57:26.343] iteration 6868 : loss: 0.009383, loss_a: 0.005520
[22:57:27.671] iteration 6869 : loss: 0.022520, loss_a: 0.013247
[22:57:28.427] iteration 6870 : loss: 0.043582, loss_a: 0.025636
[22:57:29.759] iteration 6871 : loss: 0.021212, loss_a: 0.012478
[22:57:30.489] iteration 6872 : loss: 0.018631, loss_a: 0.010959
[22:57:31.833] iteration 6873 : loss: 0.037122, loss_a: 0.021837
[22:57:32.570] iteration 6874 : loss: 0.026404, loss_a: 0.015532
[22:57:33.891] iteration 6875 : loss: 0.013832, loss_a: 0.008136
[22:57:34.625] iteration 6876 : loss: 0.016221, loss_a: 0.009542
[22:57:35.961] iteration 6877 : loss: 0.040529, loss_a: 0.023841
[22:57:36.705] iteration 6878 : loss: 0.052111, loss_a: 0.030653
[22:57:38.054] iteration 6879 : loss: 0.016589, loss_a: 0.009758
[22:57:38.791] iteration 6880 : loss: 0.019080, loss_a: 0.011224
[22:57:40.138] iteration 6881 : loss: 0.025243, loss_a: 0.014849
[22:57:40.880] iteration 6882 : loss: 0.020300, loss_a: 0.011941
[22:57:42.229] iteration 6883 : loss: 0.017213, loss_a: 0.010126
[22:57:42.970] iteration 6884 : loss: 0.025042, loss_a: 0.014730
[22:57:44.294] iteration 6885 : loss: 0.013932, loss_a: 0.008195
[22:57:45.030] iteration 6886 : loss: 0.017725, loss_a: 0.010427
[22:57:46.362] iteration 6887 : loss: 0.013517, loss_a: 0.007951
[22:57:47.107] iteration 6888 : loss: 0.022670, loss_a: 0.013336
[22:57:48.487] iteration 6889 : loss: 0.031294, loss_a: 0.018408
[22:57:49.224] iteration 6890 : loss: 0.024108, loss_a: 0.014181
[22:57:50.600] iteration 6891 : loss: 0.027495, loss_a: 0.016174
[22:57:51.349] iteration 6892 : loss: 0.019548, loss_a: 0.011499
[22:57:52.692] iteration 6893 : loss: 0.018584, loss_a: 0.010931
[22:57:53.427] iteration 6894 : loss: 0.043977, loss_a: 0.025869
[22:57:54.786] iteration 6895 : loss: 0.025037, loss_a: 0.014727
[22:57:55.516] iteration 6896 : loss: 0.024107, loss_a: 0.014180
[22:57:56.859] iteration 6897 : loss: 0.020398, loss_a: 0.011999
[22:57:57.592] iteration 6898 : loss: 0.020002, loss_a: 0.011766
[22:57:58.967] iteration 6899 : loss: 0.070792, loss_a: 0.041642
[22:57:59.715] iteration 6900 : loss: 0.017437, loss_a: 0.010257
[22:58:01.086] iteration 6901 : loss: 0.035678, loss_a: 0.020987
[22:58:01.824] iteration 6902 : loss: 0.043826, loss_a: 0.025780
[22:58:03.188] iteration 6903 : loss: 0.021651, loss_a: 0.012736
[22:58:03.928] iteration 6904 : loss: 0.018120, loss_a: 0.010659
[22:58:05.250] iteration 6905 : loss: 0.021072, loss_a: 0.012395
[22:58:05.986] iteration 6906 : loss: 0.024217, loss_a: 0.014245
[22:58:07.322] iteration 6907 : loss: 0.051618, loss_a: 0.030363
[22:58:08.059] iteration 6908 : loss: 0.039409, loss_a: 0.023182
[22:58:09.367] iteration 6909 : loss: 0.022750, loss_a: 0.013382
[22:58:10.106] iteration 6910 : loss: 0.021412, loss_a: 0.012595
[22:58:11.459] iteration 6911 : loss: 0.030670, loss_a: 0.018041
[22:58:12.196] iteration 6912 : loss: 0.016148, loss_a: 0.009499
[22:58:13.551] iteration 6913 : loss: 0.048883, loss_a: 0.028755
[22:58:14.289] iteration 6914 : loss: 0.012685, loss_a: 0.007462
[22:58:15.632] iteration 6915 : loss: 0.038948, loss_a: 0.022910
[22:58:16.367] iteration 6916 : loss: 0.023045, loss_a: 0.013556
[22:58:17.718] iteration 6917 : loss: 0.043908, loss_a: 0.025828
[22:58:18.454] iteration 6918 : loss: 0.022215, loss_a: 0.013067
[22:58:19.814] iteration 6919 : loss: 0.112801, loss_a: 0.066354
[22:58:20.557] iteration 6920 : loss: 0.029311, loss_a: 0.017242
[22:58:21.902] iteration 6921 : loss: 0.037921, loss_a: 0.022306
[22:58:22.638] iteration 6922 : loss: 0.019698, loss_a: 0.011587
[22:58:23.977] iteration 6923 : loss: 0.020617, loss_a: 0.012128
[22:58:24.715] iteration 6924 : loss: 0.045231, loss_a: 0.026606
[22:58:26.038] iteration 6925 : loss: 0.089215, loss_a: 0.052480
[22:58:26.771] iteration 6926 : loss: 0.056572, loss_a: 0.033278
[22:58:28.111] iteration 6927 : loss: 0.052964, loss_a: 0.031155
[22:58:28.861] iteration 6928 : loss: 0.019672, loss_a: 0.011572
[22:58:30.232] iteration 6929 : loss: 0.029239, loss_a: 0.017199
[22:58:30.978] iteration 6930 : loss: 0.120600, loss_a: 0.070941
[22:58:32.277] iteration 6931 : loss: 0.020064, loss_a: 0.011803
[22:58:33.019] iteration 6932 : loss: 0.024403, loss_a: 0.014355
[22:58:34.375] iteration 6933 : loss: 0.025348, loss_a: 0.014910
[22:58:35.117] iteration 6934 : loss: 0.026458, loss_a: 0.015564
[22:58:36.424] iteration 6935 : loss: 0.024201, loss_a: 0.014236
[22:58:37.166] iteration 6936 : loss: 0.032387, loss_a: 0.019051
[22:58:38.476] iteration 6937 : loss: 0.025567, loss_a: 0.015039
[22:58:39.223] iteration 6938 : loss: 0.020478, loss_a: 0.012046
[22:58:40.537] iteration 6939 : loss: 0.021084, loss_a: 0.012402
[22:58:41.276] iteration 6940 : loss: 0.016658, loss_a: 0.009799
[22:58:42.620] iteration 6941 : loss: 0.030954, loss_a: 0.018208
[22:58:43.363] iteration 6942 : loss: 0.028607, loss_a: 0.016828
[22:58:44.706] iteration 6943 : loss: 0.022360, loss_a: 0.013153
[22:58:45.445] iteration 6944 : loss: 0.039710, loss_a: 0.023359
[22:58:46.784] iteration 6945 : loss: 0.025017, loss_a: 0.014716
[22:58:47.527] iteration 6946 : loss: 0.030581, loss_a: 0.017989
[22:58:48.884] iteration 6947 : loss: 0.032634, loss_a: 0.019197
[22:58:49.617] iteration 6948 : loss: 0.023407, loss_a: 0.013769
[22:58:50.959] iteration 6949 : loss: 0.027630, loss_a: 0.016253
[22:58:51.707] iteration 6950 : loss: 0.040117, loss_a: 0.023598
[22:58:53.049] iteration 6951 : loss: 0.045017, loss_a: 0.026481
[22:58:53.787] iteration 6952 : loss: 0.034164, loss_a: 0.020097
[22:58:55.152] iteration 6953 : loss: 0.031871, loss_a: 0.018748
[22:58:55.880] iteration 6954 : loss: 0.011940, loss_a: 0.007023
[22:58:57.243] iteration 6955 : loss: 0.056637, loss_a: 0.033316
[22:58:57.978] iteration 6956 : loss: 0.013832, loss_a: 0.008136
[22:58:59.308] iteration 6957 : loss: 0.032093, loss_a: 0.018878
[22:59:00.051] iteration 6958 : loss: 0.033554, loss_a: 0.019738
[22:59:01.447] iteration 6959 : loss: 0.024409, loss_a: 0.014358
[22:59:02.201] iteration 6960 : loss: 0.018837, loss_a: 0.011080
[22:59:03.565] iteration 6961 : loss: 0.029314, loss_a: 0.017244
[22:59:04.298] iteration 6962 : loss: 0.023349, loss_a: 0.013734
[22:59:05.620] iteration 6963 : loss: 0.015424, loss_a: 0.009073
[22:59:06.366] iteration 6964 : loss: 0.032342, loss_a: 0.019025
[22:59:07.712] iteration 6965 : loss: 0.017647, loss_a: 0.010381
[22:59:08.452] iteration 6966 : loss: 0.032993, loss_a: 0.019408
[22:59:09.812] iteration 6967 : loss: 0.045466, loss_a: 0.026744
[22:59:10.548] iteration 6968 : loss: 0.037855, loss_a: 0.022268
[22:59:11.897] iteration 6969 : loss: 0.019487, loss_a: 0.011463
[22:59:12.640] iteration 6970 : loss: 0.016795, loss_a: 0.009879
[22:59:13.967] iteration 6971 : loss: 0.017565, loss_a: 0.010332
[22:59:14.711] iteration 6972 : loss: 0.030614, loss_a: 0.018008
[22:59:16.056] iteration 6973 : loss: 0.065033, loss_a: 0.038255
[22:59:16.788] iteration 6974 : loss: 0.011703, loss_a: 0.006884
[22:59:18.122] iteration 6975 : loss: 0.025638, loss_a: 0.015081
[22:59:18.858] iteration 6976 : loss: 0.024898, loss_a: 0.014646
[22:59:20.195] iteration 6977 : loss: 0.055925, loss_a: 0.032897
[22:59:20.933] iteration 6978 : loss: 0.024998, loss_a: 0.014705
[22:59:22.259] iteration 6979 : loss: 0.030076, loss_a: 0.017692
[22:59:22.998] iteration 6980 : loss: 0.033239, loss_a: 0.019553
[22:59:24.321] iteration 6981 : loss: 0.025342, loss_a: 0.014907
[22:59:25.056] iteration 6982 : loss: 0.031891, loss_a: 0.018759
[22:59:26.369] iteration 6983 : loss: 0.011437, loss_a: 0.006728
[22:59:27.123] iteration 6984 : loss: 0.029304, loss_a: 0.017238
[22:59:28.438] iteration 6985 : loss: 0.019003, loss_a: 0.011178
[22:59:29.184] iteration 6986 : loss: 0.035544, loss_a: 0.020908
[22:59:30.532] iteration 6987 : loss: 0.014892, loss_a: 0.008760
[22:59:31.276] iteration 6988 : loss: 0.030744, loss_a: 0.018085
[22:59:32.589] iteration 6989 : loss: 0.024274, loss_a: 0.014279
[22:59:33.338] iteration 6990 : loss: 0.020463, loss_a: 0.012037
[22:59:34.698] iteration 6991 : loss: 0.018677, loss_a: 0.010986
[22:59:35.430] iteration 6992 : loss: 0.031322, loss_a: 0.018424
[22:59:36.765] iteration 6993 : loss: 0.027899, loss_a: 0.016411
[22:59:37.519] iteration 6994 : loss: 0.028968, loss_a: 0.017040
[22:59:38.869] iteration 6995 : loss: 0.024308, loss_a: 0.014299
[22:59:39.613] iteration 6996 : loss: 0.029594, loss_a: 0.017408
[22:59:40.981] iteration 6997 : loss: 0.051462, loss_a: 0.030272
[22:59:41.728] iteration 6998 : loss: 0.023909, loss_a: 0.014064
[22:59:43.090] iteration 6999 : loss: 0.020688, loss_a: 0.012169
[22:59:43.839] iteration 7000 : loss: 0.027490, loss_a: 0.016171
[23:00:08.437] iteration 7001 : loss: 0.014398, loss_a: 0.008470
[23:00:10.521] iteration 7002 : loss: 0.049141, loss_a: 0.028906
[23:00:11.889] iteration 7003 : loss: 0.053351, loss_a: 0.031383
[23:00:12.633] iteration 7004 : loss: 0.033253, loss_a: 0.019561
[23:00:13.986] iteration 7005 : loss: 0.024248, loss_a: 0.014264
[23:00:14.729] iteration 7006 : loss: 0.028204, loss_a: 0.016591
[23:00:16.064] iteration 7007 : loss: 0.029848, loss_a: 0.017558
[23:00:16.806] iteration 7008 : loss: 0.027898, loss_a: 0.016410
[23:00:18.180] iteration 7009 : loss: 0.012514, loss_a: 0.007361
[23:00:18.923] iteration 7010 : loss: 0.052665, loss_a: 0.030980
[23:00:20.253] iteration 7011 : loss: 0.028379, loss_a: 0.016693
[23:00:20.999] iteration 7012 : loss: 0.018272, loss_a: 0.010749
[23:00:22.301] iteration 7013 : loss: 0.025832, loss_a: 0.015195
[23:00:23.036] iteration 7014 : loss: 0.047171, loss_a: 0.027748
[23:00:24.356] iteration 7015 : loss: 0.020134, loss_a: 0.011843
[23:00:25.100] iteration 7016 : loss: 0.027221, loss_a: 0.016012
[23:00:26.421] iteration 7017 : loss: 0.029940, loss_a: 0.017612
[23:00:27.159] iteration 7018 : loss: 0.055232, loss_a: 0.032489
[23:00:28.511] iteration 7019 : loss: 0.076900, loss_a: 0.045235
[23:00:29.252] iteration 7020 : loss: 0.030453, loss_a: 0.017913
[23:00:30.612] iteration 7021 : loss: 0.025530, loss_a: 0.015018
[23:00:31.351] iteration 7022 : loss: 0.026750, loss_a: 0.015736
[23:00:32.682] iteration 7023 : loss: 0.017987, loss_a: 0.010580
[23:00:33.420] iteration 7024 : loss: 0.043816, loss_a: 0.025774
[23:00:34.746] iteration 7025 : loss: 0.028278, loss_a: 0.016634
[23:00:35.495] iteration 7026 : loss: 0.022461, loss_a: 0.013212
[23:00:36.850] iteration 7027 : loss: 0.027999, loss_a: 0.016470
[23:00:37.587] iteration 7028 : loss: 0.032542, loss_a: 0.019142
[23:00:38.918] iteration 7029 : loss: 0.053767, loss_a: 0.031627
[23:00:39.649] iteration 7030 : loss: 0.014351, loss_a: 0.008442
[23:00:40.969] iteration 7031 : loss: 0.027481, loss_a: 0.016166
[23:00:41.709] iteration 7032 : loss: 0.032169, loss_a: 0.018923
[23:00:43.029] iteration 7033 : loss: 0.047591, loss_a: 0.027995
[23:00:43.765] iteration 7034 : loss: 0.038080, loss_a: 0.022400
[23:00:45.091] iteration 7035 : loss: 0.014348, loss_a: 0.008440
[23:00:45.830] iteration 7036 : loss: 0.030118, loss_a: 0.017717
[23:00:47.175] iteration 7037 : loss: 0.017651, loss_a: 0.010383
[23:00:47.914] iteration 7038 : loss: 0.017058, loss_a: 0.010034
[23:00:49.229] iteration 7039 : loss: 0.034978, loss_a: 0.020575
[23:00:49.972] iteration 7040 : loss: 0.041023, loss_a: 0.024131
[23:00:51.286] iteration 7041 : loss: 0.022647, loss_a: 0.013322
[23:00:52.028] iteration 7042 : loss: 0.035829, loss_a: 0.021076
[23:00:53.375] iteration 7043 : loss: 0.018744, loss_a: 0.011026
[23:00:54.125] iteration 7044 : loss: 0.018763, loss_a: 0.011037
[23:00:55.457] iteration 7045 : loss: 0.024457, loss_a: 0.014387
[23:00:56.204] iteration 7046 : loss: 0.022355, loss_a: 0.013150
[23:00:57.542] iteration 7047 : loss: 0.028115, loss_a: 0.016538
[23:00:58.292] iteration 7048 : loss: 0.025266, loss_a: 0.014862
[23:00:59.616] iteration 7049 : loss: 0.036782, loss_a: 0.021636
[23:01:00.355] iteration 7050 : loss: 0.018067, loss_a: 0.010628
[23:01:01.674] iteration 7051 : loss: 0.019671, loss_a: 0.011571
[23:01:02.409] iteration 7052 : loss: 0.019641, loss_a: 0.011553
[23:01:03.718] iteration 7053 : loss: 0.020498, loss_a: 0.012058
[23:01:04.457] iteration 7054 : loss: 0.051695, loss_a: 0.030409
[23:01:05.800] iteration 7055 : loss: 0.024391, loss_a: 0.014347
[23:01:06.545] iteration 7056 : loss: 0.029167, loss_a: 0.017157
[23:01:07.890] iteration 7057 : loss: 0.069838, loss_a: 0.041081
[23:01:08.629] iteration 7058 : loss: 0.019196, loss_a: 0.011292
[23:01:09.941] iteration 7059 : loss: 0.028595, loss_a: 0.016821
[23:01:10.696] iteration 7060 : loss: 0.015742, loss_a: 0.009260
[23:01:12.023] iteration 7061 : loss: 0.021267, loss_a: 0.012510
[23:01:12.765] iteration 7062 : loss: 0.021554, loss_a: 0.012679
[23:01:14.120] iteration 7063 : loss: 0.034524, loss_a: 0.020309
[23:01:14.851] iteration 7064 : loss: 0.024908, loss_a: 0.014652
[23:01:16.177] iteration 7065 : loss: 0.058230, loss_a: 0.034253
[23:01:16.914] iteration 7066 : loss: 0.019256, loss_a: 0.011327
[23:01:18.242] iteration 7067 : loss: 0.022993, loss_a: 0.013525
[23:01:18.979] iteration 7068 : loss: 0.022141, loss_a: 0.013024
[23:01:20.293] iteration 7069 : loss: 0.022090, loss_a: 0.012994
[23:01:21.024] iteration 7070 : loss: 0.040327, loss_a: 0.023722
[23:01:22.366] iteration 7071 : loss: 0.039750, loss_a: 0.023383
[23:01:23.105] iteration 7072 : loss: 0.019931, loss_a: 0.011724
[23:01:24.449] iteration 7073 : loss: 0.025979, loss_a: 0.015282
[23:01:25.190] iteration 7074 : loss: 0.020600, loss_a: 0.012118
[23:01:26.539] iteration 7075 : loss: 0.047277, loss_a: 0.027810
[23:01:27.279] iteration 7076 : loss: 0.012376, loss_a: 0.007280
[23:01:28.605] iteration 7077 : loss: 0.032367, loss_a: 0.019040
[23:01:29.356] iteration 7078 : loss: 0.023475, loss_a: 0.013809
[23:01:30.715] iteration 7079 : loss: 0.030532, loss_a: 0.017960
[23:01:31.448] iteration 7080 : loss: 0.036570, loss_a: 0.021512
[23:01:32.799] iteration 7081 : loss: 0.015083, loss_a: 0.008872
[23:01:33.542] iteration 7082 : loss: 0.015316, loss_a: 0.009010
[23:01:34.901] iteration 7083 : loss: 0.031713, loss_a: 0.018655
[23:01:35.638] iteration 7084 : loss: 0.030017, loss_a: 0.017657
[23:01:36.965] iteration 7085 : loss: 0.024210, loss_a: 0.014241
[23:01:37.708] iteration 7086 : loss: 0.041663, loss_a: 0.024508
[23:01:39.083] iteration 7087 : loss: 0.053281, loss_a: 0.031342
[23:01:39.818] iteration 7088 : loss: 0.047441, loss_a: 0.027907
[23:01:41.145] iteration 7089 : loss: 0.024428, loss_a: 0.014369
[23:01:41.880] iteration 7090 : loss: 0.019407, loss_a: 0.011416
[23:01:43.204] iteration 7091 : loss: 0.069202, loss_a: 0.040707
[23:01:43.940] iteration 7092 : loss: 0.054282, loss_a: 0.031931
[23:01:45.302] iteration 7093 : loss: 0.031698, loss_a: 0.018646
[23:01:46.043] iteration 7094 : loss: 0.023007, loss_a: 0.013534
[23:01:47.394] iteration 7095 : loss: 0.020845, loss_a: 0.012262
[23:01:48.128] iteration 7096 : loss: 0.021727, loss_a: 0.012781
[23:01:49.465] iteration 7097 : loss: 0.023465, loss_a: 0.013803
[23:01:50.210] iteration 7098 : loss: 0.019631, loss_a: 0.011548
[23:01:51.538] iteration 7099 : loss: 0.030687, loss_a: 0.018051
[23:01:52.283] iteration 7100 : loss: 0.052823, loss_a: 0.031073
[23:01:53.626] iteration 7101 : loss: 0.019241, loss_a: 0.011318
[23:01:54.375] iteration 7102 : loss: 0.026084, loss_a: 0.015343
[23:01:55.697] iteration 7103 : loss: 0.063504, loss_a: 0.037355
[23:01:56.441] iteration 7104 : loss: 0.030185, loss_a: 0.017756
[23:01:57.807] iteration 7105 : loss: 0.035760, loss_a: 0.021035
[23:01:58.541] iteration 7106 : loss: 0.024968, loss_a: 0.014687
[23:01:59.869] iteration 7107 : loss: 0.037376, loss_a: 0.021986
[23:02:00.608] iteration 7108 : loss: 0.029940, loss_a: 0.017612
[23:02:01.987] iteration 7109 : loss: 0.027877, loss_a: 0.016398
[23:02:02.732] iteration 7110 : loss: 0.037588, loss_a: 0.022110
[23:02:04.051] iteration 7111 : loss: 0.017164, loss_a: 0.010096
[23:02:04.791] iteration 7112 : loss: 0.052498, loss_a: 0.030881
[23:02:06.145] iteration 7113 : loss: 0.026177, loss_a: 0.015398
[23:02:06.889] iteration 7114 : loss: 0.034405, loss_a: 0.020238
[23:02:08.215] iteration 7115 : loss: 0.037814, loss_a: 0.022244
[23:02:08.950] iteration 7116 : loss: 0.016934, loss_a: 0.009961
[23:02:10.278] iteration 7117 : loss: 0.023466, loss_a: 0.013803
[23:02:11.011] iteration 7118 : loss: 0.015392, loss_a: 0.009054
[23:02:12.352] iteration 7119 : loss: 0.012056, loss_a: 0.007092
[23:02:13.100] iteration 7120 : loss: 0.021854, loss_a: 0.012855
[23:02:14.456] iteration 7121 : loss: 0.033571, loss_a: 0.019747
[23:02:15.197] iteration 7122 : loss: 0.013455, loss_a: 0.007915
[23:02:16.561] iteration 7123 : loss: 0.041839, loss_a: 0.024611
[23:02:17.308] iteration 7124 : loss: 0.062100, loss_a: 0.036530
[23:02:18.634] iteration 7125 : loss: 0.020755, loss_a: 0.012209
[23:02:19.372] iteration 7126 : loss: 0.018637, loss_a: 0.010963
[23:02:20.725] iteration 7127 : loss: 0.021712, loss_a: 0.012772
[23:02:21.485] iteration 7128 : loss: 0.054739, loss_a: 0.032199
[23:02:22.841] iteration 7129 : loss: 0.034587, loss_a: 0.020345
[23:02:23.577] iteration 7130 : loss: 0.022821, loss_a: 0.013424
[23:02:24.921] iteration 7131 : loss: 0.017375, loss_a: 0.010220
[23:02:25.666] iteration 7132 : loss: 0.036132, loss_a: 0.021254
[23:02:27.002] iteration 7133 : loss: 0.050397, loss_a: 0.029646
[23:02:27.750] iteration 7134 : loss: 0.051974, loss_a: 0.030573
[23:02:29.073] iteration 7135 : loss: 0.020554, loss_a: 0.012091
[23:02:29.810] iteration 7136 : loss: 0.044803, loss_a: 0.026354
[23:02:31.176] iteration 7137 : loss: 0.026096, loss_a: 0.015350
[23:02:31.920] iteration 7138 : loss: 0.017352, loss_a: 0.010207
[23:02:33.270] iteration 7139 : loss: 0.054436, loss_a: 0.032021
[23:02:33.998] iteration 7140 : loss: 0.014734, loss_a: 0.008667
[23:02:35.322] iteration 7141 : loss: 0.039138, loss_a: 0.023022
[23:02:36.057] iteration 7142 : loss: 0.029966, loss_a: 0.017627
[23:02:37.372] iteration 7143 : loss: 0.022228, loss_a: 0.013075
[23:02:38.118] iteration 7144 : loss: 0.032021, loss_a: 0.018836
[23:02:39.498] iteration 7145 : loss: 0.027945, loss_a: 0.016438
[23:02:40.238] iteration 7146 : loss: 0.035896, loss_a: 0.021116
[23:02:41.592] iteration 7147 : loss: 0.031671, loss_a: 0.018630
[23:02:42.327] iteration 7148 : loss: 0.021708, loss_a: 0.012770
[23:02:43.637] iteration 7149 : loss: 0.037520, loss_a: 0.022071
[23:02:44.376] iteration 7150 : loss: 0.012060, loss_a: 0.007094
[23:02:45.698] iteration 7151 : loss: 0.033125, loss_a: 0.019485
[23:02:46.439] iteration 7152 : loss: 0.019694, loss_a: 0.011585
[23:02:47.760] iteration 7153 : loss: 0.030335, loss_a: 0.017844
[23:02:48.495] iteration 7154 : loss: 0.056061, loss_a: 0.032977
[23:02:49.847] iteration 7155 : loss: 0.061960, loss_a: 0.036447
[23:02:50.588] iteration 7156 : loss: 0.053184, loss_a: 0.031285
[23:02:51.915] iteration 7157 : loss: 0.029945, loss_a: 0.017615
[23:02:52.666] iteration 7158 : loss: 0.031248, loss_a: 0.018381
[23:02:54.034] iteration 7159 : loss: 0.028449, loss_a: 0.016735
[23:02:54.774] iteration 7160 : loss: 0.041053, loss_a: 0.024149
[23:02:56.104] iteration 7161 : loss: 0.030375, loss_a: 0.017868
[23:02:56.860] iteration 7162 : loss: 0.038255, loss_a: 0.022503
[23:02:58.192] iteration 7163 : loss: 0.023227, loss_a: 0.013663
[23:02:58.931] iteration 7164 : loss: 0.021336, loss_a: 0.012550
[23:03:00.292] iteration 7165 : loss: 0.031404, loss_a: 0.018473
[23:03:01.035] iteration 7166 : loss: 0.025498, loss_a: 0.014999
[23:03:02.350] iteration 7167 : loss: 0.031612, loss_a: 0.018595
[23:03:03.096] iteration 7168 : loss: 0.044491, loss_a: 0.026171
[23:03:04.430] iteration 7169 : loss: 0.037208, loss_a: 0.021887
[23:03:05.162] iteration 7170 : loss: 0.070149, loss_a: 0.041264
[23:03:06.518] iteration 7171 : loss: 0.020698, loss_a: 0.012175
[23:03:07.263] iteration 7172 : loss: 0.027878, loss_a: 0.016399
[23:03:08.606] iteration 7173 : loss: 0.052538, loss_a: 0.030904
[23:03:09.346] iteration 7174 : loss: 0.026402, loss_a: 0.015531
[23:03:10.662] iteration 7175 : loss: 0.025160, loss_a: 0.014800
[23:03:11.410] iteration 7176 : loss: 0.036343, loss_a: 0.021378
[23:03:12.736] iteration 7177 : loss: 0.025739, loss_a: 0.015141
[23:03:13.473] iteration 7178 : loss: 0.025249, loss_a: 0.014852
[23:03:14.820] iteration 7179 : loss: 0.032750, loss_a: 0.019265
[23:03:15.562] iteration 7180 : loss: 0.024888, loss_a: 0.014640
[23:03:16.936] iteration 7181 : loss: 0.043283, loss_a: 0.025460
[23:03:17.673] iteration 7182 : loss: 0.020095, loss_a: 0.011820
[23:03:19.006] iteration 7183 : loss: 0.029354, loss_a: 0.017267
[23:03:19.748] iteration 7184 : loss: 0.024895, loss_a: 0.014644
[23:03:21.076] iteration 7185 : loss: 0.025664, loss_a: 0.015097
[23:03:21.812] iteration 7186 : loss: 0.012150, loss_a: 0.007147
[23:03:23.147] iteration 7187 : loss: 0.022081, loss_a: 0.012989
[23:03:23.888] iteration 7188 : loss: 0.019886, loss_a: 0.011698
[23:03:25.213] iteration 7189 : loss: 0.015115, loss_a: 0.008891
[23:03:25.954] iteration 7190 : loss: 0.035245, loss_a: 0.020733
[23:03:27.295] iteration 7191 : loss: 0.026551, loss_a: 0.015618
[23:03:28.038] iteration 7192 : loss: 0.034821, loss_a: 0.020483
[23:03:29.383] iteration 7193 : loss: 0.015173, loss_a: 0.008925
[23:03:30.121] iteration 7194 : loss: 0.084098, loss_a: 0.049469
[23:03:31.437] iteration 7195 : loss: 0.045562, loss_a: 0.026801
[23:03:32.176] iteration 7196 : loss: 0.036061, loss_a: 0.021212
[23:03:33.519] iteration 7197 : loss: 0.028902, loss_a: 0.017001
[23:03:34.253] iteration 7198 : loss: 0.021277, loss_a: 0.012516
[23:03:35.582] iteration 7199 : loss: 0.040418, loss_a: 0.023775
[23:03:36.325] iteration 7200 : loss: 0.021972, loss_a: 0.012925
[23:04:01.043] iteration 7201 : loss: 0.020849, loss_a: 0.012264
[23:04:03.205] iteration 7202 : loss: 0.048580, loss_a: 0.028577
[23:04:04.530] iteration 7203 : loss: 0.020552, loss_a: 0.012089
[23:04:05.275] iteration 7204 : loss: 0.029346, loss_a: 0.017262
[23:04:06.597] iteration 7205 : loss: 0.051804, loss_a: 0.030473
[23:04:07.344] iteration 7206 : loss: 0.041784, loss_a: 0.024579
[23:04:08.705] iteration 7207 : loss: 0.025539, loss_a: 0.015023
[23:04:09.440] iteration 7208 : loss: 0.015375, loss_a: 0.009044
[23:04:10.775] iteration 7209 : loss: 0.015448, loss_a: 0.009087
[23:04:11.518] iteration 7210 : loss: 0.027821, loss_a: 0.016365
[23:04:12.863] iteration 7211 : loss: 0.026629, loss_a: 0.015664
[23:04:13.599] iteration 7212 : loss: 0.045182, loss_a: 0.026577
[23:04:14.957] iteration 7213 : loss: 0.019261, loss_a: 0.011330
[23:04:15.697] iteration 7214 : loss: 0.016286, loss_a: 0.009580
[23:04:17.024] iteration 7215 : loss: 0.041829, loss_a: 0.024605
[23:04:17.775] iteration 7216 : loss: 0.049184, loss_a: 0.028932
[23:04:19.137] iteration 7217 : loss: 0.021575, loss_a: 0.012691
[23:04:19.893] iteration 7218 : loss: 0.017274, loss_a: 0.010161
[23:04:21.235] iteration 7219 : loss: 0.036367, loss_a: 0.021392
[23:04:21.997] iteration 7220 : loss: 0.032296, loss_a: 0.018998
[23:04:23.343] iteration 7221 : loss: 0.026404, loss_a: 0.015532
[23:04:24.086] iteration 7222 : loss: 0.017454, loss_a: 0.010267
[23:04:25.410] iteration 7223 : loss: 0.038460, loss_a: 0.022624
[23:04:26.142] iteration 7224 : loss: 0.016751, loss_a: 0.009854
[23:04:27.452] iteration 7225 : loss: 0.031390, loss_a: 0.018464
[23:04:28.178] iteration 7226 : loss: 0.014758, loss_a: 0.008681
[23:04:29.499] iteration 7227 : loss: 0.051677, loss_a: 0.030398
[23:04:30.233] iteration 7228 : loss: 0.018237, loss_a: 0.010728
[23:04:31.574] iteration 7229 : loss: 0.044889, loss_a: 0.026405
[23:04:32.313] iteration 7230 : loss: 0.043270, loss_a: 0.025453
[23:04:33.649] iteration 7231 : loss: 0.033325, loss_a: 0.019603
[23:04:34.387] iteration 7232 : loss: 0.014054, loss_a: 0.008267
[23:04:35.739] iteration 7233 : loss: 0.025716, loss_a: 0.015127
[23:04:36.486] iteration 7234 : loss: 0.027247, loss_a: 0.016028
[23:04:37.807] iteration 7235 : loss: 0.028855, loss_a: 0.016973
[23:04:38.544] iteration 7236 : loss: 0.022831, loss_a: 0.013430
[23:04:39.864] iteration 7237 : loss: 0.040228, loss_a: 0.023663
[23:04:40.613] iteration 7238 : loss: 0.036121, loss_a: 0.021248
[23:04:41.934] iteration 7239 : loss: 0.011849, loss_a: 0.006970
[23:04:42.664] iteration 7240 : loss: 0.014902, loss_a: 0.008766
[23:04:43.994] iteration 7241 : loss: 0.033963, loss_a: 0.019978
[23:04:44.735] iteration 7242 : loss: 0.047282, loss_a: 0.027813
[23:04:46.079] iteration 7243 : loss: 0.035816, loss_a: 0.021068
[23:04:46.825] iteration 7244 : loss: 0.056606, loss_a: 0.033298
[23:04:48.160] iteration 7245 : loss: 0.023245, loss_a: 0.013673
[23:04:48.894] iteration 7246 : loss: 0.042995, loss_a: 0.025291
[23:04:50.231] iteration 7247 : loss: 0.010457, loss_a: 0.006151
[23:04:50.974] iteration 7248 : loss: 0.069131, loss_a: 0.040665
[23:04:52.329] iteration 7249 : loss: 0.037469, loss_a: 0.022041
[23:04:53.069] iteration 7250 : loss: 0.024029, loss_a: 0.014135
[23:04:54.415] iteration 7251 : loss: 0.018447, loss_a: 0.010851
[23:04:55.159] iteration 7252 : loss: 0.021657, loss_a: 0.012740
[23:04:56.502] iteration 7253 : loss: 0.057221, loss_a: 0.033659
[23:04:57.233] iteration 7254 : loss: 0.012677, loss_a: 0.007457
[23:04:58.557] iteration 7255 : loss: 0.017294, loss_a: 0.010173
[23:04:59.299] iteration 7256 : loss: 0.038787, loss_a: 0.022816
[23:05:00.652] iteration 7257 : loss: 0.037320, loss_a: 0.021953
[23:05:01.392] iteration 7258 : loss: 0.021977, loss_a: 0.012928
[23:05:02.751] iteration 7259 : loss: 0.026537, loss_a: 0.015610
[23:05:03.496] iteration 7260 : loss: 0.041683, loss_a: 0.024519
[23:05:04.823] iteration 7261 : loss: 0.028604, loss_a: 0.016826
[23:05:05.564] iteration 7262 : loss: 0.022231, loss_a: 0.013077
[23:05:06.909] iteration 7263 : loss: 0.015716, loss_a: 0.009245
[23:05:07.648] iteration 7264 : loss: 0.021582, loss_a: 0.012695
[23:05:09.010] iteration 7265 : loss: 0.046177, loss_a: 0.027163
[23:05:09.745] iteration 7266 : loss: 0.015814, loss_a: 0.009302
[23:05:11.082] iteration 7267 : loss: 0.018257, loss_a: 0.010739
[23:05:11.826] iteration 7268 : loss: 0.042580, loss_a: 0.025047
[23:05:13.165] iteration 7269 : loss: 0.022449, loss_a: 0.013205
[23:05:13.894] iteration 7270 : loss: 0.017095, loss_a: 0.010056
[23:05:15.240] iteration 7271 : loss: 0.038197, loss_a: 0.022469
[23:05:15.975] iteration 7272 : loss: 0.027388, loss_a: 0.016110
[23:05:17.317] iteration 7273 : loss: 0.037388, loss_a: 0.021993
[23:05:18.057] iteration 7274 : loss: 0.024036, loss_a: 0.014139
[23:05:19.420] iteration 7275 : loss: 0.033526, loss_a: 0.019721
[23:05:20.155] iteration 7276 : loss: 0.035684, loss_a: 0.020991
[23:05:21.481] iteration 7277 : loss: 0.014322, loss_a: 0.008424
[23:05:22.262] iteration 7278 : loss: 0.048783, loss_a: 0.028696
[23:05:23.576] iteration 7279 : loss: 0.022303, loss_a: 0.013120
[23:05:24.311] iteration 7280 : loss: 0.020126, loss_a: 0.011839
[23:05:25.671] iteration 7281 : loss: 0.029605, loss_a: 0.017415
[23:05:26.415] iteration 7282 : loss: 0.049876, loss_a: 0.029339
[23:05:27.738] iteration 7283 : loss: 0.052011, loss_a: 0.030595
[23:05:28.473] iteration 7284 : loss: 0.010980, loss_a: 0.006459
[23:05:29.809] iteration 7285 : loss: 0.009132, loss_a: 0.005372
[23:05:30.546] iteration 7286 : loss: 0.018189, loss_a: 0.010699
[23:05:31.883] iteration 7287 : loss: 0.016721, loss_a: 0.009836
[23:05:32.618] iteration 7288 : loss: 0.023796, loss_a: 0.013998
[23:05:33.955] iteration 7289 : loss: 0.014916, loss_a: 0.008774
[23:05:34.705] iteration 7290 : loss: 0.015714, loss_a: 0.009244
[23:05:36.077] iteration 7291 : loss: 0.042790, loss_a: 0.025170
[23:05:36.812] iteration 7292 : loss: 0.011542, loss_a: 0.006790
[23:05:38.174] iteration 7293 : loss: 0.021194, loss_a: 0.012467
[23:05:38.920] iteration 7294 : loss: 0.065306, loss_a: 0.038415
[23:05:40.242] iteration 7295 : loss: 0.027342, loss_a: 0.016083
[23:05:40.994] iteration 7296 : loss: 0.088285, loss_a: 0.051932
[23:05:42.321] iteration 7297 : loss: 0.009873, loss_a: 0.005808
[23:05:43.065] iteration 7298 : loss: 0.029719, loss_a: 0.017482
[23:05:44.411] iteration 7299 : loss: 0.033561, loss_a: 0.019742
[23:05:45.149] iteration 7300 : loss: 0.029412, loss_a: 0.017301
[23:05:46.480] iteration 7301 : loss: 0.057580, loss_a: 0.033870
[23:05:47.217] iteration 7302 : loss: 0.029431, loss_a: 0.017312
[23:05:48.540] iteration 7303 : loss: 0.018788, loss_a: 0.011052
[23:05:49.284] iteration 7304 : loss: 0.033038, loss_a: 0.019434
[23:05:50.628] iteration 7305 : loss: 0.011624, loss_a: 0.006838
[23:05:51.370] iteration 7306 : loss: 0.028610, loss_a: 0.016829
[23:05:52.698] iteration 7307 : loss: 0.044026, loss_a: 0.025898
[23:05:53.440] iteration 7308 : loss: 0.059320, loss_a: 0.034894
[23:05:54.765] iteration 7309 : loss: 0.024590, loss_a: 0.014465
[23:05:55.500] iteration 7310 : loss: 0.026243, loss_a: 0.015437
[23:05:56.836] iteration 7311 : loss: 0.040369, loss_a: 0.023746
[23:05:57.586] iteration 7312 : loss: 0.050972, loss_a: 0.029983
[23:05:58.934] iteration 7313 : loss: 0.035318, loss_a: 0.020775
[23:05:59.674] iteration 7314 : loss: 0.018498, loss_a: 0.010881
[23:06:01.023] iteration 7315 : loss: 0.040248, loss_a: 0.023675
[23:06:01.754] iteration 7316 : loss: 0.022125, loss_a: 0.013015
[23:06:03.099] iteration 7317 : loss: 0.023529, loss_a: 0.013841
[23:06:03.835] iteration 7318 : loss: 0.031421, loss_a: 0.018483
[23:06:05.155] iteration 7319 : loss: 0.016140, loss_a: 0.009494
[23:06:05.898] iteration 7320 : loss: 0.042391, loss_a: 0.024936
[23:06:07.237] iteration 7321 : loss: 0.034033, loss_a: 0.020019
[23:06:07.985] iteration 7322 : loss: 0.048570, loss_a: 0.028570
[23:06:09.346] iteration 7323 : loss: 0.053079, loss_a: 0.031223
[23:06:10.090] iteration 7324 : loss: 0.056285, loss_a: 0.033109
[23:06:11.416] iteration 7325 : loss: 0.028361, loss_a: 0.016683
[23:06:12.150] iteration 7326 : loss: 0.038661, loss_a: 0.022742
[23:06:13.462] iteration 7327 : loss: 0.016937, loss_a: 0.009963
[23:06:14.208] iteration 7328 : loss: 0.046966, loss_a: 0.027627
[23:06:15.536] iteration 7329 : loss: 0.022408, loss_a: 0.013181
[23:06:16.268] iteration 7330 : loss: 0.022405, loss_a: 0.013179
[23:06:17.657] iteration 7331 : loss: 0.078991, loss_a: 0.046466
[23:06:18.412] iteration 7332 : loss: 0.026828, loss_a: 0.015781
[23:06:19.753] iteration 7333 : loss: 0.036506, loss_a: 0.021474
[23:06:20.497] iteration 7334 : loss: 0.041052, loss_a: 0.024148
[23:06:21.829] iteration 7335 : loss: 0.026510, loss_a: 0.015594
[23:06:22.568] iteration 7336 : loss: 0.015075, loss_a: 0.008868
[23:06:23.897] iteration 7337 : loss: 0.043723, loss_a: 0.025719
[23:06:24.636] iteration 7338 : loss: 0.022349, loss_a: 0.013146
[23:06:25.948] iteration 7339 : loss: 0.036544, loss_a: 0.021497
[23:06:26.686] iteration 7340 : loss: 0.028149, loss_a: 0.016558
[23:06:28.020] iteration 7341 : loss: 0.027169, loss_a: 0.015982
[23:06:28.761] iteration 7342 : loss: 0.041210, loss_a: 0.024241
[23:06:30.127] iteration 7343 : loss: 0.052206, loss_a: 0.030709
[23:06:30.867] iteration 7344 : loss: 0.024956, loss_a: 0.014680
[23:06:32.191] iteration 7345 : loss: 0.031027, loss_a: 0.018251
[23:06:32.946] iteration 7346 : loss: 0.056103, loss_a: 0.033002
[23:06:34.246] iteration 7347 : loss: 0.035529, loss_a: 0.020899
[23:06:34.989] iteration 7348 : loss: 0.053750, loss_a: 0.031618
[23:06:36.300] iteration 7349 : loss: 0.027291, loss_a: 0.016054
[23:06:37.040] iteration 7350 : loss: 0.017698, loss_a: 0.010411
[23:06:38.366] iteration 7351 : loss: 0.050541, loss_a: 0.029730
[23:06:39.110] iteration 7352 : loss: 0.050427, loss_a: 0.029663
[23:06:40.457] iteration 7353 : loss: 0.028627, loss_a: 0.016839
[23:06:41.195] iteration 7354 : loss: 0.022374, loss_a: 0.013161
[23:06:42.534] iteration 7355 : loss: 0.026495, loss_a: 0.015586
[23:06:43.276] iteration 7356 : loss: 0.030383, loss_a: 0.017872
[23:06:44.657] iteration 7357 : loss: 0.045502, loss_a: 0.026766
[23:06:45.403] iteration 7358 : loss: 0.047920, loss_a: 0.028188
[23:06:46.733] iteration 7359 : loss: 0.024756, loss_a: 0.014562
[23:06:47.480] iteration 7360 : loss: 0.039957, loss_a: 0.023504
[23:06:48.803] iteration 7361 : loss: 0.028945, loss_a: 0.017026
[23:06:49.546] iteration 7362 : loss: 0.017187, loss_a: 0.010110
[23:06:50.902] iteration 7363 : loss: 0.030394, loss_a: 0.017879
[23:06:51.652] iteration 7364 : loss: 0.041295, loss_a: 0.024291
[23:06:52.995] iteration 7365 : loss: 0.020608, loss_a: 0.012122
[23:06:53.727] iteration 7366 : loss: 0.037017, loss_a: 0.021775
[23:06:55.056] iteration 7367 : loss: 0.020772, loss_a: 0.012219
[23:06:55.804] iteration 7368 : loss: 0.045488, loss_a: 0.026758
[23:06:57.142] iteration 7369 : loss: 0.017325, loss_a: 0.010191
[23:06:57.884] iteration 7370 : loss: 0.047805, loss_a: 0.028120
[23:06:59.211] iteration 7371 : loss: 0.037418, loss_a: 0.022011
[23:06:59.951] iteration 7372 : loss: 0.039345, loss_a: 0.023144
[23:07:01.300] iteration 7373 : loss: 0.024565, loss_a: 0.014450
[23:07:02.041] iteration 7374 : loss: 0.018254, loss_a: 0.010737
[23:07:03.382] iteration 7375 : loss: 0.026886, loss_a: 0.015815
[23:07:04.116] iteration 7376 : loss: 0.020970, loss_a: 0.012335
[23:07:05.471] iteration 7377 : loss: 0.027884, loss_a: 0.016402
[23:07:06.208] iteration 7378 : loss: 0.027409, loss_a: 0.016123
[23:07:07.528] iteration 7379 : loss: 0.025788, loss_a: 0.015170
[23:07:08.273] iteration 7380 : loss: 0.027338, loss_a: 0.016081
[23:07:09.635] iteration 7381 : loss: 0.020283, loss_a: 0.011931
[23:07:10.380] iteration 7382 : loss: 0.045171, loss_a: 0.026571
[23:07:11.735] iteration 7383 : loss: 0.028399, loss_a: 0.016705
[23:07:12.474] iteration 7384 : loss: 0.023240, loss_a: 0.013670
[23:07:13.803] iteration 7385 : loss: 0.161247, loss_a: 0.094851
[23:07:14.532] iteration 7386 : loss: 0.039430, loss_a: 0.023194
[23:07:15.852] iteration 7387 : loss: 0.030102, loss_a: 0.017707
[23:07:16.592] iteration 7388 : loss: 0.047213, loss_a: 0.027772
[23:07:17.913] iteration 7389 : loss: 0.026503, loss_a: 0.015590
[23:07:18.649] iteration 7390 : loss: 0.018102, loss_a: 0.010648
[23:07:19.979] iteration 7391 : loss: 0.030910, loss_a: 0.018183
[23:07:20.714] iteration 7392 : loss: 0.020603, loss_a: 0.012120
[23:07:22.039] iteration 7393 : loss: 0.042155, loss_a: 0.024797
[23:07:22.780] iteration 7394 : loss: 0.059281, loss_a: 0.034871
[23:07:24.136] iteration 7395 : loss: 0.035915, loss_a: 0.021127
[23:07:24.870] iteration 7396 : loss: 0.035751, loss_a: 0.021030
[23:07:26.217] iteration 7397 : loss: 0.041685, loss_a: 0.024521
[23:07:26.966] iteration 7398 : loss: 0.029123, loss_a: 0.017131
[23:07:28.285] iteration 7399 : loss: 0.064652, loss_a: 0.038031
[23:07:29.025] iteration 7400 : loss: 0.041147, loss_a: 0.024204
[23:07:53.645] iteration 7401 : loss: 0.037244, loss_a: 0.021908
[23:07:55.951] iteration 7402 : loss: 0.025540, loss_a: 0.015024
[23:07:57.283] iteration 7403 : loss: 0.032770, loss_a: 0.019277
[23:07:58.035] iteration 7404 : loss: 0.041251, loss_a: 0.024265
[23:07:59.386] iteration 7405 : loss: 0.032176, loss_a: 0.018927
[23:08:00.123] iteration 7406 : loss: 0.022182, loss_a: 0.013049
[23:08:01.448] iteration 7407 : loss: 0.036177, loss_a: 0.021280
[23:08:02.197] iteration 7408 : loss: 0.021461, loss_a: 0.012624
[23:08:03.557] iteration 7409 : loss: 0.044048, loss_a: 0.025910
[23:08:04.293] iteration 7410 : loss: 0.036246, loss_a: 0.021321
[23:08:05.643] iteration 7411 : loss: 0.033676, loss_a: 0.019810
[23:08:06.395] iteration 7412 : loss: 0.048455, loss_a: 0.028503
[23:08:07.702] iteration 7413 : loss: 0.035499, loss_a: 0.020882
[23:08:08.445] iteration 7414 : loss: 0.026140, loss_a: 0.015376
[23:08:09.794] iteration 7415 : loss: 0.045168, loss_a: 0.026569
[23:08:10.539] iteration 7416 : loss: 0.031600, loss_a: 0.018588
[23:08:11.883] iteration 7417 : loss: 0.029655, loss_a: 0.017444
[23:08:12.619] iteration 7418 : loss: 0.031514, loss_a: 0.018538
[23:08:13.968] iteration 7419 : loss: 0.052195, loss_a: 0.030703
[23:08:14.705] iteration 7420 : loss: 0.018446, loss_a: 0.010851
[23:08:16.030] iteration 7421 : loss: 0.026670, loss_a: 0.015688
[23:08:16.768] iteration 7422 : loss: 0.033930, loss_a: 0.019959
[23:08:18.116] iteration 7423 : loss: 0.032100, loss_a: 0.018883
[23:08:18.862] iteration 7424 : loss: 0.041721, loss_a: 0.024542
[23:08:20.213] iteration 7425 : loss: 0.033877, loss_a: 0.019928
[23:08:20.959] iteration 7426 : loss: 0.022460, loss_a: 0.013212
[23:08:22.289] iteration 7427 : loss: 0.018401, loss_a: 0.010824
[23:08:23.021] iteration 7428 : loss: 0.014709, loss_a: 0.008652
[23:08:24.381] iteration 7429 : loss: 0.014937, loss_a: 0.008786
[23:08:25.110] iteration 7430 : loss: 0.021311, loss_a: 0.012536
[23:08:26.443] iteration 7431 : loss: 0.026443, loss_a: 0.015555
[23:08:27.179] iteration 7432 : loss: 0.010660, loss_a: 0.006271
[23:08:28.513] iteration 7433 : loss: 0.017141, loss_a: 0.010083
[23:08:29.251] iteration 7434 : loss: 0.016806, loss_a: 0.009886
[23:08:30.588] iteration 7435 : loss: 0.026947, loss_a: 0.015851
[23:08:31.326] iteration 7436 : loss: 0.021589, loss_a: 0.012700
[23:08:32.679] iteration 7437 : loss: 0.025656, loss_a: 0.015092
[23:08:33.425] iteration 7438 : loss: 0.043067, loss_a: 0.025333
[23:08:34.775] iteration 7439 : loss: 0.039224, loss_a: 0.023073
[23:08:35.515] iteration 7440 : loss: 0.033844, loss_a: 0.019908
[23:08:36.829] iteration 7441 : loss: 0.025650, loss_a: 0.015088
[23:08:37.574] iteration 7442 : loss: 0.039739, loss_a: 0.023376
[23:08:38.894] iteration 7443 : loss: 0.026010, loss_a: 0.015300
[23:08:39.635] iteration 7444 : loss: 0.039131, loss_a: 0.023018
[23:08:40.960] iteration 7445 : loss: 0.022797, loss_a: 0.013410
[23:08:41.694] iteration 7446 : loss: 0.019800, loss_a: 0.011647
[23:08:43.029] iteration 7447 : loss: 0.070006, loss_a: 0.041180
[23:08:43.782] iteration 7448 : loss: 0.019987, loss_a: 0.011757
[23:08:45.109] iteration 7449 : loss: 0.016624, loss_a: 0.009779
[23:08:45.850] iteration 7450 : loss: 0.012419, loss_a: 0.007305
[23:08:47.177] iteration 7451 : loss: 0.018910, loss_a: 0.011123
[23:08:47.920] iteration 7452 : loss: 0.034630, loss_a: 0.020371
[23:08:49.288] iteration 7453 : loss: 0.048643, loss_a: 0.028613
[23:08:50.022] iteration 7454 : loss: 0.016163, loss_a: 0.009507
[23:08:51.350] iteration 7455 : loss: 0.013304, loss_a: 0.007826
[23:08:52.085] iteration 7456 : loss: 0.019140, loss_a: 0.011259
[23:08:53.435] iteration 7457 : loss: 0.038788, loss_a: 0.022816
[23:08:54.174] iteration 7458 : loss: 0.028417, loss_a: 0.016716
[23:08:55.551] iteration 7459 : loss: 0.039506, loss_a: 0.023239
[23:08:56.287] iteration 7460 : loss: 0.025202, loss_a: 0.014825
[23:08:57.633] iteration 7461 : loss: 0.020311, loss_a: 0.011947
[23:08:58.369] iteration 7462 : loss: 0.049448, loss_a: 0.029087
[23:08:59.699] iteration 7463 : loss: 0.043588, loss_a: 0.025640
[23:09:00.445] iteration 7464 : loss: 0.022154, loss_a: 0.013032
[23:09:01.778] iteration 7465 : loss: 0.042315, loss_a: 0.024891
[23:09:02.526] iteration 7466 : loss: 0.016677, loss_a: 0.009810
[23:09:03.864] iteration 7467 : loss: 0.042100, loss_a: 0.024765
[23:09:04.602] iteration 7468 : loss: 0.025153, loss_a: 0.014796
[23:09:05.926] iteration 7469 : loss: 0.037365, loss_a: 0.021979
[23:09:06.664] iteration 7470 : loss: 0.029799, loss_a: 0.017529
[23:09:07.999] iteration 7471 : loss: 0.021117, loss_a: 0.012422
[23:09:08.745] iteration 7472 : loss: 0.051213, loss_a: 0.030126
[23:09:10.070] iteration 7473 : loss: 0.026284, loss_a: 0.015461
[23:09:10.824] iteration 7474 : loss: 0.047495, loss_a: 0.027938
[23:09:12.164] iteration 7475 : loss: 0.047766, loss_a: 0.028097
[23:09:12.903] iteration 7476 : loss: 0.034616, loss_a: 0.020362
[23:09:14.252] iteration 7477 : loss: 0.032448, loss_a: 0.019087
[23:09:14.992] iteration 7478 : loss: 0.029211, loss_a: 0.017183
[23:09:16.310] iteration 7479 : loss: 0.043207, loss_a: 0.025416
[23:09:17.061] iteration 7480 : loss: 0.096060, loss_a: 0.056506
[23:09:18.385] iteration 7481 : loss: 0.025827, loss_a: 0.015193
[23:09:19.127] iteration 7482 : loss: 0.023435, loss_a: 0.013785
[23:09:20.467] iteration 7483 : loss: 0.065986, loss_a: 0.038815
[23:09:21.205] iteration 7484 : loss: 0.042091, loss_a: 0.024760
[23:09:22.538] iteration 7485 : loss: 0.024555, loss_a: 0.014444
[23:09:23.280] iteration 7486 : loss: 0.028853, loss_a: 0.016972
[23:09:24.596] iteration 7487 : loss: 0.030438, loss_a: 0.017905
[23:09:25.340] iteration 7488 : loss: 0.041490, loss_a: 0.024406
[23:09:26.697] iteration 7489 : loss: 0.036342, loss_a: 0.021377
[23:09:27.435] iteration 7490 : loss: 0.024456, loss_a: 0.014386
[23:09:28.766] iteration 7491 : loss: 0.031015, loss_a: 0.018244
[23:09:29.504] iteration 7492 : loss: 0.027755, loss_a: 0.016326
[23:09:30.805] iteration 7493 : loss: 0.022520, loss_a: 0.013247
[23:09:31.550] iteration 7494 : loss: 0.024848, loss_a: 0.014616
[23:09:32.893] iteration 7495 : loss: 0.015035, loss_a: 0.008844
[23:09:33.630] iteration 7496 : loss: 0.021551, loss_a: 0.012677
[23:09:34.972] iteration 7497 : loss: 0.016396, loss_a: 0.009645
[23:09:35.717] iteration 7498 : loss: 0.035570, loss_a: 0.020924
[23:09:37.068] iteration 7499 : loss: 0.035891, loss_a: 0.021112
[23:09:37.817] iteration 7500 : loss: 0.057015, loss_a: 0.033538
[23:09:39.128] iteration 7501 : loss: 0.027187, loss_a: 0.015992
[23:09:39.864] iteration 7502 : loss: 0.023652, loss_a: 0.013913
[23:09:41.192] iteration 7503 : loss: 0.023610, loss_a: 0.013888
[23:09:41.935] iteration 7504 : loss: 0.023580, loss_a: 0.013870
[23:09:43.284] iteration 7505 : loss: 0.031554, loss_a: 0.018561
[23:09:44.026] iteration 7506 : loss: 0.053055, loss_a: 0.031209
[23:09:45.388] iteration 7507 : loss: 0.052828, loss_a: 0.031075
[23:09:46.135] iteration 7508 : loss: 0.024151, loss_a: 0.014206
[23:09:47.468] iteration 7509 : loss: 0.022227, loss_a: 0.013075
[23:09:48.206] iteration 7510 : loss: 0.032860, loss_a: 0.019329
[23:09:49.566] iteration 7511 : loss: 0.028469, loss_a: 0.016746
[23:09:50.310] iteration 7512 : loss: 0.015448, loss_a: 0.009087
[23:09:51.630] iteration 7513 : loss: 0.050089, loss_a: 0.029464
[23:09:52.362] iteration 7514 : loss: 0.022077, loss_a: 0.012987
[23:09:53.690] iteration 7515 : loss: 0.037756, loss_a: 0.022209
[23:09:54.419] iteration 7516 : loss: 0.019889, loss_a: 0.011700
[23:09:55.763] iteration 7517 : loss: 0.031337, loss_a: 0.018434
[23:09:56.508] iteration 7518 : loss: 0.035635, loss_a: 0.020962
[23:09:57.866] iteration 7519 : loss: 0.025191, loss_a: 0.014818
[23:09:58.605] iteration 7520 : loss: 0.017707, loss_a: 0.010416
[23:09:59.972] iteration 7521 : loss: 0.060734, loss_a: 0.035726
[23:10:00.719] iteration 7522 : loss: 0.023530, loss_a: 0.013841
[23:10:02.042] iteration 7523 : loss: 0.026381, loss_a: 0.015518
[23:10:02.792] iteration 7524 : loss: 0.036557, loss_a: 0.021504
[23:10:04.163] iteration 7525 : loss: 0.025819, loss_a: 0.015188
[23:10:04.924] iteration 7526 : loss: 0.029081, loss_a: 0.017106
[23:10:06.277] iteration 7527 : loss: 0.028673, loss_a: 0.016866
[23:10:07.024] iteration 7528 : loss: 0.031515, loss_a: 0.018538
[23:10:08.392] iteration 7529 : loss: 0.033275, loss_a: 0.019574
[23:10:09.141] iteration 7530 : loss: 0.021143, loss_a: 0.012437
[23:10:10.494] iteration 7531 : loss: 0.024149, loss_a: 0.014205
[23:10:11.234] iteration 7532 : loss: 0.019302, loss_a: 0.011354
[23:10:12.568] iteration 7533 : loss: 0.022633, loss_a: 0.013313
[23:10:13.309] iteration 7534 : loss: 0.017729, loss_a: 0.010429
[23:10:14.612] iteration 7535 : loss: 0.021130, loss_a: 0.012430
[23:10:15.344] iteration 7536 : loss: 0.022096, loss_a: 0.012997
[23:10:16.669] iteration 7537 : loss: 0.020455, loss_a: 0.012032
[23:10:17.417] iteration 7538 : loss: 0.021225, loss_a: 0.012485
[23:10:18.792] iteration 7539 : loss: 0.042914, loss_a: 0.025243
[23:10:19.530] iteration 7540 : loss: 0.023942, loss_a: 0.014084
[23:10:20.893] iteration 7541 : loss: 0.052409, loss_a: 0.030829
[23:10:21.634] iteration 7542 : loss: 0.032029, loss_a: 0.018840
[23:10:22.993] iteration 7543 : loss: 0.021044, loss_a: 0.012379
[23:10:23.737] iteration 7544 : loss: 0.019797, loss_a: 0.011645
[23:10:25.064] iteration 7545 : loss: 0.023738, loss_a: 0.013964
[23:10:25.817] iteration 7546 : loss: 0.057389, loss_a: 0.033758
[23:10:27.140] iteration 7547 : loss: 0.019216, loss_a: 0.011304
[23:10:27.883] iteration 7548 : loss: 0.035810, loss_a: 0.021065
[23:10:29.220] iteration 7549 : loss: 0.035342, loss_a: 0.020789
[23:10:29.969] iteration 7550 : loss: 0.018686, loss_a: 0.010992
[23:10:31.320] iteration 7551 : loss: 0.055360, loss_a: 0.032564
[23:10:32.052] iteration 7552 : loss: 0.024551, loss_a: 0.014442
[23:10:33.374] iteration 7553 : loss: 0.015849, loss_a: 0.009323
[23:10:34.118] iteration 7554 : loss: 0.016247, loss_a: 0.009557
[23:10:35.468] iteration 7555 : loss: 0.019368, loss_a: 0.011393
[23:10:36.206] iteration 7556 : loss: 0.017168, loss_a: 0.010099
[23:10:37.537] iteration 7557 : loss: 0.074872, loss_a: 0.044042
[23:10:38.281] iteration 7558 : loss: 0.032725, loss_a: 0.019250
[23:10:39.645] iteration 7559 : loss: 0.028792, loss_a: 0.016936
[23:10:40.389] iteration 7560 : loss: 0.025881, loss_a: 0.015224
[23:10:41.751] iteration 7561 : loss: 0.049187, loss_a: 0.028933
[23:10:42.501] iteration 7562 : loss: 0.031150, loss_a: 0.018323
[23:10:43.832] iteration 7563 : loss: 0.039142, loss_a: 0.023024
[23:10:44.598] iteration 7564 : loss: 0.037741, loss_a: 0.022200
[23:10:45.923] iteration 7565 : loss: 0.019711, loss_a: 0.011595
[23:10:46.657] iteration 7566 : loss: 0.031111, loss_a: 0.018300
[23:10:48.013] iteration 7567 : loss: 0.021382, loss_a: 0.012578
[23:10:48.755] iteration 7568 : loss: 0.021732, loss_a: 0.012784
[23:10:50.107] iteration 7569 : loss: 0.019867, loss_a: 0.011687
[23:10:50.847] iteration 7570 : loss: 0.020001, loss_a: 0.011766
[23:10:52.210] iteration 7571 : loss: 0.025708, loss_a: 0.015122
[23:10:52.949] iteration 7572 : loss: 0.022943, loss_a: 0.013496
[23:10:54.306] iteration 7573 : loss: 0.019244, loss_a: 0.011320
[23:10:55.047] iteration 7574 : loss: 0.024241, loss_a: 0.014260
[23:10:56.380] iteration 7575 : loss: 0.045471, loss_a: 0.026748
[23:10:57.121] iteration 7576 : loss: 0.025513, loss_a: 0.015008
[23:10:58.443] iteration 7577 : loss: 0.032232, loss_a: 0.018960
[23:10:59.191] iteration 7578 : loss: 0.017581, loss_a: 0.010342
[23:11:00.523] iteration 7579 : loss: 0.024069, loss_a: 0.014158
[23:11:01.265] iteration 7580 : loss: 0.015773, loss_a: 0.009278
[23:11:02.617] iteration 7581 : loss: 0.031722, loss_a: 0.018660
[23:11:03.353] iteration 7582 : loss: 0.046915, loss_a: 0.027597
[23:11:04.713] iteration 7583 : loss: 0.018658, loss_a: 0.010975
[23:11:05.450] iteration 7584 : loss: 0.022340, loss_a: 0.013141
[23:11:06.821] iteration 7585 : loss: 0.033272, loss_a: 0.019572
[23:11:07.560] iteration 7586 : loss: 0.035317, loss_a: 0.020774
[23:11:08.888] iteration 7587 : loss: 0.040412, loss_a: 0.023772
[23:11:09.634] iteration 7588 : loss: 0.045777, loss_a: 0.026928
[23:11:10.977] iteration 7589 : loss: 0.039265, loss_a: 0.023097
[23:11:11.734] iteration 7590 : loss: 0.039090, loss_a: 0.022994
[23:11:13.084] iteration 7591 : loss: 0.090090, loss_a: 0.052994
[23:11:13.833] iteration 7592 : loss: 0.017629, loss_a: 0.010370
[23:11:15.171] iteration 7593 : loss: 0.023545, loss_a: 0.013850
[23:11:15.913] iteration 7594 : loss: 0.016574, loss_a: 0.009750
[23:11:17.251] iteration 7595 : loss: 0.021349, loss_a: 0.012558
[23:11:18.001] iteration 7596 : loss: 0.100588, loss_a: 0.059169
[23:11:19.357] iteration 7597 : loss: 0.023546, loss_a: 0.013850
[23:11:20.105] iteration 7598 : loss: 0.024231, loss_a: 0.014254
[23:11:21.424] iteration 7599 : loss: 0.034261, loss_a: 0.020153
[23:11:22.179] iteration 7600 : loss: 0.044245, loss_a: 0.026026
[23:11:46.839] iteration 7601 : loss: 0.045050, loss_a: 0.026500
[23:11:49.164] iteration 7602 : loss: 0.028967, loss_a: 0.017039
[23:11:50.551] iteration 7603 : loss: 0.042110, loss_a: 0.024771
[23:11:51.287] iteration 7604 : loss: 0.025211, loss_a: 0.014830
[23:11:52.655] iteration 7605 : loss: 0.099973, loss_a: 0.058808
[23:11:53.392] iteration 7606 : loss: 0.045620, loss_a: 0.026835
[23:11:54.718] iteration 7607 : loss: 0.033451, loss_a: 0.019677
[23:11:55.458] iteration 7608 : loss: 0.036556, loss_a: 0.021503
[23:11:56.826] iteration 7609 : loss: 0.033831, loss_a: 0.019901
[23:11:57.562] iteration 7610 : loss: 0.016483, loss_a: 0.009696
[23:11:58.936] iteration 7611 : loss: 0.065295, loss_a: 0.038409
[23:11:59.688] iteration 7612 : loss: 0.023980, loss_a: 0.014106
[23:12:01.000] iteration 7613 : loss: 0.020064, loss_a: 0.011802
[23:12:01.744] iteration 7614 : loss: 0.041679, loss_a: 0.024517
[23:12:03.092] iteration 7615 : loss: 0.032565, loss_a: 0.019156
[23:12:03.829] iteration 7616 : loss: 0.036957, loss_a: 0.021739
[23:12:05.151] iteration 7617 : loss: 0.023170, loss_a: 0.013629
[23:12:05.889] iteration 7618 : loss: 0.031452, loss_a: 0.018501
[23:12:07.208] iteration 7619 : loss: 0.030799, loss_a: 0.018117
[23:12:07.954] iteration 7620 : loss: 0.031368, loss_a: 0.018452
[23:12:09.306] iteration 7621 : loss: 0.039472, loss_a: 0.023219
[23:12:10.054] iteration 7622 : loss: 0.030256, loss_a: 0.017798
[23:12:11.407] iteration 7623 : loss: 0.028391, loss_a: 0.016701
[23:12:12.150] iteration 7624 : loss: 0.049625, loss_a: 0.029191
[23:12:13.475] iteration 7625 : loss: 0.024847, loss_a: 0.014616
[23:12:14.231] iteration 7626 : loss: 0.031082, loss_a: 0.018284
[23:12:15.577] iteration 7627 : loss: 0.042593, loss_a: 0.025055
[23:12:16.315] iteration 7628 : loss: 0.026427, loss_a: 0.015545
[23:12:17.648] iteration 7629 : loss: 0.029769, loss_a: 0.017511
[23:12:18.389] iteration 7630 : loss: 0.061202, loss_a: 0.036001
[23:12:19.719] iteration 7631 : loss: 0.025036, loss_a: 0.014727
[23:12:20.455] iteration 7632 : loss: 0.027821, loss_a: 0.016365
[23:12:21.806] iteration 7633 : loss: 0.025188, loss_a: 0.014816
[23:12:22.540] iteration 7634 : loss: 0.037765, loss_a: 0.022215
[23:12:23.909] iteration 7635 : loss: 0.039451, loss_a: 0.023206
[23:12:24.641] iteration 7636 : loss: 0.012815, loss_a: 0.007538
[23:12:25.977] iteration 7637 : loss: 0.022400, loss_a: 0.013177
[23:12:26.723] iteration 7638 : loss: 0.029931, loss_a: 0.017606
[23:12:28.055] iteration 7639 : loss: 0.061479, loss_a: 0.036164
[23:12:28.805] iteration 7640 : loss: 0.061272, loss_a: 0.036043
[23:12:30.137] iteration 7641 : loss: 0.047930, loss_a: 0.028194
[23:12:30.886] iteration 7642 : loss: 0.072509, loss_a: 0.042652
[23:12:32.219] iteration 7643 : loss: 0.024697, loss_a: 0.014528
[23:12:32.950] iteration 7644 : loss: 0.021689, loss_a: 0.012758
[23:12:34.268] iteration 7645 : loss: 0.060738, loss_a: 0.035728
[23:12:35.016] iteration 7646 : loss: 0.056369, loss_a: 0.033158
[23:12:36.368] iteration 7647 : loss: 0.058472, loss_a: 0.034395
[23:12:37.120] iteration 7648 : loss: 0.045842, loss_a: 0.026966
[23:12:38.443] iteration 7649 : loss: 0.020482, loss_a: 0.012048
[23:12:39.188] iteration 7650 : loss: 0.035093, loss_a: 0.020643
[23:12:40.520] iteration 7651 : loss: 0.030998, loss_a: 0.018234
[23:12:41.262] iteration 7652 : loss: 0.023753, loss_a: 0.013972
[23:12:42.584] iteration 7653 : loss: 0.026332, loss_a: 0.015489
[23:12:43.324] iteration 7654 : loss: 0.026220, loss_a: 0.015423
[23:12:44.660] iteration 7655 : loss: 0.016269, loss_a: 0.009570
[23:12:45.405] iteration 7656 : loss: 0.053515, loss_a: 0.031480
[23:12:46.771] iteration 7657 : loss: 0.031794, loss_a: 0.018703
[23:12:47.522] iteration 7658 : loss: 0.029343, loss_a: 0.017261
[23:12:48.862] iteration 7659 : loss: 0.019767, loss_a: 0.011628
[23:12:49.596] iteration 7660 : loss: 0.030207, loss_a: 0.017769
[23:12:50.947] iteration 7661 : loss: 0.048481, loss_a: 0.028518
[23:12:51.699] iteration 7662 : loss: 0.060536, loss_a: 0.035609
[23:12:53.026] iteration 7663 : loss: 0.009797, loss_a: 0.005763
[23:12:53.771] iteration 7664 : loss: 0.033862, loss_a: 0.019919
[23:12:55.126] iteration 7665 : loss: 0.025930, loss_a: 0.015253
[23:12:55.891] iteration 7666 : loss: 0.032811, loss_a: 0.019300
[23:12:57.251] iteration 7667 : loss: 0.021692, loss_a: 0.012760
[23:12:57.984] iteration 7668 : loss: 0.015794, loss_a: 0.009291
[23:12:59.306] iteration 7669 : loss: 0.021170, loss_a: 0.012453
[23:13:00.052] iteration 7670 : loss: 0.016901, loss_a: 0.009942
[23:13:01.378] iteration 7671 : loss: 0.034995, loss_a: 0.020585
[23:13:02.111] iteration 7672 : loss: 0.024880, loss_a: 0.014635
[23:13:03.463] iteration 7673 : loss: 0.018389, loss_a: 0.010817
[23:13:04.200] iteration 7674 : loss: 0.019645, loss_a: 0.011556
[23:13:05.562] iteration 7675 : loss: 0.023894, loss_a: 0.014056
[23:13:06.309] iteration 7676 : loss: 0.029957, loss_a: 0.017622
[23:13:07.655] iteration 7677 : loss: 0.040059, loss_a: 0.023564
[23:13:08.392] iteration 7678 : loss: 0.020220, loss_a: 0.011894
[23:13:09.730] iteration 7679 : loss: 0.045189, loss_a: 0.026582
[23:13:10.471] iteration 7680 : loss: 0.080936, loss_a: 0.047610
[23:13:11.830] iteration 7681 : loss: 0.025308, loss_a: 0.014887
[23:13:12.575] iteration 7682 : loss: 0.022989, loss_a: 0.013523
[23:13:13.916] iteration 7683 : loss: 0.013938, loss_a: 0.008199
[23:13:14.668] iteration 7684 : loss: 0.027794, loss_a: 0.016350
[23:13:15.990] iteration 7685 : loss: 0.020182, loss_a: 0.011872
[23:13:16.721] iteration 7686 : loss: 0.057097, loss_a: 0.033586
[23:13:18.030] iteration 7687 : loss: 0.028194, loss_a: 0.016585
[23:13:18.775] iteration 7688 : loss: 0.025243, loss_a: 0.014849
[23:13:20.134] iteration 7689 : loss: 0.042823, loss_a: 0.025190
[23:13:20.875] iteration 7690 : loss: 0.036015, loss_a: 0.021185
[23:13:22.240] iteration 7691 : loss: 0.053558, loss_a: 0.031505
[23:13:22.979] iteration 7692 : loss: 0.028079, loss_a: 0.016517
[23:13:24.288] iteration 7693 : loss: 0.019903, loss_a: 0.011708
[23:13:25.024] iteration 7694 : loss: 0.024781, loss_a: 0.014577
[23:13:26.392] iteration 7695 : loss: 0.033075, loss_a: 0.019456
[23:13:27.128] iteration 7696 : loss: 0.014961, loss_a: 0.008801
[23:13:28.466] iteration 7697 : loss: 0.032146, loss_a: 0.018910
[23:13:29.213] iteration 7698 : loss: 0.045986, loss_a: 0.027050
[23:13:30.559] iteration 7699 : loss: 0.020984, loss_a: 0.012343
[23:13:31.307] iteration 7700 : loss: 0.032955, loss_a: 0.019385
[23:13:32.650] iteration 7701 : loss: 0.017797, loss_a: 0.010469
[23:13:33.386] iteration 7702 : loss: 0.032470, loss_a: 0.019100
[23:13:34.716] iteration 7703 : loss: 0.038003, loss_a: 0.022354
[23:13:35.457] iteration 7704 : loss: 0.038349, loss_a: 0.022558
[23:13:36.792] iteration 7705 : loss: 0.028539, loss_a: 0.016788
[23:13:37.534] iteration 7706 : loss: 0.034315, loss_a: 0.020185
[23:13:38.857] iteration 7707 : loss: 0.015677, loss_a: 0.009222
[23:13:39.595] iteration 7708 : loss: 0.054742, loss_a: 0.032201
[23:13:40.961] iteration 7709 : loss: 0.048246, loss_a: 0.028380
[23:13:41.704] iteration 7710 : loss: 0.032223, loss_a: 0.018955
[23:13:43.024] iteration 7711 : loss: 0.040669, loss_a: 0.023923
[23:13:43.762] iteration 7712 : loss: 0.021959, loss_a: 0.012917
[23:13:45.076] iteration 7713 : loss: 0.027449, loss_a: 0.016147
[23:13:45.818] iteration 7714 : loss: 0.083515, loss_a: 0.049126
[23:13:47.145] iteration 7715 : loss: 0.021137, loss_a: 0.012434
[23:13:47.891] iteration 7716 : loss: 0.032891, loss_a: 0.019348
[23:13:49.244] iteration 7717 : loss: 0.029110, loss_a: 0.017124
[23:13:49.994] iteration 7718 : loss: 0.032928, loss_a: 0.019369
[23:13:51.336] iteration 7719 : loss: 0.025082, loss_a: 0.014754
[23:13:52.075] iteration 7720 : loss: 0.031857, loss_a: 0.018739
[23:13:53.403] iteration 7721 : loss: 0.061912, loss_a: 0.036419
[23:13:54.153] iteration 7722 : loss: 0.019518, loss_a: 0.011481
[23:13:55.497] iteration 7723 : loss: 0.032369, loss_a: 0.019041
[23:13:56.248] iteration 7724 : loss: 0.024268, loss_a: 0.014275
[23:13:57.617] iteration 7725 : loss: 0.085158, loss_a: 0.050093
[23:13:58.354] iteration 7726 : loss: 0.060783, loss_a: 0.035755
[23:13:59.681] iteration 7727 : loss: 0.013064, loss_a: 0.007684
[23:14:00.436] iteration 7728 : loss: 0.018453, loss_a: 0.010855
[23:14:01.789] iteration 7729 : loss: 0.060693, loss_a: 0.035702
[23:14:02.528] iteration 7730 : loss: 0.030207, loss_a: 0.017769
[23:14:03.836] iteration 7731 : loss: 0.017193, loss_a: 0.010113
[23:14:04.584] iteration 7732 : loss: 0.073232, loss_a: 0.043077
[23:14:05.938] iteration 7733 : loss: 0.022085, loss_a: 0.012991
[23:14:06.677] iteration 7734 : loss: 0.026175, loss_a: 0.015397
[23:14:08.005] iteration 7735 : loss: 0.023911, loss_a: 0.014065
[23:14:08.753] iteration 7736 : loss: 0.044622, loss_a: 0.026248
[23:14:10.113] iteration 7737 : loss: 0.032020, loss_a: 0.018835
[23:14:10.859] iteration 7738 : loss: 0.026152, loss_a: 0.015384
[23:14:12.189] iteration 7739 : loss: 0.036449, loss_a: 0.021441
[23:14:12.928] iteration 7740 : loss: 0.034959, loss_a: 0.020564
[23:14:14.291] iteration 7741 : loss: 0.026840, loss_a: 0.015788
[23:14:15.083] iteration 7742 : loss: 0.020615, loss_a: 0.012126
[23:14:16.393] iteration 7743 : loss: 0.017653, loss_a: 0.010384
[23:14:17.137] iteration 7744 : loss: 0.034498, loss_a: 0.020293
[23:14:18.484] iteration 7745 : loss: 0.046853, loss_a: 0.027561
[23:14:19.228] iteration 7746 : loss: 0.018210, loss_a: 0.010712
[23:14:20.565] iteration 7747 : loss: 0.045343, loss_a: 0.026672
[23:14:21.298] iteration 7748 : loss: 0.026707, loss_a: 0.015710
[23:14:22.623] iteration 7749 : loss: 0.053996, loss_a: 0.031762
[23:14:23.359] iteration 7750 : loss: 0.028667, loss_a: 0.016863
[23:14:24.708] iteration 7751 : loss: 0.023079, loss_a: 0.013576
[23:14:25.450] iteration 7752 : loss: 0.020198, loss_a: 0.011881
[23:14:26.788] iteration 7753 : loss: 0.036391, loss_a: 0.021406
[23:14:27.527] iteration 7754 : loss: 0.018702, loss_a: 0.011001
[23:14:28.908] iteration 7755 : loss: 0.013562, loss_a: 0.007978
[23:14:29.645] iteration 7756 : loss: 0.021940, loss_a: 0.012906
[23:14:30.972] iteration 7757 : loss: 0.041473, loss_a: 0.024396
[23:14:31.714] iteration 7758 : loss: 0.029085, loss_a: 0.017109
[23:14:33.072] iteration 7759 : loss: 0.025682, loss_a: 0.015107
[23:14:33.814] iteration 7760 : loss: 0.026381, loss_a: 0.015518
[23:14:35.154] iteration 7761 : loss: 0.055838, loss_a: 0.032846
[23:14:35.911] iteration 7762 : loss: 0.077663, loss_a: 0.045684
[23:14:37.271] iteration 7763 : loss: 0.033438, loss_a: 0.019669
[23:14:38.018] iteration 7764 : loss: 0.031648, loss_a: 0.018616
[23:14:39.319] iteration 7765 : loss: 0.018080, loss_a: 0.010635
[23:14:40.073] iteration 7766 : loss: 0.068436, loss_a: 0.040256
[23:14:41.427] iteration 7767 : loss: 0.029767, loss_a: 0.017510
[23:14:42.171] iteration 7768 : loss: 0.052350, loss_a: 0.030794
[23:14:43.522] iteration 7769 : loss: 0.021973, loss_a: 0.012926
[23:14:44.253] iteration 7770 : loss: 0.025746, loss_a: 0.015145
[23:14:45.616] iteration 7771 : loss: 0.061211, loss_a: 0.036007
[23:14:46.357] iteration 7772 : loss: 0.019723, loss_a: 0.011602
[23:14:47.706] iteration 7773 : loss: 0.066477, loss_a: 0.039104
[23:14:48.446] iteration 7774 : loss: 0.037227, loss_a: 0.021898
[23:14:49.770] iteration 7775 : loss: 0.028513, loss_a: 0.016773
[23:14:50.508] iteration 7776 : loss: 0.017437, loss_a: 0.010257
[23:14:51.870] iteration 7777 : loss: 0.041406, loss_a: 0.024357
[23:14:52.621] iteration 7778 : loss: 0.022311, loss_a: 0.013124
[23:14:53.970] iteration 7779 : loss: 0.018614, loss_a: 0.010949
[23:14:54.714] iteration 7780 : loss: 0.034732, loss_a: 0.020431
[23:14:56.056] iteration 7781 : loss: 0.027843, loss_a: 0.016379
[23:14:56.814] iteration 7782 : loss: 0.034238, loss_a: 0.020140
[23:14:58.154] iteration 7783 : loss: 0.025442, loss_a: 0.014966
[23:14:58.907] iteration 7784 : loss: 0.046137, loss_a: 0.027139
[23:15:00.263] iteration 7785 : loss: 0.022095, loss_a: 0.012997
[23:15:01.002] iteration 7786 : loss: 0.022833, loss_a: 0.013431
[23:15:02.337] iteration 7787 : loss: 0.019314, loss_a: 0.011361
[23:15:03.087] iteration 7788 : loss: 0.042514, loss_a: 0.025008
[23:15:04.458] iteration 7789 : loss: 0.033536, loss_a: 0.019727
[23:15:05.202] iteration 7790 : loss: 0.042750, loss_a: 0.025147
[23:15:06.522] iteration 7791 : loss: 0.017872, loss_a: 0.010513
[23:15:07.253] iteration 7792 : loss: 0.035004, loss_a: 0.020591
[23:15:08.613] iteration 7793 : loss: 0.048643, loss_a: 0.028614
[23:15:09.357] iteration 7794 : loss: 0.029965, loss_a: 0.017626
[23:15:10.674] iteration 7795 : loss: 0.026373, loss_a: 0.015513
[23:15:11.416] iteration 7796 : loss: 0.020315, loss_a: 0.011950
[23:15:12.748] iteration 7797 : loss: 0.020713, loss_a: 0.012184
[23:15:13.486] iteration 7798 : loss: 0.038401, loss_a: 0.022589
[23:15:14.817] iteration 7799 : loss: 0.022515, loss_a: 0.013244
[23:15:15.576] iteration 7800 : loss: 0.034836, loss_a: 0.020492
[23:15:39.140] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_7800_dice_0.8991.pth
[23:15:40.459] iteration 7801 : loss: 0.018593, loss_a: 0.010937
[23:15:42.682] iteration 7802 : loss: 0.024856, loss_a: 0.014621
[23:15:44.044] iteration 7803 : loss: 0.047352, loss_a: 0.027854
[23:15:44.776] iteration 7804 : loss: 0.015792, loss_a: 0.009290
[23:15:46.127] iteration 7805 : loss: 0.019513, loss_a: 0.011478
[23:15:46.864] iteration 7806 : loss: 0.349532, loss_a: 0.205607
[23:15:48.211] iteration 7807 : loss: 0.036146, loss_a: 0.021262
[23:15:48.960] iteration 7808 : loss: 0.020925, loss_a: 0.012309
[23:15:50.283] iteration 7809 : loss: 0.026184, loss_a: 0.015403
[23:15:51.039] iteration 7810 : loss: 0.026908, loss_a: 0.015828
[23:15:52.356] iteration 7811 : loss: 0.024721, loss_a: 0.014542
[23:15:53.094] iteration 7812 : loss: 0.026646, loss_a: 0.015674
[23:15:54.431] iteration 7813 : loss: 0.031454, loss_a: 0.018502
[23:15:55.165] iteration 7814 : loss: 0.023955, loss_a: 0.014091
[23:15:56.509] iteration 7815 : loss: 0.025669, loss_a: 0.015099
[23:15:57.245] iteration 7816 : loss: 0.014186, loss_a: 0.008345
[23:15:58.563] iteration 7817 : loss: 0.018626, loss_a: 0.010957
[23:15:59.300] iteration 7818 : loss: 0.017181, loss_a: 0.010107
[23:16:00.664] iteration 7819 : loss: 0.017938, loss_a: 0.010552
[23:16:01.414] iteration 7820 : loss: 0.024206, loss_a: 0.014239
[23:16:02.745] iteration 7821 : loss: 0.039183, loss_a: 0.023049
[23:16:03.480] iteration 7822 : loss: 0.026624, loss_a: 0.015661
[23:16:04.799] iteration 7823 : loss: 0.038065, loss_a: 0.022391
[23:16:05.552] iteration 7824 : loss: 0.019528, loss_a: 0.011487
[23:16:06.910] iteration 7825 : loss: 0.070907, loss_a: 0.041710
[23:16:07.649] iteration 7826 : loss: 0.024492, loss_a: 0.014407
[23:16:09.000] iteration 7827 : loss: 0.016781, loss_a: 0.009871
[23:16:09.740] iteration 7828 : loss: 0.054059, loss_a: 0.031799
[23:16:11.114] iteration 7829 : loss: 0.041726, loss_a: 0.024544
[23:16:11.883] iteration 7830 : loss: 0.017456, loss_a: 0.010268
[23:16:13.213] iteration 7831 : loss: 0.017969, loss_a: 0.010570
[23:16:13.963] iteration 7832 : loss: 0.054914, loss_a: 0.032302
[23:16:15.331] iteration 7833 : loss: 0.018831, loss_a: 0.011077
[23:16:16.079] iteration 7834 : loss: 0.011223, loss_a: 0.006601
[23:16:17.433] iteration 7835 : loss: 0.027445, loss_a: 0.016144
[23:16:18.175] iteration 7836 : loss: 0.046430, loss_a: 0.027312
[23:16:19.517] iteration 7837 : loss: 0.024576, loss_a: 0.014457
[23:16:20.253] iteration 7838 : loss: 0.014248, loss_a: 0.008381
[23:16:21.582] iteration 7839 : loss: 0.028379, loss_a: 0.016694
[23:16:22.330] iteration 7840 : loss: 0.027068, loss_a: 0.015923
[23:16:23.690] iteration 7841 : loss: 0.015945, loss_a: 0.009380
[23:16:24.426] iteration 7842 : loss: 0.020344, loss_a: 0.011967
[23:16:25.738] iteration 7843 : loss: 0.032407, loss_a: 0.019063
[23:16:26.483] iteration 7844 : loss: 0.043607, loss_a: 0.025651
[23:16:27.807] iteration 7845 : loss: 0.029058, loss_a: 0.017093
[23:16:28.539] iteration 7846 : loss: 0.026135, loss_a: 0.015373
[23:16:29.853] iteration 7847 : loss: 0.022266, loss_a: 0.013098
[23:16:30.596] iteration 7848 : loss: 0.022622, loss_a: 0.013307
[23:16:31.928] iteration 7849 : loss: 0.020075, loss_a: 0.011809
[23:16:32.675] iteration 7850 : loss: 0.021088, loss_a: 0.012405
[23:16:34.038] iteration 7851 : loss: 0.028914, loss_a: 0.017008
[23:16:34.777] iteration 7852 : loss: 0.020164, loss_a: 0.011861
[23:16:36.140] iteration 7853 : loss: 0.036436, loss_a: 0.021433
[23:16:36.871] iteration 7854 : loss: 0.047985, loss_a: 0.028227
[23:16:38.218] iteration 7855 : loss: 0.020195, loss_a: 0.011880
[23:16:38.967] iteration 7856 : loss: 0.043550, loss_a: 0.025618
[23:16:40.328] iteration 7857 : loss: 0.018897, loss_a: 0.011116
[23:16:41.068] iteration 7858 : loss: 0.036484, loss_a: 0.021461
[23:16:42.387] iteration 7859 : loss: 0.020240, loss_a: 0.011906
[23:16:43.129] iteration 7860 : loss: 0.030512, loss_a: 0.017948
[23:16:44.450] iteration 7861 : loss: 0.026683, loss_a: 0.015696
[23:16:45.206] iteration 7862 : loss: 0.025006, loss_a: 0.014710
[23:16:46.516] iteration 7863 : loss: 0.025771, loss_a: 0.015160
[23:16:47.257] iteration 7864 : loss: 0.024331, loss_a: 0.014312
[23:16:48.598] iteration 7865 : loss: 0.030786, loss_a: 0.018110
[23:16:49.343] iteration 7866 : loss: 0.028392, loss_a: 0.016701
[23:16:50.698] iteration 7867 : loss: 0.078430, loss_a: 0.046135
[23:16:51.443] iteration 7868 : loss: 0.029915, loss_a: 0.017597
[23:16:52.802] iteration 7869 : loss: 0.027778, loss_a: 0.016340
[23:16:53.540] iteration 7870 : loss: 0.013987, loss_a: 0.008227
[23:16:54.866] iteration 7871 : loss: 0.024223, loss_a: 0.014249
[23:16:55.605] iteration 7872 : loss: 0.017677, loss_a: 0.010398
[23:16:56.938] iteration 7873 : loss: 0.028890, loss_a: 0.016994
[23:16:57.684] iteration 7874 : loss: 0.038420, loss_a: 0.022600
[23:16:59.031] iteration 7875 : loss: 0.031885, loss_a: 0.018756
[23:16:59.778] iteration 7876 : loss: 0.036771, loss_a: 0.021630
[23:17:01.137] iteration 7877 : loss: 0.029460, loss_a: 0.017330
[23:17:01.881] iteration 7878 : loss: 0.031350, loss_a: 0.018441
[23:17:03.227] iteration 7879 : loss: 0.026319, loss_a: 0.015482
[23:17:03.959] iteration 7880 : loss: 0.065891, loss_a: 0.038760
[23:17:05.304] iteration 7881 : loss: 0.029870, loss_a: 0.017571
[23:17:06.037] iteration 7882 : loss: 0.014239, loss_a: 0.008376
[23:17:07.386] iteration 7883 : loss: 0.014075, loss_a: 0.008280
[23:17:08.126] iteration 7884 : loss: 0.024196, loss_a: 0.014233
[23:17:09.472] iteration 7885 : loss: 0.048048, loss_a: 0.028263
[23:17:10.221] iteration 7886 : loss: 0.020138, loss_a: 0.011846
[23:17:11.599] iteration 7887 : loss: 0.036801, loss_a: 0.021648
[23:17:12.334] iteration 7888 : loss: 0.022946, loss_a: 0.013498
[23:17:13.698] iteration 7889 : loss: 0.036710, loss_a: 0.021594
[23:17:14.439] iteration 7890 : loss: 0.025835, loss_a: 0.015197
[23:17:15.791] iteration 7891 : loss: 0.029209, loss_a: 0.017182
[23:17:16.531] iteration 7892 : loss: 0.033317, loss_a: 0.019598
[23:17:17.868] iteration 7893 : loss: 0.029559, loss_a: 0.017387
[23:17:18.615] iteration 7894 : loss: 0.017243, loss_a: 0.010143
[23:17:19.966] iteration 7895 : loss: 0.023536, loss_a: 0.013845
[23:17:20.698] iteration 7896 : loss: 0.014111, loss_a: 0.008300
[23:17:22.025] iteration 7897 : loss: 0.026329, loss_a: 0.015488
[23:17:22.764] iteration 7898 : loss: 0.024453, loss_a: 0.014384
[23:17:24.078] iteration 7899 : loss: 0.025873, loss_a: 0.015220
[23:17:24.812] iteration 7900 : loss: 0.019029, loss_a: 0.011194
[23:17:26.171] iteration 7901 : loss: 0.022659, loss_a: 0.013329
[23:17:26.916] iteration 7902 : loss: 0.039283, loss_a: 0.023108
[23:17:28.278] iteration 7903 : loss: 0.027536, loss_a: 0.016198
[23:17:29.029] iteration 7904 : loss: 0.018470, loss_a: 0.010865
[23:17:30.353] iteration 7905 : loss: 0.038368, loss_a: 0.022570
[23:17:31.102] iteration 7906 : loss: 0.018433, loss_a: 0.010843
[23:17:32.410] iteration 7907 : loss: 0.028472, loss_a: 0.016748
[23:17:33.147] iteration 7908 : loss: 0.016899, loss_a: 0.009940
[23:17:34.491] iteration 7909 : loss: 0.018847, loss_a: 0.011086
[23:17:35.243] iteration 7910 : loss: 0.037676, loss_a: 0.022162
[23:17:36.586] iteration 7911 : loss: 0.026558, loss_a: 0.015622
[23:17:37.323] iteration 7912 : loss: 0.026399, loss_a: 0.015529
[23:17:38.637] iteration 7913 : loss: 0.023506, loss_a: 0.013827
[23:17:39.385] iteration 7914 : loss: 0.019147, loss_a: 0.011263
[23:17:40.718] iteration 7915 : loss: 0.016030, loss_a: 0.009430
[23:17:41.469] iteration 7916 : loss: 0.036269, loss_a: 0.021334
[23:17:42.813] iteration 7917 : loss: 0.057927, loss_a: 0.034075
[23:17:43.567] iteration 7918 : loss: 0.040980, loss_a: 0.024106
[23:17:44.947] iteration 7919 : loss: 0.042022, loss_a: 0.024719
[23:17:45.680] iteration 7920 : loss: 0.024451, loss_a: 0.014383
[23:17:47.018] iteration 7921 : loss: 0.030253, loss_a: 0.017796
[23:17:47.758] iteration 7922 : loss: 0.023446, loss_a: 0.013792
[23:17:49.107] iteration 7923 : loss: 0.038805, loss_a: 0.022827
[23:17:49.848] iteration 7924 : loss: 0.029781, loss_a: 0.017518
[23:17:51.164] iteration 7925 : loss: 0.031708, loss_a: 0.018652
[23:17:51.903] iteration 7926 : loss: 0.030396, loss_a: 0.017880
[23:17:53.218] iteration 7927 : loss: 0.019075, loss_a: 0.011220
[23:17:53.971] iteration 7928 : loss: 0.048994, loss_a: 0.028820
[23:17:55.291] iteration 7929 : loss: 0.027190, loss_a: 0.015994
[23:17:56.029] iteration 7930 : loss: 0.021687, loss_a: 0.012757
[23:17:57.351] iteration 7931 : loss: 0.029974, loss_a: 0.017632
[23:17:58.096] iteration 7932 : loss: 0.036094, loss_a: 0.021232
[23:17:59.433] iteration 7933 : loss: 0.018374, loss_a: 0.010808
[23:18:00.197] iteration 7934 : loss: 0.020110, loss_a: 0.011829
[23:18:01.519] iteration 7935 : loss: 0.017336, loss_a: 0.010198
[23:18:02.259] iteration 7936 : loss: 0.033311, loss_a: 0.019595
[23:18:03.643] iteration 7937 : loss: 0.051911, loss_a: 0.030536
[23:18:04.384] iteration 7938 : loss: 0.030268, loss_a: 0.017804
[23:18:05.716] iteration 7939 : loss: 0.017984, loss_a: 0.010579
[23:18:06.465] iteration 7940 : loss: 0.031515, loss_a: 0.018538
[23:18:07.789] iteration 7941 : loss: 0.044921, loss_a: 0.026424
[23:18:08.524] iteration 7942 : loss: 0.020885, loss_a: 0.012286
[23:18:09.886] iteration 7943 : loss: 0.017417, loss_a: 0.010245
[23:18:10.635] iteration 7944 : loss: 0.028750, loss_a: 0.016912
[23:18:11.993] iteration 7945 : loss: 0.041981, loss_a: 0.024695
[23:18:12.731] iteration 7946 : loss: 0.030675, loss_a: 0.018044
[23:18:14.066] iteration 7947 : loss: 0.022613, loss_a: 0.013302
[23:18:14.801] iteration 7948 : loss: 0.027472, loss_a: 0.016160
[23:18:16.162] iteration 7949 : loss: 0.054014, loss_a: 0.031773
[23:18:16.906] iteration 7950 : loss: 0.015474, loss_a: 0.009102
[23:18:18.271] iteration 7951 : loss: 0.032418, loss_a: 0.019070
[23:18:19.012] iteration 7952 : loss: 0.019388, loss_a: 0.011405
[23:18:20.376] iteration 7953 : loss: 0.067532, loss_a: 0.039724
[23:18:21.119] iteration 7954 : loss: 0.038705, loss_a: 0.022767
[23:18:22.456] iteration 7955 : loss: 0.044847, loss_a: 0.026380
[23:18:23.190] iteration 7956 : loss: 0.014184, loss_a: 0.008344
[23:18:24.531] iteration 7957 : loss: 0.022172, loss_a: 0.013042
[23:18:25.268] iteration 7958 : loss: 0.019222, loss_a: 0.011307
[23:18:26.630] iteration 7959 : loss: 0.056545, loss_a: 0.033262
[23:18:27.366] iteration 7960 : loss: 0.024329, loss_a: 0.014311
[23:18:28.690] iteration 7961 : loss: 0.019998, loss_a: 0.011763
[23:18:29.443] iteration 7962 : loss: 0.038726, loss_a: 0.022780
[23:18:30.782] iteration 7963 : loss: 0.032409, loss_a: 0.019064
[23:18:31.523] iteration 7964 : loss: 0.046490, loss_a: 0.027347
[23:18:32.882] iteration 7965 : loss: 0.037040, loss_a: 0.021788
[23:18:33.631] iteration 7966 : loss: 0.040245, loss_a: 0.023674
[23:18:34.980] iteration 7967 : loss: 0.018984, loss_a: 0.011167
[23:18:35.715] iteration 7968 : loss: 0.022818, loss_a: 0.013423
[23:18:37.062] iteration 7969 : loss: 0.013392, loss_a: 0.007877
[23:18:37.795] iteration 7970 : loss: 0.020933, loss_a: 0.012314
[23:18:39.124] iteration 7971 : loss: 0.023981, loss_a: 0.014106
[23:18:39.871] iteration 7972 : loss: 0.025161, loss_a: 0.014801
[23:18:41.181] iteration 7973 : loss: 0.036651, loss_a: 0.021560
[23:18:41.912] iteration 7974 : loss: 0.012863, loss_a: 0.007567
[23:18:43.254] iteration 7975 : loss: 0.041326, loss_a: 0.024309
[23:18:43.991] iteration 7976 : loss: 0.077157, loss_a: 0.045386
[23:18:45.324] iteration 7977 : loss: 0.047603, loss_a: 0.028002
[23:18:46.065] iteration 7978 : loss: 0.029812, loss_a: 0.017537
[23:18:47.414] iteration 7979 : loss: 0.041588, loss_a: 0.024464
[23:18:48.158] iteration 7980 : loss: 0.018505, loss_a: 0.010885
[23:18:49.490] iteration 7981 : loss: 0.025529, loss_a: 0.015017
[23:18:50.232] iteration 7982 : loss: 0.021326, loss_a: 0.012545
[23:18:51.576] iteration 7983 : loss: 0.095038, loss_a: 0.055905
[23:18:52.313] iteration 7984 : loss: 0.029898, loss_a: 0.017587
[23:18:53.669] iteration 7985 : loss: 0.060494, loss_a: 0.035584
[23:18:54.414] iteration 7986 : loss: 0.019546, loss_a: 0.011497
[23:18:55.740] iteration 7987 : loss: 0.033738, loss_a: 0.019846
[23:18:56.491] iteration 7988 : loss: 0.018978, loss_a: 0.011163
[23:18:57.848] iteration 7989 : loss: 0.067889, loss_a: 0.039935
[23:18:58.595] iteration 7990 : loss: 0.024823, loss_a: 0.014602
[23:18:59.955] iteration 7991 : loss: 0.031319, loss_a: 0.018423
[23:19:00.697] iteration 7992 : loss: 0.024922, loss_a: 0.014660
[23:19:02.017] iteration 7993 : loss: 0.026950, loss_a: 0.015853
[23:19:02.756] iteration 7994 : loss: 0.057770, loss_a: 0.033983
[23:19:04.076] iteration 7995 : loss: 0.024673, loss_a: 0.014513
[23:19:04.806] iteration 7996 : loss: 0.014855, loss_a: 0.008738
[23:19:06.157] iteration 7997 : loss: 0.030576, loss_a: 0.017986
[23:19:06.892] iteration 7998 : loss: 0.015363, loss_a: 0.009037
[23:19:08.248] iteration 7999 : loss: 0.035470, loss_a: 0.020865
[23:19:08.987] iteration 8000 : loss: 0.026921, loss_a: 0.015836
[23:19:33.607] iteration 8001 : loss: 0.018798, loss_a: 0.011058
[23:19:35.761] iteration 8002 : loss: 0.032734, loss_a: 0.019255
[23:19:37.111] iteration 8003 : loss: 0.052447, loss_a: 0.030851
[23:19:37.860] iteration 8004 : loss: 0.071080, loss_a: 0.041812
[23:19:39.217] iteration 8005 : loss: 0.022735, loss_a: 0.013374
[23:19:39.966] iteration 8006 : loss: 0.030048, loss_a: 0.017675
[23:19:41.296] iteration 8007 : loss: 0.033063, loss_a: 0.019449
[23:19:42.047] iteration 8008 : loss: 0.040114, loss_a: 0.023596
[23:19:43.371] iteration 8009 : loss: 0.022348, loss_a: 0.013146
[23:19:44.112] iteration 8010 : loss: 0.017755, loss_a: 0.010444
[23:19:45.456] iteration 8011 : loss: 0.054632, loss_a: 0.032137
[23:19:46.200] iteration 8012 : loss: 0.029060, loss_a: 0.017094
[23:19:47.557] iteration 8013 : loss: 0.043268, loss_a: 0.025452
[23:19:48.307] iteration 8014 : loss: 0.019594, loss_a: 0.011526
[23:19:49.638] iteration 8015 : loss: 0.015738, loss_a: 0.009258
[23:19:50.394] iteration 8016 : loss: 0.037615, loss_a: 0.022126
[23:19:51.758] iteration 8017 : loss: 0.039994, loss_a: 0.023526
[23:19:52.502] iteration 8018 : loss: 0.069392, loss_a: 0.040819
[23:19:53.848] iteration 8019 : loss: 0.032967, loss_a: 0.019392
[23:19:54.589] iteration 8020 : loss: 0.011486, loss_a: 0.006756
[23:19:55.948] iteration 8021 : loss: 0.037485, loss_a: 0.022050
[23:19:56.697] iteration 8022 : loss: 0.026058, loss_a: 0.015328
[23:19:58.053] iteration 8023 : loss: 0.016077, loss_a: 0.009457
[23:19:58.792] iteration 8024 : loss: 0.022206, loss_a: 0.013062
[23:20:00.151] iteration 8025 : loss: 0.016243, loss_a: 0.009555
[23:20:00.902] iteration 8026 : loss: 0.052028, loss_a: 0.030605
[23:20:02.274] iteration 8027 : loss: 0.032918, loss_a: 0.019363
[23:20:03.016] iteration 8028 : loss: 0.043861, loss_a: 0.025800
[23:20:04.358] iteration 8029 : loss: 0.103350, loss_a: 0.060794
[23:20:05.106] iteration 8030 : loss: 0.045112, loss_a: 0.026537
[23:20:06.477] iteration 8031 : loss: 0.050058, loss_a: 0.029446
[23:20:07.217] iteration 8032 : loss: 0.023195, loss_a: 0.013644
[23:20:08.563] iteration 8033 : loss: 0.024748, loss_a: 0.014558
[23:20:09.321] iteration 8034 : loss: 0.035806, loss_a: 0.021062
[23:20:10.671] iteration 8035 : loss: 0.023354, loss_a: 0.013737
[23:20:11.420] iteration 8036 : loss: 0.029718, loss_a: 0.017481
[23:20:12.737] iteration 8037 : loss: 0.028893, loss_a: 0.016996
[23:20:13.479] iteration 8038 : loss: 0.020030, loss_a: 0.011782
[23:20:14.812] iteration 8039 : loss: 0.022297, loss_a: 0.013116
[23:20:15.555] iteration 8040 : loss: 0.025241, loss_a: 0.014848
[23:20:16.900] iteration 8041 : loss: 0.018556, loss_a: 0.010915
[23:20:17.639] iteration 8042 : loss: 0.019507, loss_a: 0.011475
[23:20:18.986] iteration 8043 : loss: 0.064763, loss_a: 0.038096
[23:20:19.741] iteration 8044 : loss: 0.053281, loss_a: 0.031342
[23:20:21.089] iteration 8045 : loss: 0.035398, loss_a: 0.020822
[23:20:21.839] iteration 8046 : loss: 0.025981, loss_a: 0.015283
[23:20:23.173] iteration 8047 : loss: 0.043630, loss_a: 0.025664
[23:20:23.920] iteration 8048 : loss: 0.043120, loss_a: 0.025365
[23:20:25.232] iteration 8049 : loss: 0.031865, loss_a: 0.018744
[23:20:25.986] iteration 8050 : loss: 0.058403, loss_a: 0.034355
[23:20:27.341] iteration 8051 : loss: 0.040713, loss_a: 0.023949
[23:20:28.087] iteration 8052 : loss: 0.023159, loss_a: 0.013623
[23:20:29.453] iteration 8053 : loss: 0.042168, loss_a: 0.024805
[23:20:30.209] iteration 8054 : loss: 0.052945, loss_a: 0.031144
[23:20:31.555] iteration 8055 : loss: 0.025843, loss_a: 0.015202
[23:20:32.298] iteration 8056 : loss: 0.026642, loss_a: 0.015672
[23:20:33.653] iteration 8057 : loss: 0.018231, loss_a: 0.010724
[23:20:34.400] iteration 8058 : loss: 0.019460, loss_a: 0.011447
[23:20:35.748] iteration 8059 : loss: 0.034705, loss_a: 0.020415
[23:20:36.493] iteration 8060 : loss: 0.018578, loss_a: 0.010929
[23:20:37.845] iteration 8061 : loss: 0.016954, loss_a: 0.009973
[23:20:38.595] iteration 8062 : loss: 0.027368, loss_a: 0.016099
[23:20:39.947] iteration 8063 : loss: 0.022812, loss_a: 0.013419
[23:20:40.695] iteration 8064 : loss: 0.013045, loss_a: 0.007673
[23:20:42.076] iteration 8065 : loss: 0.037610, loss_a: 0.022123
[23:20:42.830] iteration 8066 : loss: 0.041541, loss_a: 0.024436
[23:20:44.189] iteration 8067 : loss: 0.067367, loss_a: 0.039628
[23:20:44.932] iteration 8068 : loss: 0.025202, loss_a: 0.014825
[23:20:46.270] iteration 8069 : loss: 0.035693, loss_a: 0.020996
[23:20:47.010] iteration 8070 : loss: 0.033547, loss_a: 0.019734
[23:20:48.314] iteration 8071 : loss: 0.016829, loss_a: 0.009899
[23:20:49.054] iteration 8072 : loss: 0.032492, loss_a: 0.019113
[23:20:50.414] iteration 8073 : loss: 0.026863, loss_a: 0.015802
[23:20:51.155] iteration 8074 : loss: 0.039944, loss_a: 0.023496
[23:20:52.522] iteration 8075 : loss: 0.028850, loss_a: 0.016970
[23:20:53.265] iteration 8076 : loss: 0.036539, loss_a: 0.021494
[23:20:54.578] iteration 8077 : loss: 0.020069, loss_a: 0.011805
[23:20:55.334] iteration 8078 : loss: 0.027603, loss_a: 0.016237
[23:20:56.667] iteration 8079 : loss: 0.073468, loss_a: 0.043217
[23:20:57.400] iteration 8080 : loss: 0.027234, loss_a: 0.016020
[23:20:58.755] iteration 8081 : loss: 0.029653, loss_a: 0.017443
[23:20:59.510] iteration 8082 : loss: 0.039709, loss_a: 0.023359
[23:21:00.866] iteration 8083 : loss: 0.024334, loss_a: 0.014314
[23:21:01.603] iteration 8084 : loss: 0.014351, loss_a: 0.008442
[23:21:02.956] iteration 8085 : loss: 0.025217, loss_a: 0.014834
[23:21:03.706] iteration 8086 : loss: 0.022023, loss_a: 0.012955
[23:21:05.061] iteration 8087 : loss: 0.113688, loss_a: 0.066875
[23:21:05.800] iteration 8088 : loss: 0.018174, loss_a: 0.010690
[23:21:07.162] iteration 8089 : loss: 0.026652, loss_a: 0.015678
[23:21:07.911] iteration 8090 : loss: 0.018206, loss_a: 0.010710
[23:21:09.246] iteration 8091 : loss: 0.058371, loss_a: 0.034336
[23:21:09.992] iteration 8092 : loss: 0.026024, loss_a: 0.015308
[23:21:11.361] iteration 8093 : loss: 0.035673, loss_a: 0.020984
[23:21:12.110] iteration 8094 : loss: 0.011261, loss_a: 0.006624
[23:21:13.429] iteration 8095 : loss: 0.016854, loss_a: 0.009914
[23:21:14.175] iteration 8096 : loss: 0.020791, loss_a: 0.012230
[23:21:15.495] iteration 8097 : loss: 0.021573, loss_a: 0.012690
[23:21:16.249] iteration 8098 : loss: 0.049880, loss_a: 0.029341
[23:21:17.591] iteration 8099 : loss: 0.113334, loss_a: 0.066667
[23:21:18.334] iteration 8100 : loss: 0.078389, loss_a: 0.046111
[23:21:19.690] iteration 8101 : loss: 0.012229, loss_a: 0.007194
[23:21:20.434] iteration 8102 : loss: 0.042002, loss_a: 0.024707
[23:21:21.793] iteration 8103 : loss: 0.034040, loss_a: 0.020024
[23:21:22.554] iteration 8104 : loss: 0.047576, loss_a: 0.027986
[23:21:23.929] iteration 8105 : loss: 0.035627, loss_a: 0.020957
[23:21:24.668] iteration 8106 : loss: 0.022833, loss_a: 0.013431
[23:21:26.000] iteration 8107 : loss: 0.034610, loss_a: 0.020359
[23:21:26.755] iteration 8108 : loss: 0.058267, loss_a: 0.034275
[23:21:28.106] iteration 8109 : loss: 0.024688, loss_a: 0.014523
[23:21:28.845] iteration 8110 : loss: 0.029909, loss_a: 0.017593
[23:21:30.164] iteration 8111 : loss: 0.042810, loss_a: 0.025182
[23:21:30.911] iteration 8112 : loss: 0.023181, loss_a: 0.013636
[23:21:32.264] iteration 8113 : loss: 0.036272, loss_a: 0.021336
[23:21:32.998] iteration 8114 : loss: 0.028341, loss_a: 0.016671
[23:21:34.351] iteration 8115 : loss: 0.034948, loss_a: 0.020558
[23:21:35.095] iteration 8116 : loss: 0.036010, loss_a: 0.021182
[23:21:36.412] iteration 8117 : loss: 0.030820, loss_a: 0.018129
[23:21:37.144] iteration 8118 : loss: 0.015866, loss_a: 0.009333
[23:21:38.470] iteration 8119 : loss: 0.029576, loss_a: 0.017398
[23:21:39.209] iteration 8120 : loss: 0.026027, loss_a: 0.015310
[23:21:40.531] iteration 8121 : loss: 0.012803, loss_a: 0.007531
[23:21:41.277] iteration 8122 : loss: 0.040751, loss_a: 0.023971
[23:21:42.621] iteration 8123 : loss: 0.062214, loss_a: 0.036596
[23:21:43.359] iteration 8124 : loss: 0.039040, loss_a: 0.022965
[23:21:44.700] iteration 8125 : loss: 0.019166, loss_a: 0.011274
[23:21:45.444] iteration 8126 : loss: 0.027564, loss_a: 0.016214
[23:21:46.780] iteration 8127 : loss: 0.026781, loss_a: 0.015753
[23:21:47.520] iteration 8128 : loss: 0.051170, loss_a: 0.030100
[23:21:48.852] iteration 8129 : loss: 0.054545, loss_a: 0.032085
[23:21:49.599] iteration 8130 : loss: 0.031837, loss_a: 0.018728
[23:21:50.923] iteration 8131 : loss: 0.012843, loss_a: 0.007555
[23:21:51.666] iteration 8132 : loss: 0.019411, loss_a: 0.011418
[23:21:53.012] iteration 8133 : loss: 0.033067, loss_a: 0.019451
[23:21:53.763] iteration 8134 : loss: 0.041180, loss_a: 0.024223
[23:21:55.136] iteration 8135 : loss: 0.030242, loss_a: 0.017789
[23:21:55.881] iteration 8136 : loss: 0.029716, loss_a: 0.017480
[23:21:57.234] iteration 8137 : loss: 0.025022, loss_a: 0.014719
[23:21:57.978] iteration 8138 : loss: 0.017641, loss_a: 0.010377
[23:21:59.312] iteration 8139 : loss: 0.083233, loss_a: 0.048960
[23:22:00.055] iteration 8140 : loss: 0.019738, loss_a: 0.011610
[23:22:01.381] iteration 8141 : loss: 0.044180, loss_a: 0.025988
[23:22:02.127] iteration 8142 : loss: 0.024239, loss_a: 0.014258
[23:22:03.445] iteration 8143 : loss: 0.041154, loss_a: 0.024208
[23:22:04.201] iteration 8144 : loss: 0.023855, loss_a: 0.014032
[23:22:05.522] iteration 8145 : loss: 0.055274, loss_a: 0.032514
[23:22:06.258] iteration 8146 : loss: 0.009679, loss_a: 0.005694
[23:22:07.585] iteration 8147 : loss: 0.027628, loss_a: 0.016252
[23:22:08.349] iteration 8148 : loss: 0.036916, loss_a: 0.021715
[23:22:09.663] iteration 8149 : loss: 0.016725, loss_a: 0.009839
[23:22:10.407] iteration 8150 : loss: 0.023471, loss_a: 0.013806
[23:22:11.776] iteration 8151 : loss: 0.031726, loss_a: 0.018662
[23:22:12.530] iteration 8152 : loss: 0.021456, loss_a: 0.012621
[23:22:13.894] iteration 8153 : loss: 0.018232, loss_a: 0.010725
[23:22:14.638] iteration 8154 : loss: 0.031720, loss_a: 0.018659
[23:22:15.990] iteration 8155 : loss: 0.039560, loss_a: 0.023270
[23:22:16.736] iteration 8156 : loss: 0.022413, loss_a: 0.013184
[23:22:18.104] iteration 8157 : loss: 0.034146, loss_a: 0.020086
[23:22:18.845] iteration 8158 : loss: 0.017159, loss_a: 0.010094
[23:22:20.165] iteration 8159 : loss: 0.013909, loss_a: 0.008182
[23:22:20.911] iteration 8160 : loss: 0.030135, loss_a: 0.017726
[23:22:22.243] iteration 8161 : loss: 0.018268, loss_a: 0.010746
[23:22:22.985] iteration 8162 : loss: 0.015832, loss_a: 0.009313
[23:22:24.344] iteration 8163 : loss: 0.023618, loss_a: 0.013893
[23:22:25.093] iteration 8164 : loss: 0.018136, loss_a: 0.010668
[23:22:26.455] iteration 8165 : loss: 0.014035, loss_a: 0.008256
[23:22:27.188] iteration 8166 : loss: 0.045158, loss_a: 0.026564
[23:22:28.535] iteration 8167 : loss: 0.025808, loss_a: 0.015181
[23:22:29.288] iteration 8168 : loss: 0.030483, loss_a: 0.017931
[23:22:30.629] iteration 8169 : loss: 0.020575, loss_a: 0.012103
[23:22:31.372] iteration 8170 : loss: 0.027496, loss_a: 0.016174
[23:22:32.705] iteration 8171 : loss: 0.017820, loss_a: 0.010482
[23:22:33.442] iteration 8172 : loss: 0.020344, loss_a: 0.011967
[23:22:34.772] iteration 8173 : loss: 0.010517, loss_a: 0.006187
[23:22:35.537] iteration 8174 : loss: 0.035135, loss_a: 0.020667
[23:22:36.863] iteration 8175 : loss: 0.014163, loss_a: 0.008331
[23:22:37.615] iteration 8176 : loss: 0.053181, loss_a: 0.031283
[23:22:38.982] iteration 8177 : loss: 0.015870, loss_a: 0.009335
[23:22:39.726] iteration 8178 : loss: 0.023403, loss_a: 0.013767
[23:22:41.088] iteration 8179 : loss: 0.025293, loss_a: 0.014878
[23:22:41.839] iteration 8180 : loss: 0.032008, loss_a: 0.018828
[23:22:43.190] iteration 8181 : loss: 0.036299, loss_a: 0.021353
[23:22:43.931] iteration 8182 : loss: 0.031819, loss_a: 0.018717
[23:22:45.263] iteration 8183 : loss: 0.011228, loss_a: 0.006605
[23:22:46.008] iteration 8184 : loss: 0.029999, loss_a: 0.017647
[23:22:47.337] iteration 8185 : loss: 0.051110, loss_a: 0.030065
[23:22:48.087] iteration 8186 : loss: 0.046720, loss_a: 0.027482
[23:22:49.419] iteration 8187 : loss: 0.030749, loss_a: 0.018088
[23:22:50.162] iteration 8188 : loss: 0.044434, loss_a: 0.026138
[23:22:51.485] iteration 8189 : loss: 0.028182, loss_a: 0.016578
[23:22:52.224] iteration 8190 : loss: 0.019918, loss_a: 0.011717
[23:22:53.538] iteration 8191 : loss: 0.026145, loss_a: 0.015379
[23:22:54.323] iteration 8192 : loss: 0.518255, loss_a: 0.304856
[23:22:55.641] iteration 8193 : loss: 0.027959, loss_a: 0.016446
[23:22:56.388] iteration 8194 : loss: 0.038621, loss_a: 0.022718
[23:22:57.734] iteration 8195 : loss: 0.073995, loss_a: 0.043526
[23:22:58.486] iteration 8196 : loss: 0.014468, loss_a: 0.008511
[23:22:59.832] iteration 8197 : loss: 0.075668, loss_a: 0.044511
[23:23:00.571] iteration 8198 : loss: 0.028821, loss_a: 0.016953
[23:23:01.909] iteration 8199 : loss: 0.016135, loss_a: 0.009491
[23:23:02.652] iteration 8200 : loss: 0.077392, loss_a: 0.045525
[23:23:27.326] iteration 8201 : loss: 0.041232, loss_a: 0.024254
[23:23:29.444] iteration 8202 : loss: 0.015434, loss_a: 0.009079
[23:23:30.790] iteration 8203 : loss: 0.018120, loss_a: 0.010659
[23:23:31.534] iteration 8204 : loss: 0.024892, loss_a: 0.014643
[23:23:32.894] iteration 8205 : loss: 0.030541, loss_a: 0.017966
[23:23:33.631] iteration 8206 : loss: 0.051125, loss_a: 0.030074
[23:23:34.987] iteration 8207 : loss: 0.071791, loss_a: 0.042230
[23:23:35.735] iteration 8208 : loss: 0.037656, loss_a: 0.022151
[23:23:37.101] iteration 8209 : loss: 0.028246, loss_a: 0.016615
[23:23:37.836] iteration 8210 : loss: 0.019399, loss_a: 0.011411
[23:23:39.197] iteration 8211 : loss: 0.037433, loss_a: 0.022019
[23:23:39.948] iteration 8212 : loss: 0.047493, loss_a: 0.027937
[23:23:41.313] iteration 8213 : loss: 0.032096, loss_a: 0.018880
[23:23:42.062] iteration 8214 : loss: 0.029138, loss_a: 0.017140
[23:23:43.390] iteration 8215 : loss: 0.022729, loss_a: 0.013370
[23:23:44.136] iteration 8216 : loss: 0.023098, loss_a: 0.013587
[23:23:45.501] iteration 8217 : loss: 0.038375, loss_a: 0.022574
[23:23:46.245] iteration 8218 : loss: 0.029986, loss_a: 0.017639
[23:23:47.570] iteration 8219 : loss: 0.037726, loss_a: 0.022192
[23:23:48.307] iteration 8220 : loss: 0.018902, loss_a: 0.011119
[23:23:49.633] iteration 8221 : loss: 0.056948, loss_a: 0.033499
[23:23:50.374] iteration 8222 : loss: 0.028318, loss_a: 0.016658
[23:23:51.714] iteration 8223 : loss: 0.011978, loss_a: 0.007046
[23:23:52.461] iteration 8224 : loss: 0.018513, loss_a: 0.010890
[23:23:53.792] iteration 8225 : loss: 0.023107, loss_a: 0.013592
[23:23:54.535] iteration 8226 : loss: 0.035627, loss_a: 0.020957
[23:23:55.854] iteration 8227 : loss: 0.036772, loss_a: 0.021630
[23:23:56.601] iteration 8228 : loss: 0.034345, loss_a: 0.020203
[23:23:57.937] iteration 8229 : loss: 0.018839, loss_a: 0.011082
[23:23:58.683] iteration 8230 : loss: 0.020161, loss_a: 0.011859
[23:24:00.032] iteration 8231 : loss: 0.022380, loss_a: 0.013165
[23:24:00.772] iteration 8232 : loss: 0.023901, loss_a: 0.014059
[23:24:02.147] iteration 8233 : loss: 0.035036, loss_a: 0.020609
[23:24:02.888] iteration 8234 : loss: 0.019003, loss_a: 0.011178
[23:24:04.224] iteration 8235 : loss: 0.028531, loss_a: 0.016783
[23:24:04.956] iteration 8236 : loss: 0.018856, loss_a: 0.011092
[23:24:06.306] iteration 8237 : loss: 0.024349, loss_a: 0.014323
[23:24:07.054] iteration 8238 : loss: 0.014483, loss_a: 0.008520
[23:24:08.407] iteration 8239 : loss: 0.015021, loss_a: 0.008836
[23:24:09.140] iteration 8240 : loss: 0.021427, loss_a: 0.012604
[23:24:10.495] iteration 8241 : loss: 0.017530, loss_a: 0.010312
[23:24:11.242] iteration 8242 : loss: 0.028713, loss_a: 0.016890
[23:24:12.598] iteration 8243 : loss: 0.056897, loss_a: 0.033469
[23:24:13.336] iteration 8244 : loss: 0.027313, loss_a: 0.016066
[23:24:14.647] iteration 8245 : loss: 0.028863, loss_a: 0.016978
[23:24:15.388] iteration 8246 : loss: 0.024139, loss_a: 0.014200
[23:24:16.739] iteration 8247 : loss: 0.021844, loss_a: 0.012849
[23:24:17.478] iteration 8248 : loss: 0.014971, loss_a: 0.008806
[23:24:18.803] iteration 8249 : loss: 0.068597, loss_a: 0.040351
[23:24:19.550] iteration 8250 : loss: 0.024173, loss_a: 0.014219
[23:24:20.885] iteration 8251 : loss: 0.038641, loss_a: 0.022730
[23:24:21.620] iteration 8252 : loss: 0.024701, loss_a: 0.014530
[23:24:22.956] iteration 8253 : loss: 0.021599, loss_a: 0.012705
[23:24:23.695] iteration 8254 : loss: 0.021881, loss_a: 0.012871
[23:24:25.009] iteration 8255 : loss: 0.026208, loss_a: 0.015416
[23:24:25.744] iteration 8256 : loss: 0.021383, loss_a: 0.012578
[23:24:27.096] iteration 8257 : loss: 0.018108, loss_a: 0.010652
[23:24:27.846] iteration 8258 : loss: 0.028190, loss_a: 0.016582
[23:24:29.209] iteration 8259 : loss: 0.086520, loss_a: 0.050894
[23:24:29.947] iteration 8260 : loss: 0.021122, loss_a: 0.012425
[23:24:31.295] iteration 8261 : loss: 0.015747, loss_a: 0.009263
[23:24:32.050] iteration 8262 : loss: 0.022756, loss_a: 0.013386
[23:24:33.397] iteration 8263 : loss: 0.041066, loss_a: 0.024156
[23:24:34.135] iteration 8264 : loss: 0.020780, loss_a: 0.012224
[23:24:35.468] iteration 8265 : loss: 0.029715, loss_a: 0.017479
[23:24:36.207] iteration 8266 : loss: 0.028862, loss_a: 0.016978
[23:24:37.560] iteration 8267 : loss: 0.038540, loss_a: 0.022670
[23:24:38.300] iteration 8268 : loss: 0.024117, loss_a: 0.014186
[23:24:39.659] iteration 8269 : loss: 0.153707, loss_a: 0.090416
[23:24:40.390] iteration 8270 : loss: 0.027822, loss_a: 0.016366
[23:24:41.720] iteration 8271 : loss: 0.042598, loss_a: 0.025058
[23:24:42.458] iteration 8272 : loss: 0.025988, loss_a: 0.015287
[23:24:43.779] iteration 8273 : loss: 0.018789, loss_a: 0.011052
[23:24:44.517] iteration 8274 : loss: 0.054497, loss_a: 0.032057
[23:24:45.872] iteration 8275 : loss: 0.012922, loss_a: 0.007601
[23:24:46.601] iteration 8276 : loss: 0.029064, loss_a: 0.017097
[23:24:47.952] iteration 8277 : loss: 0.027622, loss_a: 0.016248
[23:24:48.688] iteration 8278 : loss: 0.016231, loss_a: 0.009547
[23:24:50.050] iteration 8279 : loss: 0.024851, loss_a: 0.014618
[23:24:50.796] iteration 8280 : loss: 0.028357, loss_a: 0.016680
[23:24:52.147] iteration 8281 : loss: 0.037303, loss_a: 0.021943
[23:24:52.891] iteration 8282 : loss: 0.029133, loss_a: 0.017137
[23:24:54.243] iteration 8283 : loss: 0.046700, loss_a: 0.027470
[23:24:54.994] iteration 8284 : loss: 0.029151, loss_a: 0.017148
[23:24:56.349] iteration 8285 : loss: 0.026230, loss_a: 0.015430
[23:24:57.083] iteration 8286 : loss: 0.016761, loss_a: 0.009859
[23:24:58.445] iteration 8287 : loss: 0.027646, loss_a: 0.016263
[23:24:59.183] iteration 8288 : loss: 0.033349, loss_a: 0.019617
[23:25:00.535] iteration 8289 : loss: 0.035641, loss_a: 0.020965
[23:25:01.271] iteration 8290 : loss: 0.039463, loss_a: 0.023213
[23:25:02.627] iteration 8291 : loss: 0.029438, loss_a: 0.017316
[23:25:03.367] iteration 8292 : loss: 0.031240, loss_a: 0.018376
[23:25:04.720] iteration 8293 : loss: 0.047652, loss_a: 0.028031
[23:25:05.458] iteration 8294 : loss: 0.026988, loss_a: 0.015875
[23:25:06.777] iteration 8295 : loss: 0.022321, loss_a: 0.013130
[23:25:07.513] iteration 8296 : loss: 0.017337, loss_a: 0.010198
[23:25:08.840] iteration 8297 : loss: 0.023264, loss_a: 0.013685
[23:25:09.580] iteration 8298 : loss: 0.026901, loss_a: 0.015824
[23:25:10.935] iteration 8299 : loss: 0.030400, loss_a: 0.017882
[23:25:11.674] iteration 8300 : loss: 0.018882, loss_a: 0.011107
[23:25:13.044] iteration 8301 : loss: 0.056719, loss_a: 0.033364
[23:25:13.788] iteration 8302 : loss: 0.023963, loss_a: 0.014096
[23:25:15.124] iteration 8303 : loss: 0.018221, loss_a: 0.010718
[23:25:15.863] iteration 8304 : loss: 0.015293, loss_a: 0.008996
[23:25:17.198] iteration 8305 : loss: 0.019817, loss_a: 0.011657
[23:25:17.937] iteration 8306 : loss: 0.027778, loss_a: 0.016340
[23:25:19.289] iteration 8307 : loss: 0.025329, loss_a: 0.014899
[23:25:20.034] iteration 8308 : loss: 0.018837, loss_a: 0.011081
[23:25:21.390] iteration 8309 : loss: 0.051890, loss_a: 0.030524
[23:25:22.138] iteration 8310 : loss: 0.026777, loss_a: 0.015751
[23:25:23.503] iteration 8311 : loss: 0.049704, loss_a: 0.029237
[23:25:24.246] iteration 8312 : loss: 0.057785, loss_a: 0.033991
[23:25:25.637] iteration 8313 : loss: 0.034673, loss_a: 0.020396
[23:25:26.375] iteration 8314 : loss: 0.023098, loss_a: 0.013587
[23:25:27.739] iteration 8315 : loss: 0.027515, loss_a: 0.016185
[23:25:28.489] iteration 8316 : loss: 0.037050, loss_a: 0.021794
[23:25:29.844] iteration 8317 : loss: 0.024849, loss_a: 0.014617
[23:25:30.589] iteration 8318 : loss: 0.027473, loss_a: 0.016161
[23:25:31.951] iteration 8319 : loss: 0.058888, loss_a: 0.034640
[23:25:32.690] iteration 8320 : loss: 0.024954, loss_a: 0.014679
[23:25:34.012] iteration 8321 : loss: 0.036600, loss_a: 0.021529
[23:25:34.767] iteration 8322 : loss: 0.028354, loss_a: 0.016679
[23:25:36.155] iteration 8323 : loss: 0.080048, loss_a: 0.047087
[23:25:36.906] iteration 8324 : loss: 0.021147, loss_a: 0.012439
[23:25:38.256] iteration 8325 : loss: 0.041589, loss_a: 0.024464
[23:25:39.008] iteration 8326 : loss: 0.037143, loss_a: 0.021849
[23:25:40.353] iteration 8327 : loss: 0.033776, loss_a: 0.019868
[23:25:41.087] iteration 8328 : loss: 0.052263, loss_a: 0.030743
[23:25:42.429] iteration 8329 : loss: 0.034872, loss_a: 0.020513
[23:25:43.173] iteration 8330 : loss: 0.021388, loss_a: 0.012581
[23:25:44.518] iteration 8331 : loss: 0.015370, loss_a: 0.009041
[23:25:45.261] iteration 8332 : loss: 0.031418, loss_a: 0.018481
[23:25:46.602] iteration 8333 : loss: 0.027704, loss_a: 0.016296
[23:25:47.348] iteration 8334 : loss: 0.026069, loss_a: 0.015335
[23:25:48.692] iteration 8335 : loss: 0.072275, loss_a: 0.042514
[23:25:49.428] iteration 8336 : loss: 0.039571, loss_a: 0.023277
[23:25:50.821] iteration 8337 : loss: 0.019948, loss_a: 0.011734
[23:25:51.547] iteration 8338 : loss: 0.013211, loss_a: 0.007771
[23:25:52.898] iteration 8339 : loss: 0.022206, loss_a: 0.013063
[23:25:53.643] iteration 8340 : loss: 0.029523, loss_a: 0.017366
[23:25:55.012] iteration 8341 : loss: 0.053978, loss_a: 0.031752
[23:25:55.750] iteration 8342 : loss: 0.048074, loss_a: 0.028279
[23:25:57.088] iteration 8343 : loss: 0.018597, loss_a: 0.010940
[23:25:57.829] iteration 8344 : loss: 0.043919, loss_a: 0.025835
[23:25:59.174] iteration 8345 : loss: 0.017972, loss_a: 0.010572
[23:25:59.913] iteration 8346 : loss: 0.031441, loss_a: 0.018495
[23:26:01.271] iteration 8347 : loss: 0.030974, loss_a: 0.018220
[23:26:02.018] iteration 8348 : loss: 0.016565, loss_a: 0.009744
[23:26:03.362] iteration 8349 : loss: 0.021388, loss_a: 0.012581
[23:26:04.098] iteration 8350 : loss: 0.019194, loss_a: 0.011291
[23:26:05.422] iteration 8351 : loss: 0.026401, loss_a: 0.015530
[23:26:06.171] iteration 8352 : loss: 0.026893, loss_a: 0.015819
[23:26:07.493] iteration 8353 : loss: 0.018240, loss_a: 0.010730
[23:26:08.230] iteration 8354 : loss: 0.037399, loss_a: 0.021999
[23:26:09.568] iteration 8355 : loss: 0.045679, loss_a: 0.026870
[23:26:10.310] iteration 8356 : loss: 0.018694, loss_a: 0.010996
[23:26:11.644] iteration 8357 : loss: 0.025718, loss_a: 0.015128
[23:26:12.388] iteration 8358 : loss: 0.041132, loss_a: 0.024195
[23:26:13.733] iteration 8359 : loss: 0.036294, loss_a: 0.021349
[23:26:14.472] iteration 8360 : loss: 0.023022, loss_a: 0.013543
[23:26:15.833] iteration 8361 : loss: 0.040855, loss_a: 0.024032
[23:26:16.569] iteration 8362 : loss: 0.040853, loss_a: 0.024031
[23:26:17.938] iteration 8363 : loss: 0.036800, loss_a: 0.021647
[23:26:18.674] iteration 8364 : loss: 0.033071, loss_a: 0.019453
[23:26:20.019] iteration 8365 : loss: 0.015373, loss_a: 0.009043
[23:26:20.773] iteration 8366 : loss: 0.037338, loss_a: 0.021964
[23:26:22.097] iteration 8367 : loss: 0.019395, loss_a: 0.011409
[23:26:22.830] iteration 8368 : loss: 0.020774, loss_a: 0.012220
[23:26:24.181] iteration 8369 : loss: 0.031583, loss_a: 0.018578
[23:26:24.931] iteration 8370 : loss: 0.027880, loss_a: 0.016400
[23:26:26.276] iteration 8371 : loss: 0.020101, loss_a: 0.011824
[23:26:27.020] iteration 8372 : loss: 0.032032, loss_a: 0.018843
[23:26:28.347] iteration 8373 : loss: 0.047771, loss_a: 0.028101
[23:26:29.087] iteration 8374 : loss: 0.034918, loss_a: 0.020540
[23:26:30.453] iteration 8375 : loss: 0.044397, loss_a: 0.026116
[23:26:31.197] iteration 8376 : loss: 0.022744, loss_a: 0.013379
[23:26:32.548] iteration 8377 : loss: 0.034804, loss_a: 0.020473
[23:26:33.284] iteration 8378 : loss: 0.019430, loss_a: 0.011429
[23:26:34.636] iteration 8379 : loss: 0.024217, loss_a: 0.014245
[23:26:35.371] iteration 8380 : loss: 0.025982, loss_a: 0.015284
[23:26:36.698] iteration 8381 : loss: 0.297635, loss_a: 0.175079
[23:26:37.428] iteration 8382 : loss: 0.026216, loss_a: 0.015421
[23:26:38.783] iteration 8383 : loss: 0.036864, loss_a: 0.021685
[23:26:39.524] iteration 8384 : loss: 0.059278, loss_a: 0.034869
[23:26:40.873] iteration 8385 : loss: 0.025828, loss_a: 0.015193
[23:26:41.616] iteration 8386 : loss: 0.038939, loss_a: 0.022905
[23:26:42.956] iteration 8387 : loss: 0.018826, loss_a: 0.011074
[23:26:43.698] iteration 8388 : loss: 0.021934, loss_a: 0.012902
[23:26:45.035] iteration 8389 : loss: 0.055494, loss_a: 0.032644
[23:26:45.777] iteration 8390 : loss: 0.032649, loss_a: 0.019205
[23:26:47.116] iteration 8391 : loss: 0.033573, loss_a: 0.019749
[23:26:47.857] iteration 8392 : loss: 0.022354, loss_a: 0.013149
[23:26:49.181] iteration 8393 : loss: 0.040301, loss_a: 0.023706
[23:26:49.925] iteration 8394 : loss: 0.036930, loss_a: 0.021723
[23:26:51.262] iteration 8395 : loss: 0.029807, loss_a: 0.017533
[23:26:52.010] iteration 8396 : loss: 0.020569, loss_a: 0.012100
[23:26:53.381] iteration 8397 : loss: 0.047444, loss_a: 0.027908
[23:26:54.133] iteration 8398 : loss: 0.027729, loss_a: 0.016311
[23:26:55.496] iteration 8399 : loss: 0.051546, loss_a: 0.030321
[23:26:56.232] iteration 8400 : loss: 0.021727, loss_a: 0.012781
[23:27:20.845] iteration 8401 : loss: 0.024422, loss_a: 0.014366
[23:27:22.995] iteration 8402 : loss: 0.019154, loss_a: 0.011267
[23:27:24.311] iteration 8403 : loss: 0.021914, loss_a: 0.012890
[23:27:25.051] iteration 8404 : loss: 0.020139, loss_a: 0.011846
[23:27:26.359] iteration 8405 : loss: 0.025260, loss_a: 0.014859
[23:27:27.100] iteration 8406 : loss: 0.037167, loss_a: 0.021863
[23:27:28.443] iteration 8407 : loss: 0.031211, loss_a: 0.018360
[23:27:29.185] iteration 8408 : loss: 0.024765, loss_a: 0.014568
[23:27:30.517] iteration 8409 : loss: 0.020493, loss_a: 0.012055
[23:27:31.251] iteration 8410 : loss: 0.039092, loss_a: 0.022995
[23:27:32.613] iteration 8411 : loss: 0.030372, loss_a: 0.017866
[23:27:33.338] iteration 8412 : loss: 0.019960, loss_a: 0.011741
[23:27:34.679] iteration 8413 : loss: 0.046833, loss_a: 0.027549
[23:27:35.418] iteration 8414 : loss: 0.033666, loss_a: 0.019803
[23:27:36.779] iteration 8415 : loss: 0.021353, loss_a: 0.012561
[23:27:37.521] iteration 8416 : loss: 0.018369, loss_a: 0.010806
[23:27:38.842] iteration 8417 : loss: 0.039842, loss_a: 0.023436
[23:27:39.594] iteration 8418 : loss: 0.045599, loss_a: 0.026823
[23:27:40.939] iteration 8419 : loss: 0.045982, loss_a: 0.027048
[23:27:41.669] iteration 8420 : loss: 0.018185, loss_a: 0.010697
[23:27:42.997] iteration 8421 : loss: 0.047043, loss_a: 0.027672
[23:27:43.741] iteration 8422 : loss: 0.029045, loss_a: 0.017085
[23:27:45.067] iteration 8423 : loss: 0.057071, loss_a: 0.033571
[23:27:45.818] iteration 8424 : loss: 0.035455, loss_a: 0.020856
[23:27:47.171] iteration 8425 : loss: 0.026959, loss_a: 0.015858
[23:27:47.909] iteration 8426 : loss: 0.023259, loss_a: 0.013682
[23:27:49.223] iteration 8427 : loss: 0.047416, loss_a: 0.027892
[23:27:49.962] iteration 8428 : loss: 0.030235, loss_a: 0.017785
[23:27:51.275] iteration 8429 : loss: 0.015626, loss_a: 0.009192
[23:27:52.023] iteration 8430 : loss: 0.023913, loss_a: 0.014066
[23:27:53.377] iteration 8431 : loss: 0.029869, loss_a: 0.017570
[23:27:54.126] iteration 8432 : loss: 0.027000, loss_a: 0.015883
[23:27:55.431] iteration 8433 : loss: 0.032958, loss_a: 0.019387
[23:27:56.183] iteration 8434 : loss: 0.042495, loss_a: 0.024997
[23:27:57.549] iteration 8435 : loss: 0.070315, loss_a: 0.041362
[23:27:58.284] iteration 8436 : loss: 0.018749, loss_a: 0.011029
[23:27:59.648] iteration 8437 : loss: 0.027116, loss_a: 0.015950
[23:28:00.400] iteration 8438 : loss: 0.024917, loss_a: 0.014657
[23:28:01.718] iteration 8439 : loss: 0.026014, loss_a: 0.015302
[23:28:02.465] iteration 8440 : loss: 0.022488, loss_a: 0.013228
[23:28:03.787] iteration 8441 : loss: 0.023460, loss_a: 0.013800
[23:28:04.532] iteration 8442 : loss: 0.017661, loss_a: 0.010389
[23:28:05.855] iteration 8443 : loss: 0.010431, loss_a: 0.006136
[23:28:06.605] iteration 8444 : loss: 0.035631, loss_a: 0.020959
[23:28:07.968] iteration 8445 : loss: 0.017227, loss_a: 0.010134
[23:28:08.704] iteration 8446 : loss: 0.010089, loss_a: 0.005935
[23:28:10.025] iteration 8447 : loss: 0.015946, loss_a: 0.009380
[23:28:10.782] iteration 8448 : loss: 0.037822, loss_a: 0.022248
[23:28:12.114] iteration 8449 : loss: 0.023752, loss_a: 0.013972
[23:28:12.856] iteration 8450 : loss: 0.016401, loss_a: 0.009648
[23:28:14.186] iteration 8451 : loss: 0.021969, loss_a: 0.012923
[23:28:14.925] iteration 8452 : loss: 0.057232, loss_a: 0.033666
[23:28:16.223] iteration 8453 : loss: 0.022150, loss_a: 0.013030
[23:28:16.973] iteration 8454 : loss: 0.023745, loss_a: 0.013968
[23:28:18.296] iteration 8455 : loss: 0.055444, loss_a: 0.032614
[23:28:19.028] iteration 8456 : loss: 0.016280, loss_a: 0.009576
[23:28:20.384] iteration 8457 : loss: 0.029327, loss_a: 0.017251
[23:28:21.129] iteration 8458 : loss: 0.095009, loss_a: 0.055888
[23:28:22.490] iteration 8459 : loss: 0.025204, loss_a: 0.014826
[23:28:23.239] iteration 8460 : loss: 0.033152, loss_a: 0.019501
[23:28:24.604] iteration 8461 : loss: 0.063439, loss_a: 0.037317
[23:28:25.339] iteration 8462 : loss: 0.015395, loss_a: 0.009056
[23:28:26.681] iteration 8463 : loss: 0.020555, loss_a: 0.012091
[23:28:27.432] iteration 8464 : loss: 0.054260, loss_a: 0.031918
[23:28:28.749] iteration 8465 : loss: 0.041396, loss_a: 0.024351
[23:28:29.500] iteration 8466 : loss: 0.029169, loss_a: 0.017158
[23:28:30.851] iteration 8467 : loss: 0.034095, loss_a: 0.020056
[23:28:31.595] iteration 8468 : loss: 0.021645, loss_a: 0.012732
[23:28:32.921] iteration 8469 : loss: 0.025207, loss_a: 0.014828
[23:28:33.667] iteration 8470 : loss: 0.032505, loss_a: 0.019120
[23:28:35.001] iteration 8471 : loss: 0.017093, loss_a: 0.010055
[23:28:35.738] iteration 8472 : loss: 0.033693, loss_a: 0.019820
[23:28:37.091] iteration 8473 : loss: 0.031334, loss_a: 0.018432
[23:28:37.836] iteration 8474 : loss: 0.026533, loss_a: 0.015607
[23:28:39.191] iteration 8475 : loss: 0.015784, loss_a: 0.009285
[23:28:39.923] iteration 8476 : loss: 0.031615, loss_a: 0.018597
[23:28:41.243] iteration 8477 : loss: 0.023047, loss_a: 0.013557
[23:28:41.986] iteration 8478 : loss: 0.044863, loss_a: 0.026390
[23:28:43.335] iteration 8479 : loss: 0.029921, loss_a: 0.017601
[23:28:44.069] iteration 8480 : loss: 0.019353, loss_a: 0.011384
[23:28:45.430] iteration 8481 : loss: 0.041771, loss_a: 0.024571
[23:28:46.176] iteration 8482 : loss: 0.020739, loss_a: 0.012200
[23:28:47.493] iteration 8483 : loss: 0.023409, loss_a: 0.013770
[23:28:48.235] iteration 8484 : loss: 0.059362, loss_a: 0.034919
[23:28:49.587] iteration 8485 : loss: 0.030766, loss_a: 0.018098
[23:28:50.320] iteration 8486 : loss: 0.041565, loss_a: 0.024450
[23:28:51.641] iteration 8487 : loss: 0.018414, loss_a: 0.010832
[23:28:52.382] iteration 8488 : loss: 0.029002, loss_a: 0.017060
[23:28:53.703] iteration 8489 : loss: 0.016607, loss_a: 0.009769
[23:28:54.450] iteration 8490 : loss: 0.042439, loss_a: 0.024964
[23:28:55.813] iteration 8491 : loss: 0.030445, loss_a: 0.017909
[23:28:56.555] iteration 8492 : loss: 0.021566, loss_a: 0.012686
[23:28:57.896] iteration 8493 : loss: 0.025231, loss_a: 0.014842
[23:28:58.644] iteration 8494 : loss: 0.033628, loss_a: 0.019781
[23:28:59.979] iteration 8495 : loss: 0.009969, loss_a: 0.005864
[23:29:00.718] iteration 8496 : loss: 0.015964, loss_a: 0.009390
[23:29:02.045] iteration 8497 : loss: 0.029047, loss_a: 0.017087
[23:29:02.792] iteration 8498 : loss: 0.032678, loss_a: 0.019222
[23:29:04.114] iteration 8499 : loss: 0.022322, loss_a: 0.013130
[23:29:04.849] iteration 8500 : loss: 0.019250, loss_a: 0.011324
[23:29:06.201] iteration 8501 : loss: 0.032430, loss_a: 0.019077
[23:29:06.947] iteration 8502 : loss: 0.022238, loss_a: 0.013081
[23:29:08.291] iteration 8503 : loss: 0.044094, loss_a: 0.025938
[23:29:09.028] iteration 8504 : loss: 0.029303, loss_a: 0.017237
[23:29:10.376] iteration 8505 : loss: 0.019317, loss_a: 0.011363
[23:29:11.111] iteration 8506 : loss: 0.014031, loss_a: 0.008254
[23:29:12.426] iteration 8507 : loss: 0.016381, loss_a: 0.009636
[23:29:13.171] iteration 8508 : loss: 0.039545, loss_a: 0.023262
[23:29:14.541] iteration 8509 : loss: 0.025141, loss_a: 0.014789
[23:29:15.284] iteration 8510 : loss: 0.038184, loss_a: 0.022461
[23:29:16.590] iteration 8511 : loss: 0.020216, loss_a: 0.011892
[23:29:17.336] iteration 8512 : loss: 0.030769, loss_a: 0.018099
[23:29:18.671] iteration 8513 : loss: 0.017672, loss_a: 0.010395
[23:29:19.403] iteration 8514 : loss: 0.050124, loss_a: 0.029484
[23:29:20.759] iteration 8515 : loss: 0.044907, loss_a: 0.026416
[23:29:21.496] iteration 8516 : loss: 0.017271, loss_a: 0.010159
[23:29:22.828] iteration 8517 : loss: 0.053285, loss_a: 0.031344
[23:29:23.581] iteration 8518 : loss: 0.069372, loss_a: 0.040807
[23:29:24.917] iteration 8519 : loss: 0.023647, loss_a: 0.013910
[23:29:25.662] iteration 8520 : loss: 0.058410, loss_a: 0.034359
[23:29:26.998] iteration 8521 : loss: 0.027646, loss_a: 0.016262
[23:29:27.748] iteration 8522 : loss: 0.066087, loss_a: 0.038875
[23:29:29.057] iteration 8523 : loss: 0.038502, loss_a: 0.022648
[23:29:29.803] iteration 8524 : loss: 0.022496, loss_a: 0.013233
[23:29:31.149] iteration 8525 : loss: 0.015599, loss_a: 0.009176
[23:29:31.889] iteration 8526 : loss: 0.022504, loss_a: 0.013238
[23:29:33.211] iteration 8527 : loss: 0.015009, loss_a: 0.008829
[23:29:33.950] iteration 8528 : loss: 0.057697, loss_a: 0.033939
[23:29:35.270] iteration 8529 : loss: 0.016285, loss_a: 0.009579
[23:29:36.007] iteration 8530 : loss: 0.019219, loss_a: 0.011305
[23:29:37.366] iteration 8531 : loss: 0.036233, loss_a: 0.021314
[23:29:38.098] iteration 8532 : loss: 0.013260, loss_a: 0.007800
[23:29:39.455] iteration 8533 : loss: 0.025192, loss_a: 0.014819
[23:29:40.193] iteration 8534 : loss: 0.070996, loss_a: 0.041762
[23:29:41.497] iteration 8535 : loss: 0.022708, loss_a: 0.013357
[23:29:42.243] iteration 8536 : loss: 0.020255, loss_a: 0.011915
[23:29:43.572] iteration 8537 : loss: 0.026293, loss_a: 0.015467
[23:29:44.314] iteration 8538 : loss: 0.041223, loss_a: 0.024249
[23:29:45.653] iteration 8539 : loss: 0.015976, loss_a: 0.009397
[23:29:46.388] iteration 8540 : loss: 0.021750, loss_a: 0.012794
[23:29:47.768] iteration 8541 : loss: 0.028773, loss_a: 0.016925
[23:29:48.504] iteration 8542 : loss: 0.025153, loss_a: 0.014796
[23:29:49.851] iteration 8543 : loss: 0.031757, loss_a: 0.018681
[23:29:50.586] iteration 8544 : loss: 0.017671, loss_a: 0.010395
[23:29:51.934] iteration 8545 : loss: 0.016614, loss_a: 0.009773
[23:29:52.665] iteration 8546 : loss: 0.027820, loss_a: 0.016364
[23:29:53.994] iteration 8547 : loss: 0.028953, loss_a: 0.017031
[23:29:54.737] iteration 8548 : loss: 0.026946, loss_a: 0.015851
[23:29:56.062] iteration 8549 : loss: 0.026399, loss_a: 0.015529
[23:29:56.804] iteration 8550 : loss: 0.077024, loss_a: 0.045308
[23:29:58.161] iteration 8551 : loss: 0.018131, loss_a: 0.010665
[23:29:58.909] iteration 8552 : loss: 0.016958, loss_a: 0.009975
[23:30:00.237] iteration 8553 : loss: 0.022340, loss_a: 0.013141
[23:30:00.973] iteration 8554 : loss: 0.020174, loss_a: 0.011867
[23:30:02.291] iteration 8555 : loss: 0.023409, loss_a: 0.013770
[23:30:03.031] iteration 8556 : loss: 0.020504, loss_a: 0.012061
[23:30:04.370] iteration 8557 : loss: 0.022096, loss_a: 0.012997
[23:30:05.105] iteration 8558 : loss: 0.030755, loss_a: 0.018091
[23:30:06.446] iteration 8559 : loss: 0.033875, loss_a: 0.019927
[23:30:07.186] iteration 8560 : loss: 0.022263, loss_a: 0.013096
[23:30:08.534] iteration 8561 : loss: 0.027422, loss_a: 0.016130
[23:30:09.270] iteration 8562 : loss: 0.022987, loss_a: 0.013522
[23:30:10.595] iteration 8563 : loss: 0.038984, loss_a: 0.022932
[23:30:11.343] iteration 8564 : loss: 0.013908, loss_a: 0.008181
[23:30:12.656] iteration 8565 : loss: 0.021796, loss_a: 0.012821
[23:30:13.390] iteration 8566 : loss: 0.022535, loss_a: 0.013256
[23:30:14.717] iteration 8567 : loss: 0.026326, loss_a: 0.015486
[23:30:15.456] iteration 8568 : loss: 0.022456, loss_a: 0.013209
[23:30:16.807] iteration 8569 : loss: 0.015380, loss_a: 0.009047
[23:30:17.547] iteration 8570 : loss: 0.018685, loss_a: 0.010991
[23:30:18.877] iteration 8571 : loss: 0.023834, loss_a: 0.014020
[23:30:19.615] iteration 8572 : loss: 0.022185, loss_a: 0.013050
[23:30:20.945] iteration 8573 : loss: 0.026381, loss_a: 0.015518
[23:30:21.689] iteration 8574 : loss: 0.031972, loss_a: 0.018807
[23:30:23.029] iteration 8575 : loss: 0.055062, loss_a: 0.032390
[23:30:23.776] iteration 8576 : loss: 0.030259, loss_a: 0.017799
[23:30:25.118] iteration 8577 : loss: 0.037081, loss_a: 0.021812
[23:30:25.866] iteration 8578 : loss: 0.022386, loss_a: 0.013168
[23:30:27.208] iteration 8579 : loss: 0.014900, loss_a: 0.008765
[23:30:27.963] iteration 8580 : loss: 0.037370, loss_a: 0.021982
[23:30:29.276] iteration 8581 : loss: 0.033684, loss_a: 0.019814
[23:30:30.021] iteration 8582 : loss: 0.035587, loss_a: 0.020934
[23:30:31.365] iteration 8583 : loss: 0.022328, loss_a: 0.013134
[23:30:32.096] iteration 8584 : loss: 0.019613, loss_a: 0.011537
[23:30:33.458] iteration 8585 : loss: 0.025418, loss_a: 0.014952
[23:30:34.204] iteration 8586 : loss: 0.022746, loss_a: 0.013380
[23:30:35.589] iteration 8587 : loss: 0.037559, loss_a: 0.022094
[23:30:36.341] iteration 8588 : loss: 0.041308, loss_a: 0.024299
[23:30:37.648] iteration 8589 : loss: 0.022894, loss_a: 0.013467
[23:30:38.390] iteration 8590 : loss: 0.042631, loss_a: 0.025077
[23:30:39.736] iteration 8591 : loss: 0.016129, loss_a: 0.009488
[23:30:40.479] iteration 8592 : loss: 0.027726, loss_a: 0.016310
[23:30:41.822] iteration 8593 : loss: 0.031346, loss_a: 0.018439
[23:30:42.561] iteration 8594 : loss: 0.064945, loss_a: 0.038203
[23:30:43.925] iteration 8595 : loss: 0.038387, loss_a: 0.022580
[23:30:44.666] iteration 8596 : loss: 0.018417, loss_a: 0.010834
[23:30:46.026] iteration 8597 : loss: 0.049205, loss_a: 0.028944
[23:30:46.768] iteration 8598 : loss: 0.030094, loss_a: 0.017702
[23:30:48.090] iteration 8599 : loss: 0.032732, loss_a: 0.019254
[23:30:48.837] iteration 8600 : loss: 0.033753, loss_a: 0.019855
[23:31:13.467] iteration 8601 : loss: 0.026366, loss_a: 0.015509
[23:31:15.602] iteration 8602 : loss: 0.052252, loss_a: 0.030736
[23:31:16.938] iteration 8603 : loss: 0.035402, loss_a: 0.020825
[23:31:17.689] iteration 8604 : loss: 0.044029, loss_a: 0.025900
[23:31:19.027] iteration 8605 : loss: 0.019363, loss_a: 0.011390
[23:31:19.765] iteration 8606 : loss: 0.021888, loss_a: 0.012875
[23:31:21.088] iteration 8607 : loss: 0.054734, loss_a: 0.032196
[23:31:21.834] iteration 8608 : loss: 0.032843, loss_a: 0.019320
[23:31:23.176] iteration 8609 : loss: 0.018309, loss_a: 0.010770
[23:31:23.933] iteration 8610 : loss: 0.048166, loss_a: 0.028333
[23:31:25.252] iteration 8611 : loss: 0.031228, loss_a: 0.018369
[23:31:25.997] iteration 8612 : loss: 0.020894, loss_a: 0.012291
[23:31:27.324] iteration 8613 : loss: 0.028414, loss_a: 0.016714
[23:31:28.065] iteration 8614 : loss: 0.041896, loss_a: 0.024645
[23:31:29.411] iteration 8615 : loss: 0.046565, loss_a: 0.027391
[23:31:30.153] iteration 8616 : loss: 0.039176, loss_a: 0.023044
[23:31:31.490] iteration 8617 : loss: 0.030282, loss_a: 0.017813
[23:31:32.243] iteration 8618 : loss: 0.033186, loss_a: 0.019521
[23:31:33.592] iteration 8619 : loss: 0.067653, loss_a: 0.039796
[23:31:34.338] iteration 8620 : loss: 0.029426, loss_a: 0.017309
[23:31:35.653] iteration 8621 : loss: 0.027114, loss_a: 0.015949
[23:31:36.391] iteration 8622 : loss: 0.018993, loss_a: 0.011172
[23:31:37.743] iteration 8623 : loss: 0.024739, loss_a: 0.014552
[23:31:38.482] iteration 8624 : loss: 0.046852, loss_a: 0.027560
[23:31:39.795] iteration 8625 : loss: 0.028708, loss_a: 0.016887
[23:31:40.538] iteration 8626 : loss: 0.034561, loss_a: 0.020330
[23:31:41.860] iteration 8627 : loss: 0.047025, loss_a: 0.027662
[23:31:42.596] iteration 8628 : loss: 0.017762, loss_a: 0.010448
[23:31:43.938] iteration 8629 : loss: 0.020154, loss_a: 0.011855
[23:31:44.675] iteration 8630 : loss: 0.061146, loss_a: 0.035968
[23:31:46.003] iteration 8631 : loss: 0.031408, loss_a: 0.018475
[23:31:46.750] iteration 8632 : loss: 0.021821, loss_a: 0.012836
[23:31:48.058] iteration 8633 : loss: 0.023845, loss_a: 0.014027
[23:31:48.801] iteration 8634 : loss: 0.024787, loss_a: 0.014580
[23:31:50.108] iteration 8635 : loss: 0.051737, loss_a: 0.030433
[23:31:50.857] iteration 8636 : loss: 0.059377, loss_a: 0.034927
[23:31:52.190] iteration 8637 : loss: 0.024493, loss_a: 0.014408
[23:31:52.925] iteration 8638 : loss: 0.032787, loss_a: 0.019286
[23:31:54.255] iteration 8639 : loss: 0.042303, loss_a: 0.024884
[23:31:54.983] iteration 8640 : loss: 0.014076, loss_a: 0.008280
[23:31:56.340] iteration 8641 : loss: 0.022957, loss_a: 0.013504
[23:31:57.081] iteration 8642 : loss: 0.023707, loss_a: 0.013945
[23:31:58.431] iteration 8643 : loss: 0.035091, loss_a: 0.020642
[23:31:59.168] iteration 8644 : loss: 0.026661, loss_a: 0.015683
[23:32:00.496] iteration 8645 : loss: 0.023866, loss_a: 0.014039
[23:32:01.245] iteration 8646 : loss: 0.024322, loss_a: 0.014307
[23:32:02.604] iteration 8647 : loss: 0.037362, loss_a: 0.021978
[23:32:03.359] iteration 8648 : loss: 0.022666, loss_a: 0.013333
[23:32:04.686] iteration 8649 : loss: 0.016073, loss_a: 0.009455
[23:32:05.438] iteration 8650 : loss: 0.026458, loss_a: 0.015564
[23:32:06.802] iteration 8651 : loss: 0.025924, loss_a: 0.015249
[23:32:07.553] iteration 8652 : loss: 0.029523, loss_a: 0.017367
[23:32:08.928] iteration 8653 : loss: 0.031263, loss_a: 0.018390
[23:32:09.673] iteration 8654 : loss: 0.046689, loss_a: 0.027464
[23:32:11.013] iteration 8655 : loss: 0.040806, loss_a: 0.024003
[23:32:11.753] iteration 8656 : loss: 0.027612, loss_a: 0.016242
[23:32:13.083] iteration 8657 : loss: 0.019459, loss_a: 0.011447
[23:32:13.821] iteration 8658 : loss: 0.039537, loss_a: 0.023257
[23:32:15.134] iteration 8659 : loss: 0.024908, loss_a: 0.014652
[23:32:15.883] iteration 8660 : loss: 0.037990, loss_a: 0.022347
[23:32:17.204] iteration 8661 : loss: 0.031627, loss_a: 0.018604
[23:32:17.956] iteration 8662 : loss: 0.019754, loss_a: 0.011620
[23:32:19.308] iteration 8663 : loss: 0.034416, loss_a: 0.020245
[23:32:20.057] iteration 8664 : loss: 0.063185, loss_a: 0.037168
[23:32:21.360] iteration 8665 : loss: 0.025704, loss_a: 0.015120
[23:32:22.115] iteration 8666 : loss: 0.028624, loss_a: 0.016838
[23:32:23.431] iteration 8667 : loss: 0.029258, loss_a: 0.017211
[23:32:24.181] iteration 8668 : loss: 0.038116, loss_a: 0.022421
[23:32:25.510] iteration 8669 : loss: 0.018074, loss_a: 0.010632
[23:32:26.242] iteration 8670 : loss: 0.019185, loss_a: 0.011285
[23:32:27.561] iteration 8671 : loss: 0.016647, loss_a: 0.009792
[23:32:28.307] iteration 8672 : loss: 0.031147, loss_a: 0.018322
[23:32:29.641] iteration 8673 : loss: 0.026552, loss_a: 0.015619
[23:32:30.378] iteration 8674 : loss: 0.018186, loss_a: 0.010697
[23:32:31.701] iteration 8675 : loss: 0.028272, loss_a: 0.016631
[23:32:32.437] iteration 8676 : loss: 0.023899, loss_a: 0.014058
[23:32:33.790] iteration 8677 : loss: 0.036560, loss_a: 0.021506
[23:32:34.529] iteration 8678 : loss: 0.018684, loss_a: 0.010990
[23:32:35.877] iteration 8679 : loss: 0.029961, loss_a: 0.017624
[23:32:36.617] iteration 8680 : loss: 0.017519, loss_a: 0.010305
[23:32:37.970] iteration 8681 : loss: 0.035513, loss_a: 0.020890
[23:32:38.712] iteration 8682 : loss: 0.017264, loss_a: 0.010155
[23:32:40.055] iteration 8683 : loss: 0.043269, loss_a: 0.025452
[23:32:40.809] iteration 8684 : loss: 0.029066, loss_a: 0.017098
[23:32:42.131] iteration 8685 : loss: 0.030017, loss_a: 0.017657
[23:32:42.872] iteration 8686 : loss: 0.017065, loss_a: 0.010038
[23:32:44.199] iteration 8687 : loss: 0.024205, loss_a: 0.014238
[23:32:44.939] iteration 8688 : loss: 0.019879, loss_a: 0.011694
[23:32:46.275] iteration 8689 : loss: 0.013569, loss_a: 0.007982
[23:32:47.019] iteration 8690 : loss: 0.040232, loss_a: 0.023666
[23:32:48.364] iteration 8691 : loss: 0.016149, loss_a: 0.009499
[23:32:49.109] iteration 8692 : loss: 0.016557, loss_a: 0.009740
[23:32:50.456] iteration 8693 : loss: 0.026900, loss_a: 0.015823
[23:32:51.194] iteration 8694 : loss: 0.020782, loss_a: 0.012224
[23:32:52.553] iteration 8695 : loss: 0.017791, loss_a: 0.010465
[23:32:53.297] iteration 8696 : loss: 0.033660, loss_a: 0.019800
[23:32:54.608] iteration 8697 : loss: 0.030314, loss_a: 0.017831
[23:32:55.344] iteration 8698 : loss: 0.020469, loss_a: 0.012040
[23:32:56.664] iteration 8699 : loss: 0.020046, loss_a: 0.011792
[23:32:57.417] iteration 8700 : loss: 0.038115, loss_a: 0.022421
[23:32:58.761] iteration 8701 : loss: 0.029616, loss_a: 0.017421
[23:32:59.504] iteration 8702 : loss: 0.026733, loss_a: 0.015725
[23:33:00.864] iteration 8703 : loss: 0.018299, loss_a: 0.010764
[23:33:01.611] iteration 8704 : loss: 0.027667, loss_a: 0.016275
[23:33:02.939] iteration 8705 : loss: 0.014750, loss_a: 0.008676
[23:33:03.688] iteration 8706 : loss: 0.025472, loss_a: 0.014983
[23:33:05.014] iteration 8707 : loss: 0.019600, loss_a: 0.011529
[23:33:05.743] iteration 8708 : loss: 0.014510, loss_a: 0.008536
[23:33:07.101] iteration 8709 : loss: 0.029847, loss_a: 0.017557
[23:33:07.863] iteration 8710 : loss: 0.047801, loss_a: 0.028118
[23:33:09.213] iteration 8711 : loss: 0.046608, loss_a: 0.027416
[23:33:09.953] iteration 8712 : loss: 0.056754, loss_a: 0.033385
[23:33:11.307] iteration 8713 : loss: 0.031033, loss_a: 0.018255
[23:33:12.052] iteration 8714 : loss: 0.034464, loss_a: 0.020273
[23:33:13.375] iteration 8715 : loss: 0.030750, loss_a: 0.018088
[23:33:14.123] iteration 8716 : loss: 0.045472, loss_a: 0.026748
[23:33:15.473] iteration 8717 : loss: 0.018734, loss_a: 0.011020
[23:33:16.206] iteration 8718 : loss: 0.015261, loss_a: 0.008977
[23:33:17.560] iteration 8719 : loss: 0.040177, loss_a: 0.023633
[23:33:18.296] iteration 8720 : loss: 0.019084, loss_a: 0.011226
[23:33:19.632] iteration 8721 : loss: 0.024898, loss_a: 0.014646
[23:33:20.369] iteration 8722 : loss: 0.030564, loss_a: 0.017979
[23:33:21.689] iteration 8723 : loss: 0.027271, loss_a: 0.016042
[23:33:22.435] iteration 8724 : loss: 0.026749, loss_a: 0.015735
[23:33:23.749] iteration 8725 : loss: 0.031261, loss_a: 0.018389
[23:33:24.494] iteration 8726 : loss: 0.019378, loss_a: 0.011399
[23:33:25.814] iteration 8727 : loss: 0.015358, loss_a: 0.009034
[23:33:26.557] iteration 8728 : loss: 0.043467, loss_a: 0.025569
[23:33:27.904] iteration 8729 : loss: 0.025416, loss_a: 0.014951
[23:33:28.640] iteration 8730 : loss: 0.026295, loss_a: 0.015468
[23:33:29.980] iteration 8731 : loss: 0.015726, loss_a: 0.009251
[23:33:30.729] iteration 8732 : loss: 0.053774, loss_a: 0.031632
[23:33:32.086] iteration 8733 : loss: 0.013384, loss_a: 0.007873
[23:33:32.835] iteration 8734 : loss: 0.035443, loss_a: 0.020849
[23:33:34.178] iteration 8735 : loss: 0.020009, loss_a: 0.011770
[23:33:34.925] iteration 8736 : loss: 0.022714, loss_a: 0.013361
[23:33:36.240] iteration 8737 : loss: 0.024269, loss_a: 0.014276
[23:33:36.988] iteration 8738 : loss: 0.025795, loss_a: 0.015173
[23:33:38.302] iteration 8739 : loss: 0.023289, loss_a: 0.013699
[23:33:39.043] iteration 8740 : loss: 0.017766, loss_a: 0.010451
[23:33:40.419] iteration 8741 : loss: 0.083768, loss_a: 0.049275
[23:33:41.162] iteration 8742 : loss: 0.014565, loss_a: 0.008568
[23:33:42.503] iteration 8743 : loss: 0.027646, loss_a: 0.016262
[23:33:43.231] iteration 8744 : loss: 0.019056, loss_a: 0.011209
[23:33:44.577] iteration 8745 : loss: 0.028005, loss_a: 0.016473
[23:33:45.311] iteration 8746 : loss: 0.069063, loss_a: 0.040625
[23:33:46.661] iteration 8747 : loss: 0.039744, loss_a: 0.023379
[23:33:47.398] iteration 8748 : loss: 0.026294, loss_a: 0.015467
[23:33:48.750] iteration 8749 : loss: 0.020629, loss_a: 0.012135
[23:33:49.483] iteration 8750 : loss: 0.023624, loss_a: 0.013897
[23:33:50.803] iteration 8751 : loss: 0.037532, loss_a: 0.022078
[23:33:51.546] iteration 8752 : loss: 0.021048, loss_a: 0.012381
[23:33:52.867] iteration 8753 : loss: 0.019058, loss_a: 0.011210
[23:33:53.608] iteration 8754 : loss: 0.016058, loss_a: 0.009446
[23:33:54.937] iteration 8755 : loss: 0.026779, loss_a: 0.015753
[23:33:55.671] iteration 8756 : loss: 0.027154, loss_a: 0.015973
[23:33:56.995] iteration 8757 : loss: 0.015551, loss_a: 0.009147
[23:33:57.727] iteration 8758 : loss: 0.029609, loss_a: 0.017417
[23:33:59.046] iteration 8759 : loss: 0.023395, loss_a: 0.013762
[23:33:59.783] iteration 8760 : loss: 0.024521, loss_a: 0.014424
[23:34:01.138] iteration 8761 : loss: 0.019409, loss_a: 0.011417
[23:34:01.872] iteration 8762 : loss: 0.017476, loss_a: 0.010280
[23:34:03.202] iteration 8763 : loss: 0.022578, loss_a: 0.013281
[23:34:03.939] iteration 8764 : loss: 0.015091, loss_a: 0.008877
[23:34:05.270] iteration 8765 : loss: 0.023348, loss_a: 0.013734
[23:34:06.009] iteration 8766 : loss: 0.025176, loss_a: 0.014810
[23:34:07.370] iteration 8767 : loss: 0.030571, loss_a: 0.017983
[23:34:08.110] iteration 8768 : loss: 0.031125, loss_a: 0.018309
[23:34:09.431] iteration 8769 : loss: 0.039570, loss_a: 0.023276
[23:34:10.171] iteration 8770 : loss: 0.024542, loss_a: 0.014436
[23:34:11.515] iteration 8771 : loss: 0.027088, loss_a: 0.015934
[23:34:12.257] iteration 8772 : loss: 0.015300, loss_a: 0.009000
[23:34:13.570] iteration 8773 : loss: 0.014314, loss_a: 0.008420
[23:34:14.310] iteration 8774 : loss: 0.060381, loss_a: 0.035518
[23:34:15.670] iteration 8775 : loss: 0.024522, loss_a: 0.014425
[23:34:16.414] iteration 8776 : loss: 0.031405, loss_a: 0.018474
[23:34:17.726] iteration 8777 : loss: 0.050415, loss_a: 0.029656
[23:34:18.474] iteration 8778 : loss: 0.031312, loss_a: 0.018419
[23:34:19.813] iteration 8779 : loss: 0.018694, loss_a: 0.010996
[23:34:20.545] iteration 8780 : loss: 0.038502, loss_a: 0.022648
[23:34:21.873] iteration 8781 : loss: 0.036035, loss_a: 0.021197
[23:34:22.600] iteration 8782 : loss: 0.024985, loss_a: 0.014697
[23:34:23.933] iteration 8783 : loss: 0.023897, loss_a: 0.014057
[23:34:24.677] iteration 8784 : loss: 0.030183, loss_a: 0.017755
[23:34:26.046] iteration 8785 : loss: 0.049120, loss_a: 0.028894
[23:34:26.783] iteration 8786 : loss: 0.021643, loss_a: 0.012731
[23:34:28.113] iteration 8787 : loss: 0.048593, loss_a: 0.028584
[23:34:28.858] iteration 8788 : loss: 0.020856, loss_a: 0.012268
[23:34:30.202] iteration 8789 : loss: 0.044270, loss_a: 0.026041
[23:34:30.946] iteration 8790 : loss: 0.033284, loss_a: 0.019579
[23:34:32.256] iteration 8791 : loss: 0.023420, loss_a: 0.013776
[23:34:32.990] iteration 8792 : loss: 0.020610, loss_a: 0.012124
[23:34:34.301] iteration 8793 : loss: 0.013760, loss_a: 0.008094
[23:34:35.045] iteration 8794 : loss: 0.034482, loss_a: 0.020283
[23:34:36.399] iteration 8795 : loss: 0.027356, loss_a: 0.016092
[23:34:37.139] iteration 8796 : loss: 0.029660, loss_a: 0.017447
[23:34:38.456] iteration 8797 : loss: 0.028945, loss_a: 0.017026
[23:34:39.189] iteration 8798 : loss: 0.012777, loss_a: 0.007516
[23:34:40.548] iteration 8799 : loss: 0.036068, loss_a: 0.021216
[23:34:41.289] iteration 8800 : loss: 0.037095, loss_a: 0.021821
[23:35:05.887] iteration 8801 : loss: 0.016917, loss_a: 0.009951
[23:35:08.093] iteration 8802 : loss: 0.022537, loss_a: 0.013257
[23:35:09.423] iteration 8803 : loss: 0.019547, loss_a: 0.011498
[23:35:10.163] iteration 8804 : loss: 0.025976, loss_a: 0.015280
[23:35:11.512] iteration 8805 : loss: 0.061343, loss_a: 0.036084
[23:35:12.253] iteration 8806 : loss: 0.022588, loss_a: 0.013287
[23:35:13.594] iteration 8807 : loss: 0.016180, loss_a: 0.009518
[23:35:14.338] iteration 8808 : loss: 0.018797, loss_a: 0.011057
[23:35:15.656] iteration 8809 : loss: 0.019273, loss_a: 0.011337
[23:35:16.397] iteration 8810 : loss: 0.017552, loss_a: 0.010325
[23:35:17.745] iteration 8811 : loss: 0.027333, loss_a: 0.016079
[23:35:18.485] iteration 8812 : loss: 0.018733, loss_a: 0.011019
[23:35:19.826] iteration 8813 : loss: 0.039265, loss_a: 0.023097
[23:35:20.565] iteration 8814 : loss: 0.042378, loss_a: 0.024928
[23:35:21.893] iteration 8815 : loss: 0.010656, loss_a: 0.006268
[23:35:22.641] iteration 8816 : loss: 0.030674, loss_a: 0.018044
[23:35:23.964] iteration 8817 : loss: 0.016422, loss_a: 0.009660
[23:35:24.701] iteration 8818 : loss: 0.029368, loss_a: 0.017275
[23:35:26.032] iteration 8819 : loss: 0.040651, loss_a: 0.023912
[23:35:26.780] iteration 8820 : loss: 0.028955, loss_a: 0.017032
[23:35:28.111] iteration 8821 : loss: 0.025951, loss_a: 0.015265
[23:35:28.859] iteration 8822 : loss: 0.053342, loss_a: 0.031378
[23:35:30.215] iteration 8823 : loss: 0.042448, loss_a: 0.024969
[23:35:30.961] iteration 8824 : loss: 0.030867, loss_a: 0.018157
[23:35:32.268] iteration 8825 : loss: 0.017107, loss_a: 0.010063
[23:35:33.007] iteration 8826 : loss: 0.026877, loss_a: 0.015810
[23:35:34.329] iteration 8827 : loss: 0.031142, loss_a: 0.018319
[23:35:35.071] iteration 8828 : loss: 0.019360, loss_a: 0.011389
[23:35:36.411] iteration 8829 : loss: 0.015705, loss_a: 0.009238
[23:35:37.152] iteration 8830 : loss: 0.019815, loss_a: 0.011656
[23:35:38.462] iteration 8831 : loss: 0.027396, loss_a: 0.016115
[23:35:39.203] iteration 8832 : loss: 0.063418, loss_a: 0.037305
[23:35:40.567] iteration 8833 : loss: 0.036729, loss_a: 0.021605
[23:35:41.305] iteration 8834 : loss: 0.013493, loss_a: 0.007937
[23:35:42.676] iteration 8835 : loss: 0.100700, loss_a: 0.059235
[23:35:43.417] iteration 8836 : loss: 0.049555, loss_a: 0.029150
[23:35:44.766] iteration 8837 : loss: 0.036918, loss_a: 0.021716
[23:35:45.510] iteration 8838 : loss: 0.027401, loss_a: 0.016118
[23:35:46.884] iteration 8839 : loss: 0.043818, loss_a: 0.025776
[23:35:47.637] iteration 8840 : loss: 0.059458, loss_a: 0.034975
[23:35:48.942] iteration 8841 : loss: 0.029921, loss_a: 0.017600
[23:35:49.671] iteration 8842 : loss: 0.036828, loss_a: 0.021664
[23:35:50.986] iteration 8843 : loss: 0.023365, loss_a: 0.013744
[23:35:51.731] iteration 8844 : loss: 0.023148, loss_a: 0.013616
[23:35:53.056] iteration 8845 : loss: 0.032027, loss_a: 0.018839
[23:35:53.789] iteration 8846 : loss: 0.019663, loss_a: 0.011567
[23:35:55.147] iteration 8847 : loss: 0.029874, loss_a: 0.017573
[23:35:55.884] iteration 8848 : loss: 0.026798, loss_a: 0.015764
[23:35:57.247] iteration 8849 : loss: 0.024546, loss_a: 0.014439
[23:35:57.985] iteration 8850 : loss: 0.024758, loss_a: 0.014563
[23:35:59.321] iteration 8851 : loss: 0.021436, loss_a: 0.012610
[23:36:00.058] iteration 8852 : loss: 0.022596, loss_a: 0.013292
[23:36:01.377] iteration 8853 : loss: 0.025252, loss_a: 0.014854
[23:36:02.125] iteration 8854 : loss: 0.019446, loss_a: 0.011439
[23:36:03.477] iteration 8855 : loss: 0.021645, loss_a: 0.012732
[23:36:04.228] iteration 8856 : loss: 0.028437, loss_a: 0.016727
[23:36:05.553] iteration 8857 : loss: 0.017441, loss_a: 0.010259
[23:36:06.289] iteration 8858 : loss: 0.018270, loss_a: 0.010747
[23:36:07.610] iteration 8859 : loss: 0.021336, loss_a: 0.012551
[23:36:08.353] iteration 8860 : loss: 0.019309, loss_a: 0.011358
[23:36:09.714] iteration 8861 : loss: 0.023742, loss_a: 0.013966
[23:36:10.456] iteration 8862 : loss: 0.015629, loss_a: 0.009193
[23:36:11.765] iteration 8863 : loss: 0.035676, loss_a: 0.020986
[23:36:12.508] iteration 8864 : loss: 0.028799, loss_a: 0.016941
[23:36:13.844] iteration 8865 : loss: 0.026954, loss_a: 0.015855
[23:36:14.580] iteration 8866 : loss: 0.016554, loss_a: 0.009737
[23:36:15.928] iteration 8867 : loss: 0.034506, loss_a: 0.020298
[23:36:16.660] iteration 8868 : loss: 0.013240, loss_a: 0.007788
[23:36:18.029] iteration 8869 : loss: 0.018823, loss_a: 0.011073
[23:36:18.771] iteration 8870 : loss: 0.024534, loss_a: 0.014432
[23:36:20.127] iteration 8871 : loss: 0.026469, loss_a: 0.015570
[23:36:20.860] iteration 8872 : loss: 0.040254, loss_a: 0.023679
[23:36:22.177] iteration 8873 : loss: 0.018532, loss_a: 0.010901
[23:36:22.940] iteration 8874 : loss: 0.051925, loss_a: 0.030544
[23:36:24.316] iteration 8875 : loss: 0.043937, loss_a: 0.025845
[23:36:25.058] iteration 8876 : loss: 0.019554, loss_a: 0.011502
[23:36:26.410] iteration 8877 : loss: 0.036152, loss_a: 0.021266
[23:36:27.149] iteration 8878 : loss: 0.032668, loss_a: 0.019216
[23:36:28.480] iteration 8879 : loss: 0.040640, loss_a: 0.023906
[23:36:29.217] iteration 8880 : loss: 0.013272, loss_a: 0.007807
[23:36:30.567] iteration 8881 : loss: 0.050720, loss_a: 0.029835
[23:36:31.309] iteration 8882 : loss: 0.037477, loss_a: 0.022045
[23:36:32.650] iteration 8883 : loss: 0.033923, loss_a: 0.019955
[23:36:33.406] iteration 8884 : loss: 0.051232, loss_a: 0.030137
[23:36:34.712] iteration 8885 : loss: 0.022967, loss_a: 0.013510
[23:36:35.446] iteration 8886 : loss: 0.026189, loss_a: 0.015405
[23:36:36.784] iteration 8887 : loss: 0.052791, loss_a: 0.031053
[23:36:37.529] iteration 8888 : loss: 0.028894, loss_a: 0.016996
[23:36:38.882] iteration 8889 : loss: 0.043845, loss_a: 0.025791
[23:36:39.632] iteration 8890 : loss: 0.042937, loss_a: 0.025257
[23:36:40.985] iteration 8891 : loss: 0.044922, loss_a: 0.026425
[23:36:41.714] iteration 8892 : loss: 0.010589, loss_a: 0.006229
[23:36:43.086] iteration 8893 : loss: 0.017355, loss_a: 0.010209
[23:36:43.830] iteration 8894 : loss: 0.031762, loss_a: 0.018684
[23:36:45.188] iteration 8895 : loss: 0.034533, loss_a: 0.020314
[23:36:45.934] iteration 8896 : loss: 0.036820, loss_a: 0.021659
[23:36:47.290] iteration 8897 : loss: 0.026137, loss_a: 0.015375
[23:36:48.045] iteration 8898 : loss: 0.041289, loss_a: 0.024288
[23:36:49.410] iteration 8899 : loss: 0.030226, loss_a: 0.017780
[23:36:50.158] iteration 8900 : loss: 0.047877, loss_a: 0.028163
[23:36:51.505] iteration 8901 : loss: 0.031628, loss_a: 0.018605
[23:36:52.243] iteration 8902 : loss: 0.022797, loss_a: 0.013410
[23:36:53.573] iteration 8903 : loss: 0.030974, loss_a: 0.018220
[23:36:54.313] iteration 8904 : loss: 0.019701, loss_a: 0.011589
[23:36:55.652] iteration 8905 : loss: 0.037838, loss_a: 0.022257
[23:36:56.397] iteration 8906 : loss: 0.055100, loss_a: 0.032412
[23:36:57.733] iteration 8907 : loss: 0.022499, loss_a: 0.013235
[23:36:58.471] iteration 8908 : loss: 0.024480, loss_a: 0.014400
[23:36:59.849] iteration 8909 : loss: 0.031999, loss_a: 0.018823
[23:37:00.607] iteration 8910 : loss: 0.029401, loss_a: 0.017295
[23:37:01.924] iteration 8911 : loss: 0.052103, loss_a: 0.030649
[23:37:02.666] iteration 8912 : loss: 0.022137, loss_a: 0.013022
[23:37:04.031] iteration 8913 : loss: 0.032342, loss_a: 0.019024
[23:37:04.761] iteration 8914 : loss: 0.014708, loss_a: 0.008652
[23:37:06.112] iteration 8915 : loss: 0.029707, loss_a: 0.017475
[23:37:06.850] iteration 8916 : loss: 0.014063, loss_a: 0.008272
[23:37:08.201] iteration 8917 : loss: 0.024388, loss_a: 0.014346
[23:37:08.943] iteration 8918 : loss: 0.036543, loss_a: 0.021496
[23:37:10.311] iteration 8919 : loss: 0.033698, loss_a: 0.019822
[23:37:11.058] iteration 8920 : loss: 0.026087, loss_a: 0.015345
[23:37:12.406] iteration 8921 : loss: 0.034089, loss_a: 0.020052
[23:37:13.148] iteration 8922 : loss: 0.058946, loss_a: 0.034674
[23:37:14.496] iteration 8923 : loss: 0.029070, loss_a: 0.017100
[23:37:15.254] iteration 8924 : loss: 0.027361, loss_a: 0.016095
[23:37:16.601] iteration 8925 : loss: 0.020686, loss_a: 0.012168
[23:37:17.353] iteration 8926 : loss: 0.023886, loss_a: 0.014051
[23:37:18.681] iteration 8927 : loss: 0.041725, loss_a: 0.024544
[23:37:19.432] iteration 8928 : loss: 0.054713, loss_a: 0.032184
[23:37:20.779] iteration 8929 : loss: 0.024477, loss_a: 0.014398
[23:37:21.516] iteration 8930 : loss: 0.020410, loss_a: 0.012006
[23:37:22.867] iteration 8931 : loss: 0.015645, loss_a: 0.009203
[23:37:23.619] iteration 8932 : loss: 0.036755, loss_a: 0.021620
[23:37:24.954] iteration 8933 : loss: 0.064895, loss_a: 0.038173
[23:37:25.685] iteration 8934 : loss: 0.021836, loss_a: 0.012845
[23:37:27.024] iteration 8935 : loss: 0.021145, loss_a: 0.012438
[23:37:27.756] iteration 8936 : loss: 0.018922, loss_a: 0.011131
[23:37:29.079] iteration 8937 : loss: 0.012930, loss_a: 0.007606
[23:37:29.834] iteration 8938 : loss: 0.015537, loss_a: 0.009139
[23:37:31.208] iteration 8939 : loss: 0.045869, loss_a: 0.026982
[23:37:31.938] iteration 8940 : loss: 0.026073, loss_a: 0.015337
[23:37:33.270] iteration 8941 : loss: 0.023799, loss_a: 0.013999
[23:37:34.005] iteration 8942 : loss: 0.021317, loss_a: 0.012540
[23:37:35.365] iteration 8943 : loss: 0.029824, loss_a: 0.017543
[23:37:36.108] iteration 8944 : loss: 0.018356, loss_a: 0.010798
[23:37:37.414] iteration 8945 : loss: 0.022614, loss_a: 0.013302
[23:37:38.157] iteration 8946 : loss: 0.026996, loss_a: 0.015880
[23:37:39.474] iteration 8947 : loss: 0.014414, loss_a: 0.008479
[23:37:40.222] iteration 8948 : loss: 0.026079, loss_a: 0.015341
[23:37:41.585] iteration 8949 : loss: 0.018665, loss_a: 0.010979
[23:37:42.325] iteration 8950 : loss: 0.018211, loss_a: 0.010712
[23:37:43.673] iteration 8951 : loss: 0.020335, loss_a: 0.011962
[23:37:44.407] iteration 8952 : loss: 0.015019, loss_a: 0.008835
[23:37:45.736] iteration 8953 : loss: 0.029625, loss_a: 0.017426
[23:37:46.476] iteration 8954 : loss: 0.028936, loss_a: 0.017021
[23:37:47.828] iteration 8955 : loss: 0.018085, loss_a: 0.010638
[23:37:48.570] iteration 8956 : loss: 0.060269, loss_a: 0.035452
[23:37:49.922] iteration 8957 : loss: 0.039812, loss_a: 0.023419
[23:37:50.660] iteration 8958 : loss: 0.018106, loss_a: 0.010651
[23:37:51.991] iteration 8959 : loss: 0.011766, loss_a: 0.006921
[23:37:52.737] iteration 8960 : loss: 0.036015, loss_a: 0.021185
[23:37:54.088] iteration 8961 : loss: 0.024703, loss_a: 0.014531
[23:37:54.825] iteration 8962 : loss: 0.021482, loss_a: 0.012637
[23:37:56.172] iteration 8963 : loss: 0.026119, loss_a: 0.015364
[23:37:56.922] iteration 8964 : loss: 0.057275, loss_a: 0.033691
[23:37:58.290] iteration 8965 : loss: 0.047535, loss_a: 0.027962
[23:37:59.039] iteration 8966 : loss: 0.035806, loss_a: 0.021062
[23:38:00.354] iteration 8967 : loss: 0.019887, loss_a: 0.011698
[23:38:01.113] iteration 8968 : loss: 0.032538, loss_a: 0.019140
[23:38:02.440] iteration 8969 : loss: 0.024632, loss_a: 0.014490
[23:38:03.190] iteration 8970 : loss: 0.041144, loss_a: 0.024203
[23:38:04.555] iteration 8971 : loss: 0.016705, loss_a: 0.009827
[23:38:05.297] iteration 8972 : loss: 0.015029, loss_a: 0.008840
[23:38:06.608] iteration 8973 : loss: 0.040673, loss_a: 0.023925
[23:38:07.354] iteration 8974 : loss: 0.015926, loss_a: 0.009368
[23:38:08.697] iteration 8975 : loss: 0.036897, loss_a: 0.021704
[23:38:09.446] iteration 8976 : loss: 0.027016, loss_a: 0.015892
[23:38:10.786] iteration 8977 : loss: 0.032502, loss_a: 0.019119
[23:38:11.540] iteration 8978 : loss: 0.035025, loss_a: 0.020603
[23:38:12.863] iteration 8979 : loss: 0.024219, loss_a: 0.014247
[23:38:13.602] iteration 8980 : loss: 0.028384, loss_a: 0.016697
[23:38:14.964] iteration 8981 : loss: 0.031078, loss_a: 0.018281
[23:38:15.703] iteration 8982 : loss: 0.024171, loss_a: 0.014219
[23:38:17.055] iteration 8983 : loss: 0.023348, loss_a: 0.013734
[23:38:17.803] iteration 8984 : loss: 0.023504, loss_a: 0.013826
[23:38:19.159] iteration 8985 : loss: 0.034404, loss_a: 0.020237
[23:38:19.899] iteration 8986 : loss: 0.039854, loss_a: 0.023444
[23:38:21.247] iteration 8987 : loss: 0.023914, loss_a: 0.014067
[23:38:21.987] iteration 8988 : loss: 0.027150, loss_a: 0.015971
[23:38:23.321] iteration 8989 : loss: 0.026939, loss_a: 0.015846
[23:38:24.051] iteration 8990 : loss: 0.029873, loss_a: 0.017572
[23:38:25.406] iteration 8991 : loss: 0.025739, loss_a: 0.015141
[23:38:26.146] iteration 8992 : loss: 0.021490, loss_a: 0.012641
[23:38:27.511] iteration 8993 : loss: 0.018383, loss_a: 0.010814
[23:38:28.259] iteration 8994 : loss: 0.019948, loss_a: 0.011734
[23:38:29.583] iteration 8995 : loss: 0.021882, loss_a: 0.012872
[23:38:30.334] iteration 8996 : loss: 0.029505, loss_a: 0.017356
[23:38:31.658] iteration 8997 : loss: 0.022890, loss_a: 0.013465
[23:38:32.411] iteration 8998 : loss: 0.023927, loss_a: 0.014075
[23:38:33.768] iteration 8999 : loss: 0.031194, loss_a: 0.018350
[23:38:34.515] iteration 9000 : loss: 0.026731, loss_a: 0.015724
[23:38:59.208] iteration 9001 : loss: 0.065866, loss_a: 0.038745
[23:39:01.295] iteration 9002 : loss: 0.033288, loss_a: 0.019581
[23:39:02.630] iteration 9003 : loss: 0.041901, loss_a: 0.024648
[23:39:03.377] iteration 9004 : loss: 0.019164, loss_a: 0.011273
[23:39:04.693] iteration 9005 : loss: 0.040606, loss_a: 0.023886
[23:39:05.435] iteration 9006 : loss: 0.025987, loss_a: 0.015286
[23:39:06.766] iteration 9007 : loss: 0.012486, loss_a: 0.007345
[23:39:07.504] iteration 9008 : loss: 0.016079, loss_a: 0.009458
[23:39:08.837] iteration 9009 : loss: 0.028005, loss_a: 0.016474
[23:39:09.581] iteration 9010 : loss: 0.024674, loss_a: 0.014514
[23:39:10.950] iteration 9011 : loss: 0.026394, loss_a: 0.015526
[23:39:11.695] iteration 9012 : loss: 0.025563, loss_a: 0.015037
[23:39:13.038] iteration 9013 : loss: 0.021086, loss_a: 0.012404
[23:39:13.788] iteration 9014 : loss: 0.036713, loss_a: 0.021596
[23:39:15.138] iteration 9015 : loss: 0.015587, loss_a: 0.009169
[23:39:15.872] iteration 9016 : loss: 0.018164, loss_a: 0.010685
[23:39:17.202] iteration 9017 : loss: 0.023502, loss_a: 0.013825
[23:39:17.941] iteration 9018 : loss: 0.073703, loss_a: 0.043355
[23:39:19.300] iteration 9019 : loss: 0.019768, loss_a: 0.011628
[23:39:20.029] iteration 9020 : loss: 0.023022, loss_a: 0.013542
[23:39:21.371] iteration 9021 : loss: 0.026447, loss_a: 0.015557
[23:39:22.110] iteration 9022 : loss: 0.034901, loss_a: 0.020530
[23:39:23.422] iteration 9023 : loss: 0.022685, loss_a: 0.013344
[23:39:24.174] iteration 9024 : loss: 0.031928, loss_a: 0.018781
[23:39:25.523] iteration 9025 : loss: 0.018008, loss_a: 0.010593
[23:39:26.270] iteration 9026 : loss: 0.028316, loss_a: 0.016656
[23:39:27.599] iteration 9027 : loss: 0.015256, loss_a: 0.008974
[23:39:28.336] iteration 9028 : loss: 0.015121, loss_a: 0.008895
[23:39:29.651] iteration 9029 : loss: 0.262978, loss_a: 0.154693
[23:39:30.390] iteration 9030 : loss: 0.059703, loss_a: 0.035120
[23:39:31.714] iteration 9031 : loss: 0.025475, loss_a: 0.014985
[23:39:32.456] iteration 9032 : loss: 0.029194, loss_a: 0.017173
[23:39:33.801] iteration 9033 : loss: 0.032419, loss_a: 0.019070
[23:39:34.560] iteration 9034 : loss: 0.030302, loss_a: 0.017824
[23:39:35.915] iteration 9035 : loss: 0.028476, loss_a: 0.016751
[23:39:36.670] iteration 9036 : loss: 0.027086, loss_a: 0.015933
[23:39:38.024] iteration 9037 : loss: 0.034193, loss_a: 0.020114
[23:39:38.767] iteration 9038 : loss: 0.024523, loss_a: 0.014425
[23:39:40.116] iteration 9039 : loss: 0.022150, loss_a: 0.013029
[23:39:40.852] iteration 9040 : loss: 0.025742, loss_a: 0.015142
[23:39:42.209] iteration 9041 : loss: 0.032288, loss_a: 0.018993
[23:39:42.960] iteration 9042 : loss: 0.048521, loss_a: 0.028542
[23:39:44.310] iteration 9043 : loss: 0.036461, loss_a: 0.021448
[23:39:45.052] iteration 9044 : loss: 0.021005, loss_a: 0.012356
[23:39:46.392] iteration 9045 : loss: 0.018123, loss_a: 0.010660
[23:39:47.140] iteration 9046 : loss: 0.028124, loss_a: 0.016544
[23:39:48.509] iteration 9047 : loss: 0.096058, loss_a: 0.056504
[23:39:49.243] iteration 9048 : loss: 0.025162, loss_a: 0.014801
[23:39:50.566] iteration 9049 : loss: 0.027892, loss_a: 0.016407
[23:39:51.313] iteration 9050 : loss: 0.030935, loss_a: 0.018197
[23:39:52.660] iteration 9051 : loss: 0.031243, loss_a: 0.018378
[23:39:53.402] iteration 9052 : loss: 0.041463, loss_a: 0.024390
[23:39:54.736] iteration 9053 : loss: 0.028102, loss_a: 0.016531
[23:39:55.477] iteration 9054 : loss: 0.018928, loss_a: 0.011134
[23:39:56.807] iteration 9055 : loss: 0.017580, loss_a: 0.010341
[23:39:57.560] iteration 9056 : loss: 0.091595, loss_a: 0.053879
[23:39:58.891] iteration 9057 : loss: 0.022322, loss_a: 0.013131
[23:39:59.638] iteration 9058 : loss: 0.033228, loss_a: 0.019546
[23:40:00.982] iteration 9059 : loss: 0.062089, loss_a: 0.036523
[23:40:01.722] iteration 9060 : loss: 0.026556, loss_a: 0.015621
[23:40:03.062] iteration 9061 : loss: 0.073724, loss_a: 0.043367
[23:40:03.806] iteration 9062 : loss: 0.022640, loss_a: 0.013317
[23:40:05.126] iteration 9063 : loss: 0.024751, loss_a: 0.014559
[23:40:05.869] iteration 9064 : loss: 0.040032, loss_a: 0.023548
[23:40:07.229] iteration 9065 : loss: 0.029227, loss_a: 0.017192
[23:40:07.965] iteration 9066 : loss: 0.016589, loss_a: 0.009758
[23:40:09.306] iteration 9067 : loss: 0.063846, loss_a: 0.037557
[23:40:10.039] iteration 9068 : loss: 0.033245, loss_a: 0.019556
[23:40:11.362] iteration 9069 : loss: 0.017052, loss_a: 0.010031
[23:40:12.110] iteration 9070 : loss: 0.032229, loss_a: 0.018958
[23:40:13.465] iteration 9071 : loss: 0.090173, loss_a: 0.053043
[23:40:14.200] iteration 9072 : loss: 0.016950, loss_a: 0.009970
[23:40:15.550] iteration 9073 : loss: 0.042446, loss_a: 0.024968
[23:40:16.290] iteration 9074 : loss: 0.024775, loss_a: 0.014574
[23:40:17.636] iteration 9075 : loss: 0.018603, loss_a: 0.010943
[23:40:18.393] iteration 9076 : loss: 0.043549, loss_a: 0.025617
[23:40:19.711] iteration 9077 : loss: 0.024102, loss_a: 0.014178
[23:40:20.454] iteration 9078 : loss: 0.046524, loss_a: 0.027367
[23:40:21.802] iteration 9079 : loss: 0.031757, loss_a: 0.018681
[23:40:22.551] iteration 9080 : loss: 0.031412, loss_a: 0.018478
[23:40:23.887] iteration 9081 : loss: 0.028370, loss_a: 0.016688
[23:40:24.633] iteration 9082 : loss: 0.037564, loss_a: 0.022097
[23:40:26.017] iteration 9083 : loss: 0.023860, loss_a: 0.014036
[23:40:26.762] iteration 9084 : loss: 0.036997, loss_a: 0.021763
[23:40:28.112] iteration 9085 : loss: 0.028434, loss_a: 0.016726
[23:40:28.852] iteration 9086 : loss: 0.046272, loss_a: 0.027219
[23:40:30.186] iteration 9087 : loss: 0.033959, loss_a: 0.019976
[23:40:30.940] iteration 9088 : loss: 0.027596, loss_a: 0.016233
[23:40:32.280] iteration 9089 : loss: 0.032403, loss_a: 0.019061
[23:40:33.017] iteration 9090 : loss: 0.025920, loss_a: 0.015247
[23:40:34.365] iteration 9091 : loss: 0.023765, loss_a: 0.013980
[23:40:35.102] iteration 9092 : loss: 0.018371, loss_a: 0.010806
[23:40:36.463] iteration 9093 : loss: 0.027616, loss_a: 0.016244
[23:40:37.214] iteration 9094 : loss: 0.024572, loss_a: 0.014454
[23:40:38.577] iteration 9095 : loss: 0.038566, loss_a: 0.022686
[23:40:39.312] iteration 9096 : loss: 0.043703, loss_a: 0.025708
[23:40:40.630] iteration 9097 : loss: 0.023501, loss_a: 0.013824
[23:40:41.365] iteration 9098 : loss: 0.016180, loss_a: 0.009518
[23:40:42.704] iteration 9099 : loss: 0.029780, loss_a: 0.017518
[23:40:43.439] iteration 9100 : loss: 0.019128, loss_a: 0.011252
[23:40:44.773] iteration 9101 : loss: 0.038983, loss_a: 0.022931
[23:40:45.512] iteration 9102 : loss: 0.030735, loss_a: 0.018080
[23:40:46.869] iteration 9103 : loss: 0.044011, loss_a: 0.025889
[23:40:47.603] iteration 9104 : loss: 0.022971, loss_a: 0.013512
[23:40:48.955] iteration 9105 : loss: 0.018290, loss_a: 0.010759
[23:40:49.699] iteration 9106 : loss: 0.017235, loss_a: 0.010138
[23:40:51.032] iteration 9107 : loss: 0.032410, loss_a: 0.019064
[23:40:51.774] iteration 9108 : loss: 0.027717, loss_a: 0.016304
[23:40:53.113] iteration 9109 : loss: 0.026174, loss_a: 0.015397
[23:40:53.862] iteration 9110 : loss: 0.027249, loss_a: 0.016029
[23:40:55.189] iteration 9111 : loss: 0.059365, loss_a: 0.034920
[23:40:55.938] iteration 9112 : loss: 0.022626, loss_a: 0.013309
[23:40:57.238] iteration 9113 : loss: 0.014785, loss_a: 0.008697
[23:40:57.977] iteration 9114 : loss: 0.017157, loss_a: 0.010092
[23:40:59.332] iteration 9115 : loss: 0.014008, loss_a: 0.008240
[23:41:00.068] iteration 9116 : loss: 0.012116, loss_a: 0.007127
[23:41:01.410] iteration 9117 : loss: 0.028916, loss_a: 0.017009
[23:41:02.144] iteration 9118 : loss: 0.014776, loss_a: 0.008692
[23:41:03.478] iteration 9119 : loss: 0.058145, loss_a: 0.034203
[23:41:04.217] iteration 9120 : loss: 0.039409, loss_a: 0.023182
[23:41:05.552] iteration 9121 : loss: 0.014263, loss_a: 0.008390
[23:41:06.295] iteration 9122 : loss: 0.025874, loss_a: 0.015220
[23:41:07.616] iteration 9123 : loss: 0.021935, loss_a: 0.012903
[23:41:08.355] iteration 9124 : loss: 0.019363, loss_a: 0.011390
[23:41:09.672] iteration 9125 : loss: 0.012419, loss_a: 0.007305
[23:41:10.415] iteration 9126 : loss: 0.048495, loss_a: 0.028526
[23:41:11.730] iteration 9127 : loss: 0.035595, loss_a: 0.020938
[23:41:12.466] iteration 9128 : loss: 0.018912, loss_a: 0.011124
[23:41:13.813] iteration 9129 : loss: 0.022018, loss_a: 0.012952
[23:41:14.562] iteration 9130 : loss: 0.035365, loss_a: 0.020803
[23:41:15.917] iteration 9131 : loss: 0.030805, loss_a: 0.018121
[23:41:16.655] iteration 9132 : loss: 0.027272, loss_a: 0.016043
[23:41:18.004] iteration 9133 : loss: 0.027283, loss_a: 0.016049
[23:41:18.747] iteration 9134 : loss: 0.028633, loss_a: 0.016843
[23:41:20.105] iteration 9135 : loss: 0.017328, loss_a: 0.010193
[23:41:20.855] iteration 9136 : loss: 0.047043, loss_a: 0.027672
[23:41:22.201] iteration 9137 : loss: 0.047672, loss_a: 0.028042
[23:41:22.946] iteration 9138 : loss: 0.026355, loss_a: 0.015503
[23:41:24.324] iteration 9139 : loss: 0.031621, loss_a: 0.018600
[23:41:25.068] iteration 9140 : loss: 0.054986, loss_a: 0.032345
[23:41:26.388] iteration 9141 : loss: 0.033969, loss_a: 0.019981
[23:41:27.133] iteration 9142 : loss: 0.027638, loss_a: 0.016257
[23:41:28.470] iteration 9143 : loss: 0.045957, loss_a: 0.027034
[23:41:29.205] iteration 9144 : loss: 0.020676, loss_a: 0.012162
[23:41:30.545] iteration 9145 : loss: 0.023466, loss_a: 0.013804
[23:41:31.300] iteration 9146 : loss: 0.017887, loss_a: 0.010522
[23:41:32.648] iteration 9147 : loss: 0.023650, loss_a: 0.013912
[23:41:33.390] iteration 9148 : loss: 0.024329, loss_a: 0.014311
[23:41:34.704] iteration 9149 : loss: 0.019056, loss_a: 0.011210
[23:41:35.451] iteration 9150 : loss: 0.018193, loss_a: 0.010702
[23:41:36.810] iteration 9151 : loss: 0.023011, loss_a: 0.013536
[23:41:37.553] iteration 9152 : loss: 0.023304, loss_a: 0.013708
[23:41:38.916] iteration 9153 : loss: 0.039859, loss_a: 0.023446
[23:41:39.661] iteration 9154 : loss: 0.019197, loss_a: 0.011293
[23:41:41.024] iteration 9155 : loss: 0.014237, loss_a: 0.008375
[23:41:41.769] iteration 9156 : loss: 0.037739, loss_a: 0.022200
[23:41:43.122] iteration 9157 : loss: 0.024848, loss_a: 0.014616
[23:41:43.865] iteration 9158 : loss: 0.048821, loss_a: 0.028718
[23:41:45.195] iteration 9159 : loss: 0.044146, loss_a: 0.025968
[23:41:45.928] iteration 9160 : loss: 0.052835, loss_a: 0.031080
[23:41:47.283] iteration 9161 : loss: 0.031860, loss_a: 0.018741
[23:41:48.015] iteration 9162 : loss: 0.048460, loss_a: 0.028506
[23:41:49.343] iteration 9163 : loss: 0.023863, loss_a: 0.014037
[23:41:50.073] iteration 9164 : loss: 0.029342, loss_a: 0.017260
[23:41:51.386] iteration 9165 : loss: 0.020092, loss_a: 0.011819
[23:41:52.132] iteration 9166 : loss: 0.026869, loss_a: 0.015805
[23:41:53.482] iteration 9167 : loss: 0.023018, loss_a: 0.013540
[23:41:54.227] iteration 9168 : loss: 0.023989, loss_a: 0.014111
[23:41:55.549] iteration 9169 : loss: 0.018710, loss_a: 0.011006
[23:41:56.284] iteration 9170 : loss: 0.026975, loss_a: 0.015868
[23:41:57.626] iteration 9171 : loss: 0.019924, loss_a: 0.011720
[23:41:58.366] iteration 9172 : loss: 0.021187, loss_a: 0.012463
[23:41:59.705] iteration 9173 : loss: 0.013931, loss_a: 0.008194
[23:42:00.444] iteration 9174 : loss: 0.024124, loss_a: 0.014190
[23:42:01.762] iteration 9175 : loss: 0.017456, loss_a: 0.010268
[23:42:02.504] iteration 9176 : loss: 0.281480, loss_a: 0.165577
[23:42:03.836] iteration 9177 : loss: 0.020242, loss_a: 0.011907
[23:42:04.574] iteration 9178 : loss: 0.040605, loss_a: 0.023885
[23:42:05.893] iteration 9179 : loss: 0.020332, loss_a: 0.011960
[23:42:06.633] iteration 9180 : loss: 0.019917, loss_a: 0.011716
[23:42:07.960] iteration 9181 : loss: 0.020271, loss_a: 0.011924
[23:42:08.709] iteration 9182 : loss: 0.034454, loss_a: 0.020267
[23:42:10.046] iteration 9183 : loss: 0.022482, loss_a: 0.013225
[23:42:10.790] iteration 9184 : loss: 0.062170, loss_a: 0.036571
[23:42:12.146] iteration 9185 : loss: 0.032341, loss_a: 0.019024
[23:42:12.896] iteration 9186 : loss: 0.024833, loss_a: 0.014608
[23:42:14.233] iteration 9187 : loss: 0.033952, loss_a: 0.019972
[23:42:14.970] iteration 9188 : loss: 0.015215, loss_a: 0.008950
[23:42:16.332] iteration 9189 : loss: 0.040907, loss_a: 0.024063
[23:42:17.081] iteration 9190 : loss: 0.036393, loss_a: 0.021407
[23:42:18.408] iteration 9191 : loss: 0.019361, loss_a: 0.011389
[23:42:19.154] iteration 9192 : loss: 0.020133, loss_a: 0.011843
[23:42:20.480] iteration 9193 : loss: 0.022091, loss_a: 0.012995
[23:42:21.231] iteration 9194 : loss: 0.023950, loss_a: 0.014088
[23:42:22.571] iteration 9195 : loss: 0.039106, loss_a: 0.023004
[23:42:23.313] iteration 9196 : loss: 0.015554, loss_a: 0.009150
[23:42:24.619] iteration 9197 : loss: 0.018125, loss_a: 0.010662
[23:42:25.356] iteration 9198 : loss: 0.021773, loss_a: 0.012807
[23:42:26.706] iteration 9199 : loss: 0.016608, loss_a: 0.009769
[23:42:27.441] iteration 9200 : loss: 0.038350, loss_a: 0.022559
[23:42:52.083] iteration 9201 : loss: 0.033445, loss_a: 0.019674
[23:42:54.232] iteration 9202 : loss: 0.022923, loss_a: 0.013484
[23:42:55.556] iteration 9203 : loss: 0.019344, loss_a: 0.011379
[23:42:56.305] iteration 9204 : loss: 0.019247, loss_a: 0.011321
[23:42:57.611] iteration 9205 : loss: 0.018073, loss_a: 0.010631
[23:42:58.341] iteration 9206 : loss: 0.016243, loss_a: 0.009555
[23:42:59.680] iteration 9207 : loss: 0.029695, loss_a: 0.017468
[23:43:00.417] iteration 9208 : loss: 0.030926, loss_a: 0.018192
[23:43:01.731] iteration 9209 : loss: 0.016535, loss_a: 0.009726
[23:43:02.466] iteration 9210 : loss: 0.018264, loss_a: 0.010744
[23:43:03.783] iteration 9211 : loss: 0.021494, loss_a: 0.012643
[23:43:04.517] iteration 9212 : loss: 0.016286, loss_a: 0.009580
[23:43:05.871] iteration 9213 : loss: 0.018678, loss_a: 0.010987
[23:43:06.620] iteration 9214 : loss: 0.089167, loss_a: 0.052451
[23:43:07.980] iteration 9215 : loss: 0.021572, loss_a: 0.012689
[23:43:08.714] iteration 9216 : loss: 0.026762, loss_a: 0.015742
[23:43:10.033] iteration 9217 : loss: 0.023384, loss_a: 0.013755
[23:43:10.774] iteration 9218 : loss: 0.019616, loss_a: 0.011539
[23:43:12.100] iteration 9219 : loss: 0.035179, loss_a: 0.020693
[23:43:12.842] iteration 9220 : loss: 0.064437, loss_a: 0.037904
[23:43:14.162] iteration 9221 : loss: 0.053090, loss_a: 0.031230
[23:43:14.899] iteration 9222 : loss: 0.015500, loss_a: 0.009118
[23:43:16.223] iteration 9223 : loss: 0.020115, loss_a: 0.011832
[23:43:16.996] iteration 9224 : loss: 0.530278, loss_a: 0.311928
[23:43:18.324] iteration 9225 : loss: 0.029367, loss_a: 0.017275
[23:43:19.057] iteration 9226 : loss: 0.018598, loss_a: 0.010940
[23:43:20.397] iteration 9227 : loss: 0.082138, loss_a: 0.048317
[23:43:21.127] iteration 9228 : loss: 0.017995, loss_a: 0.010585
[23:43:22.492] iteration 9229 : loss: 0.035571, loss_a: 0.020924
[23:43:23.221] iteration 9230 : loss: 0.014270, loss_a: 0.008394
[23:43:24.546] iteration 9231 : loss: 0.053991, loss_a: 0.031759
[23:43:25.293] iteration 9232 : loss: 0.056421, loss_a: 0.033189
[23:43:26.620] iteration 9233 : loss: 0.016264, loss_a: 0.009567
[23:43:27.359] iteration 9234 : loss: 0.022120, loss_a: 0.013012
[23:43:28.684] iteration 9235 : loss: 0.022211, loss_a: 0.013065
[23:43:29.429] iteration 9236 : loss: 0.022667, loss_a: 0.013334
[23:43:30.756] iteration 9237 : loss: 0.021697, loss_a: 0.012763
[23:43:31.494] iteration 9238 : loss: 0.019745, loss_a: 0.011615
[23:43:32.843] iteration 9239 : loss: 0.049398, loss_a: 0.029057
[23:43:33.581] iteration 9240 : loss: 0.022438, loss_a: 0.013199
[23:43:34.903] iteration 9241 : loss: 0.026483, loss_a: 0.015578
[23:43:35.646] iteration 9242 : loss: 0.067618, loss_a: 0.039776
[23:43:37.011] iteration 9243 : loss: 0.063827, loss_a: 0.037546
[23:43:37.751] iteration 9244 : loss: 0.019852, loss_a: 0.011678
[23:43:39.087] iteration 9245 : loss: 0.030295, loss_a: 0.017821
[23:43:39.840] iteration 9246 : loss: 0.046825, loss_a: 0.027544
[23:43:41.188] iteration 9247 : loss: 0.034840, loss_a: 0.020494
[23:43:41.924] iteration 9248 : loss: 0.022428, loss_a: 0.013193
[23:43:43.269] iteration 9249 : loss: 0.019194, loss_a: 0.011291
[23:43:44.016] iteration 9250 : loss: 0.083095, loss_a: 0.048880
[23:43:45.370] iteration 9251 : loss: 0.022035, loss_a: 0.012962
[23:43:46.104] iteration 9252 : loss: 0.017272, loss_a: 0.010160
[23:43:47.452] iteration 9253 : loss: 0.036306, loss_a: 0.021356
[23:43:48.191] iteration 9254 : loss: 0.021594, loss_a: 0.012703
[23:43:49.512] iteration 9255 : loss: 0.017161, loss_a: 0.010095
[23:43:50.253] iteration 9256 : loss: 0.032561, loss_a: 0.019153
[23:43:51.622] iteration 9257 : loss: 0.047691, loss_a: 0.028053
[23:43:52.357] iteration 9258 : loss: 0.021421, loss_a: 0.012600
[23:43:53.694] iteration 9259 : loss: 0.032074, loss_a: 0.018867
[23:43:54.444] iteration 9260 : loss: 0.050480, loss_a: 0.029694
[23:43:55.771] iteration 9261 : loss: 0.029024, loss_a: 0.017073
[23:43:56.505] iteration 9262 : loss: 0.024058, loss_a: 0.014152
[23:43:57.845] iteration 9263 : loss: 0.017144, loss_a: 0.010085
[23:43:58.583] iteration 9264 : loss: 0.017931, loss_a: 0.010547
[23:43:59.895] iteration 9265 : loss: 0.046682, loss_a: 0.027460
[23:44:00.631] iteration 9266 : loss: 0.022207, loss_a: 0.013063
[23:44:01.990] iteration 9267 : loss: 0.025683, loss_a: 0.015108
[23:44:02.742] iteration 9268 : loss: 0.060466, loss_a: 0.035568
[23:44:04.095] iteration 9269 : loss: 0.027619, loss_a: 0.016247
[23:44:04.830] iteration 9270 : loss: 0.024244, loss_a: 0.014261
[23:44:06.189] iteration 9271 : loss: 0.034006, loss_a: 0.020004
[23:44:06.918] iteration 9272 : loss: 0.016438, loss_a: 0.009670
[23:44:08.251] iteration 9273 : loss: 0.014511, loss_a: 0.008536
[23:44:08.992] iteration 9274 : loss: 0.022885, loss_a: 0.013462
[23:44:10.318] iteration 9275 : loss: 0.024886, loss_a: 0.014639
[23:44:11.079] iteration 9276 : loss: 0.022920, loss_a: 0.013482
[23:44:12.447] iteration 9277 : loss: 0.022192, loss_a: 0.013054
[23:44:13.192] iteration 9278 : loss: 0.021366, loss_a: 0.012568
[23:44:14.510] iteration 9279 : loss: 0.009818, loss_a: 0.005775
[23:44:15.253] iteration 9280 : loss: 0.027087, loss_a: 0.015934
[23:44:16.572] iteration 9281 : loss: 0.028053, loss_a: 0.016502
[23:44:17.303] iteration 9282 : loss: 0.029363, loss_a: 0.017272
[23:44:18.647] iteration 9283 : loss: 0.021694, loss_a: 0.012761
[23:44:19.390] iteration 9284 : loss: 0.054972, loss_a: 0.032336
[23:44:20.747] iteration 9285 : loss: 0.014523, loss_a: 0.008543
[23:44:21.486] iteration 9286 : loss: 0.029355, loss_a: 0.017268
[23:44:22.856] iteration 9287 : loss: 0.091000, loss_a: 0.053529
[23:44:23.592] iteration 9288 : loss: 0.033991, loss_a: 0.019995
[23:44:24.916] iteration 9289 : loss: 0.012867, loss_a: 0.007569
[23:44:25.655] iteration 9290 : loss: 0.016558, loss_a: 0.009740
[23:44:26.971] iteration 9291 : loss: 0.013127, loss_a: 0.007722
[23:44:27.705] iteration 9292 : loss: 0.020580, loss_a: 0.012106
[23:44:29.026] iteration 9293 : loss: 0.046764, loss_a: 0.027508
[23:44:29.771] iteration 9294 : loss: 0.050394, loss_a: 0.029644
[23:44:31.106] iteration 9295 : loss: 0.059880, loss_a: 0.035224
[23:44:31.859] iteration 9296 : loss: 0.022024, loss_a: 0.012955
[23:44:33.188] iteration 9297 : loss: 0.041865, loss_a: 0.024627
[23:44:33.934] iteration 9298 : loss: 0.020628, loss_a: 0.012134
[23:44:35.237] iteration 9299 : loss: 0.011177, loss_a: 0.006575
[23:44:35.983] iteration 9300 : loss: 0.023010, loss_a: 0.013535
[23:44:37.305] iteration 9301 : loss: 0.026821, loss_a: 0.015777
[23:44:38.053] iteration 9302 : loss: 0.017072, loss_a: 0.010042
[23:44:39.380] iteration 9303 : loss: 0.028988, loss_a: 0.017052
[23:44:40.115] iteration 9304 : loss: 0.030769, loss_a: 0.018099
[23:44:41.425] iteration 9305 : loss: 0.012637, loss_a: 0.007434
[23:44:42.160] iteration 9306 : loss: 0.023143, loss_a: 0.013614
[23:44:43.494] iteration 9307 : loss: 0.019363, loss_a: 0.011390
[23:44:44.233] iteration 9308 : loss: 0.021067, loss_a: 0.012392
[23:44:45.564] iteration 9309 : loss: 0.020625, loss_a: 0.012132
[23:44:46.305] iteration 9310 : loss: 0.022455, loss_a: 0.013209
[23:44:47.626] iteration 9311 : loss: 0.045311, loss_a: 0.026653
[23:44:48.366] iteration 9312 : loss: 0.045560, loss_a: 0.026800
[23:44:49.693] iteration 9313 : loss: 0.019800, loss_a: 0.011647
[23:44:50.437] iteration 9314 : loss: 0.032171, loss_a: 0.018924
[23:44:51.761] iteration 9315 : loss: 0.019628, loss_a: 0.011546
[23:44:52.497] iteration 9316 : loss: 0.033975, loss_a: 0.019985
[23:44:53.858] iteration 9317 : loss: 0.031741, loss_a: 0.018671
[23:44:54.604] iteration 9318 : loss: 0.027433, loss_a: 0.016137
[23:44:55.924] iteration 9319 : loss: 0.024298, loss_a: 0.014293
[23:44:56.667] iteration 9320 : loss: 0.023291, loss_a: 0.013701
[23:44:57.998] iteration 9321 : loss: 0.034389, loss_a: 0.020229
[23:44:58.734] iteration 9322 : loss: 0.018655, loss_a: 0.010974
[23:45:00.058] iteration 9323 : loss: 0.035782, loss_a: 0.021048
[23:45:00.796] iteration 9324 : loss: 0.018996, loss_a: 0.011174
[23:45:02.168] iteration 9325 : loss: 0.027598, loss_a: 0.016234
[23:45:02.910] iteration 9326 : loss: 0.017348, loss_a: 0.010205
[23:45:04.239] iteration 9327 : loss: 0.045151, loss_a: 0.026559
[23:45:04.980] iteration 9328 : loss: 0.016839, loss_a: 0.009905
[23:45:06.335] iteration 9329 : loss: 0.032954, loss_a: 0.019385
[23:45:07.082] iteration 9330 : loss: 0.042035, loss_a: 0.024726
[23:45:08.440] iteration 9331 : loss: 0.014686, loss_a: 0.008639
[23:45:09.178] iteration 9332 : loss: 0.022984, loss_a: 0.013520
[23:45:10.507] iteration 9333 : loss: 0.020227, loss_a: 0.011898
[23:45:11.255] iteration 9334 : loss: 0.021029, loss_a: 0.012370
[23:45:12.611] iteration 9335 : loss: 0.009461, loss_a: 0.005566
[23:45:13.359] iteration 9336 : loss: 0.026925, loss_a: 0.015838
[23:45:14.712] iteration 9337 : loss: 0.039028, loss_a: 0.022958
[23:45:15.449] iteration 9338 : loss: 0.022362, loss_a: 0.013154
[23:45:16.797] iteration 9339 : loss: 0.028142, loss_a: 0.016554
[23:45:17.537] iteration 9340 : loss: 0.011220, loss_a: 0.006600
[23:45:18.866] iteration 9341 : loss: 0.010073, loss_a: 0.005925
[23:45:19.607] iteration 9342 : loss: 0.036379, loss_a: 0.021399
[23:45:20.930] iteration 9343 : loss: 0.023006, loss_a: 0.013533
[23:45:21.660] iteration 9344 : loss: 0.014875, loss_a: 0.008750
[23:45:23.010] iteration 9345 : loss: 0.023650, loss_a: 0.013912
[23:45:23.745] iteration 9346 : loss: 0.015462, loss_a: 0.009095
[23:45:25.099] iteration 9347 : loss: 0.025232, loss_a: 0.014842
[23:45:25.868] iteration 9348 : loss: 0.038674, loss_a: 0.022750
[23:45:27.213] iteration 9349 : loss: 0.017439, loss_a: 0.010258
[23:45:27.957] iteration 9350 : loss: 0.036471, loss_a: 0.021454
[23:45:29.280] iteration 9351 : loss: 0.015268, loss_a: 0.008981
[23:45:30.027] iteration 9352 : loss: 0.054245, loss_a: 0.031909
[23:45:31.374] iteration 9353 : loss: 0.033879, loss_a: 0.019929
[23:45:32.109] iteration 9354 : loss: 0.016965, loss_a: 0.009979
[23:45:33.430] iteration 9355 : loss: 0.015197, loss_a: 0.008939
[23:45:34.163] iteration 9356 : loss: 0.028944, loss_a: 0.017026
[23:45:35.493] iteration 9357 : loss: 0.024238, loss_a: 0.014258
[23:45:36.231] iteration 9358 : loss: 0.035275, loss_a: 0.020750
[23:45:37.566] iteration 9359 : loss: 0.026381, loss_a: 0.015518
[23:45:38.299] iteration 9360 : loss: 0.017257, loss_a: 0.010151
[23:45:39.664] iteration 9361 : loss: 0.041663, loss_a: 0.024507
[23:45:40.404] iteration 9362 : loss: 0.016783, loss_a: 0.009873
[23:45:41.758] iteration 9363 : loss: 0.022502, loss_a: 0.013236
[23:45:42.494] iteration 9364 : loss: 0.028025, loss_a: 0.016485
[23:45:43.843] iteration 9365 : loss: 0.025984, loss_a: 0.015285
[23:45:44.589] iteration 9366 : loss: 0.044472, loss_a: 0.026160
[23:45:45.942] iteration 9367 : loss: 0.017789, loss_a: 0.010464
[23:45:46.683] iteration 9368 : loss: 0.017518, loss_a: 0.010305
[23:45:48.016] iteration 9369 : loss: 0.017908, loss_a: 0.010534
[23:45:48.760] iteration 9370 : loss: 0.034805, loss_a: 0.020473
[23:45:50.109] iteration 9371 : loss: 0.021181, loss_a: 0.012459
[23:45:50.849] iteration 9372 : loss: 0.030949, loss_a: 0.018205
[23:45:52.199] iteration 9373 : loss: 0.016603, loss_a: 0.009766
[23:45:52.933] iteration 9374 : loss: 0.019118, loss_a: 0.011246
[23:45:54.297] iteration 9375 : loss: 0.032046, loss_a: 0.018851
[23:45:55.044] iteration 9376 : loss: 0.027745, loss_a: 0.016321
[23:45:56.378] iteration 9377 : loss: 0.059889, loss_a: 0.035229
[23:45:57.121] iteration 9378 : loss: 0.038519, loss_a: 0.022658
[23:45:58.443] iteration 9379 : loss: 0.015170, loss_a: 0.008923
[23:45:59.173] iteration 9380 : loss: 0.016748, loss_a: 0.009852
[23:46:00.516] iteration 9381 : loss: 0.017729, loss_a: 0.010429
[23:46:01.249] iteration 9382 : loss: 0.049568, loss_a: 0.029158
[23:46:02.589] iteration 9383 : loss: 0.029291, loss_a: 0.017230
[23:46:03.333] iteration 9384 : loss: 0.031843, loss_a: 0.018731
[23:46:04.661] iteration 9385 : loss: 0.014058, loss_a: 0.008269
[23:46:05.400] iteration 9386 : loss: 0.024565, loss_a: 0.014450
[23:46:06.774] iteration 9387 : loss: 0.020275, loss_a: 0.011927
[23:46:07.509] iteration 9388 : loss: 0.056617, loss_a: 0.033304
[23:46:08.839] iteration 9389 : loss: 0.010676, loss_a: 0.006280
[23:46:09.582] iteration 9390 : loss: 0.026816, loss_a: 0.015774
[23:46:10.903] iteration 9391 : loss: 0.015899, loss_a: 0.009352
[23:46:11.642] iteration 9392 : loss: 0.024128, loss_a: 0.014193
[23:46:12.972] iteration 9393 : loss: 0.051857, loss_a: 0.030504
[23:46:13.712] iteration 9394 : loss: 0.016962, loss_a: 0.009978
[23:46:15.043] iteration 9395 : loss: 0.023680, loss_a: 0.013929
[23:46:15.788] iteration 9396 : loss: 0.027187, loss_a: 0.015993
[23:46:17.116] iteration 9397 : loss: 0.017849, loss_a: 0.010500
[23:46:17.858] iteration 9398 : loss: 0.035151, loss_a: 0.020677
[23:46:19.201] iteration 9399 : loss: 0.047374, loss_a: 0.027867
[23:46:19.937] iteration 9400 : loss: 0.019165, loss_a: 0.011274
[23:46:44.584] iteration 9401 : loss: 0.082018, loss_a: 0.048246
[23:46:46.802] iteration 9402 : loss: 0.019000, loss_a: 0.011176
[23:46:48.128] iteration 9403 : loss: 0.030522, loss_a: 0.017954
[23:46:48.873] iteration 9404 : loss: 0.039603, loss_a: 0.023296
[23:46:50.202] iteration 9405 : loss: 0.028970, loss_a: 0.017041
[23:46:50.939] iteration 9406 : loss: 0.031228, loss_a: 0.018369
[23:46:52.271] iteration 9407 : loss: 0.040540, loss_a: 0.023847
[23:46:53.024] iteration 9408 : loss: 0.038898, loss_a: 0.022881
[23:46:54.378] iteration 9409 : loss: 0.048125, loss_a: 0.028309
[23:46:55.118] iteration 9410 : loss: 0.080082, loss_a: 0.047107
[23:46:56.472] iteration 9411 : loss: 0.023214, loss_a: 0.013655
[23:46:57.234] iteration 9412 : loss: 0.025012, loss_a: 0.014713
[23:46:58.549] iteration 9413 : loss: 0.041779, loss_a: 0.024576
[23:46:59.295] iteration 9414 : loss: 0.046138, loss_a: 0.027140
[23:47:00.616] iteration 9415 : loss: 0.030982, loss_a: 0.018225
[23:47:01.367] iteration 9416 : loss: 0.024693, loss_a: 0.014525
[23:47:02.721] iteration 9417 : loss: 0.025812, loss_a: 0.015184
[23:47:03.462] iteration 9418 : loss: 0.032687, loss_a: 0.019228
[23:47:04.778] iteration 9419 : loss: 0.024032, loss_a: 0.014137
[23:47:05.516] iteration 9420 : loss: 0.018525, loss_a: 0.010897
[23:47:06.857] iteration 9421 : loss: 0.027294, loss_a: 0.016055
[23:47:07.611] iteration 9422 : loss: 0.052803, loss_a: 0.031060
[23:47:08.955] iteration 9423 : loss: 0.038752, loss_a: 0.022796
[23:47:09.694] iteration 9424 : loss: 0.027258, loss_a: 0.016034
[23:47:11.001] iteration 9425 : loss: 0.016760, loss_a: 0.009859
[23:47:11.754] iteration 9426 : loss: 0.030901, loss_a: 0.018177
[23:47:13.073] iteration 9427 : loss: 0.055149, loss_a: 0.032441
[23:47:13.825] iteration 9428 : loss: 0.032184, loss_a: 0.018932
[23:47:15.197] iteration 9429 : loss: 0.026923, loss_a: 0.015837
[23:47:15.936] iteration 9430 : loss: 0.022059, loss_a: 0.012976
[23:47:17.260] iteration 9431 : loss: 0.017555, loss_a: 0.010326
[23:47:18.014] iteration 9432 : loss: 0.030346, loss_a: 0.017851
[23:47:19.362] iteration 9433 : loss: 0.028976, loss_a: 0.017045
[23:47:20.094] iteration 9434 : loss: 0.024503, loss_a: 0.014413
[23:47:21.415] iteration 9435 : loss: 0.039097, loss_a: 0.022998
[23:47:22.167] iteration 9436 : loss: 0.028056, loss_a: 0.016504
[23:47:23.520] iteration 9437 : loss: 0.025411, loss_a: 0.014948
[23:47:24.270] iteration 9438 : loss: 0.049373, loss_a: 0.029043
[23:47:25.599] iteration 9439 : loss: 0.019675, loss_a: 0.011573
[23:47:26.346] iteration 9440 : loss: 0.041960, loss_a: 0.024682
[23:47:27.684] iteration 9441 : loss: 0.041690, loss_a: 0.024523
[23:47:28.425] iteration 9442 : loss: 0.052207, loss_a: 0.030710
[23:47:29.773] iteration 9443 : loss: 0.039686, loss_a: 0.023345
[23:47:30.521] iteration 9444 : loss: 0.033856, loss_a: 0.019915
[23:47:31.856] iteration 9445 : loss: 0.023817, loss_a: 0.014010
[23:47:32.603] iteration 9446 : loss: 0.021216, loss_a: 0.012480
[23:47:33.953] iteration 9447 : loss: 0.041482, loss_a: 0.024401
[23:47:34.686] iteration 9448 : loss: 0.012854, loss_a: 0.007561
[23:47:36.058] iteration 9449 : loss: 0.023134, loss_a: 0.013608
[23:47:36.806] iteration 9450 : loss: 0.020480, loss_a: 0.012047
[23:47:38.137] iteration 9451 : loss: 0.013781, loss_a: 0.008106
[23:47:38.872] iteration 9452 : loss: 0.027321, loss_a: 0.016071
[23:47:40.205] iteration 9453 : loss: 0.030538, loss_a: 0.017964
[23:47:40.945] iteration 9454 : loss: 0.024665, loss_a: 0.014509
[23:47:42.284] iteration 9455 : loss: 0.030264, loss_a: 0.017802
[23:47:43.020] iteration 9456 : loss: 0.018261, loss_a: 0.010742
[23:47:44.354] iteration 9457 : loss: 0.021108, loss_a: 0.012417
[23:47:45.085] iteration 9458 : loss: 0.022027, loss_a: 0.012957
[23:47:46.414] iteration 9459 : loss: 0.055279, loss_a: 0.032517
[23:47:47.153] iteration 9460 : loss: 0.027670, loss_a: 0.016277
[23:47:48.485] iteration 9461 : loss: 0.019435, loss_a: 0.011432
[23:47:49.234] iteration 9462 : loss: 0.029207, loss_a: 0.017181
[23:47:50.565] iteration 9463 : loss: 0.039837, loss_a: 0.023434
[23:47:51.301] iteration 9464 : loss: 0.018094, loss_a: 0.010643
[23:47:52.620] iteration 9465 : loss: 0.052219, loss_a: 0.030717
[23:47:53.353] iteration 9466 : loss: 0.017991, loss_a: 0.010583
[23:47:54.685] iteration 9467 : loss: 0.023729, loss_a: 0.013958
[23:47:55.425] iteration 9468 : loss: 0.026204, loss_a: 0.015414
[23:47:56.772] iteration 9469 : loss: 0.056650, loss_a: 0.033323
[23:47:57.513] iteration 9470 : loss: 0.040551, loss_a: 0.023854
[23:47:58.853] iteration 9471 : loss: 0.028891, loss_a: 0.016994
[23:47:59.594] iteration 9472 : loss: 0.042940, loss_a: 0.025259
[23:48:00.904] iteration 9473 : loss: 0.027131, loss_a: 0.015959
[23:48:01.644] iteration 9474 : loss: 0.048747, loss_a: 0.028675
[23:48:02.996] iteration 9475 : loss: 0.017987, loss_a: 0.010581
[23:48:03.730] iteration 9476 : loss: 0.019064, loss_a: 0.011214
[23:48:05.100] iteration 9477 : loss: 0.040256, loss_a: 0.023680
[23:48:05.850] iteration 9478 : loss: 0.057370, loss_a: 0.033747
[23:48:07.194] iteration 9479 : loss: 0.066001, loss_a: 0.038824
[23:48:07.937] iteration 9480 : loss: 0.035296, loss_a: 0.020762
[23:48:09.282] iteration 9481 : loss: 0.034272, loss_a: 0.020160
[23:48:10.027] iteration 9482 : loss: 0.069915, loss_a: 0.041126
[23:48:11.360] iteration 9483 : loss: 0.036718, loss_a: 0.021599
[23:48:12.102] iteration 9484 : loss: 0.018582, loss_a: 0.010930
[23:48:13.408] iteration 9485 : loss: 0.026051, loss_a: 0.015324
[23:48:14.146] iteration 9486 : loss: 0.020767, loss_a: 0.012216
[23:48:15.462] iteration 9487 : loss: 0.040231, loss_a: 0.023665
[23:48:16.215] iteration 9488 : loss: 0.030056, loss_a: 0.017680
[23:48:17.523] iteration 9489 : loss: 0.024141, loss_a: 0.014201
[23:48:18.263] iteration 9490 : loss: 0.020320, loss_a: 0.011953
[23:48:19.614] iteration 9491 : loss: 0.032166, loss_a: 0.018921
[23:48:20.351] iteration 9492 : loss: 0.023206, loss_a: 0.013650
[23:48:21.698] iteration 9493 : loss: 0.021909, loss_a: 0.012888
[23:48:22.430] iteration 9494 : loss: 0.021572, loss_a: 0.012690
[23:48:23.742] iteration 9495 : loss: 0.012829, loss_a: 0.007547
[23:48:24.483] iteration 9496 : loss: 0.027066, loss_a: 0.015921
[23:48:25.850] iteration 9497 : loss: 0.032204, loss_a: 0.018944
[23:48:26.594] iteration 9498 : loss: 0.038183, loss_a: 0.022460
[23:48:27.942] iteration 9499 : loss: 0.029056, loss_a: 0.017092
[23:48:28.687] iteration 9500 : loss: 0.022850, loss_a: 0.013441
[23:48:30.049] iteration 9501 : loss: 0.077199, loss_a: 0.045411
[23:48:30.798] iteration 9502 : loss: 0.025139, loss_a: 0.014787
[23:48:32.117] iteration 9503 : loss: 0.017179, loss_a: 0.010105
[23:48:32.861] iteration 9504 : loss: 0.035117, loss_a: 0.020657
[23:48:34.221] iteration 9505 : loss: 0.026571, loss_a: 0.015630
[23:48:34.959] iteration 9506 : loss: 0.018139, loss_a: 0.010670
[23:48:36.294] iteration 9507 : loss: 0.026788, loss_a: 0.015758
[23:48:37.030] iteration 9508 : loss: 0.030504, loss_a: 0.017944
[23:48:38.376] iteration 9509 : loss: 0.023358, loss_a: 0.013740
[23:48:39.110] iteration 9510 : loss: 0.026934, loss_a: 0.015844
[23:48:40.431] iteration 9511 : loss: 0.039601, loss_a: 0.023295
[23:48:41.175] iteration 9512 : loss: 0.042050, loss_a: 0.024735
[23:48:42.537] iteration 9513 : loss: 0.045773, loss_a: 0.026925
[23:48:43.279] iteration 9514 : loss: 0.031898, loss_a: 0.018764
[23:48:44.638] iteration 9515 : loss: 0.041266, loss_a: 0.024274
[23:48:45.387] iteration 9516 : loss: 0.028731, loss_a: 0.016900
[23:48:46.692] iteration 9517 : loss: 0.015920, loss_a: 0.009365
[23:48:47.432] iteration 9518 : loss: 0.026903, loss_a: 0.015825
[23:48:48.791] iteration 9519 : loss: 0.040699, loss_a: 0.023941
[23:48:49.527] iteration 9520 : loss: 0.020762, loss_a: 0.012213
[23:48:50.858] iteration 9521 : loss: 0.025939, loss_a: 0.015259
[23:48:51.593] iteration 9522 : loss: 0.018971, loss_a: 0.011159
[23:48:52.941] iteration 9523 : loss: 0.015464, loss_a: 0.009096
[23:48:53.675] iteration 9524 : loss: 0.016386, loss_a: 0.009639
[23:48:55.020] iteration 9525 : loss: 0.038587, loss_a: 0.022698
[23:48:55.759] iteration 9526 : loss: 0.015243, loss_a: 0.008966
[23:48:57.112] iteration 9527 : loss: 0.022368, loss_a: 0.013157
[23:48:57.853] iteration 9528 : loss: 0.018735, loss_a: 0.011020
[23:48:59.180] iteration 9529 : loss: 0.023439, loss_a: 0.013788
[23:48:59.928] iteration 9530 : loss: 0.016319, loss_a: 0.009599
[23:49:01.276] iteration 9531 : loss: 0.016496, loss_a: 0.009704
[23:49:02.019] iteration 9532 : loss: 0.036727, loss_a: 0.021604
[23:49:03.345] iteration 9533 : loss: 0.015476, loss_a: 0.009104
[23:49:04.086] iteration 9534 : loss: 0.027384, loss_a: 0.016108
[23:49:05.432] iteration 9535 : loss: 0.019585, loss_a: 0.011521
[23:49:06.183] iteration 9536 : loss: 0.018898, loss_a: 0.011117
[23:49:07.519] iteration 9537 : loss: 0.041751, loss_a: 0.024559
[23:49:08.256] iteration 9538 : loss: 0.014673, loss_a: 0.008631
[23:49:09.605] iteration 9539 : loss: 0.033157, loss_a: 0.019504
[23:49:10.345] iteration 9540 : loss: 0.062534, loss_a: 0.036785
[23:49:11.713] iteration 9541 : loss: 0.028219, loss_a: 0.016600
[23:49:12.451] iteration 9542 : loss: 0.015978, loss_a: 0.009399
[23:49:13.796] iteration 9543 : loss: 0.053608, loss_a: 0.031534
[23:49:14.540] iteration 9544 : loss: 0.031404, loss_a: 0.018473
[23:49:15.873] iteration 9545 : loss: 0.022341, loss_a: 0.013142
[23:49:16.618] iteration 9546 : loss: 0.019031, loss_a: 0.011195
[23:49:17.962] iteration 9547 : loss: 0.025665, loss_a: 0.015097
[23:49:18.689] iteration 9548 : loss: 0.011689, loss_a: 0.006876
[23:49:20.018] iteration 9549 : loss: 0.031243, loss_a: 0.018378
[23:49:20.767] iteration 9550 : loss: 0.032890, loss_a: 0.019347
[23:49:22.117] iteration 9551 : loss: 0.042957, loss_a: 0.025269
[23:49:22.851] iteration 9552 : loss: 0.014574, loss_a: 0.008573
[23:49:24.178] iteration 9553 : loss: 0.023004, loss_a: 0.013532
[23:49:24.911] iteration 9554 : loss: 0.032502, loss_a: 0.019119
[23:49:26.227] iteration 9555 : loss: 0.018811, loss_a: 0.011065
[23:49:26.973] iteration 9556 : loss: 0.067668, loss_a: 0.039805
[23:49:28.333] iteration 9557 : loss: 0.060417, loss_a: 0.035540
[23:49:29.079] iteration 9558 : loss: 0.036052, loss_a: 0.021207
[23:49:30.432] iteration 9559 : loss: 0.026595, loss_a: 0.015644
[23:49:31.174] iteration 9560 : loss: 0.043471, loss_a: 0.025571
[23:49:32.501] iteration 9561 : loss: 0.022149, loss_a: 0.013029
[23:49:33.235] iteration 9562 : loss: 0.012818, loss_a: 0.007540
[23:49:34.567] iteration 9563 : loss: 0.036041, loss_a: 0.021201
[23:49:35.318] iteration 9564 : loss: 0.037005, loss_a: 0.021767
[23:49:36.651] iteration 9565 : loss: 0.019168, loss_a: 0.011275
[23:49:37.387] iteration 9566 : loss: 0.054476, loss_a: 0.032045
[23:49:38.739] iteration 9567 : loss: 0.023728, loss_a: 0.013958
[23:49:39.474] iteration 9568 : loss: 0.019820, loss_a: 0.011659
[23:49:40.825] iteration 9569 : loss: 0.024299, loss_a: 0.014293
[23:49:41.568] iteration 9570 : loss: 0.070170, loss_a: 0.041277
[23:49:42.896] iteration 9571 : loss: 0.023185, loss_a: 0.013638
[23:49:43.638] iteration 9572 : loss: 0.024304, loss_a: 0.014297
[23:49:44.960] iteration 9573 : loss: 0.016259, loss_a: 0.009564
[23:49:45.699] iteration 9574 : loss: 0.014188, loss_a: 0.008346
[23:49:47.014] iteration 9575 : loss: 0.022036, loss_a: 0.012962
[23:49:47.769] iteration 9576 : loss: 0.023139, loss_a: 0.013611
[23:49:49.084] iteration 9577 : loss: 0.013994, loss_a: 0.008232
[23:49:49.828] iteration 9578 : loss: 0.028999, loss_a: 0.017058
[23:49:51.162] iteration 9579 : loss: 0.029824, loss_a: 0.017544
[23:49:51.901] iteration 9580 : loss: 0.017903, loss_a: 0.010531
[23:49:53.216] iteration 9581 : loss: 0.034162, loss_a: 0.020095
[23:49:53.971] iteration 9582 : loss: 0.057864, loss_a: 0.034038
[23:49:55.342] iteration 9583 : loss: 0.037875, loss_a: 0.022280
[23:49:56.072] iteration 9584 : loss: 0.021063, loss_a: 0.012390
[23:49:57.422] iteration 9585 : loss: 0.046072, loss_a: 0.027101
[23:49:58.158] iteration 9586 : loss: 0.012545, loss_a: 0.007379
[23:49:59.521] iteration 9587 : loss: 0.052900, loss_a: 0.031118
[23:50:00.250] iteration 9588 : loss: 0.011250, loss_a: 0.006618
[23:50:01.605] iteration 9589 : loss: 0.021666, loss_a: 0.012745
[23:50:02.349] iteration 9590 : loss: 0.013159, loss_a: 0.007740
[23:50:03.675] iteration 9591 : loss: 0.049669, loss_a: 0.029217
[23:50:04.402] iteration 9592 : loss: 0.031939, loss_a: 0.018787
[23:50:05.729] iteration 9593 : loss: 0.031521, loss_a: 0.018542
[23:50:06.471] iteration 9594 : loss: 0.031131, loss_a: 0.018312
[23:50:07.819] iteration 9595 : loss: 0.030617, loss_a: 0.018010
[23:50:08.566] iteration 9596 : loss: 0.033047, loss_a: 0.019439
[23:50:09.913] iteration 9597 : loss: 0.029045, loss_a: 0.017086
[23:50:10.646] iteration 9598 : loss: 0.023294, loss_a: 0.013702
[23:50:11.970] iteration 9599 : loss: 0.026029, loss_a: 0.015311
[23:50:12.700] iteration 9600 : loss: 0.016245, loss_a: 0.009556
[23:50:37.329] iteration 9601 : loss: 0.036150, loss_a: 0.021265
[23:50:39.474] iteration 9602 : loss: 0.030477, loss_a: 0.017928
[23:50:40.810] iteration 9603 : loss: 0.051520, loss_a: 0.030306
[23:50:41.565] iteration 9604 : loss: 0.021259, loss_a: 0.012505
[23:50:42.906] iteration 9605 : loss: 0.010159, loss_a: 0.005976
[23:50:43.643] iteration 9606 : loss: 0.030548, loss_a: 0.017969
[23:50:44.963] iteration 9607 : loss: 0.042649, loss_a: 0.025087
[23:50:45.694] iteration 9608 : loss: 0.009328, loss_a: 0.005487
[23:50:47.049] iteration 9609 : loss: 0.039283, loss_a: 0.023108
[23:50:47.785] iteration 9610 : loss: 0.019928, loss_a: 0.011722
[23:50:49.156] iteration 9611 : loss: 0.019139, loss_a: 0.011258
[23:50:49.902] iteration 9612 : loss: 0.040298, loss_a: 0.023704
[23:50:51.258] iteration 9613 : loss: 0.198826, loss_a: 0.116956
[23:50:51.996] iteration 9614 : loss: 0.029347, loss_a: 0.017263
[23:50:53.306] iteration 9615 : loss: 0.015194, loss_a: 0.008938
[23:50:54.064] iteration 9616 : loss: 0.053794, loss_a: 0.031643
[23:50:55.378] iteration 9617 : loss: 0.034181, loss_a: 0.020106
[23:50:56.124] iteration 9618 : loss: 0.017558, loss_a: 0.010328
[23:50:57.521] iteration 9619 : loss: 0.294151, loss_a: 0.173030
[23:50:58.260] iteration 9620 : loss: 0.029653, loss_a: 0.017443
[23:50:59.615] iteration 9621 : loss: 0.032320, loss_a: 0.019012
[23:51:00.360] iteration 9622 : loss: 0.037136, loss_a: 0.021844
[23:51:01.698] iteration 9623 : loss: 0.029348, loss_a: 0.017264
[23:51:02.446] iteration 9624 : loss: 0.021516, loss_a: 0.012656
[23:51:03.817] iteration 9625 : loss: 0.027179, loss_a: 0.015988
[23:51:04.569] iteration 9626 : loss: 0.053940, loss_a: 0.031729
[23:51:05.900] iteration 9627 : loss: 0.018674, loss_a: 0.010985
[23:51:06.632] iteration 9628 : loss: 0.017364, loss_a: 0.010214
[23:51:07.962] iteration 9629 : loss: 0.015459, loss_a: 0.009094
[23:51:08.710] iteration 9630 : loss: 0.030751, loss_a: 0.018089
[23:51:10.062] iteration 9631 : loss: 0.021142, loss_a: 0.012436
[23:51:10.806] iteration 9632 : loss: 0.031889, loss_a: 0.018758
[23:51:12.138] iteration 9633 : loss: 0.020304, loss_a: 0.011944
[23:51:12.877] iteration 9634 : loss: 0.029081, loss_a: 0.017106
[23:51:14.192] iteration 9635 : loss: 0.027036, loss_a: 0.015903
[23:51:14.950] iteration 9636 : loss: 0.031295, loss_a: 0.018409
[23:51:16.296] iteration 9637 : loss: 0.046286, loss_a: 0.027227
[23:51:17.022] iteration 9638 : loss: 0.024182, loss_a: 0.014225
[23:51:18.374] iteration 9639 : loss: 0.021669, loss_a: 0.012747
[23:51:19.117] iteration 9640 : loss: 0.014747, loss_a: 0.008675
[23:51:20.471] iteration 9641 : loss: 0.039555, loss_a: 0.023267
[23:51:21.221] iteration 9642 : loss: 0.021948, loss_a: 0.012911
[23:51:22.575] iteration 9643 : loss: 0.074878, loss_a: 0.044046
[23:51:23.320] iteration 9644 : loss: 0.024126, loss_a: 0.014192
[23:51:24.678] iteration 9645 : loss: 0.043974, loss_a: 0.025867
[23:51:25.406] iteration 9646 : loss: 0.017558, loss_a: 0.010328
[23:51:26.762] iteration 9647 : loss: 0.025145, loss_a: 0.014791
[23:51:27.499] iteration 9648 : loss: 0.031339, loss_a: 0.018435
[23:51:28.801] iteration 9649 : loss: 0.018444, loss_a: 0.010849
[23:51:29.540] iteration 9650 : loss: 0.035661, loss_a: 0.020977
[23:51:30.886] iteration 9651 : loss: 0.041017, loss_a: 0.024127
[23:51:31.619] iteration 9652 : loss: 0.022991, loss_a: 0.013524
[23:51:32.967] iteration 9653 : loss: 0.013599, loss_a: 0.008000
[23:51:33.703] iteration 9654 : loss: 0.034434, loss_a: 0.020255
[23:51:35.014] iteration 9655 : loss: 0.019506, loss_a: 0.011474
[23:51:35.760] iteration 9656 : loss: 0.019894, loss_a: 0.011702
[23:51:37.104] iteration 9657 : loss: 0.025750, loss_a: 0.015147
[23:51:37.843] iteration 9658 : loss: 0.017603, loss_a: 0.010355
[23:51:39.187] iteration 9659 : loss: 0.033039, loss_a: 0.019435
[23:51:39.935] iteration 9660 : loss: 0.021611, loss_a: 0.012713
[23:51:41.271] iteration 9661 : loss: 0.010054, loss_a: 0.005914
[23:51:42.018] iteration 9662 : loss: 0.042147, loss_a: 0.024793
[23:51:43.340] iteration 9663 : loss: 0.034551, loss_a: 0.020324
[23:51:44.079] iteration 9664 : loss: 0.022460, loss_a: 0.013212
[23:51:45.398] iteration 9665 : loss: 0.019982, loss_a: 0.011754
[23:51:46.139] iteration 9666 : loss: 0.087934, loss_a: 0.051726
[23:51:47.461] iteration 9667 : loss: 0.034336, loss_a: 0.020198
[23:51:48.201] iteration 9668 : loss: 0.028676, loss_a: 0.016868
[23:51:49.533] iteration 9669 : loss: 0.032405, loss_a: 0.019062
[23:51:50.290] iteration 9670 : loss: 0.041533, loss_a: 0.024431
[23:51:51.636] iteration 9671 : loss: 0.023625, loss_a: 0.013897
[23:51:52.374] iteration 9672 : loss: 0.017655, loss_a: 0.010385
[23:51:53.698] iteration 9673 : loss: 0.013990, loss_a: 0.008230
[23:51:54.445] iteration 9674 : loss: 0.026390, loss_a: 0.015523
[23:51:55.789] iteration 9675 : loss: 0.047230, loss_a: 0.027783
[23:51:56.526] iteration 9676 : loss: 0.034483, loss_a: 0.020284
[23:51:57.850] iteration 9677 : loss: 0.032944, loss_a: 0.019379
[23:51:58.579] iteration 9678 : loss: 0.038832, loss_a: 0.022842
[23:51:59.905] iteration 9679 : loss: 0.032662, loss_a: 0.019213
[23:52:00.646] iteration 9680 : loss: 0.024601, loss_a: 0.014471
[23:52:01.979] iteration 9681 : loss: 0.014798, loss_a: 0.008705
[23:52:02.722] iteration 9682 : loss: 0.017219, loss_a: 0.010129
[23:52:04.042] iteration 9683 : loss: 0.020572, loss_a: 0.012101
[23:52:04.783] iteration 9684 : loss: 0.029047, loss_a: 0.017086
[23:52:06.129] iteration 9685 : loss: 0.018284, loss_a: 0.010755
[23:52:06.862] iteration 9686 : loss: 0.021056, loss_a: 0.012386
[23:52:08.247] iteration 9687 : loss: 0.023393, loss_a: 0.013761
[23:52:08.986] iteration 9688 : loss: 0.022699, loss_a: 0.013352
[23:52:10.345] iteration 9689 : loss: 0.027014, loss_a: 0.015890
[23:52:11.089] iteration 9690 : loss: 0.050866, loss_a: 0.029921
[23:52:12.439] iteration 9691 : loss: 0.055920, loss_a: 0.032894
[23:52:13.184] iteration 9692 : loss: 0.025802, loss_a: 0.015178
[23:52:14.513] iteration 9693 : loss: 0.020455, loss_a: 0.012032
[23:52:15.259] iteration 9694 : loss: 0.025490, loss_a: 0.014994
[23:52:16.608] iteration 9695 : loss: 0.024211, loss_a: 0.014242
[23:52:17.350] iteration 9696 : loss: 0.032838, loss_a: 0.019316
[23:52:18.672] iteration 9697 : loss: 0.020333, loss_a: 0.011961
[23:52:19.428] iteration 9698 : loss: 0.029071, loss_a: 0.017101
[23:52:20.757] iteration 9699 : loss: 0.038872, loss_a: 0.022866
[23:52:21.511] iteration 9700 : loss: 0.043854, loss_a: 0.025796
[23:52:22.816] iteration 9701 : loss: 0.013918, loss_a: 0.008187
[23:52:23.552] iteration 9702 : loss: 0.016649, loss_a: 0.009794
[23:52:24.915] iteration 9703 : loss: 0.040278, loss_a: 0.023693
[23:52:25.658] iteration 9704 : loss: 0.024865, loss_a: 0.014626
[23:52:26.994] iteration 9705 : loss: 0.024269, loss_a: 0.014276
[23:52:27.734] iteration 9706 : loss: 0.022102, loss_a: 0.013001
[23:52:29.114] iteration 9707 : loss: 0.025436, loss_a: 0.014962
[23:52:29.865] iteration 9708 : loss: 0.023006, loss_a: 0.013533
[23:52:31.210] iteration 9709 : loss: 0.027958, loss_a: 0.016446
[23:52:31.951] iteration 9710 : loss: 0.025354, loss_a: 0.014914
[23:52:33.277] iteration 9711 : loss: 0.045808, loss_a: 0.026946
[23:52:34.011] iteration 9712 : loss: 0.063490, loss_a: 0.037347
[23:52:35.355] iteration 9713 : loss: 0.037918, loss_a: 0.022305
[23:52:36.103] iteration 9714 : loss: 0.029629, loss_a: 0.017429
[23:52:37.451] iteration 9715 : loss: 0.021697, loss_a: 0.012763
[23:52:38.185] iteration 9716 : loss: 0.050992, loss_a: 0.029995
[23:52:39.526] iteration 9717 : loss: 0.038881, loss_a: 0.022871
[23:52:40.263] iteration 9718 : loss: 0.016618, loss_a: 0.009775
[23:52:41.630] iteration 9719 : loss: 0.017671, loss_a: 0.010395
[23:52:42.372] iteration 9720 : loss: 0.025455, loss_a: 0.014973
[23:52:43.744] iteration 9721 : loss: 0.035799, loss_a: 0.021058
[23:52:44.487] iteration 9722 : loss: 0.014159, loss_a: 0.008329
[23:52:45.817] iteration 9723 : loss: 0.042466, loss_a: 0.024980
[23:52:46.555] iteration 9724 : loss: 0.042012, loss_a: 0.024713
[23:52:47.880] iteration 9725 : loss: 0.015164, loss_a: 0.008920
[23:52:48.623] iteration 9726 : loss: 0.023265, loss_a: 0.013685
[23:52:49.946] iteration 9727 : loss: 0.020298, loss_a: 0.011940
[23:52:50.686] iteration 9728 : loss: 0.049870, loss_a: 0.029335
[23:52:52.051] iteration 9729 : loss: 0.072449, loss_a: 0.042617
[23:52:52.782] iteration 9730 : loss: 0.042974, loss_a: 0.025279
[23:52:54.118] iteration 9731 : loss: 0.029326, loss_a: 0.017251
[23:52:54.854] iteration 9732 : loss: 0.021495, loss_a: 0.012644
[23:52:56.178] iteration 9733 : loss: 0.013725, loss_a: 0.008074
[23:52:56.912] iteration 9734 : loss: 0.013901, loss_a: 0.008177
[23:52:58.278] iteration 9735 : loss: 0.043937, loss_a: 0.025845
[23:52:59.014] iteration 9736 : loss: 0.020592, loss_a: 0.012113
[23:53:00.344] iteration 9737 : loss: 0.050453, loss_a: 0.029678
[23:53:01.100] iteration 9738 : loss: 0.027554, loss_a: 0.016208
[23:53:02.490] iteration 9739 : loss: 0.041470, loss_a: 0.024394
[23:53:03.229] iteration 9740 : loss: 0.025475, loss_a: 0.014985
[23:53:04.542] iteration 9741 : loss: 0.042399, loss_a: 0.024941
[23:53:05.291] iteration 9742 : loss: 0.018547, loss_a: 0.010910
[23:53:06.668] iteration 9743 : loss: 0.026137, loss_a: 0.015375
[23:53:07.402] iteration 9744 : loss: 0.032368, loss_a: 0.019040
[23:53:08.750] iteration 9745 : loss: 0.022286, loss_a: 0.013110
[23:53:09.510] iteration 9746 : loss: 0.025155, loss_a: 0.014797
[23:53:10.886] iteration 9747 : loss: 0.034578, loss_a: 0.020340
[23:53:11.630] iteration 9748 : loss: 0.038704, loss_a: 0.022767
[23:53:12.995] iteration 9749 : loss: 0.024812, loss_a: 0.014595
[23:53:13.730] iteration 9750 : loss: 0.024226, loss_a: 0.014250
[23:53:15.078] iteration 9751 : loss: 0.022230, loss_a: 0.013077
[23:53:15.812] iteration 9752 : loss: 0.013877, loss_a: 0.008163
[23:53:17.164] iteration 9753 : loss: 0.036819, loss_a: 0.021658
[23:53:17.904] iteration 9754 : loss: 0.038022, loss_a: 0.022366
[23:53:19.241] iteration 9755 : loss: 0.035789, loss_a: 0.021052
[23:53:19.980] iteration 9756 : loss: 0.021556, loss_a: 0.012680
[23:53:21.315] iteration 9757 : loss: 0.025899, loss_a: 0.015235
[23:53:22.062] iteration 9758 : loss: 0.032197, loss_a: 0.018939
[23:53:23.397] iteration 9759 : loss: 0.033650, loss_a: 0.019794
[23:53:24.139] iteration 9760 : loss: 0.028029, loss_a: 0.016488
[23:53:25.477] iteration 9761 : loss: 0.056547, loss_a: 0.033263
[23:53:26.226] iteration 9762 : loss: 0.015802, loss_a: 0.009295
[23:53:27.566] iteration 9763 : loss: 0.026663, loss_a: 0.015684
[23:53:28.312] iteration 9764 : loss: 0.020329, loss_a: 0.011958
[23:53:29.644] iteration 9765 : loss: 0.027672, loss_a: 0.016278
[23:53:30.387] iteration 9766 : loss: 0.025560, loss_a: 0.015035
[23:53:31.729] iteration 9767 : loss: 0.027262, loss_a: 0.016036
[23:53:32.465] iteration 9768 : loss: 0.014412, loss_a: 0.008478
[23:53:33.805] iteration 9769 : loss: 0.015432, loss_a: 0.009078
[23:53:34.537] iteration 9770 : loss: 0.014857, loss_a: 0.008740
[23:53:35.872] iteration 9771 : loss: 0.019079, loss_a: 0.011223
[23:53:36.614] iteration 9772 : loss: 0.022432, loss_a: 0.013196
[23:53:37.927] iteration 9773 : loss: 0.014994, loss_a: 0.008820
[23:53:38.672] iteration 9774 : loss: 0.026547, loss_a: 0.015616
[23:53:40.028] iteration 9775 : loss: 0.024488, loss_a: 0.014405
[23:53:40.762] iteration 9776 : loss: 0.017743, loss_a: 0.010437
[23:53:42.133] iteration 9777 : loss: 0.039346, loss_a: 0.023145
[23:53:42.888] iteration 9778 : loss: 0.061637, loss_a: 0.036257
[23:53:44.217] iteration 9779 : loss: 0.016397, loss_a: 0.009645
[23:53:44.969] iteration 9780 : loss: 0.046146, loss_a: 0.027145
[23:53:46.296] iteration 9781 : loss: 0.019841, loss_a: 0.011671
[23:53:47.043] iteration 9782 : loss: 0.040408, loss_a: 0.023770
[23:53:48.412] iteration 9783 : loss: 0.044114, loss_a: 0.025950
[23:53:49.146] iteration 9784 : loss: 0.020060, loss_a: 0.011800
[23:53:50.491] iteration 9785 : loss: 0.013712, loss_a: 0.008066
[23:53:51.232] iteration 9786 : loss: 0.026566, loss_a: 0.015627
[23:53:52.565] iteration 9787 : loss: 0.022317, loss_a: 0.013128
[23:53:53.302] iteration 9788 : loss: 0.026383, loss_a: 0.015519
[23:53:54.648] iteration 9789 : loss: 0.016055, loss_a: 0.009444
[23:53:55.394] iteration 9790 : loss: 0.024799, loss_a: 0.014588
[23:53:56.744] iteration 9791 : loss: 0.062712, loss_a: 0.036889
[23:53:57.499] iteration 9792 : loss: 0.030029, loss_a: 0.017664
[23:53:58.858] iteration 9793 : loss: 0.017791, loss_a: 0.010465
[23:53:59.602] iteration 9794 : loss: 0.017951, loss_a: 0.010559
[23:54:00.950] iteration 9795 : loss: 0.020304, loss_a: 0.011943
[23:54:01.698] iteration 9796 : loss: 0.047315, loss_a: 0.027833
[23:54:03.038] iteration 9797 : loss: 0.046859, loss_a: 0.027564
[23:54:03.777] iteration 9798 : loss: 0.034600, loss_a: 0.020353
[23:54:05.127] iteration 9799 : loss: 0.019381, loss_a: 0.011401
[23:54:05.867] iteration 9800 : loss: 0.015996, loss_a: 0.009409
[23:54:30.530] iteration 9801 : loss: 0.031024, loss_a: 0.018250
[23:54:32.822] iteration 9802 : loss: 0.029040, loss_a: 0.017082
[23:54:34.147] iteration 9803 : loss: 0.013628, loss_a: 0.008016
[23:54:34.885] iteration 9804 : loss: 0.030750, loss_a: 0.018088
[23:54:36.242] iteration 9805 : loss: 0.062872, loss_a: 0.036984
[23:54:36.985] iteration 9806 : loss: 0.017957, loss_a: 0.010563
[23:54:38.320] iteration 9807 : loss: 0.034234, loss_a: 0.020138
[23:54:39.073] iteration 9808 : loss: 0.033844, loss_a: 0.019908
[23:54:40.417] iteration 9809 : loss: 0.016927, loss_a: 0.009957
[23:54:41.153] iteration 9810 : loss: 0.024415, loss_a: 0.014362
[23:54:42.503] iteration 9811 : loss: 0.028251, loss_a: 0.016618
[23:54:43.241] iteration 9812 : loss: 0.024256, loss_a: 0.014268
[23:54:44.591] iteration 9813 : loss: 0.035491, loss_a: 0.020877
[23:54:45.335] iteration 9814 : loss: 0.027129, loss_a: 0.015958
[23:54:46.719] iteration 9815 : loss: 0.044483, loss_a: 0.026167
[23:54:47.461] iteration 9816 : loss: 0.026274, loss_a: 0.015455
[23:54:48.817] iteration 9817 : loss: 0.023719, loss_a: 0.013952
[23:54:49.555] iteration 9818 : loss: 0.031541, loss_a: 0.018553
[23:54:50.912] iteration 9819 : loss: 0.015568, loss_a: 0.009158
[23:54:51.674] iteration 9820 : loss: 0.029991, loss_a: 0.017642
[23:54:53.018] iteration 9821 : loss: 0.020292, loss_a: 0.011936
[23:54:53.747] iteration 9822 : loss: 0.028412, loss_a: 0.016713
[23:54:55.096] iteration 9823 : loss: 0.038011, loss_a: 0.022360
[23:54:55.835] iteration 9824 : loss: 0.025696, loss_a: 0.015115
[23:54:57.189] iteration 9825 : loss: 0.019089, loss_a: 0.011229
[23:54:57.921] iteration 9826 : loss: 0.022511, loss_a: 0.013242
[23:54:59.249] iteration 9827 : loss: 0.017806, loss_a: 0.010474
[23:54:59.985] iteration 9828 : loss: 0.021786, loss_a: 0.012815
[23:55:01.347] iteration 9829 : loss: 0.031581, loss_a: 0.018577
[23:55:02.088] iteration 9830 : loss: 0.016680, loss_a: 0.009812
[23:55:03.408] iteration 9831 : loss: 0.016543, loss_a: 0.009731
[23:55:04.149] iteration 9832 : loss: 0.041530, loss_a: 0.024429
[23:55:05.497] iteration 9833 : loss: 0.024059, loss_a: 0.014152
[23:55:06.241] iteration 9834 : loss: 0.043337, loss_a: 0.025492
[23:55:07.592] iteration 9835 : loss: 0.037591, loss_a: 0.022112
[23:55:08.335] iteration 9836 : loss: 0.021826, loss_a: 0.012839
[23:55:09.653] iteration 9837 : loss: 0.045389, loss_a: 0.026699
[23:55:10.401] iteration 9838 : loss: 0.019928, loss_a: 0.011722
[23:55:11.724] iteration 9839 : loss: 0.024968, loss_a: 0.014687
[23:55:12.472] iteration 9840 : loss: 0.063719, loss_a: 0.037482
[23:55:13.808] iteration 9841 : loss: 0.023307, loss_a: 0.013710
[23:55:14.543] iteration 9842 : loss: 0.015210, loss_a: 0.008947
[23:55:15.887] iteration 9843 : loss: 0.028607, loss_a: 0.016827
[23:55:16.634] iteration 9844 : loss: 0.020874, loss_a: 0.012279
[23:55:17.958] iteration 9845 : loss: 0.022599, loss_a: 0.013294
[23:55:18.693] iteration 9846 : loss: 0.020473, loss_a: 0.012043
[23:55:20.031] iteration 9847 : loss: 0.014356, loss_a: 0.008445
[23:55:20.769] iteration 9848 : loss: 0.023101, loss_a: 0.013589
[23:55:22.114] iteration 9849 : loss: 0.019785, loss_a: 0.011638
[23:55:22.869] iteration 9850 : loss: 0.030510, loss_a: 0.017947
[23:55:24.195] iteration 9851 : loss: 0.019198, loss_a: 0.011293
[23:55:24.927] iteration 9852 : loss: 0.022224, loss_a: 0.013073
[23:55:26.252] iteration 9853 : loss: 0.017463, loss_a: 0.010272
[23:55:26.986] iteration 9854 : loss: 0.025947, loss_a: 0.015263
[23:55:28.332] iteration 9855 : loss: 0.019274, loss_a: 0.011337
[23:55:29.071] iteration 9856 : loss: 0.016459, loss_a: 0.009682
[23:55:30.399] iteration 9857 : loss: 0.016565, loss_a: 0.009744
[23:55:31.137] iteration 9858 : loss: 0.028248, loss_a: 0.016616
[23:55:32.472] iteration 9859 : loss: 0.047505, loss_a: 0.027944
[23:55:33.223] iteration 9860 : loss: 0.034650, loss_a: 0.020382
[23:55:34.552] iteration 9861 : loss: 0.028540, loss_a: 0.016788
[23:55:35.296] iteration 9862 : loss: 0.052149, loss_a: 0.030676
[23:55:36.613] iteration 9863 : loss: 0.023650, loss_a: 0.013912
[23:55:37.347] iteration 9864 : loss: 0.036997, loss_a: 0.021763
[23:55:38.700] iteration 9865 : loss: 0.024372, loss_a: 0.014337
[23:55:39.448] iteration 9866 : loss: 0.031859, loss_a: 0.018741
[23:55:40.791] iteration 9867 : loss: 0.015614, loss_a: 0.009185
[23:55:41.530] iteration 9868 : loss: 0.019131, loss_a: 0.011253
[23:55:42.903] iteration 9869 : loss: 0.045704, loss_a: 0.026885
[23:55:43.657] iteration 9870 : loss: 0.039840, loss_a: 0.023435
[23:55:44.990] iteration 9871 : loss: 0.028113, loss_a: 0.016537
[23:55:45.737] iteration 9872 : loss: 0.040463, loss_a: 0.023802
[23:55:47.055] iteration 9873 : loss: 0.019543, loss_a: 0.011496
[23:55:47.797] iteration 9874 : loss: 0.028257, loss_a: 0.016622
[23:55:49.134] iteration 9875 : loss: 0.050135, loss_a: 0.029491
[23:55:49.877] iteration 9876 : loss: 0.015534, loss_a: 0.009137
[23:55:51.211] iteration 9877 : loss: 0.025863, loss_a: 0.015213
[23:55:51.954] iteration 9878 : loss: 0.028028, loss_a: 0.016487
[23:55:53.311] iteration 9879 : loss: 0.023014, loss_a: 0.013538
[23:55:54.062] iteration 9880 : loss: 0.069132, loss_a: 0.040666
[23:55:55.374] iteration 9881 : loss: 0.025866, loss_a: 0.015216
[23:55:56.123] iteration 9882 : loss: 0.021206, loss_a: 0.012474
[23:55:57.461] iteration 9883 : loss: 0.060757, loss_a: 0.035740
[23:55:58.210] iteration 9884 : loss: 0.041821, loss_a: 0.024601
[23:55:59.574] iteration 9885 : loss: 0.031600, loss_a: 0.018588
[23:56:00.323] iteration 9886 : loss: 0.057464, loss_a: 0.033802
[23:56:01.693] iteration 9887 : loss: 0.025621, loss_a: 0.015071
[23:56:02.438] iteration 9888 : loss: 0.025358, loss_a: 0.014916
[23:56:03.754] iteration 9889 : loss: 0.019503, loss_a: 0.011473
[23:56:04.505] iteration 9890 : loss: 0.032624, loss_a: 0.019191
[23:56:05.839] iteration 9891 : loss: 0.030089, loss_a: 0.017699
[23:56:06.579] iteration 9892 : loss: 0.017365, loss_a: 0.010215
[23:56:07.926] iteration 9893 : loss: 0.030686, loss_a: 0.018050
[23:56:08.669] iteration 9894 : loss: 0.040384, loss_a: 0.023755
[23:56:10.004] iteration 9895 : loss: 0.047934, loss_a: 0.028197
[23:56:10.745] iteration 9896 : loss: 0.031999, loss_a: 0.018823
[23:56:12.089] iteration 9897 : loss: 0.029635, loss_a: 0.017432
[23:56:12.839] iteration 9898 : loss: 0.020556, loss_a: 0.012092
[23:56:14.153] iteration 9899 : loss: 0.038840, loss_a: 0.022847
[23:56:14.887] iteration 9900 : loss: 0.020006, loss_a: 0.011768
[23:56:16.242] iteration 9901 : loss: 0.049636, loss_a: 0.029198
[23:56:16.980] iteration 9902 : loss: 0.022576, loss_a: 0.013280
[23:56:18.328] iteration 9903 : loss: 0.026165, loss_a: 0.015391
[23:56:19.085] iteration 9904 : loss: 0.021858, loss_a: 0.012857
[23:56:20.440] iteration 9905 : loss: 0.013223, loss_a: 0.007778
[23:56:21.176] iteration 9906 : loss: 0.033954, loss_a: 0.019973
[23:56:22.492] iteration 9907 : loss: 0.017432, loss_a: 0.010254
[23:56:23.231] iteration 9908 : loss: 0.014421, loss_a: 0.008483
[23:56:24.575] iteration 9909 : loss: 0.035007, loss_a: 0.020592
[23:56:25.313] iteration 9910 : loss: 0.028087, loss_a: 0.016522
[23:56:26.676] iteration 9911 : loss: 0.024813, loss_a: 0.014596
[23:56:27.411] iteration 9912 : loss: 0.023946, loss_a: 0.014086
[23:56:28.724] iteration 9913 : loss: 0.022514, loss_a: 0.013244
[23:56:29.474] iteration 9914 : loss: 0.054327, loss_a: 0.031957
[23:56:30.832] iteration 9915 : loss: 0.024288, loss_a: 0.014287
[23:56:31.576] iteration 9916 : loss: 0.029587, loss_a: 0.017404
[23:56:32.939] iteration 9917 : loss: 0.046657, loss_a: 0.027445
[23:56:33.675] iteration 9918 : loss: 0.027458, loss_a: 0.016152
[23:56:35.031] iteration 9919 : loss: 0.024710, loss_a: 0.014536
[23:56:35.769] iteration 9920 : loss: 0.042267, loss_a: 0.024863
[23:56:37.126] iteration 9921 : loss: 0.023421, loss_a: 0.013777
[23:56:37.866] iteration 9922 : loss: 0.055666, loss_a: 0.032745
[23:56:39.188] iteration 9923 : loss: 0.049355, loss_a: 0.029033
[23:56:39.936] iteration 9924 : loss: 0.021398, loss_a: 0.012587
[23:56:41.253] iteration 9925 : loss: 0.026114, loss_a: 0.015361
[23:56:41.996] iteration 9926 : loss: 0.020341, loss_a: 0.011965
[23:56:43.357] iteration 9927 : loss: 0.021077, loss_a: 0.012398
[23:56:44.100] iteration 9928 : loss: 0.018233, loss_a: 0.010726
[23:56:45.462] iteration 9929 : loss: 0.070642, loss_a: 0.041554
[23:56:46.191] iteration 9930 : loss: 0.035636, loss_a: 0.020962
[23:56:47.552] iteration 9931 : loss: 0.019898, loss_a: 0.011705
[23:56:48.304] iteration 9932 : loss: 0.036942, loss_a: 0.021731
[23:56:49.647] iteration 9933 : loss: 0.037207, loss_a: 0.021887
[23:56:50.396] iteration 9934 : loss: 0.027143, loss_a: 0.015966
[23:56:51.721] iteration 9935 : loss: 0.029883, loss_a: 0.017578
[23:56:52.471] iteration 9936 : loss: 0.036394, loss_a: 0.021408
[23:56:53.789] iteration 9937 : loss: 0.018942, loss_a: 0.011142
[23:56:54.526] iteration 9938 : loss: 0.026200, loss_a: 0.015412
[23:56:55.850] iteration 9939 : loss: 0.014111, loss_a: 0.008301
[23:56:56.583] iteration 9940 : loss: 0.015258, loss_a: 0.008975
[23:56:57.916] iteration 9941 : loss: 0.026893, loss_a: 0.015819
[23:56:58.660] iteration 9942 : loss: 0.047431, loss_a: 0.027901
[23:57:00.009] iteration 9943 : loss: 0.025107, loss_a: 0.014769
[23:57:00.759] iteration 9944 : loss: 0.052493, loss_a: 0.030878
[23:57:02.095] iteration 9945 : loss: 0.027895, loss_a: 0.016409
[23:57:02.836] iteration 9946 : loss: 0.057895, loss_a: 0.034056
[23:57:04.175] iteration 9947 : loss: 0.042856, loss_a: 0.025209
[23:57:04.910] iteration 9948 : loss: 0.014700, loss_a: 0.008647
[23:57:06.250] iteration 9949 : loss: 0.017636, loss_a: 0.010374
[23:57:06.984] iteration 9950 : loss: 0.012968, loss_a: 0.007628
[23:57:08.314] iteration 9951 : loss: 0.021823, loss_a: 0.012837
[23:57:09.056] iteration 9952 : loss: 0.019853, loss_a: 0.011678
[23:57:10.409] iteration 9953 : loss: 0.053887, loss_a: 0.031698
[23:57:11.160] iteration 9954 : loss: 0.011617, loss_a: 0.006834
[23:57:12.521] iteration 9955 : loss: 0.037415, loss_a: 0.022009
[23:57:13.257] iteration 9956 : loss: 0.024397, loss_a: 0.014351
[23:57:14.600] iteration 9957 : loss: 0.019631, loss_a: 0.011548
[23:57:15.342] iteration 9958 : loss: 0.026967, loss_a: 0.015863
[23:57:16.688] iteration 9959 : loss: 0.050221, loss_a: 0.029542
[23:57:17.431] iteration 9960 : loss: 0.031349, loss_a: 0.018440
[23:57:18.759] iteration 9961 : loss: 0.013788, loss_a: 0.008111
[23:57:19.506] iteration 9962 : loss: 0.034275, loss_a: 0.020162
[23:57:20.819] iteration 9963 : loss: 0.011645, loss_a: 0.006850
[23:57:21.555] iteration 9964 : loss: 0.022929, loss_a: 0.013488
[23:57:22.913] iteration 9965 : loss: 0.021937, loss_a: 0.012904
[23:57:23.652] iteration 9966 : loss: 0.022906, loss_a: 0.013474
[23:57:24.986] iteration 9967 : loss: 0.027202, loss_a: 0.016001
[23:57:25.721] iteration 9968 : loss: 0.014009, loss_a: 0.008241
[23:57:27.073] iteration 9969 : loss: 0.037842, loss_a: 0.022260
[23:57:27.822] iteration 9970 : loss: 0.029146, loss_a: 0.017145
[23:57:29.185] iteration 9971 : loss: 0.022559, loss_a: 0.013270
[23:57:29.924] iteration 9972 : loss: 0.029133, loss_a: 0.017137
[23:57:31.292] iteration 9973 : loss: 0.057780, loss_a: 0.033988
[23:57:32.026] iteration 9974 : loss: 0.018376, loss_a: 0.010810
[23:57:33.377] iteration 9975 : loss: 0.065394, loss_a: 0.038467
[23:57:34.123] iteration 9976 : loss: 0.030297, loss_a: 0.017822
[23:57:35.446] iteration 9977 : loss: 0.031827, loss_a: 0.018722
[23:57:36.184] iteration 9978 : loss: 0.026376, loss_a: 0.015515
[23:57:37.519] iteration 9979 : loss: 0.019986, loss_a: 0.011757
[23:57:38.265] iteration 9980 : loss: 0.019547, loss_a: 0.011498
[23:57:39.573] iteration 9981 : loss: 0.034897, loss_a: 0.020527
[23:57:40.314] iteration 9982 : loss: 0.022550, loss_a: 0.013265
[23:57:41.649] iteration 9983 : loss: 0.023382, loss_a: 0.013754
[23:57:42.396] iteration 9984 : loss: 0.045646, loss_a: 0.026850
[23:57:43.738] iteration 9985 : loss: 0.094213, loss_a: 0.055419
[23:57:44.471] iteration 9986 : loss: 0.023543, loss_a: 0.013849
[23:57:45.805] iteration 9987 : loss: 0.022107, loss_a: 0.013004
[23:57:46.545] iteration 9988 : loss: 0.039245, loss_a: 0.023086
[23:57:47.903] iteration 9989 : loss: 0.032815, loss_a: 0.019303
[23:57:48.641] iteration 9990 : loss: 0.020596, loss_a: 0.012115
[23:57:49.977] iteration 9991 : loss: 0.017671, loss_a: 0.010394
[23:57:50.711] iteration 9992 : loss: 0.014781, loss_a: 0.008695
[23:57:52.081] iteration 9993 : loss: 0.039692, loss_a: 0.023348
[23:57:52.817] iteration 9994 : loss: 0.018291, loss_a: 0.010760
[23:57:54.128] iteration 9995 : loss: 0.041979, loss_a: 0.024694
[23:57:54.872] iteration 9996 : loss: 0.038777, loss_a: 0.022810
[23:57:56.211] iteration 9997 : loss: 0.021601, loss_a: 0.012706
[23:57:56.960] iteration 9998 : loss: 0.043882, loss_a: 0.025813
[23:57:58.331] iteration 9999 : loss: 0.045956, loss_a: 0.027033
[23:57:59.070] iteration 10000 : loss: 0.024974, loss_a: 0.014691
[23:58:23.739] iteration 10001 : loss: 0.027613, loss_a: 0.016243
[23:58:25.948] iteration 10002 : loss: 0.026075, loss_a: 0.015339
[23:58:27.282] iteration 10003 : loss: 0.018937, loss_a: 0.011140
[23:58:28.018] iteration 10004 : loss: 0.023202, loss_a: 0.013648
[23:58:29.389] iteration 10005 : loss: 0.054699, loss_a: 0.032176
[23:58:30.128] iteration 10006 : loss: 0.024202, loss_a: 0.014237
[23:58:31.485] iteration 10007 : loss: 0.018798, loss_a: 0.011058
[23:58:32.223] iteration 10008 : loss: 0.032426, loss_a: 0.019074
[23:58:33.541] iteration 10009 : loss: 0.015844, loss_a: 0.009320
[23:58:34.277] iteration 10010 : loss: 0.019483, loss_a: 0.011461
[23:58:35.635] iteration 10011 : loss: 0.018934, loss_a: 0.011138
[23:58:36.367] iteration 10012 : loss: 0.022520, loss_a: 0.013247
[23:58:37.721] iteration 10013 : loss: 0.030314, loss_a: 0.017832
[23:58:38.458] iteration 10014 : loss: 0.014378, loss_a: 0.008458
[23:58:39.798] iteration 10015 : loss: 0.027136, loss_a: 0.015962
[23:58:40.533] iteration 10016 : loss: 0.009620, loss_a: 0.005659
[23:58:41.887] iteration 10017 : loss: 0.012376, loss_a: 0.007280
[23:58:42.623] iteration 10018 : loss: 0.028619, loss_a: 0.016835
[23:58:43.983] iteration 10019 : loss: 0.031059, loss_a: 0.018270
[23:58:44.726] iteration 10020 : loss: 0.023392, loss_a: 0.013760
[23:58:46.076] iteration 10021 : loss: 0.021153, loss_a: 0.012443
[23:58:46.812] iteration 10022 : loss: 0.029113, loss_a: 0.017125
[23:58:48.137] iteration 10023 : loss: 0.020506, loss_a: 0.012062
[23:58:48.878] iteration 10024 : loss: 0.042802, loss_a: 0.025178
[23:58:50.209] iteration 10025 : loss: 0.047980, loss_a: 0.028224
[23:58:50.951] iteration 10026 : loss: 0.026479, loss_a: 0.015576
[23:58:52.273] iteration 10027 : loss: 0.030710, loss_a: 0.018065
[23:58:53.020] iteration 10028 : loss: 0.038118, loss_a: 0.022423
[23:58:54.374] iteration 10029 : loss: 0.026971, loss_a: 0.015866
[23:58:55.105] iteration 10030 : loss: 0.015833, loss_a: 0.009313
[23:58:56.450] iteration 10031 : loss: 0.028519, loss_a: 0.016776
[23:58:57.183] iteration 10032 : loss: 0.042057, loss_a: 0.024739
[23:58:58.499] iteration 10033 : loss: 0.044592, loss_a: 0.026230
[23:58:59.232] iteration 10034 : loss: 0.019406, loss_a: 0.011415
[23:59:00.577] iteration 10035 : loss: 0.053494, loss_a: 0.031467
[23:59:01.311] iteration 10036 : loss: 0.016632, loss_a: 0.009783
[23:59:02.650] iteration 10037 : loss: 0.041371, loss_a: 0.024336
[23:59:03.398] iteration 10038 : loss: 0.032330, loss_a: 0.019018
[23:59:04.707] iteration 10039 : loss: 0.028586, loss_a: 0.016815
[23:59:05.447] iteration 10040 : loss: 0.039469, loss_a: 0.023217
[23:59:06.797] iteration 10041 : loss: 0.023315, loss_a: 0.013715
[23:59:07.537] iteration 10042 : loss: 0.033702, loss_a: 0.019825
[23:59:08.888] iteration 10043 : loss: 0.019885, loss_a: 0.011697
[23:59:09.624] iteration 10044 : loss: 0.037129, loss_a: 0.021840
[23:59:10.977] iteration 10045 : loss: 0.023807, loss_a: 0.014004
[23:59:11.718] iteration 10046 : loss: 0.033692, loss_a: 0.019819
[23:59:13.063] iteration 10047 : loss: 0.024580, loss_a: 0.014459
[23:59:13.798] iteration 10048 : loss: 0.023208, loss_a: 0.013652
[23:59:15.150] iteration 10049 : loss: 0.029376, loss_a: 0.017280
[23:59:15.888] iteration 10050 : loss: 0.045956, loss_a: 0.027033
[23:59:17.240] iteration 10051 : loss: 0.018724, loss_a: 0.011014
[23:59:17.984] iteration 10052 : loss: 0.022019, loss_a: 0.012953
[23:59:19.379] iteration 10053 : loss: 0.042109, loss_a: 0.024770
[23:59:20.114] iteration 10054 : loss: 0.015691, loss_a: 0.009230
[23:59:21.447] iteration 10055 : loss: 0.022046, loss_a: 0.012968
[23:59:22.200] iteration 10056 : loss: 0.025848, loss_a: 0.015205
[23:59:23.522] iteration 10057 : loss: 0.024159, loss_a: 0.014211
[23:59:24.259] iteration 10058 : loss: 0.021010, loss_a: 0.012359
[23:59:25.583] iteration 10059 : loss: 0.019641, loss_a: 0.011553
[23:59:26.329] iteration 10060 : loss: 0.026635, loss_a: 0.015668
[23:59:27.651] iteration 10061 : loss: 0.025927, loss_a: 0.015251
[23:59:28.393] iteration 10062 : loss: 0.022340, loss_a: 0.013141
[23:59:29.716] iteration 10063 : loss: 0.042759, loss_a: 0.025153
[23:59:30.460] iteration 10064 : loss: 0.029431, loss_a: 0.017313
[23:59:31.815] iteration 10065 : loss: 0.014154, loss_a: 0.008326
[23:59:32.546] iteration 10066 : loss: 0.058061, loss_a: 0.034154
[23:59:33.858] iteration 10067 : loss: 0.054331, loss_a: 0.031959
[23:59:34.594] iteration 10068 : loss: 0.021450, loss_a: 0.012618
[23:59:35.960] iteration 10069 : loss: 0.044018, loss_a: 0.025893
[23:59:36.702] iteration 10070 : loss: 0.030698, loss_a: 0.018058
[23:59:38.050] iteration 10071 : loss: 0.019112, loss_a: 0.011242
[23:59:38.788] iteration 10072 : loss: 0.026415, loss_a: 0.015538
[23:59:40.132] iteration 10073 : loss: 0.058267, loss_a: 0.034275
[23:59:40.876] iteration 10074 : loss: 0.043755, loss_a: 0.025738
[23:59:42.219] iteration 10075 : loss: 0.022070, loss_a: 0.012983
[23:59:42.964] iteration 10076 : loss: 0.022135, loss_a: 0.013021
[23:59:44.326] iteration 10077 : loss: 0.040688, loss_a: 0.023934
[23:59:45.071] iteration 10078 : loss: 0.042622, loss_a: 0.025072
[23:59:46.416] iteration 10079 : loss: 0.034186, loss_a: 0.020109
[23:59:47.178] iteration 10080 : loss: 0.039426, loss_a: 0.023192
[23:59:48.498] iteration 10081 : loss: 0.025305, loss_a: 0.014885
[23:59:49.245] iteration 10082 : loss: 0.034342, loss_a: 0.020201
[23:59:50.572] iteration 10083 : loss: 0.023832, loss_a: 0.014019
[23:59:51.308] iteration 10084 : loss: 0.014627, loss_a: 0.008604
[23:59:52.640] iteration 10085 : loss: 0.018606, loss_a: 0.010945
[23:59:53.386] iteration 10086 : loss: 0.023362, loss_a: 0.013742
[23:59:54.695] iteration 10087 : loss: 0.016715, loss_a: 0.009833
[23:59:55.432] iteration 10088 : loss: 0.035759, loss_a: 0.021035
[23:59:56.778] iteration 10089 : loss: 0.026612, loss_a: 0.015654
[23:59:57.511] iteration 10090 : loss: 0.019013, loss_a: 0.011184
[23:59:58.850] iteration 10091 : loss: 0.021955, loss_a: 0.012914
[23:59:59.612] iteration 10092 : loss: 0.028017, loss_a: 0.016480
[00:00:00.967] iteration 10093 : loss: 0.013511, loss_a: 0.007948
[00:00:01.712] iteration 10094 : loss: 0.026435, loss_a: 0.015550
[00:00:03.033] iteration 10095 : loss: 0.026980, loss_a: 0.015871
[00:00:03.773] iteration 10096 : loss: 0.019969, loss_a: 0.011746
[00:00:05.117] iteration 10097 : loss: 0.017813, loss_a: 0.010478
[00:00:05.861] iteration 10098 : loss: 0.028001, loss_a: 0.016471
[00:00:07.223] iteration 10099 : loss: 0.018274, loss_a: 0.010749
[00:00:07.970] iteration 10100 : loss: 0.017295, loss_a: 0.010174
[00:00:09.299] iteration 10101 : loss: 0.025245, loss_a: 0.014850
[00:00:10.045] iteration 10102 : loss: 0.027904, loss_a: 0.016414
[00:00:11.376] iteration 10103 : loss: 0.018518, loss_a: 0.010893
[00:00:12.110] iteration 10104 : loss: 0.024272, loss_a: 0.014278
[00:00:13.479] iteration 10105 : loss: 0.075699, loss_a: 0.044529
[00:00:14.236] iteration 10106 : loss: 0.074762, loss_a: 0.043978
[00:00:15.603] iteration 10107 : loss: 0.032541, loss_a: 0.019142
[00:00:16.343] iteration 10108 : loss: 0.016428, loss_a: 0.009663
[00:00:17.660] iteration 10109 : loss: 0.019578, loss_a: 0.011516
[00:00:18.395] iteration 10110 : loss: 0.032661, loss_a: 0.019212
[00:00:19.721] iteration 10111 : loss: 0.034964, loss_a: 0.020567
[00:00:20.459] iteration 10112 : loss: 0.027502, loss_a: 0.016178
[00:00:21.774] iteration 10113 : loss: 0.032578, loss_a: 0.019164
[00:00:22.508] iteration 10114 : loss: 0.027551, loss_a: 0.016207
[00:00:23.830] iteration 10115 : loss: 0.014474, loss_a: 0.008514
[00:00:24.576] iteration 10116 : loss: 0.021346, loss_a: 0.012557
[00:00:25.921] iteration 10117 : loss: 0.025610, loss_a: 0.015065
[00:00:26.648] iteration 10118 : loss: 0.013188, loss_a: 0.007758
[00:00:27.974] iteration 10119 : loss: 0.028367, loss_a: 0.016686
[00:00:28.725] iteration 10120 : loss: 0.100863, loss_a: 0.059331
[00:00:30.084] iteration 10121 : loss: 0.017201, loss_a: 0.010118
[00:00:30.821] iteration 10122 : loss: 0.035611, loss_a: 0.020948
[00:00:32.171] iteration 10123 : loss: 0.048978, loss_a: 0.028810
[00:00:32.903] iteration 10124 : loss: 0.019437, loss_a: 0.011433
[00:00:34.243] iteration 10125 : loss: 0.015431, loss_a: 0.009077
[00:00:34.981] iteration 10126 : loss: 0.014775, loss_a: 0.008691
[00:00:36.350] iteration 10127 : loss: 0.033354, loss_a: 0.019620
[00:00:37.094] iteration 10128 : loss: 0.021948, loss_a: 0.012910
[00:00:38.432] iteration 10129 : loss: 0.032585, loss_a: 0.019168
[00:00:39.175] iteration 10130 : loss: 0.044233, loss_a: 0.026019
[00:00:40.543] iteration 10131 : loss: 0.040954, loss_a: 0.024090
[00:00:41.285] iteration 10132 : loss: 0.025361, loss_a: 0.014918
[00:00:42.604] iteration 10133 : loss: 0.018956, loss_a: 0.011151
[00:00:43.348] iteration 10134 : loss: 0.027379, loss_a: 0.016105
[00:00:44.705] iteration 10135 : loss: 0.018589, loss_a: 0.010935
[00:00:45.443] iteration 10136 : loss: 0.022746, loss_a: 0.013380
[00:00:46.792] iteration 10137 : loss: 0.028903, loss_a: 0.017002
[00:00:47.530] iteration 10138 : loss: 0.018233, loss_a: 0.010725
[00:00:48.868] iteration 10139 : loss: 0.025976, loss_a: 0.015280
[00:00:49.612] iteration 10140 : loss: 0.010807, loss_a: 0.006357
[00:00:50.943] iteration 10141 : loss: 0.016718, loss_a: 0.009834
[00:00:51.690] iteration 10142 : loss: 0.023279, loss_a: 0.013693
[00:00:53.050] iteration 10143 : loss: 0.051883, loss_a: 0.030519
[00:00:53.783] iteration 10144 : loss: 0.022964, loss_a: 0.013508
[00:00:55.145] iteration 10145 : loss: 0.032124, loss_a: 0.018897
[00:00:55.873] iteration 10146 : loss: 0.015691, loss_a: 0.009230
[00:00:57.194] iteration 10147 : loss: 0.030612, loss_a: 0.018007
[00:00:57.944] iteration 10148 : loss: 0.072858, loss_a: 0.042858
[00:00:59.278] iteration 10149 : loss: 0.016677, loss_a: 0.009810
[00:01:00.015] iteration 10150 : loss: 0.016317, loss_a: 0.009598
[00:01:01.349] iteration 10151 : loss: 0.015667, loss_a: 0.009216
[00:01:02.098] iteration 10152 : loss: 0.040523, loss_a: 0.023837
[00:01:03.419] iteration 10153 : loss: 0.038694, loss_a: 0.022761
[00:01:04.161] iteration 10154 : loss: 0.023024, loss_a: 0.013544
[00:01:05.496] iteration 10155 : loss: 0.014513, loss_a: 0.008537
[00:01:06.226] iteration 10156 : loss: 0.025003, loss_a: 0.014708
[00:01:07.559] iteration 10157 : loss: 0.030029, loss_a: 0.017664
[00:01:08.290] iteration 10158 : loss: 0.025946, loss_a: 0.015262
[00:01:09.610] iteration 10159 : loss: 0.020091, loss_a: 0.011818
[00:01:10.352] iteration 10160 : loss: 0.019164, loss_a: 0.011273
[00:01:11.687] iteration 10161 : loss: 0.024099, loss_a: 0.014176
[00:01:12.426] iteration 10162 : loss: 0.026600, loss_a: 0.015647
[00:01:13.766] iteration 10163 : loss: 0.019518, loss_a: 0.011481
[00:01:14.504] iteration 10164 : loss: 0.034375, loss_a: 0.020220
[00:01:15.811] iteration 10165 : loss: 0.042753, loss_a: 0.025149
[00:01:16.550] iteration 10166 : loss: 0.025802, loss_a: 0.015178
[00:01:17.855] iteration 10167 : loss: 0.102214, loss_a: 0.060126
[00:01:18.593] iteration 10168 : loss: 0.024765, loss_a: 0.014568
[00:01:19.922] iteration 10169 : loss: 0.022534, loss_a: 0.013255
[00:01:20.665] iteration 10170 : loss: 0.025184, loss_a: 0.014814
[00:01:22.000] iteration 10171 : loss: 0.018392, loss_a: 0.010819
[00:01:22.743] iteration 10172 : loss: 0.014468, loss_a: 0.008510
[00:01:24.079] iteration 10173 : loss: 0.015537, loss_a: 0.009139
[00:01:24.817] iteration 10174 : loss: 0.022769, loss_a: 0.013394
[00:01:26.172] iteration 10175 : loss: 0.034708, loss_a: 0.020416
[00:01:26.911] iteration 10176 : loss: 0.028428, loss_a: 0.016722
[00:01:28.255] iteration 10177 : loss: 0.014646, loss_a: 0.008615
[00:01:28.995] iteration 10178 : loss: 0.014472, loss_a: 0.008513
[00:01:30.318] iteration 10179 : loss: 0.038383, loss_a: 0.022578
[00:01:31.053] iteration 10180 : loss: 0.022894, loss_a: 0.013467
[00:01:32.375] iteration 10181 : loss: 0.019830, loss_a: 0.011665
[00:01:33.108] iteration 10182 : loss: 0.024598, loss_a: 0.014469
[00:01:34.485] iteration 10183 : loss: 0.032523, loss_a: 0.019131
[00:01:35.220] iteration 10184 : loss: 0.027206, loss_a: 0.016003
[00:01:36.568] iteration 10185 : loss: 0.014139, loss_a: 0.008317
[00:01:37.309] iteration 10186 : loss: 0.049825, loss_a: 0.029309
[00:01:38.646] iteration 10187 : loss: 0.016778, loss_a: 0.009869
[00:01:39.376] iteration 10188 : loss: 0.017271, loss_a: 0.010160
[00:01:40.696] iteration 10189 : loss: 0.020180, loss_a: 0.011871
[00:01:41.442] iteration 10190 : loss: 0.023715, loss_a: 0.013950
[00:01:42.798] iteration 10191 : loss: 0.017385, loss_a: 0.010227
[00:01:43.540] iteration 10192 : loss: 0.041561, loss_a: 0.024448
[00:01:44.862] iteration 10193 : loss: 0.024306, loss_a: 0.014297
[00:01:45.607] iteration 10194 : loss: 0.031698, loss_a: 0.018646
[00:01:46.972] iteration 10195 : loss: 0.041783, loss_a: 0.024578
[00:01:47.711] iteration 10196 : loss: 0.024495, loss_a: 0.014409
[00:01:49.057] iteration 10197 : loss: 0.021631, loss_a: 0.012724
[00:01:49.793] iteration 10198 : loss: 0.014515, loss_a: 0.008538
[00:01:51.149] iteration 10199 : loss: 0.043953, loss_a: 0.025854
[00:01:51.890] iteration 10200 : loss: 0.023807, loss_a: 0.014004
[00:02:16.496] iteration 10201 : loss: 0.019674, loss_a: 0.011573
[00:02:18.638] iteration 10202 : loss: 0.016425, loss_a: 0.009662
[00:02:19.985] iteration 10203 : loss: 0.025788, loss_a: 0.015170
[00:02:20.745] iteration 10204 : loss: 0.024818, loss_a: 0.014599
[00:02:22.070] iteration 10205 : loss: 0.015571, loss_a: 0.009159
[00:02:22.809] iteration 10206 : loss: 0.015741, loss_a: 0.009259
[00:02:24.167] iteration 10207 : loss: 0.015162, loss_a: 0.008919
[00:02:24.904] iteration 10208 : loss: 0.018332, loss_a: 0.010783
[00:02:26.255] iteration 10209 : loss: 0.016747, loss_a: 0.009851
[00:02:27.008] iteration 10210 : loss: 0.026117, loss_a: 0.015363
[00:02:28.362] iteration 10211 : loss: 0.039145, loss_a: 0.023026
[00:02:29.103] iteration 10212 : loss: 0.019456, loss_a: 0.011444
[00:02:30.419] iteration 10213 : loss: 0.012481, loss_a: 0.007342
[00:02:31.147] iteration 10214 : loss: 0.012653, loss_a: 0.007443
[00:02:32.461] iteration 10215 : loss: 0.021254, loss_a: 0.012502
[00:02:33.200] iteration 10216 : loss: 0.060148, loss_a: 0.035381
[00:02:34.523] iteration 10217 : loss: 0.015803, loss_a: 0.009296
[00:02:35.255] iteration 10218 : loss: 0.019736, loss_a: 0.011609
[00:02:36.600] iteration 10219 : loss: 0.037203, loss_a: 0.021884
[00:02:37.341] iteration 10220 : loss: 0.033238, loss_a: 0.019552
[00:02:38.702] iteration 10221 : loss: 0.045146, loss_a: 0.026556
[00:02:39.455] iteration 10222 : loss: 0.040378, loss_a: 0.023752
[00:02:40.790] iteration 10223 : loss: 0.020082, loss_a: 0.011813
[00:02:41.526] iteration 10224 : loss: 0.014315, loss_a: 0.008421
[00:02:42.870] iteration 10225 : loss: 0.061171, loss_a: 0.035983
[00:02:43.607] iteration 10226 : loss: 0.016850, loss_a: 0.009912
[00:02:44.940] iteration 10227 : loss: 0.017573, loss_a: 0.010337
[00:02:45.683] iteration 10228 : loss: 0.026262, loss_a: 0.015448
[00:02:47.022] iteration 10229 : loss: 0.011865, loss_a: 0.006979
[00:02:47.761] iteration 10230 : loss: 0.056943, loss_a: 0.033496
[00:02:49.107] iteration 10231 : loss: 0.055043, loss_a: 0.032378
[00:02:49.848] iteration 10232 : loss: 0.038060, loss_a: 0.022388
[00:02:51.211] iteration 10233 : loss: 0.033971, loss_a: 0.019983
[00:02:51.943] iteration 10234 : loss: 0.024231, loss_a: 0.014254
[00:02:53.300] iteration 10235 : loss: 0.023780, loss_a: 0.013988
[00:02:54.033] iteration 10236 : loss: 0.019397, loss_a: 0.011410
[00:02:55.364] iteration 10237 : loss: 0.026716, loss_a: 0.015716
[00:02:56.130] iteration 10238 : loss: 0.058695, loss_a: 0.034527
[00:02:57.493] iteration 10239 : loss: 0.034943, loss_a: 0.020555
[00:02:58.226] iteration 10240 : loss: 0.021286, loss_a: 0.012521
[00:02:59.542] iteration 10241 : loss: 0.023116, loss_a: 0.013597
[00:03:00.273] iteration 10242 : loss: 0.020129, loss_a: 0.011841
[00:03:01.584] iteration 10243 : loss: 0.041719, loss_a: 0.024541
[00:03:02.329] iteration 10244 : loss: 0.045363, loss_a: 0.026684
[00:03:03.664] iteration 10245 : loss: 0.020761, loss_a: 0.012213
[00:03:04.393] iteration 10246 : loss: 0.010770, loss_a: 0.006335
[00:03:05.745] iteration 10247 : loss: 0.029967, loss_a: 0.017628
[00:03:06.484] iteration 10248 : loss: 0.020021, loss_a: 0.011777
[00:03:07.825] iteration 10249 : loss: 0.024213, loss_a: 0.014243
[00:03:08.556] iteration 10250 : loss: 0.017540, loss_a: 0.010318
[00:03:09.883] iteration 10251 : loss: 0.039619, loss_a: 0.023305
[00:03:10.628] iteration 10252 : loss: 0.025349, loss_a: 0.014911
[00:03:11.992] iteration 10253 : loss: 0.053827, loss_a: 0.031663
[00:03:12.738] iteration 10254 : loss: 0.017223, loss_a: 0.010131
[00:03:14.053] iteration 10255 : loss: 0.020624, loss_a: 0.012132
[00:03:14.788] iteration 10256 : loss: 0.010026, loss_a: 0.005897
[00:03:16.136] iteration 10257 : loss: 0.017634, loss_a: 0.010373
[00:03:16.876] iteration 10258 : loss: 0.025705, loss_a: 0.015121
[00:03:18.223] iteration 10259 : loss: 0.034604, loss_a: 0.020355
[00:03:18.966] iteration 10260 : loss: 0.037053, loss_a: 0.021796
[00:03:20.292] iteration 10261 : loss: 0.049255, loss_a: 0.028974
[00:03:21.023] iteration 10262 : loss: 0.013610, loss_a: 0.008006
[00:03:22.366] iteration 10263 : loss: 0.028960, loss_a: 0.017035
[00:03:23.106] iteration 10264 : loss: 0.019569, loss_a: 0.011511
[00:03:24.451] iteration 10265 : loss: 0.016840, loss_a: 0.009906
[00:03:25.188] iteration 10266 : loss: 0.033374, loss_a: 0.019632
[00:03:26.529] iteration 10267 : loss: 0.033034, loss_a: 0.019432
[00:03:27.259] iteration 10268 : loss: 0.016816, loss_a: 0.009892
[00:03:28.604] iteration 10269 : loss: 0.021985, loss_a: 0.012932
[00:03:29.345] iteration 10270 : loss: 0.032841, loss_a: 0.019318
[00:03:30.700] iteration 10271 : loss: 0.013923, loss_a: 0.008190
[00:03:31.435] iteration 10272 : loss: 0.032252, loss_a: 0.018972
[00:03:32.784] iteration 10273 : loss: 0.029500, loss_a: 0.017353
[00:03:33.527] iteration 10274 : loss: 0.014147, loss_a: 0.008322
[00:03:34.863] iteration 10275 : loss: 0.028964, loss_a: 0.017038
[00:03:35.610] iteration 10276 : loss: 0.026386, loss_a: 0.015521
[00:03:36.959] iteration 10277 : loss: 0.021724, loss_a: 0.012779
[00:03:37.702] iteration 10278 : loss: 0.041585, loss_a: 0.024462
[00:03:39.050] iteration 10279 : loss: 0.037611, loss_a: 0.022124
[00:03:39.785] iteration 10280 : loss: 0.019321, loss_a: 0.011365
[00:03:41.150] iteration 10281 : loss: 0.026268, loss_a: 0.015452
[00:03:41.889] iteration 10282 : loss: 0.018252, loss_a: 0.010736
[00:03:43.201] iteration 10283 : loss: 0.044642, loss_a: 0.026260
[00:03:43.940] iteration 10284 : loss: 0.019456, loss_a: 0.011445
[00:03:45.278] iteration 10285 : loss: 0.019062, loss_a: 0.011213
[00:03:46.022] iteration 10286 : loss: 0.021245, loss_a: 0.012497
[00:03:47.362] iteration 10287 : loss: 0.037613, loss_a: 0.022125
[00:03:48.113] iteration 10288 : loss: 0.018268, loss_a: 0.010746
[00:03:49.453] iteration 10289 : loss: 0.023679, loss_a: 0.013929
[00:03:50.190] iteration 10290 : loss: 0.035038, loss_a: 0.020611
[00:03:51.536] iteration 10291 : loss: 0.018734, loss_a: 0.011020
[00:03:52.275] iteration 10292 : loss: 0.042202, loss_a: 0.024825
[00:03:53.617] iteration 10293 : loss: 0.021967, loss_a: 0.012922
[00:03:54.355] iteration 10294 : loss: 0.020563, loss_a: 0.012096
[00:03:55.713] iteration 10295 : loss: 0.022734, loss_a: 0.013373
[00:03:56.458] iteration 10296 : loss: 0.050141, loss_a: 0.029495
[00:03:57.772] iteration 10297 : loss: 0.035419, loss_a: 0.020835
[00:03:58.508] iteration 10298 : loss: 0.036210, loss_a: 0.021300
[00:03:59.845] iteration 10299 : loss: 0.036486, loss_a: 0.021462
[00:04:00.586] iteration 10300 : loss: 0.031426, loss_a: 0.018486
[00:04:01.924] iteration 10301 : loss: 0.009381, loss_a: 0.005518
[00:04:02.656] iteration 10302 : loss: 0.030286, loss_a: 0.017815
[00:04:03.993] iteration 10303 : loss: 0.020865, loss_a: 0.012274
[00:04:04.727] iteration 10304 : loss: 0.015986, loss_a: 0.009404
[00:04:06.084] iteration 10305 : loss: 0.033695, loss_a: 0.019821
[00:04:06.822] iteration 10306 : loss: 0.012997, loss_a: 0.007645
[00:04:08.156] iteration 10307 : loss: 0.044389, loss_a: 0.026111
[00:04:08.899] iteration 10308 : loss: 0.022726, loss_a: 0.013368
[00:04:10.241] iteration 10309 : loss: 0.029864, loss_a: 0.017567
[00:04:10.996] iteration 10310 : loss: 0.059249, loss_a: 0.034852
[00:04:12.342] iteration 10311 : loss: 0.017462, loss_a: 0.010272
[00:04:13.086] iteration 10312 : loss: 0.016701, loss_a: 0.009824
[00:04:14.395] iteration 10313 : loss: 0.007768, loss_a: 0.004569
[00:04:15.128] iteration 10314 : loss: 0.020820, loss_a: 0.012247
[00:04:16.459] iteration 10315 : loss: 0.033590, loss_a: 0.019759
[00:04:17.206] iteration 10316 : loss: 0.026210, loss_a: 0.015417
[00:04:18.540] iteration 10317 : loss: 0.025716, loss_a: 0.015127
[00:04:19.279] iteration 10318 : loss: 0.019735, loss_a: 0.011609
[00:04:20.631] iteration 10319 : loss: 0.025462, loss_a: 0.014978
[00:04:21.370] iteration 10320 : loss: 0.027076, loss_a: 0.015927
[00:04:22.697] iteration 10321 : loss: 0.032295, loss_a: 0.018997
[00:04:23.433] iteration 10322 : loss: 0.024031, loss_a: 0.014136
[00:04:24.775] iteration 10323 : loss: 0.022483, loss_a: 0.013225
[00:04:25.509] iteration 10324 : loss: 0.024949, loss_a: 0.014676
[00:04:26.858] iteration 10325 : loss: 0.032475, loss_a: 0.019103
[00:04:27.597] iteration 10326 : loss: 0.019489, loss_a: 0.011464
[00:04:28.919] iteration 10327 : loss: 0.036417, loss_a: 0.021422
[00:04:29.654] iteration 10328 : loss: 0.016302, loss_a: 0.009589
[00:04:31.012] iteration 10329 : loss: 0.047370, loss_a: 0.027865
[00:04:31.754] iteration 10330 : loss: 0.027006, loss_a: 0.015886
[00:04:33.072] iteration 10331 : loss: 0.015330, loss_a: 0.009018
[00:04:33.817] iteration 10332 : loss: 0.018751, loss_a: 0.011030
[00:04:35.122] iteration 10333 : loss: 0.025138, loss_a: 0.014787
[00:04:35.857] iteration 10334 : loss: 0.019484, loss_a: 0.011461
[00:04:37.200] iteration 10335 : loss: 0.025672, loss_a: 0.015101
[00:04:37.937] iteration 10336 : loss: 0.046563, loss_a: 0.027390
[00:04:39.260] iteration 10337 : loss: 0.040027, loss_a: 0.023545
[00:04:40.003] iteration 10338 : loss: 0.022625, loss_a: 0.013309
[00:04:41.365] iteration 10339 : loss: 0.034406, loss_a: 0.020239
[00:04:42.114] iteration 10340 : loss: 0.040791, loss_a: 0.023994
[00:04:43.438] iteration 10341 : loss: 0.017396, loss_a: 0.010233
[00:04:44.173] iteration 10342 : loss: 0.026244, loss_a: 0.015438
[00:04:45.523] iteration 10343 : loss: 0.015666, loss_a: 0.009215
[00:04:46.255] iteration 10344 : loss: 0.020074, loss_a: 0.011808
[00:04:47.577] iteration 10345 : loss: 0.028680, loss_a: 0.016871
[00:04:48.306] iteration 10346 : loss: 0.027337, loss_a: 0.016081
[00:04:49.654] iteration 10347 : loss: 0.016613, loss_a: 0.009772
[00:04:50.391] iteration 10348 : loss: 0.030128, loss_a: 0.017722
[00:04:51.715] iteration 10349 : loss: 0.016247, loss_a: 0.009557
[00:04:52.454] iteration 10350 : loss: 0.028961, loss_a: 0.017036
[00:04:53.773] iteration 10351 : loss: 0.025524, loss_a: 0.015014
[00:04:54.531] iteration 10352 : loss: 0.037341, loss_a: 0.021965
[00:04:55.885] iteration 10353 : loss: 0.054939, loss_a: 0.032317
[00:04:56.626] iteration 10354 : loss: 0.069366, loss_a: 0.040804
[00:04:57.972] iteration 10355 : loss: 0.070413, loss_a: 0.041420
[00:04:58.711] iteration 10356 : loss: 0.018290, loss_a: 0.010759
[00:05:00.072] iteration 10357 : loss: 0.060298, loss_a: 0.035470
[00:05:00.813] iteration 10358 : loss: 0.025897, loss_a: 0.015233
[00:05:02.164] iteration 10359 : loss: 0.031357, loss_a: 0.018445
[00:05:02.904] iteration 10360 : loss: 0.031194, loss_a: 0.018349
[00:05:04.225] iteration 10361 : loss: 0.036626, loss_a: 0.021545
[00:05:04.958] iteration 10362 : loss: 0.020678, loss_a: 0.012163
[00:05:06.306] iteration 10363 : loss: 0.054461, loss_a: 0.032036
[00:05:07.040] iteration 10364 : loss: 0.016144, loss_a: 0.009496
[00:05:08.364] iteration 10365 : loss: 0.016246, loss_a: 0.009557
[00:05:09.108] iteration 10366 : loss: 0.020760, loss_a: 0.012212
[00:05:10.419] iteration 10367 : loss: 0.036144, loss_a: 0.021261
[00:05:11.159] iteration 10368 : loss: 0.013574, loss_a: 0.007985
[00:05:12.501] iteration 10369 : loss: 0.029385, loss_a: 0.017285
[00:05:13.232] iteration 10370 : loss: 0.013777, loss_a: 0.008104
[00:05:14.577] iteration 10371 : loss: 0.044153, loss_a: 0.025972
[00:05:15.314] iteration 10372 : loss: 0.045901, loss_a: 0.027000
[00:05:16.623] iteration 10373 : loss: 0.047461, loss_a: 0.027918
[00:05:17.357] iteration 10374 : loss: 0.041919, loss_a: 0.024658
[00:05:18.703] iteration 10375 : loss: 0.047602, loss_a: 0.028001
[00:05:19.437] iteration 10376 : loss: 0.020907, loss_a: 0.012298
[00:05:20.785] iteration 10377 : loss: 0.061366, loss_a: 0.036097
[00:05:21.520] iteration 10378 : loss: 0.017619, loss_a: 0.010364
[00:05:22.849] iteration 10379 : loss: 0.029174, loss_a: 0.017161
[00:05:23.585] iteration 10380 : loss: 0.018215, loss_a: 0.010715
[00:05:24.936] iteration 10381 : loss: 0.023690, loss_a: 0.013935
[00:05:25.674] iteration 10382 : loss: 0.019802, loss_a: 0.011648
[00:05:27.012] iteration 10383 : loss: 0.033094, loss_a: 0.019467
[00:05:27.751] iteration 10384 : loss: 0.056779, loss_a: 0.033399
[00:05:29.122] iteration 10385 : loss: 0.021013, loss_a: 0.012361
[00:05:29.859] iteration 10386 : loss: 0.023559, loss_a: 0.013858
[00:05:31.206] iteration 10387 : loss: 0.020383, loss_a: 0.011990
[00:05:31.956] iteration 10388 : loss: 0.019068, loss_a: 0.011216
[00:05:33.288] iteration 10389 : loss: 0.018312, loss_a: 0.010772
[00:05:34.026] iteration 10390 : loss: 0.025671, loss_a: 0.015101
[00:05:35.364] iteration 10391 : loss: 0.026465, loss_a: 0.015568
[00:05:36.104] iteration 10392 : loss: 0.044254, loss_a: 0.026032
[00:05:37.467] iteration 10393 : loss: 0.037700, loss_a: 0.022176
[00:05:38.206] iteration 10394 : loss: 0.025437, loss_a: 0.014963
[00:05:39.523] iteration 10395 : loss: 0.040922, loss_a: 0.024072
[00:05:40.279] iteration 10396 : loss: 0.023160, loss_a: 0.013623
[00:05:41.595] iteration 10397 : loss: 0.014564, loss_a: 0.008567
[00:05:42.330] iteration 10398 : loss: 0.018218, loss_a: 0.010717
[00:05:43.661] iteration 10399 : loss: 0.043196, loss_a: 0.025410
[00:05:44.399] iteration 10400 : loss: 0.042636, loss_a: 0.025080
[00:06:07.997] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_10400_dice_0.8999.pth
[00:06:09.320] iteration 10401 : loss: 0.025266, loss_a: 0.014862
[00:06:11.568] iteration 10402 : loss: 0.058997, loss_a: 0.034704
[00:06:12.938] iteration 10403 : loss: 0.029011, loss_a: 0.017066
[00:06:13.682] iteration 10404 : loss: 0.044958, loss_a: 0.026446
[00:06:15.017] iteration 10405 : loss: 0.043404, loss_a: 0.025531
[00:06:15.757] iteration 10406 : loss: 0.021613, loss_a: 0.012713
[00:06:17.078] iteration 10407 : loss: 0.015911, loss_a: 0.009359
[00:06:17.813] iteration 10408 : loss: 0.027548, loss_a: 0.016205
[00:06:19.150] iteration 10409 : loss: 0.045462, loss_a: 0.026742
[00:06:19.891] iteration 10410 : loss: 0.016364, loss_a: 0.009626
[00:06:21.211] iteration 10411 : loss: 0.019840, loss_a: 0.011670
[00:06:21.944] iteration 10412 : loss: 0.022795, loss_a: 0.013409
[00:06:23.254] iteration 10413 : loss: 0.017192, loss_a: 0.010113
[00:06:24.000] iteration 10414 : loss: 0.028044, loss_a: 0.016496
[00:06:25.356] iteration 10415 : loss: 0.021505, loss_a: 0.012650
[00:06:26.098] iteration 10416 : loss: 0.020336, loss_a: 0.011963
[00:06:27.447] iteration 10417 : loss: 0.014448, loss_a: 0.008499
[00:06:28.185] iteration 10418 : loss: 0.018952, loss_a: 0.011148
[00:06:29.543] iteration 10419 : loss: 0.030785, loss_a: 0.018109
[00:06:30.285] iteration 10420 : loss: 0.048573, loss_a: 0.028572
[00:06:31.654] iteration 10421 : loss: 0.046948, loss_a: 0.027617
[00:06:32.387] iteration 10422 : loss: 0.023776, loss_a: 0.013986
[00:06:33.745] iteration 10423 : loss: 0.034340, loss_a: 0.020200
[00:06:34.485] iteration 10424 : loss: 0.012463, loss_a: 0.007331
[00:06:35.838] iteration 10425 : loss: 0.016508, loss_a: 0.009711
[00:06:36.574] iteration 10426 : loss: 0.019551, loss_a: 0.011501
[00:06:37.903] iteration 10427 : loss: 0.020726, loss_a: 0.012192
[00:06:38.657] iteration 10428 : loss: 0.049961, loss_a: 0.029389
[00:06:39.983] iteration 10429 : loss: 0.027185, loss_a: 0.015991
[00:06:40.718] iteration 10430 : loss: 0.017239, loss_a: 0.010140
[00:06:42.049] iteration 10431 : loss: 0.057239, loss_a: 0.033670
[00:06:42.794] iteration 10432 : loss: 0.026822, loss_a: 0.015777
[00:06:44.142] iteration 10433 : loss: 0.020038, loss_a: 0.011787
[00:06:44.884] iteration 10434 : loss: 0.034782, loss_a: 0.020460
[00:06:46.195] iteration 10435 : loss: 0.032253, loss_a: 0.018972
[00:06:46.942] iteration 10436 : loss: 0.038495, loss_a: 0.022644
[00:06:48.304] iteration 10437 : loss: 0.019854, loss_a: 0.011679
[00:06:49.049] iteration 10438 : loss: 0.026516, loss_a: 0.015597
[00:06:50.391] iteration 10439 : loss: 0.014407, loss_a: 0.008475
[00:06:51.128] iteration 10440 : loss: 0.059361, loss_a: 0.034918
[00:06:52.455] iteration 10441 : loss: 0.014426, loss_a: 0.008486
[00:06:53.207] iteration 10442 : loss: 0.016569, loss_a: 0.009746
[00:06:54.550] iteration 10443 : loss: 0.018368, loss_a: 0.010805
[00:06:55.297] iteration 10444 : loss: 0.034366, loss_a: 0.020216
[00:06:56.618] iteration 10445 : loss: 0.012343, loss_a: 0.007260
[00:06:57.362] iteration 10446 : loss: 0.086100, loss_a: 0.050647
[00:06:58.734] iteration 10447 : loss: 0.044965, loss_a: 0.026450
[00:06:59.469] iteration 10448 : loss: 0.015332, loss_a: 0.009019
[00:07:00.802] iteration 10449 : loss: 0.020083, loss_a: 0.011813
[00:07:01.551] iteration 10450 : loss: 0.050097, loss_a: 0.029469
[00:07:02.884] iteration 10451 : loss: 0.049631, loss_a: 0.029195
[00:07:03.635] iteration 10452 : loss: 0.027237, loss_a: 0.016022
[00:07:04.969] iteration 10453 : loss: 0.018744, loss_a: 0.011026
[00:07:05.716] iteration 10454 : loss: 0.036329, loss_a: 0.021370
[00:07:07.031] iteration 10455 : loss: 0.046876, loss_a: 0.027574
[00:07:07.774] iteration 10456 : loss: 0.036738, loss_a: 0.021610
[00:07:09.105] iteration 10457 : loss: 0.018028, loss_a: 0.010605
[00:07:09.843] iteration 10458 : loss: 0.021663, loss_a: 0.012743
[00:07:11.189] iteration 10459 : loss: 0.051199, loss_a: 0.030117
[00:07:11.924] iteration 10460 : loss: 0.016902, loss_a: 0.009942
[00:07:13.247] iteration 10461 : loss: 0.026520, loss_a: 0.015600
[00:07:13.987] iteration 10462 : loss: 0.022896, loss_a: 0.013468
[00:07:15.306] iteration 10463 : loss: 0.024089, loss_a: 0.014170
[00:07:16.051] iteration 10464 : loss: 0.038561, loss_a: 0.022683
[00:07:17.412] iteration 10465 : loss: 0.057747, loss_a: 0.033969
[00:07:18.150] iteration 10466 : loss: 0.030395, loss_a: 0.017880
[00:07:19.479] iteration 10467 : loss: 0.025063, loss_a: 0.014743
[00:07:20.225] iteration 10468 : loss: 0.027708, loss_a: 0.016299
[00:07:21.547] iteration 10469 : loss: 0.021362, loss_a: 0.012566
[00:07:22.286] iteration 10470 : loss: 0.025386, loss_a: 0.014933
[00:07:23.632] iteration 10471 : loss: 0.013645, loss_a: 0.008027
[00:07:24.382] iteration 10472 : loss: 0.035371, loss_a: 0.020806
[00:07:25.733] iteration 10473 : loss: 0.013113, loss_a: 0.007714
[00:07:26.471] iteration 10474 : loss: 0.037054, loss_a: 0.021796
[00:07:27.805] iteration 10475 : loss: 0.030429, loss_a: 0.017899
[00:07:28.540] iteration 10476 : loss: 0.020450, loss_a: 0.012030
[00:07:29.907] iteration 10477 : loss: 0.038664, loss_a: 0.022743
[00:07:30.645] iteration 10478 : loss: 0.058931, loss_a: 0.034665
[00:07:31.964] iteration 10479 : loss: 0.024956, loss_a: 0.014680
[00:07:32.717] iteration 10480 : loss: 0.060076, loss_a: 0.035339
[00:07:34.072] iteration 10481 : loss: 0.025507, loss_a: 0.015004
[00:07:34.818] iteration 10482 : loss: 0.039356, loss_a: 0.023150
[00:07:36.171] iteration 10483 : loss: 0.023397, loss_a: 0.013763
[00:07:36.907] iteration 10484 : loss: 0.035066, loss_a: 0.020627
[00:07:38.255] iteration 10485 : loss: 0.023546, loss_a: 0.013851
[00:07:38.992] iteration 10486 : loss: 0.025500, loss_a: 0.015000
[00:07:40.341] iteration 10487 : loss: 0.019299, loss_a: 0.011352
[00:07:41.088] iteration 10488 : loss: 0.033680, loss_a: 0.019812
[00:07:42.422] iteration 10489 : loss: 0.036245, loss_a: 0.021321
[00:07:43.163] iteration 10490 : loss: 0.014912, loss_a: 0.008772
[00:07:44.481] iteration 10491 : loss: 0.026674, loss_a: 0.015691
[00:07:45.228] iteration 10492 : loss: 0.028294, loss_a: 0.016643
[00:07:46.546] iteration 10493 : loss: 0.011597, loss_a: 0.006822
[00:07:47.284] iteration 10494 : loss: 0.020736, loss_a: 0.012197
[00:07:48.654] iteration 10495 : loss: 0.022420, loss_a: 0.013188
[00:07:49.393] iteration 10496 : loss: 0.026400, loss_a: 0.015529
[00:07:50.735] iteration 10497 : loss: 0.021672, loss_a: 0.012748
[00:07:51.475] iteration 10498 : loss: 0.022242, loss_a: 0.013083
[00:07:52.798] iteration 10499 : loss: 0.027600, loss_a: 0.016235
[00:07:53.538] iteration 10500 : loss: 0.017652, loss_a: 0.010384
[00:07:54.888] iteration 10501 : loss: 0.039341, loss_a: 0.023142
[00:07:55.626] iteration 10502 : loss: 0.034006, loss_a: 0.020003
[00:07:56.952] iteration 10503 : loss: 0.023095, loss_a: 0.013586
[00:07:57.683] iteration 10504 : loss: 0.014217, loss_a: 0.008363
[00:07:59.053] iteration 10505 : loss: 0.045539, loss_a: 0.026788
[00:07:59.785] iteration 10506 : loss: 0.016055, loss_a: 0.009444
[00:08:01.116] iteration 10507 : loss: 0.038672, loss_a: 0.022748
[00:08:01.863] iteration 10508 : loss: 0.052910, loss_a: 0.031124
[00:08:03.198] iteration 10509 : loss: 0.021074, loss_a: 0.012397
[00:08:03.947] iteration 10510 : loss: 0.044104, loss_a: 0.025943
[00:08:05.276] iteration 10511 : loss: 0.013806, loss_a: 0.008121
[00:08:06.019] iteration 10512 : loss: 0.021967, loss_a: 0.012922
[00:08:07.347] iteration 10513 : loss: 0.020555, loss_a: 0.012091
[00:08:08.093] iteration 10514 : loss: 0.026802, loss_a: 0.015766
[00:08:09.441] iteration 10515 : loss: 0.015363, loss_a: 0.009037
[00:08:10.183] iteration 10516 : loss: 0.028741, loss_a: 0.016907
[00:08:11.490] iteration 10517 : loss: 0.016747, loss_a: 0.009851
[00:08:12.240] iteration 10518 : loss: 0.076098, loss_a: 0.044764
[00:08:13.561] iteration 10519 : loss: 0.021597, loss_a: 0.012704
[00:08:14.304] iteration 10520 : loss: 0.016971, loss_a: 0.009983
[00:08:15.627] iteration 10521 : loss: 0.021229, loss_a: 0.012488
[00:08:16.381] iteration 10522 : loss: 0.041991, loss_a: 0.024701
[00:08:17.707] iteration 10523 : loss: 0.032531, loss_a: 0.019136
[00:08:18.449] iteration 10524 : loss: 0.037845, loss_a: 0.022262
[00:08:19.780] iteration 10525 : loss: 0.035434, loss_a: 0.020844
[00:08:20.523] iteration 10526 : loss: 0.043433, loss_a: 0.025549
[00:08:21.879] iteration 10527 : loss: 0.046468, loss_a: 0.027334
[00:08:22.624] iteration 10528 : loss: 0.019497, loss_a: 0.011469
[00:08:23.970] iteration 10529 : loss: 0.027802, loss_a: 0.016354
[00:08:24.701] iteration 10530 : loss: 0.011101, loss_a: 0.006530
[00:08:26.041] iteration 10531 : loss: 0.024054, loss_a: 0.014150
[00:08:26.781] iteration 10532 : loss: 0.020404, loss_a: 0.012002
[00:08:28.125] iteration 10533 : loss: 0.036285, loss_a: 0.021344
[00:08:28.867] iteration 10534 : loss: 0.030031, loss_a: 0.017665
[00:08:30.177] iteration 10535 : loss: 0.020080, loss_a: 0.011812
[00:08:30.927] iteration 10536 : loss: 0.050338, loss_a: 0.029611
[00:08:32.341] iteration 10537 : loss: 0.018511, loss_a: 0.010889
[00:08:33.082] iteration 10538 : loss: 0.019421, loss_a: 0.011424
[00:08:34.421] iteration 10539 : loss: 0.039989, loss_a: 0.023523
[00:08:35.159] iteration 10540 : loss: 0.027683, loss_a: 0.016284
[00:08:36.536] iteration 10541 : loss: 0.065306, loss_a: 0.038415
[00:08:37.273] iteration 10542 : loss: 0.025593, loss_a: 0.015055
[00:08:38.624] iteration 10543 : loss: 0.027833, loss_a: 0.016372
[00:08:39.360] iteration 10544 : loss: 0.020820, loss_a: 0.012247
[00:08:40.715] iteration 10545 : loss: 0.029092, loss_a: 0.017113
[00:08:41.455] iteration 10546 : loss: 0.020468, loss_a: 0.012040
[00:08:42.806] iteration 10547 : loss: 0.016741, loss_a: 0.009848
[00:08:43.534] iteration 10548 : loss: 0.018689, loss_a: 0.010993
[00:08:44.891] iteration 10549 : loss: 0.034565, loss_a: 0.020333
[00:08:45.628] iteration 10550 : loss: 0.026814, loss_a: 0.015773
[00:08:46.946] iteration 10551 : loss: 0.022262, loss_a: 0.013095
[00:08:47.698] iteration 10552 : loss: 0.029718, loss_a: 0.017481
[00:08:49.008] iteration 10553 : loss: 0.019526, loss_a: 0.011486
[00:08:49.744] iteration 10554 : loss: 0.018940, loss_a: 0.011141
[00:08:51.064] iteration 10555 : loss: 0.020756, loss_a: 0.012209
[00:08:51.828] iteration 10556 : loss: 0.026318, loss_a: 0.015481
[00:08:53.197] iteration 10557 : loss: 0.076417, loss_a: 0.044951
[00:08:53.942] iteration 10558 : loss: 0.030562, loss_a: 0.017977
[00:08:55.288] iteration 10559 : loss: 0.016625, loss_a: 0.009780
[00:08:56.028] iteration 10560 : loss: 0.041430, loss_a: 0.024370
[00:08:57.372] iteration 10561 : loss: 0.014741, loss_a: 0.008671
[00:08:58.106] iteration 10562 : loss: 0.024508, loss_a: 0.014417
[00:08:59.418] iteration 10563 : loss: 0.019138, loss_a: 0.011258
[00:09:00.148] iteration 10564 : loss: 0.020987, loss_a: 0.012345
[00:09:01.471] iteration 10565 : loss: 0.028406, loss_a: 0.016709
[00:09:02.216] iteration 10566 : loss: 0.038555, loss_a: 0.022679
[00:09:03.564] iteration 10567 : loss: 0.020961, loss_a: 0.012330
[00:09:04.299] iteration 10568 : loss: 0.046932, loss_a: 0.027607
[00:09:05.664] iteration 10569 : loss: 0.044257, loss_a: 0.026034
[00:09:06.416] iteration 10570 : loss: 0.021317, loss_a: 0.012540
[00:09:07.778] iteration 10571 : loss: 0.016671, loss_a: 0.009806
[00:09:08.533] iteration 10572 : loss: 0.054037, loss_a: 0.031786
[00:09:09.851] iteration 10573 : loss: 0.027122, loss_a: 0.015954
[00:09:10.593] iteration 10574 : loss: 0.026824, loss_a: 0.015779
[00:09:11.903] iteration 10575 : loss: 0.034685, loss_a: 0.020403
[00:09:12.633] iteration 10576 : loss: 0.017739, loss_a: 0.010435
[00:09:13.957] iteration 10577 : loss: 0.019373, loss_a: 0.011396
[00:09:14.697] iteration 10578 : loss: 0.021670, loss_a: 0.012747
[00:09:16.068] iteration 10579 : loss: 0.057376, loss_a: 0.033751
[00:09:16.811] iteration 10580 : loss: 0.087167, loss_a: 0.051275
[00:09:18.135] iteration 10581 : loss: 0.024154, loss_a: 0.014208
[00:09:18.867] iteration 10582 : loss: 0.016223, loss_a: 0.009543
[00:09:20.191] iteration 10583 : loss: 0.015346, loss_a: 0.009027
[00:09:20.922] iteration 10584 : loss: 0.020118, loss_a: 0.011834
[00:09:22.268] iteration 10585 : loss: 0.017204, loss_a: 0.010120
[00:09:23.007] iteration 10586 : loss: 0.015757, loss_a: 0.009269
[00:09:24.343] iteration 10587 : loss: 0.035492, loss_a: 0.020878
[00:09:25.095] iteration 10588 : loss: 0.020008, loss_a: 0.011769
[00:09:26.403] iteration 10589 : loss: 0.026768, loss_a: 0.015746
[00:09:27.146] iteration 10590 : loss: 0.015703, loss_a: 0.009237
[00:09:28.499] iteration 10591 : loss: 0.021805, loss_a: 0.012827
[00:09:29.233] iteration 10592 : loss: 0.015848, loss_a: 0.009322
[00:09:30.595] iteration 10593 : loss: 0.022890, loss_a: 0.013464
[00:09:31.332] iteration 10594 : loss: 0.033764, loss_a: 0.019861
[00:09:32.687] iteration 10595 : loss: 0.020414, loss_a: 0.012008
[00:09:33.429] iteration 10596 : loss: 0.046257, loss_a: 0.027210
[00:09:34.750] iteration 10597 : loss: 0.037562, loss_a: 0.022095
[00:09:35.501] iteration 10598 : loss: 0.026154, loss_a: 0.015384
[00:09:36.831] iteration 10599 : loss: 0.020957, loss_a: 0.012328
[00:09:37.573] iteration 10600 : loss: 0.020127, loss_a: 0.011839
[00:10:02.202] iteration 10601 : loss: 0.011125, loss_a: 0.006544
[00:10:04.329] iteration 10602 : loss: 0.022376, loss_a: 0.013162
[00:10:05.686] iteration 10603 : loss: 0.030349, loss_a: 0.017852
[00:10:06.449] iteration 10604 : loss: 0.069128, loss_a: 0.040663
[00:10:07.801] iteration 10605 : loss: 0.017280, loss_a: 0.010165
[00:10:08.582] iteration 10606 : loss: 0.454661, loss_a: 0.267448
[00:10:09.932] iteration 10607 : loss: 0.025542, loss_a: 0.015025
[00:10:10.669] iteration 10608 : loss: 0.038583, loss_a: 0.022696
[00:10:11.986] iteration 10609 : loss: 0.023389, loss_a: 0.013758
[00:10:12.724] iteration 10610 : loss: 0.018224, loss_a: 0.010720
[00:10:14.077] iteration 10611 : loss: 0.019979, loss_a: 0.011752
[00:10:14.823] iteration 10612 : loss: 0.041107, loss_a: 0.024181
[00:10:16.159] iteration 10613 : loss: 0.033495, loss_a: 0.019703
[00:10:16.894] iteration 10614 : loss: 0.019187, loss_a: 0.011286
[00:10:18.227] iteration 10615 : loss: 0.030388, loss_a: 0.017875
[00:10:18.970] iteration 10616 : loss: 0.018265, loss_a: 0.010744
[00:10:20.317] iteration 10617 : loss: 0.035173, loss_a: 0.020690
[00:10:21.051] iteration 10618 : loss: 0.051144, loss_a: 0.030085
[00:10:22.373] iteration 10619 : loss: 0.027144, loss_a: 0.015967
[00:10:23.121] iteration 10620 : loss: 0.044339, loss_a: 0.026082
[00:10:24.450] iteration 10621 : loss: 0.028381, loss_a: 0.016695
[00:10:25.191] iteration 10622 : loss: 0.017226, loss_a: 0.010133
[00:10:26.534] iteration 10623 : loss: 0.027521, loss_a: 0.016189
[00:10:27.274] iteration 10624 : loss: 0.014714, loss_a: 0.008655
[00:10:28.628] iteration 10625 : loss: 0.025580, loss_a: 0.015047
[00:10:29.384] iteration 10626 : loss: 0.040829, loss_a: 0.024017
[00:10:30.717] iteration 10627 : loss: 0.038461, loss_a: 0.022624
[00:10:31.451] iteration 10628 : loss: 0.013813, loss_a: 0.008125
[00:10:32.782] iteration 10629 : loss: 0.038744, loss_a: 0.022791
[00:10:33.530] iteration 10630 : loss: 0.020001, loss_a: 0.011765
[00:10:34.856] iteration 10631 : loss: 0.012777, loss_a: 0.007516
[00:10:35.602] iteration 10632 : loss: 0.018127, loss_a: 0.010663
[00:10:36.961] iteration 10633 : loss: 0.042094, loss_a: 0.024761
[00:10:37.707] iteration 10634 : loss: 0.043452, loss_a: 0.025560
[00:10:39.077] iteration 10635 : loss: 0.027075, loss_a: 0.015926
[00:10:39.819] iteration 10636 : loss: 0.011837, loss_a: 0.006963
[00:10:41.144] iteration 10637 : loss: 0.036284, loss_a: 0.021344
[00:10:41.919] iteration 10638 : loss: 0.031035, loss_a: 0.018256
[00:10:43.268] iteration 10639 : loss: 0.022967, loss_a: 0.013510
[00:10:44.009] iteration 10640 : loss: 0.090190, loss_a: 0.053053
[00:10:45.352] iteration 10641 : loss: 0.021092, loss_a: 0.012407
[00:10:46.093] iteration 10642 : loss: 0.030846, loss_a: 0.018145
[00:10:47.463] iteration 10643 : loss: 0.042236, loss_a: 0.024844
[00:10:48.207] iteration 10644 : loss: 0.056559, loss_a: 0.033270
[00:10:49.562] iteration 10645 : loss: 0.020644, loss_a: 0.012143
[00:10:50.309] iteration 10646 : loss: 0.022473, loss_a: 0.013219
[00:10:51.668] iteration 10647 : loss: 0.027673, loss_a: 0.016278
[00:10:52.410] iteration 10648 : loss: 0.018317, loss_a: 0.010774
[00:10:53.730] iteration 10649 : loss: 0.014957, loss_a: 0.008798
[00:10:54.467] iteration 10650 : loss: 0.025149, loss_a: 0.014793
[00:10:55.846] iteration 10651 : loss: 0.057411, loss_a: 0.033771
[00:10:56.589] iteration 10652 : loss: 0.031189, loss_a: 0.018347
[00:10:57.911] iteration 10653 : loss: 0.011335, loss_a: 0.006668
[00:10:58.652] iteration 10654 : loss: 0.012304, loss_a: 0.007238
[00:10:59.985] iteration 10655 : loss: 0.028651, loss_a: 0.016853
[00:11:00.732] iteration 10656 : loss: 0.021435, loss_a: 0.012609
[00:11:02.083] iteration 10657 : loss: 0.025442, loss_a: 0.014966
[00:11:02.818] iteration 10658 : loss: 0.024936, loss_a: 0.014668
[00:11:04.139] iteration 10659 : loss: 0.031073, loss_a: 0.018278
[00:11:04.888] iteration 10660 : loss: 0.023700, loss_a: 0.013941
[00:11:06.228] iteration 10661 : loss: 0.013075, loss_a: 0.007691
[00:11:06.971] iteration 10662 : loss: 0.017705, loss_a: 0.010415
[00:11:08.325] iteration 10663 : loss: 0.061146, loss_a: 0.035968
[00:11:09.060] iteration 10664 : loss: 0.033752, loss_a: 0.019854
[00:11:10.385] iteration 10665 : loss: 0.029036, loss_a: 0.017080
[00:11:11.140] iteration 10666 : loss: 0.052859, loss_a: 0.031094
[00:11:12.456] iteration 10667 : loss: 0.010581, loss_a: 0.006224
[00:11:13.202] iteration 10668 : loss: 0.029784, loss_a: 0.017520
[00:11:14.537] iteration 10669 : loss: 0.020414, loss_a: 0.012008
[00:11:15.281] iteration 10670 : loss: 0.041848, loss_a: 0.024617
[00:11:16.610] iteration 10671 : loss: 0.019352, loss_a: 0.011383
[00:11:17.362] iteration 10672 : loss: 0.046337, loss_a: 0.027257
[00:11:18.715] iteration 10673 : loss: 0.035793, loss_a: 0.021055
[00:11:19.463] iteration 10674 : loss: 0.020697, loss_a: 0.012175
[00:11:20.789] iteration 10675 : loss: 0.019900, loss_a: 0.011706
[00:11:21.529] iteration 10676 : loss: 0.014930, loss_a: 0.008783
[00:11:22.875] iteration 10677 : loss: 0.056647, loss_a: 0.033322
[00:11:23.615] iteration 10678 : loss: 0.037364, loss_a: 0.021979
[00:11:24.973] iteration 10679 : loss: 0.021425, loss_a: 0.012603
[00:11:25.711] iteration 10680 : loss: 0.012333, loss_a: 0.007255
[00:11:27.070] iteration 10681 : loss: 0.021071, loss_a: 0.012395
[00:11:27.804] iteration 10682 : loss: 0.014920, loss_a: 0.008776
[00:11:29.155] iteration 10683 : loss: 0.043289, loss_a: 0.025464
[00:11:29.894] iteration 10684 : loss: 0.029993, loss_a: 0.017643
[00:11:31.236] iteration 10685 : loss: 0.048809, loss_a: 0.028711
[00:11:31.977] iteration 10686 : loss: 0.014561, loss_a: 0.008566
[00:11:33.330] iteration 10687 : loss: 0.019190, loss_a: 0.011288
[00:11:34.070] iteration 10688 : loss: 0.020519, loss_a: 0.012070
[00:11:35.392] iteration 10689 : loss: 0.023540, loss_a: 0.013847
[00:11:36.128] iteration 10690 : loss: 0.017792, loss_a: 0.010466
[00:11:37.450] iteration 10691 : loss: 0.013051, loss_a: 0.007677
[00:11:38.183] iteration 10692 : loss: 0.017033, loss_a: 0.010019
[00:11:39.528] iteration 10693 : loss: 0.037344, loss_a: 0.021967
[00:11:40.269] iteration 10694 : loss: 0.037149, loss_a: 0.021852
[00:11:41.616] iteration 10695 : loss: 0.028040, loss_a: 0.016494
[00:11:42.360] iteration 10696 : loss: 0.016912, loss_a: 0.009948
[00:11:43.706] iteration 10697 : loss: 0.016489, loss_a: 0.009699
[00:11:44.443] iteration 10698 : loss: 0.024283, loss_a: 0.014284
[00:11:45.797] iteration 10699 : loss: 0.015210, loss_a: 0.008947
[00:11:46.545] iteration 10700 : loss: 0.026726, loss_a: 0.015721
[00:11:47.893] iteration 10701 : loss: 0.018044, loss_a: 0.010614
[00:11:48.621] iteration 10702 : loss: 0.068228, loss_a: 0.040134
[00:11:49.950] iteration 10703 : loss: 0.039326, loss_a: 0.023133
[00:11:50.684] iteration 10704 : loss: 0.019532, loss_a: 0.011489
[00:11:52.045] iteration 10705 : loss: 0.015887, loss_a: 0.009346
[00:11:52.786] iteration 10706 : loss: 0.014980, loss_a: 0.008812
[00:11:54.156] iteration 10707 : loss: 0.033850, loss_a: 0.019912
[00:11:54.895] iteration 10708 : loss: 0.016313, loss_a: 0.009596
[00:11:56.234] iteration 10709 : loss: 0.024674, loss_a: 0.014514
[00:11:56.982] iteration 10710 : loss: 0.017505, loss_a: 0.010297
[00:11:58.311] iteration 10711 : loss: 0.020366, loss_a: 0.011980
[00:11:59.057] iteration 10712 : loss: 0.041336, loss_a: 0.024315
[00:12:00.415] iteration 10713 : loss: 0.018559, loss_a: 0.010917
[00:12:01.152] iteration 10714 : loss: 0.025999, loss_a: 0.015294
[00:12:02.500] iteration 10715 : loss: 0.036697, loss_a: 0.021586
[00:12:03.238] iteration 10716 : loss: 0.022081, loss_a: 0.012989
[00:12:04.605] iteration 10717 : loss: 0.022719, loss_a: 0.013364
[00:12:05.362] iteration 10718 : loss: 0.072375, loss_a: 0.042573
[00:12:06.717] iteration 10719 : loss: 0.037224, loss_a: 0.021896
[00:12:07.452] iteration 10720 : loss: 0.026796, loss_a: 0.015762
[00:12:08.822] iteration 10721 : loss: 0.069753, loss_a: 0.041031
[00:12:09.567] iteration 10722 : loss: 0.023158, loss_a: 0.013622
[00:12:10.915] iteration 10723 : loss: 0.067835, loss_a: 0.039903
[00:12:11.657] iteration 10724 : loss: 0.017860, loss_a: 0.010506
[00:12:13.036] iteration 10725 : loss: 0.028483, loss_a: 0.016755
[00:12:13.771] iteration 10726 : loss: 0.014998, loss_a: 0.008823
[00:12:15.131] iteration 10727 : loss: 0.140124, loss_a: 0.082426
[00:12:15.864] iteration 10728 : loss: 0.010866, loss_a: 0.006392
[00:12:17.192] iteration 10729 : loss: 0.052654, loss_a: 0.030973
[00:12:17.949] iteration 10730 : loss: 0.039890, loss_a: 0.023465
[00:12:19.258] iteration 10731 : loss: 0.018835, loss_a: 0.011079
[00:12:20.015] iteration 10732 : loss: 0.055052, loss_a: 0.032384
[00:12:21.325] iteration 10733 : loss: 0.022744, loss_a: 0.013379
[00:12:22.073] iteration 10734 : loss: 0.036542, loss_a: 0.021495
[00:12:23.444] iteration 10735 : loss: 0.018513, loss_a: 0.010890
[00:12:24.197] iteration 10736 : loss: 0.037152, loss_a: 0.021854
[00:12:25.537] iteration 10737 : loss: 0.015333, loss_a: 0.009019
[00:12:26.281] iteration 10738 : loss: 0.020936, loss_a: 0.012315
[00:12:27.644] iteration 10739 : loss: 0.065082, loss_a: 0.038283
[00:12:28.377] iteration 10740 : loss: 0.017267, loss_a: 0.010157
[00:12:29.760] iteration 10741 : loss: 0.035642, loss_a: 0.020966
[00:12:30.497] iteration 10742 : loss: 0.021906, loss_a: 0.012886
[00:12:31.844] iteration 10743 : loss: 0.025497, loss_a: 0.014998
[00:12:32.589] iteration 10744 : loss: 0.035095, loss_a: 0.020644
[00:12:33.921] iteration 10745 : loss: 0.073459, loss_a: 0.043211
[00:12:34.670] iteration 10746 : loss: 0.031695, loss_a: 0.018644
[00:12:36.075] iteration 10747 : loss: 0.027904, loss_a: 0.016414
[00:12:36.820] iteration 10748 : loss: 0.027508, loss_a: 0.016181
[00:12:38.156] iteration 10749 : loss: 0.077581, loss_a: 0.045636
[00:12:38.899] iteration 10750 : loss: 0.049874, loss_a: 0.029337
[00:12:40.246] iteration 10751 : loss: 0.020743, loss_a: 0.012202
[00:12:40.979] iteration 10752 : loss: 0.011090, loss_a: 0.006523
[00:12:42.342] iteration 10753 : loss: 0.037348, loss_a: 0.021970
[00:12:43.084] iteration 10754 : loss: 0.040260, loss_a: 0.023683
[00:12:44.419] iteration 10755 : loss: 0.044284, loss_a: 0.026050
[00:12:45.161] iteration 10756 : loss: 0.027335, loss_a: 0.016079
[00:12:46.502] iteration 10757 : loss: 0.037932, loss_a: 0.022313
[00:12:47.237] iteration 10758 : loss: 0.025946, loss_a: 0.015262
[00:12:48.563] iteration 10759 : loss: 0.021927, loss_a: 0.012898
[00:12:49.307] iteration 10760 : loss: 0.054675, loss_a: 0.032162
[00:12:50.667] iteration 10761 : loss: 0.016297, loss_a: 0.009586
[00:12:51.408] iteration 10762 : loss: 0.016436, loss_a: 0.009668
[00:12:52.751] iteration 10763 : loss: 0.011319, loss_a: 0.006658
[00:12:53.486] iteration 10764 : loss: 0.024468, loss_a: 0.014393
[00:12:54.842] iteration 10765 : loss: 0.050169, loss_a: 0.029511
[00:12:55.591] iteration 10766 : loss: 0.019075, loss_a: 0.011220
[00:12:56.934] iteration 10767 : loss: 0.035128, loss_a: 0.020664
[00:12:57.682] iteration 10768 : loss: 0.030029, loss_a: 0.017664
[00:12:59.034] iteration 10769 : loss: 0.024932, loss_a: 0.014666
[00:12:59.777] iteration 10770 : loss: 0.030046, loss_a: 0.017674
[00:13:01.143] iteration 10771 : loss: 0.025781, loss_a: 0.015165
[00:13:01.882] iteration 10772 : loss: 0.041721, loss_a: 0.024542
[00:13:03.233] iteration 10773 : loss: 0.012795, loss_a: 0.007527
[00:13:03.972] iteration 10774 : loss: 0.030401, loss_a: 0.017883
[00:13:05.316] iteration 10775 : loss: 0.017634, loss_a: 0.010373
[00:13:06.060] iteration 10776 : loss: 0.015463, loss_a: 0.009096
[00:13:07.383] iteration 10777 : loss: 0.037372, loss_a: 0.021984
[00:13:08.135] iteration 10778 : loss: 0.031013, loss_a: 0.018243
[00:13:09.471] iteration 10779 : loss: 0.021502, loss_a: 0.012648
[00:13:10.221] iteration 10780 : loss: 0.042263, loss_a: 0.024860
[00:13:11.538] iteration 10781 : loss: 0.013436, loss_a: 0.007904
[00:13:12.277] iteration 10782 : loss: 0.023351, loss_a: 0.013736
[00:13:13.613] iteration 10783 : loss: 0.046053, loss_a: 0.027090
[00:13:14.358] iteration 10784 : loss: 0.035661, loss_a: 0.020977
[00:13:15.684] iteration 10785 : loss: 0.021410, loss_a: 0.012594
[00:13:16.432] iteration 10786 : loss: 0.026324, loss_a: 0.015485
[00:13:17.793] iteration 10787 : loss: 0.024885, loss_a: 0.014638
[00:13:18.543] iteration 10788 : loss: 0.050072, loss_a: 0.029454
[00:13:19.865] iteration 10789 : loss: 0.034471, loss_a: 0.020277
[00:13:20.626] iteration 10790 : loss: 0.059392, loss_a: 0.034936
[00:13:21.951] iteration 10791 : loss: 0.030408, loss_a: 0.017887
[00:13:22.684] iteration 10792 : loss: 0.044094, loss_a: 0.025938
[00:13:24.013] iteration 10793 : loss: 0.023922, loss_a: 0.014072
[00:13:24.748] iteration 10794 : loss: 0.013783, loss_a: 0.008108
[00:13:26.097] iteration 10795 : loss: 0.031636, loss_a: 0.018609
[00:13:26.845] iteration 10796 : loss: 0.023379, loss_a: 0.013752
[00:13:28.193] iteration 10797 : loss: 0.049284, loss_a: 0.028990
[00:13:28.942] iteration 10798 : loss: 0.018847, loss_a: 0.011086
[00:13:30.262] iteration 10799 : loss: 0.020734, loss_a: 0.012196
[00:13:31.008] iteration 10800 : loss: 0.022783, loss_a: 0.013402
[00:13:55.640] iteration 10801 : loss: 0.041302, loss_a: 0.024295
[00:13:57.843] iteration 10802 : loss: 0.022071, loss_a: 0.012983
[00:13:59.168] iteration 10803 : loss: 0.046967, loss_a: 0.027627
[00:13:59.924] iteration 10804 : loss: 0.030635, loss_a: 0.018020
[00:14:01.272] iteration 10805 : loss: 0.027478, loss_a: 0.016163
[00:14:02.007] iteration 10806 : loss: 0.022818, loss_a: 0.013422
[00:14:03.360] iteration 10807 : loss: 0.033800, loss_a: 0.019882
[00:14:04.106] iteration 10808 : loss: 0.012953, loss_a: 0.007619
[00:14:05.461] iteration 10809 : loss: 0.040439, loss_a: 0.023788
[00:14:06.197] iteration 10810 : loss: 0.031335, loss_a: 0.018432
[00:14:07.525] iteration 10811 : loss: 0.026827, loss_a: 0.015781
[00:14:08.269] iteration 10812 : loss: 0.022605, loss_a: 0.013297
[00:14:09.595] iteration 10813 : loss: 0.020256, loss_a: 0.011915
[00:14:10.344] iteration 10814 : loss: 0.030486, loss_a: 0.017933
[00:14:11.657] iteration 10815 : loss: 0.045997, loss_a: 0.027057
[00:14:12.392] iteration 10816 : loss: 0.019142, loss_a: 0.011260
[00:14:13.710] iteration 10817 : loss: 0.021675, loss_a: 0.012750
[00:14:14.449] iteration 10818 : loss: 0.024725, loss_a: 0.014544
[00:14:15.807] iteration 10819 : loss: 0.024107, loss_a: 0.014180
[00:14:16.547] iteration 10820 : loss: 0.034969, loss_a: 0.020570
[00:14:17.871] iteration 10821 : loss: 0.032850, loss_a: 0.019323
[00:14:18.613] iteration 10822 : loss: 0.020068, loss_a: 0.011805
[00:14:19.935] iteration 10823 : loss: 0.026438, loss_a: 0.015551
[00:14:20.668] iteration 10824 : loss: 0.031628, loss_a: 0.018605
[00:14:22.001] iteration 10825 : loss: 0.029189, loss_a: 0.017170
[00:14:22.734] iteration 10826 : loss: 0.018094, loss_a: 0.010644
[00:14:24.094] iteration 10827 : loss: 0.048011, loss_a: 0.028242
[00:14:24.825] iteration 10828 : loss: 0.019433, loss_a: 0.011431
[00:14:26.162] iteration 10829 : loss: 0.056342, loss_a: 0.033143
[00:14:26.909] iteration 10830 : loss: 0.030834, loss_a: 0.018137
[00:14:28.253] iteration 10831 : loss: 0.034859, loss_a: 0.020505
[00:14:28.991] iteration 10832 : loss: 0.039499, loss_a: 0.023235
[00:14:30.316] iteration 10833 : loss: 0.026957, loss_a: 0.015857
[00:14:31.058] iteration 10834 : loss: 0.021931, loss_a: 0.012900
[00:14:32.387] iteration 10835 : loss: 0.034015, loss_a: 0.020009
[00:14:33.122] iteration 10836 : loss: 0.013356, loss_a: 0.007856
[00:14:34.457] iteration 10837 : loss: 0.012507, loss_a: 0.007357
[00:14:35.193] iteration 10838 : loss: 0.017520, loss_a: 0.010306
[00:14:36.555] iteration 10839 : loss: 0.020947, loss_a: 0.012322
[00:14:37.301] iteration 10840 : loss: 0.026090, loss_a: 0.015347
[00:14:38.639] iteration 10841 : loss: 0.012489, loss_a: 0.007346
[00:14:39.388] iteration 10842 : loss: 0.060331, loss_a: 0.035489
[00:14:40.703] iteration 10843 : loss: 0.040545, loss_a: 0.023850
[00:14:41.446] iteration 10844 : loss: 0.036207, loss_a: 0.021298
[00:14:42.766] iteration 10845 : loss: 0.017956, loss_a: 0.010563
[00:14:43.512] iteration 10846 : loss: 0.016265, loss_a: 0.009568
[00:14:44.851] iteration 10847 : loss: 0.039717, loss_a: 0.023363
[00:14:45.589] iteration 10848 : loss: 0.021343, loss_a: 0.012554
[00:14:46.936] iteration 10849 : loss: 0.018449, loss_a: 0.010852
[00:14:47.681] iteration 10850 : loss: 0.021793, loss_a: 0.012819
[00:14:49.016] iteration 10851 : loss: 0.025522, loss_a: 0.015013
[00:14:49.767] iteration 10852 : loss: 0.030185, loss_a: 0.017756
[00:14:51.129] iteration 10853 : loss: 0.023760, loss_a: 0.013976
[00:14:51.870] iteration 10854 : loss: 0.015908, loss_a: 0.009358
[00:14:53.229] iteration 10855 : loss: 0.058708, loss_a: 0.034534
[00:14:53.966] iteration 10856 : loss: 0.023553, loss_a: 0.013855
[00:14:55.322] iteration 10857 : loss: 0.039069, loss_a: 0.022982
[00:14:56.053] iteration 10858 : loss: 0.022835, loss_a: 0.013432
[00:14:57.394] iteration 10859 : loss: 0.019267, loss_a: 0.011334
[00:14:58.133] iteration 10860 : loss: 0.016519, loss_a: 0.009717
[00:14:59.467] iteration 10861 : loss: 0.027408, loss_a: 0.016123
[00:15:00.210] iteration 10862 : loss: 0.024408, loss_a: 0.014358
[00:15:01.531] iteration 10863 : loss: 0.029311, loss_a: 0.017242
[00:15:02.286] iteration 10864 : loss: 0.030267, loss_a: 0.017804
[00:15:03.610] iteration 10865 : loss: 0.047049, loss_a: 0.027676
[00:15:04.364] iteration 10866 : loss: 0.036037, loss_a: 0.021199
[00:15:05.704] iteration 10867 : loss: 0.037226, loss_a: 0.021898
[00:15:06.447] iteration 10868 : loss: 0.022457, loss_a: 0.013210
[00:15:07.757] iteration 10869 : loss: 0.022192, loss_a: 0.013054
[00:15:08.500] iteration 10870 : loss: 0.030889, loss_a: 0.018170
[00:15:09.851] iteration 10871 : loss: 0.045885, loss_a: 0.026991
[00:15:10.591] iteration 10872 : loss: 0.025431, loss_a: 0.014960
[00:15:11.886] iteration 10873 : loss: 0.022182, loss_a: 0.013048
[00:15:12.635] iteration 10874 : loss: 0.062484, loss_a: 0.036755
[00:15:13.972] iteration 10875 : loss: 0.017830, loss_a: 0.010488
[00:15:14.715] iteration 10876 : loss: 0.023890, loss_a: 0.014053
[00:15:16.051] iteration 10877 : loss: 0.022831, loss_a: 0.013430
[00:15:16.795] iteration 10878 : loss: 0.024645, loss_a: 0.014497
[00:15:18.155] iteration 10879 : loss: 0.018727, loss_a: 0.011016
[00:15:18.893] iteration 10880 : loss: 0.019194, loss_a: 0.011291
[00:15:20.229] iteration 10881 : loss: 0.037206, loss_a: 0.021886
[00:15:20.970] iteration 10882 : loss: 0.039521, loss_a: 0.023248
[00:15:22.332] iteration 10883 : loss: 0.027809, loss_a: 0.016358
[00:15:23.078] iteration 10884 : loss: 0.021552, loss_a: 0.012678
[00:15:24.432] iteration 10885 : loss: 0.034981, loss_a: 0.020577
[00:15:25.179] iteration 10886 : loss: 0.044920, loss_a: 0.026423
[00:15:26.498] iteration 10887 : loss: 0.050397, loss_a: 0.029645
[00:15:27.244] iteration 10888 : loss: 0.019357, loss_a: 0.011387
[00:15:28.607] iteration 10889 : loss: 0.049763, loss_a: 0.029273
[00:15:29.344] iteration 10890 : loss: 0.032176, loss_a: 0.018927
[00:15:30.679] iteration 10891 : loss: 0.032820, loss_a: 0.019306
[00:15:31.414] iteration 10892 : loss: 0.024646, loss_a: 0.014497
[00:15:32.767] iteration 10893 : loss: 0.042912, loss_a: 0.025242
[00:15:33.506] iteration 10894 : loss: 0.027012, loss_a: 0.015890
[00:15:34.832] iteration 10895 : loss: 0.040908, loss_a: 0.024064
[00:15:35.572] iteration 10896 : loss: 0.036012, loss_a: 0.021184
[00:15:36.942] iteration 10897 : loss: 0.058400, loss_a: 0.034353
[00:15:37.693] iteration 10898 : loss: 0.024913, loss_a: 0.014654
[00:15:39.016] iteration 10899 : loss: 0.052474, loss_a: 0.030867
[00:15:39.754] iteration 10900 : loss: 0.018807, loss_a: 0.011063
[00:15:41.084] iteration 10901 : loss: 0.032585, loss_a: 0.019167
[00:15:41.824] iteration 10902 : loss: 0.022304, loss_a: 0.013120
[00:15:43.179] iteration 10903 : loss: 0.017460, loss_a: 0.010271
[00:15:43.918] iteration 10904 : loss: 0.061827, loss_a: 0.036369
[00:15:45.238] iteration 10905 : loss: 0.021220, loss_a: 0.012482
[00:15:45.985] iteration 10906 : loss: 0.019980, loss_a: 0.011753
[00:15:47.337] iteration 10907 : loss: 0.039904, loss_a: 0.023473
[00:15:48.083] iteration 10908 : loss: 0.029948, loss_a: 0.017617
[00:15:49.417] iteration 10909 : loss: 0.045522, loss_a: 0.026778
[00:15:50.151] iteration 10910 : loss: 0.016079, loss_a: 0.009458
[00:15:51.479] iteration 10911 : loss: 0.018908, loss_a: 0.011123
[00:15:52.215] iteration 10912 : loss: 0.032005, loss_a: 0.018827
[00:15:53.597] iteration 10913 : loss: 0.020414, loss_a: 0.012008
[00:15:54.340] iteration 10914 : loss: 0.021424, loss_a: 0.012602
[00:15:55.662] iteration 10915 : loss: 0.027000, loss_a: 0.015883
[00:15:56.421] iteration 10916 : loss: 0.024080, loss_a: 0.014165
[00:15:57.727] iteration 10917 : loss: 0.014268, loss_a: 0.008393
[00:15:58.475] iteration 10918 : loss: 0.031854, loss_a: 0.018738
[00:15:59.791] iteration 10919 : loss: 0.019296, loss_a: 0.011351
[00:16:00.522] iteration 10920 : loss: 0.015302, loss_a: 0.009001
[00:16:01.843] iteration 10921 : loss: 0.016361, loss_a: 0.009624
[00:16:02.579] iteration 10922 : loss: 0.015761, loss_a: 0.009271
[00:16:03.915] iteration 10923 : loss: 0.036803, loss_a: 0.021649
[00:16:04.664] iteration 10924 : loss: 0.022101, loss_a: 0.013001
[00:16:06.036] iteration 10925 : loss: 0.019205, loss_a: 0.011297
[00:16:06.775] iteration 10926 : loss: 0.038922, loss_a: 0.022895
[00:16:08.096] iteration 10927 : loss: 0.016387, loss_a: 0.009639
[00:16:08.829] iteration 10928 : loss: 0.016528, loss_a: 0.009722
[00:16:10.174] iteration 10929 : loss: 0.023994, loss_a: 0.014114
[00:16:10.921] iteration 10930 : loss: 0.019661, loss_a: 0.011565
[00:16:12.242] iteration 10931 : loss: 0.031944, loss_a: 0.018791
[00:16:12.970] iteration 10932 : loss: 0.016593, loss_a: 0.009761
[00:16:14.324] iteration 10933 : loss: 0.015641, loss_a: 0.009200
[00:16:15.068] iteration 10934 : loss: 0.040712, loss_a: 0.023948
[00:16:16.430] iteration 10935 : loss: 0.041139, loss_a: 0.024199
[00:16:17.160] iteration 10936 : loss: 0.083442, loss_a: 0.049083
[00:16:18.521] iteration 10937 : loss: 0.018967, loss_a: 0.011157
[00:16:19.268] iteration 10938 : loss: 0.018389, loss_a: 0.010817
[00:16:20.621] iteration 10939 : loss: 0.040497, loss_a: 0.023822
[00:16:21.358] iteration 10940 : loss: 0.018539, loss_a: 0.010906
[00:16:22.679] iteration 10941 : loss: 0.011826, loss_a: 0.006957
[00:16:23.412] iteration 10942 : loss: 0.011179, loss_a: 0.006576
[00:16:24.730] iteration 10943 : loss: 0.017920, loss_a: 0.010541
[00:16:25.480] iteration 10944 : loss: 0.057359, loss_a: 0.033740
[00:16:26.793] iteration 10945 : loss: 0.021881, loss_a: 0.012871
[00:16:27.542] iteration 10946 : loss: 0.087394, loss_a: 0.051408
[00:16:28.879] iteration 10947 : loss: 0.014063, loss_a: 0.008272
[00:16:29.629] iteration 10948 : loss: 0.021728, loss_a: 0.012781
[00:16:30.990] iteration 10949 : loss: 0.029636, loss_a: 0.017433
[00:16:31.722] iteration 10950 : loss: 0.035713, loss_a: 0.021008
[00:16:33.068] iteration 10951 : loss: 0.023151, loss_a: 0.013618
[00:16:33.799] iteration 10952 : loss: 0.040010, loss_a: 0.023535
[00:16:35.150] iteration 10953 : loss: 0.066419, loss_a: 0.039070
[00:16:35.889] iteration 10954 : loss: 0.036405, loss_a: 0.021415
[00:16:37.243] iteration 10955 : loss: 0.025102, loss_a: 0.014766
[00:16:37.983] iteration 10956 : loss: 0.038945, loss_a: 0.022909
[00:16:39.317] iteration 10957 : loss: 0.013005, loss_a: 0.007650
[00:16:40.061] iteration 10958 : loss: 0.032979, loss_a: 0.019399
[00:16:41.414] iteration 10959 : loss: 0.057371, loss_a: 0.033748
[00:16:42.145] iteration 10960 : loss: 0.018977, loss_a: 0.011163
[00:16:43.485] iteration 10961 : loss: 0.028930, loss_a: 0.017018
[00:16:44.250] iteration 10962 : loss: 0.030314, loss_a: 0.017832
[00:16:45.612] iteration 10963 : loss: 0.046623, loss_a: 0.027425
[00:16:46.363] iteration 10964 : loss: 0.058441, loss_a: 0.034377
[00:16:47.721] iteration 10965 : loss: 0.114749, loss_a: 0.067499
[00:16:48.463] iteration 10966 : loss: 0.036387, loss_a: 0.021404
[00:16:49.788] iteration 10967 : loss: 0.038100, loss_a: 0.022412
[00:16:50.528] iteration 10968 : loss: 0.017695, loss_a: 0.010409
[00:16:51.896] iteration 10969 : loss: 0.026989, loss_a: 0.015876
[00:16:52.629] iteration 10970 : loss: 0.036234, loss_a: 0.021314
[00:16:53.952] iteration 10971 : loss: 0.047575, loss_a: 0.027985
[00:16:54.695] iteration 10972 : loss: 0.024337, loss_a: 0.014316
[00:16:56.017] iteration 10973 : loss: 0.021738, loss_a: 0.012787
[00:16:56.767] iteration 10974 : loss: 0.041805, loss_a: 0.024591
[00:16:58.087] iteration 10975 : loss: 0.026894, loss_a: 0.015820
[00:16:58.842] iteration 10976 : loss: 0.059192, loss_a: 0.034819
[00:17:00.248] iteration 10977 : loss: 0.030051, loss_a: 0.017677
[00:17:00.990] iteration 10978 : loss: 0.055345, loss_a: 0.032556
[00:17:02.359] iteration 10979 : loss: 0.029548, loss_a: 0.017381
[00:17:03.108] iteration 10980 : loss: 0.033067, loss_a: 0.019451
[00:17:04.424] iteration 10981 : loss: 0.044090, loss_a: 0.025935
[00:17:05.164] iteration 10982 : loss: 0.037631, loss_a: 0.022136
[00:17:06.519] iteration 10983 : loss: 0.037776, loss_a: 0.022221
[00:17:07.263] iteration 10984 : loss: 0.032350, loss_a: 0.019030
[00:17:08.583] iteration 10985 : loss: 0.042805, loss_a: 0.025179
[00:17:09.323] iteration 10986 : loss: 0.021369, loss_a: 0.012570
[00:17:10.672] iteration 10987 : loss: 0.035329, loss_a: 0.020782
[00:17:11.411] iteration 10988 : loss: 0.032010, loss_a: 0.018830
[00:17:12.752] iteration 10989 : loss: 0.031625, loss_a: 0.018603
[00:17:13.491] iteration 10990 : loss: 0.031182, loss_a: 0.018342
[00:17:14.818] iteration 10991 : loss: 0.016731, loss_a: 0.009842
[00:17:15.556] iteration 10992 : loss: 0.035612, loss_a: 0.020948
[00:17:16.906] iteration 10993 : loss: 0.075925, loss_a: 0.044662
[00:17:17.660] iteration 10994 : loss: 0.025880, loss_a: 0.015223
[00:17:18.974] iteration 10995 : loss: 0.031319, loss_a: 0.018423
[00:17:19.716] iteration 10996 : loss: 0.024193, loss_a: 0.014231
[00:17:21.053] iteration 10997 : loss: 0.018139, loss_a: 0.010670
[00:17:21.786] iteration 10998 : loss: 0.058159, loss_a: 0.034211
[00:17:23.112] iteration 10999 : loss: 0.023711, loss_a: 0.013948
[00:17:23.861] iteration 11000 : loss: 0.019607, loss_a: 0.011533
[00:17:48.501] iteration 11001 : loss: 0.024308, loss_a: 0.014299
[00:17:50.782] iteration 11002 : loss: 0.053087, loss_a: 0.031228
[00:17:52.141] iteration 11003 : loss: 0.016503, loss_a: 0.009707
[00:17:52.885] iteration 11004 : loss: 0.041738, loss_a: 0.024552
[00:17:54.212] iteration 11005 : loss: 0.079138, loss_a: 0.046552
[00:17:54.954] iteration 11006 : loss: 0.045087, loss_a: 0.026522
[00:17:56.311] iteration 11007 : loss: 0.033649, loss_a: 0.019794
[00:17:57.050] iteration 11008 : loss: 0.026870, loss_a: 0.015806
[00:17:58.382] iteration 11009 : loss: 0.029103, loss_a: 0.017119
[00:17:59.123] iteration 11010 : loss: 0.030903, loss_a: 0.018178
[00:18:00.459] iteration 11011 : loss: 0.025164, loss_a: 0.014802
[00:18:01.192] iteration 11012 : loss: 0.037228, loss_a: 0.021899
[00:18:02.548] iteration 11013 : loss: 0.074639, loss_a: 0.043905
[00:18:03.293] iteration 11014 : loss: 0.088248, loss_a: 0.051911
[00:18:04.640] iteration 11015 : loss: 0.030885, loss_a: 0.018168
[00:18:05.386] iteration 11016 : loss: 0.020919, loss_a: 0.012305
[00:18:06.754] iteration 11017 : loss: 0.528841, loss_a: 0.311083
[00:18:07.493] iteration 11018 : loss: 0.026691, loss_a: 0.015701
[00:18:08.858] iteration 11019 : loss: 0.064377, loss_a: 0.037869
[00:18:09.601] iteration 11020 : loss: 0.067361, loss_a: 0.039624
[00:18:10.966] iteration 11021 : loss: 0.025641, loss_a: 0.015083
[00:18:11.713] iteration 11022 : loss: 0.046896, loss_a: 0.027586
[00:18:13.055] iteration 11023 : loss: 0.052866, loss_a: 0.031097
[00:18:13.797] iteration 11024 : loss: 0.019481, loss_a: 0.011460
[00:18:15.123] iteration 11025 : loss: 0.020420, loss_a: 0.012012
[00:18:15.864] iteration 11026 : loss: 0.031711, loss_a: 0.018653
[00:18:17.208] iteration 11027 : loss: 0.020711, loss_a: 0.012183
[00:18:17.951] iteration 11028 : loss: 0.045231, loss_a: 0.026607
[00:18:19.277] iteration 11029 : loss: 0.072355, loss_a: 0.042562
[00:18:20.025] iteration 11030 : loss: 0.036795, loss_a: 0.021644
[00:18:21.384] iteration 11031 : loss: 0.038953, loss_a: 0.022913
[00:18:22.119] iteration 11032 : loss: 0.016526, loss_a: 0.009721
[00:18:23.479] iteration 11033 : loss: 0.028625, loss_a: 0.016838
[00:18:24.232] iteration 11034 : loss: 0.030841, loss_a: 0.018142
[00:18:25.561] iteration 11035 : loss: 0.023465, loss_a: 0.013803
[00:18:26.321] iteration 11036 : loss: 0.037129, loss_a: 0.021841
[00:18:27.633] iteration 11037 : loss: 0.019988, loss_a: 0.011758
[00:18:28.377] iteration 11038 : loss: 0.022509, loss_a: 0.013241
[00:18:29.718] iteration 11039 : loss: 0.014383, loss_a: 0.008460
[00:18:30.449] iteration 11040 : loss: 0.022315, loss_a: 0.013127
[00:18:31.798] iteration 11041 : loss: 0.057478, loss_a: 0.033810
[00:18:32.541] iteration 11042 : loss: 0.025461, loss_a: 0.014977
[00:18:33.892] iteration 11043 : loss: 0.031973, loss_a: 0.018807
[00:18:34.640] iteration 11044 : loss: 0.031286, loss_a: 0.018403
[00:18:35.946] iteration 11045 : loss: 0.031165, loss_a: 0.018332
[00:18:36.681] iteration 11046 : loss: 0.028404, loss_a: 0.016708
[00:18:38.015] iteration 11047 : loss: 0.037445, loss_a: 0.022027
[00:18:38.753] iteration 11048 : loss: 0.019335, loss_a: 0.011373
[00:18:40.120] iteration 11049 : loss: 0.048504, loss_a: 0.028532
[00:18:40.859] iteration 11050 : loss: 0.021845, loss_a: 0.012850
[00:18:42.193] iteration 11051 : loss: 0.046452, loss_a: 0.027325
[00:18:42.936] iteration 11052 : loss: 0.027000, loss_a: 0.015882
[00:18:44.267] iteration 11053 : loss: 0.021613, loss_a: 0.012714
[00:18:45.007] iteration 11054 : loss: 0.023598, loss_a: 0.013881
[00:18:46.331] iteration 11055 : loss: 0.021020, loss_a: 0.012365
[00:18:47.074] iteration 11056 : loss: 0.022313, loss_a: 0.013125
[00:18:48.410] iteration 11057 : loss: 0.056958, loss_a: 0.033505
[00:18:49.150] iteration 11058 : loss: 0.031554, loss_a: 0.018561
[00:18:50.496] iteration 11059 : loss: 0.028779, loss_a: 0.016929
[00:18:51.236] iteration 11060 : loss: 0.021302, loss_a: 0.012530
[00:18:52.557] iteration 11061 : loss: 0.029938, loss_a: 0.017611
[00:18:53.298] iteration 11062 : loss: 0.041096, loss_a: 0.024174
[00:18:54.620] iteration 11063 : loss: 0.015711, loss_a: 0.009242
[00:18:55.356] iteration 11064 : loss: 0.019712, loss_a: 0.011595
[00:18:56.698] iteration 11065 : loss: 0.033315, loss_a: 0.019597
[00:18:57.436] iteration 11066 : loss: 0.017736, loss_a: 0.010433
[00:18:58.777] iteration 11067 : loss: 0.013825, loss_a: 0.008132
[00:18:59.527] iteration 11068 : loss: 0.018809, loss_a: 0.011064
[00:19:00.846] iteration 11069 : loss: 0.030564, loss_a: 0.017979
[00:19:01.591] iteration 11070 : loss: 0.028140, loss_a: 0.016553
[00:19:02.915] iteration 11071 : loss: 0.053352, loss_a: 0.031384
[00:19:03.657] iteration 11072 : loss: 0.025098, loss_a: 0.014764
[00:19:04.986] iteration 11073 : loss: 0.042455, loss_a: 0.024974
[00:19:05.726] iteration 11074 : loss: 0.039724, loss_a: 0.023367
[00:19:07.058] iteration 11075 : loss: 0.024206, loss_a: 0.014239
[00:19:07.805] iteration 11076 : loss: 0.027107, loss_a: 0.015945
[00:19:09.145] iteration 11077 : loss: 0.022944, loss_a: 0.013496
[00:19:09.895] iteration 11078 : loss: 0.042074, loss_a: 0.024750
[00:19:11.280] iteration 11079 : loss: 0.026154, loss_a: 0.015385
[00:19:12.019] iteration 11080 : loss: 0.014760, loss_a: 0.008682
[00:19:13.360] iteration 11081 : loss: 0.023998, loss_a: 0.014117
[00:19:14.095] iteration 11082 : loss: 0.014211, loss_a: 0.008360
[00:19:15.421] iteration 11083 : loss: 0.033181, loss_a: 0.019518
[00:19:16.150] iteration 11084 : loss: 0.018813, loss_a: 0.011067
[00:19:17.489] iteration 11085 : loss: 0.014768, loss_a: 0.008687
[00:19:18.223] iteration 11086 : loss: 0.016478, loss_a: 0.009693
[00:19:19.589] iteration 11087 : loss: 0.022121, loss_a: 0.013012
[00:19:20.335] iteration 11088 : loss: 0.037479, loss_a: 0.022047
[00:19:21.669] iteration 11089 : loss: 0.018901, loss_a: 0.011118
[00:19:22.411] iteration 11090 : loss: 0.013926, loss_a: 0.008192
[00:19:23.759] iteration 11091 : loss: 0.019763, loss_a: 0.011625
[00:19:24.490] iteration 11092 : loss: 0.026297, loss_a: 0.015469
[00:19:25.855] iteration 11093 : loss: 0.022462, loss_a: 0.013213
[00:19:26.605] iteration 11094 : loss: 0.030713, loss_a: 0.018066
[00:19:27.939] iteration 11095 : loss: 0.016716, loss_a: 0.009833
[00:19:28.677] iteration 11096 : loss: 0.016360, loss_a: 0.009623
[00:19:30.019] iteration 11097 : loss: 0.019781, loss_a: 0.011636
[00:19:30.752] iteration 11098 : loss: 0.029177, loss_a: 0.017163
[00:19:32.108] iteration 11099 : loss: 0.010198, loss_a: 0.005999
[00:19:32.857] iteration 11100 : loss: 0.042552, loss_a: 0.025030
[00:19:34.210] iteration 11101 : loss: 0.037758, loss_a: 0.022211
[00:19:34.952] iteration 11102 : loss: 0.021101, loss_a: 0.012412
[00:19:36.264] iteration 11103 : loss: 0.023149, loss_a: 0.013617
[00:19:37.012] iteration 11104 : loss: 0.040873, loss_a: 0.024043
[00:19:38.355] iteration 11105 : loss: 0.017832, loss_a: 0.010490
[00:19:39.097] iteration 11106 : loss: 0.025523, loss_a: 0.015013
[00:19:40.420] iteration 11107 : loss: 0.032968, loss_a: 0.019393
[00:19:41.163] iteration 11108 : loss: 0.024451, loss_a: 0.014383
[00:19:42.545] iteration 11109 : loss: 0.024446, loss_a: 0.014380
[00:19:43.293] iteration 11110 : loss: 0.027993, loss_a: 0.016466
[00:19:44.627] iteration 11111 : loss: 0.012462, loss_a: 0.007330
[00:19:45.367] iteration 11112 : loss: 0.023094, loss_a: 0.013584
[00:19:46.696] iteration 11113 : loss: 0.012273, loss_a: 0.007220
[00:19:47.439] iteration 11114 : loss: 0.044845, loss_a: 0.026379
[00:19:48.769] iteration 11115 : loss: 0.033522, loss_a: 0.019719
[00:19:49.511] iteration 11116 : loss: 0.023715, loss_a: 0.013950
[00:19:50.850] iteration 11117 : loss: 0.024900, loss_a: 0.014647
[00:19:51.602] iteration 11118 : loss: 0.023766, loss_a: 0.013980
[00:19:52.961] iteration 11119 : loss: 0.048626, loss_a: 0.028604
[00:19:53.725] iteration 11120 : loss: 0.065620, loss_a: 0.038600
[00:19:55.045] iteration 11121 : loss: 0.022055, loss_a: 0.012974
[00:19:55.802] iteration 11122 : loss: 0.020297, loss_a: 0.011939
[00:19:57.127] iteration 11123 : loss: 0.053995, loss_a: 0.031762
[00:19:57.864] iteration 11124 : loss: 0.019105, loss_a: 0.011238
[00:19:59.181] iteration 11125 : loss: 0.055853, loss_a: 0.032855
[00:19:59.928] iteration 11126 : loss: 0.025366, loss_a: 0.014921
[00:20:01.283] iteration 11127 : loss: 0.038398, loss_a: 0.022587
[00:20:02.051] iteration 11128 : loss: 0.086311, loss_a: 0.050771
[00:20:03.378] iteration 11129 : loss: 0.030638, loss_a: 0.018022
[00:20:04.126] iteration 11130 : loss: 0.065518, loss_a: 0.038540
[00:20:05.501] iteration 11131 : loss: 0.083314, loss_a: 0.049008
[00:20:06.244] iteration 11132 : loss: 0.038000, loss_a: 0.022353
[00:20:07.568] iteration 11133 : loss: 0.029001, loss_a: 0.017060
[00:20:08.310] iteration 11134 : loss: 0.026200, loss_a: 0.015412
[00:20:09.634] iteration 11135 : loss: 0.019975, loss_a: 0.011750
[00:20:10.392] iteration 11136 : loss: 0.065183, loss_a: 0.038343
[00:20:11.745] iteration 11137 : loss: 0.028819, loss_a: 0.016952
[00:20:12.498] iteration 11138 : loss: 0.039351, loss_a: 0.023148
[00:20:13.849] iteration 11139 : loss: 0.039251, loss_a: 0.023089
[00:20:14.600] iteration 11140 : loss: 0.023289, loss_a: 0.013699
[00:20:15.917] iteration 11141 : loss: 0.023452, loss_a: 0.013796
[00:20:16.646] iteration 11142 : loss: 0.065759, loss_a: 0.038681
[00:20:17.986] iteration 11143 : loss: 0.023712, loss_a: 0.013948
[00:20:18.733] iteration 11144 : loss: 0.087715, loss_a: 0.051597
[00:20:20.068] iteration 11145 : loss: 0.024393, loss_a: 0.014349
[00:20:20.803] iteration 11146 : loss: 0.029158, loss_a: 0.017152
[00:20:22.139] iteration 11147 : loss: 0.017412, loss_a: 0.010242
[00:20:22.877] iteration 11148 : loss: 0.021924, loss_a: 0.012897
[00:20:24.227] iteration 11149 : loss: 0.041372, loss_a: 0.024336
[00:20:24.964] iteration 11150 : loss: 0.011449, loss_a: 0.006735
[00:20:26.301] iteration 11151 : loss: 0.049893, loss_a: 0.029349
[00:20:27.041] iteration 11152 : loss: 0.038495, loss_a: 0.022644
[00:20:28.398] iteration 11153 : loss: 0.037928, loss_a: 0.022311
[00:20:29.130] iteration 11154 : loss: 0.029190, loss_a: 0.017170
[00:20:30.446] iteration 11155 : loss: 0.017829, loss_a: 0.010488
[00:20:31.189] iteration 11156 : loss: 0.017599, loss_a: 0.010352
[00:20:32.513] iteration 11157 : loss: 0.025353, loss_a: 0.014914
[00:20:33.244] iteration 11158 : loss: 0.020388, loss_a: 0.011993
[00:20:34.588] iteration 11159 : loss: 0.035097, loss_a: 0.020646
[00:20:35.320] iteration 11160 : loss: 0.016945, loss_a: 0.009967
[00:20:36.622] iteration 11161 : loss: 0.027619, loss_a: 0.016247
[00:20:37.352] iteration 11162 : loss: 0.010280, loss_a: 0.006047
[00:20:38.671] iteration 11163 : loss: 0.023532, loss_a: 0.013842
[00:20:39.409] iteration 11164 : loss: 0.041357, loss_a: 0.024327
[00:20:40.745] iteration 11165 : loss: 0.015284, loss_a: 0.008991
[00:20:41.480] iteration 11166 : loss: 0.023753, loss_a: 0.013972
[00:20:42.839] iteration 11167 : loss: 0.018599, loss_a: 0.010941
[00:20:43.579] iteration 11168 : loss: 0.020467, loss_a: 0.012039
[00:20:44.884] iteration 11169 : loss: 0.028509, loss_a: 0.016770
[00:20:45.624] iteration 11170 : loss: 0.039256, loss_a: 0.023092
[00:20:46.943] iteration 11171 : loss: 0.025597, loss_a: 0.015057
[00:20:47.677] iteration 11172 : loss: 0.036800, loss_a: 0.021647
[00:20:49.030] iteration 11173 : loss: 0.015266, loss_a: 0.008980
[00:20:49.779] iteration 11174 : loss: 0.074460, loss_a: 0.043800
[00:20:51.122] iteration 11175 : loss: 0.021480, loss_a: 0.012635
[00:20:51.868] iteration 11176 : loss: 0.035457, loss_a: 0.020857
[00:20:53.213] iteration 11177 : loss: 0.015341, loss_a: 0.009024
[00:20:53.955] iteration 11178 : loss: 0.029897, loss_a: 0.017586
[00:20:55.318] iteration 11179 : loss: 0.034669, loss_a: 0.020393
[00:20:56.058] iteration 11180 : loss: 0.020673, loss_a: 0.012161
[00:20:57.404] iteration 11181 : loss: 0.028015, loss_a: 0.016479
[00:20:58.143] iteration 11182 : loss: 0.033498, loss_a: 0.019705
[00:20:59.456] iteration 11183 : loss: 0.040399, loss_a: 0.023764
[00:21:00.198] iteration 11184 : loss: 0.021523, loss_a: 0.012661
[00:21:01.522] iteration 11185 : loss: 0.037856, loss_a: 0.022268
[00:21:02.269] iteration 11186 : loss: 0.085025, loss_a: 0.050015
[00:21:03.620] iteration 11187 : loss: 0.029958, loss_a: 0.017622
[00:21:04.358] iteration 11188 : loss: 0.017796, loss_a: 0.010468
[00:21:05.720] iteration 11189 : loss: 0.018864, loss_a: 0.011097
[00:21:06.449] iteration 11190 : loss: 0.014421, loss_a: 0.008483
[00:21:07.789] iteration 11191 : loss: 0.049789, loss_a: 0.029288
[00:21:08.529] iteration 11192 : loss: 0.047866, loss_a: 0.028157
[00:21:09.855] iteration 11193 : loss: 0.030923, loss_a: 0.018190
[00:21:10.593] iteration 11194 : loss: 0.046415, loss_a: 0.027303
[00:21:11.947] iteration 11195 : loss: 0.057091, loss_a: 0.033583
[00:21:12.687] iteration 11196 : loss: 0.011274, loss_a: 0.006632
[00:21:14.047] iteration 11197 : loss: 0.027752, loss_a: 0.016325
[00:21:14.790] iteration 11198 : loss: 0.022030, loss_a: 0.012959
[00:21:16.145] iteration 11199 : loss: 0.050011, loss_a: 0.029418
[00:21:16.888] iteration 11200 : loss: 0.030529, loss_a: 0.017958
[00:21:41.452] iteration 11201 : loss: 0.037321, loss_a: 0.021954
[00:21:43.613] iteration 11202 : loss: 0.015360, loss_a: 0.009035
[00:21:44.966] iteration 11203 : loss: 0.046445, loss_a: 0.027321
[00:21:45.709] iteration 11204 : loss: 0.028264, loss_a: 0.016626
[00:21:47.032] iteration 11205 : loss: 0.035422, loss_a: 0.020837
[00:21:47.771] iteration 11206 : loss: 0.033115, loss_a: 0.019479
[00:21:49.083] iteration 11207 : loss: 0.019804, loss_a: 0.011649
[00:21:49.819] iteration 11208 : loss: 0.044340, loss_a: 0.026082
[00:21:51.140] iteration 11209 : loss: 0.058917, loss_a: 0.034657
[00:21:51.877] iteration 11210 : loss: 0.020115, loss_a: 0.011832
[00:21:53.232] iteration 11211 : loss: 0.177397, loss_a: 0.104351
[00:21:53.982] iteration 11212 : loss: 0.034522, loss_a: 0.020307
[00:21:55.295] iteration 11213 : loss: 0.039594, loss_a: 0.023291
[00:21:56.033] iteration 11214 : loss: 0.038970, loss_a: 0.022923
[00:21:57.382] iteration 11215 : loss: 0.041631, loss_a: 0.024489
[00:21:58.121] iteration 11216 : loss: 0.029631, loss_a: 0.017430
[00:21:59.442] iteration 11217 : loss: 0.019577, loss_a: 0.011516
[00:22:00.185] iteration 11218 : loss: 0.051629, loss_a: 0.030370
[00:22:01.516] iteration 11219 : loss: 0.025845, loss_a: 0.015203
[00:22:02.257] iteration 11220 : loss: 0.029441, loss_a: 0.017318
[00:22:03.596] iteration 11221 : loss: 0.033193, loss_a: 0.019525
[00:22:04.335] iteration 11222 : loss: 0.031660, loss_a: 0.018623
[00:22:05.654] iteration 11223 : loss: 0.028751, loss_a: 0.016912
[00:22:06.399] iteration 11224 : loss: 0.029466, loss_a: 0.017333
[00:22:07.708] iteration 11225 : loss: 0.026959, loss_a: 0.015858
[00:22:08.446] iteration 11226 : loss: 0.013604, loss_a: 0.008002
[00:22:09.789] iteration 11227 : loss: 0.045616, loss_a: 0.026833
[00:22:10.525] iteration 11228 : loss: 0.016778, loss_a: 0.009869
[00:22:11.837] iteration 11229 : loss: 0.014661, loss_a: 0.008624
[00:22:12.590] iteration 11230 : loss: 0.038182, loss_a: 0.022460
[00:22:13.947] iteration 11231 : loss: 0.021844, loss_a: 0.012849
[00:22:14.693] iteration 11232 : loss: 0.053797, loss_a: 0.031645
[00:22:16.014] iteration 11233 : loss: 0.033531, loss_a: 0.019724
[00:22:16.774] iteration 11234 : loss: 0.039161, loss_a: 0.023036
[00:22:18.131] iteration 11235 : loss: 0.020339, loss_a: 0.011964
[00:22:18.888] iteration 11236 : loss: 0.048031, loss_a: 0.028253
[00:22:20.202] iteration 11237 : loss: 0.017892, loss_a: 0.010524
[00:22:20.953] iteration 11238 : loss: 0.032684, loss_a: 0.019226
[00:22:22.274] iteration 11239 : loss: 0.024577, loss_a: 0.014457
[00:22:23.006] iteration 11240 : loss: 0.019210, loss_a: 0.011300
[00:22:24.368] iteration 11241 : loss: 0.049386, loss_a: 0.029051
[00:22:25.118] iteration 11242 : loss: 0.043915, loss_a: 0.025832
[00:22:26.466] iteration 11243 : loss: 0.024256, loss_a: 0.014268
[00:22:27.208] iteration 11244 : loss: 0.068195, loss_a: 0.040115
[00:22:28.557] iteration 11245 : loss: 0.024035, loss_a: 0.014138
[00:22:29.304] iteration 11246 : loss: 0.042015, loss_a: 0.024715
[00:22:30.674] iteration 11247 : loss: 0.040443, loss_a: 0.023790
[00:22:31.411] iteration 11248 : loss: 0.026951, loss_a: 0.015853
[00:22:32.731] iteration 11249 : loss: 0.030282, loss_a: 0.017813
[00:22:33.461] iteration 11250 : loss: 0.013120, loss_a: 0.007718
[00:22:34.827] iteration 11251 : loss: 0.028488, loss_a: 0.016758
[00:22:35.567] iteration 11252 : loss: 0.031765, loss_a: 0.018685
[00:22:36.928] iteration 11253 : loss: 0.027255, loss_a: 0.016032
[00:22:37.663] iteration 11254 : loss: 0.033609, loss_a: 0.019770
[00:22:39.021] iteration 11255 : loss: 0.016925, loss_a: 0.009956
[00:22:39.767] iteration 11256 : loss: 0.041848, loss_a: 0.024616
[00:22:41.146] iteration 11257 : loss: 0.034389, loss_a: 0.020229
[00:22:41.884] iteration 11258 : loss: 0.049990, loss_a: 0.029406
[00:22:43.209] iteration 11259 : loss: 0.015419, loss_a: 0.009070
[00:22:43.955] iteration 11260 : loss: 0.021722, loss_a: 0.012778
[00:22:45.288] iteration 11261 : loss: 0.035277, loss_a: 0.020751
[00:22:46.025] iteration 11262 : loss: 0.024967, loss_a: 0.014686
[00:22:47.367] iteration 11263 : loss: 0.016526, loss_a: 0.009721
[00:22:48.105] iteration 11264 : loss: 0.030944, loss_a: 0.018202
[00:22:49.458] iteration 11265 : loss: 0.036157, loss_a: 0.021269
[00:22:50.194] iteration 11266 : loss: 0.023669, loss_a: 0.013923
[00:22:51.519] iteration 11267 : loss: 0.031802, loss_a: 0.018707
[00:22:52.267] iteration 11268 : loss: 0.034801, loss_a: 0.020471
[00:22:53.605] iteration 11269 : loss: 0.014413, loss_a: 0.008478
[00:22:54.341] iteration 11270 : loss: 0.021545, loss_a: 0.012674
[00:22:55.689] iteration 11271 : loss: 0.017280, loss_a: 0.010165
[00:22:56.426] iteration 11272 : loss: 0.026086, loss_a: 0.015345
[00:22:57.745] iteration 11273 : loss: 0.011403, loss_a: 0.006708
[00:22:58.487] iteration 11274 : loss: 0.035379, loss_a: 0.020811
[00:22:59.835] iteration 11275 : loss: 0.063122, loss_a: 0.037130
[00:23:00.576] iteration 11276 : loss: 0.025165, loss_a: 0.014803
[00:23:01.900] iteration 11277 : loss: 0.022081, loss_a: 0.012989
[00:23:02.645] iteration 11278 : loss: 0.020228, loss_a: 0.011899
[00:23:03.997] iteration 11279 : loss: 0.033045, loss_a: 0.019438
[00:23:04.738] iteration 11280 : loss: 0.021005, loss_a: 0.012356
[00:23:06.055] iteration 11281 : loss: 0.023342, loss_a: 0.013731
[00:23:06.790] iteration 11282 : loss: 0.026175, loss_a: 0.015397
[00:23:08.133] iteration 11283 : loss: 0.024182, loss_a: 0.014225
[00:23:08.867] iteration 11284 : loss: 0.036120, loss_a: 0.021247
[00:23:10.220] iteration 11285 : loss: 0.031820, loss_a: 0.018718
[00:23:11.009] iteration 11286 : loss: 0.017890, loss_a: 0.010523
[00:23:12.323] iteration 11287 : loss: 0.053930, loss_a: 0.031724
[00:23:13.063] iteration 11288 : loss: 0.019412, loss_a: 0.011419
[00:23:14.420] iteration 11289 : loss: 0.026976, loss_a: 0.015868
[00:23:15.161] iteration 11290 : loss: 0.026886, loss_a: 0.015815
[00:23:16.494] iteration 11291 : loss: 0.047237, loss_a: 0.027786
[00:23:17.238] iteration 11292 : loss: 0.022447, loss_a: 0.013204
[00:23:18.552] iteration 11293 : loss: 0.022651, loss_a: 0.013324
[00:23:19.296] iteration 11294 : loss: 0.021354, loss_a: 0.012561
[00:23:20.627] iteration 11295 : loss: 0.020431, loss_a: 0.012018
[00:23:21.365] iteration 11296 : loss: 0.086187, loss_a: 0.050698
[00:23:22.722] iteration 11297 : loss: 0.029106, loss_a: 0.017121
[00:23:23.455] iteration 11298 : loss: 0.029061, loss_a: 0.017095
[00:23:24.786] iteration 11299 : loss: 0.019696, loss_a: 0.011586
[00:23:25.525] iteration 11300 : loss: 0.026082, loss_a: 0.015342
[00:23:26.881] iteration 11301 : loss: 0.027213, loss_a: 0.016008
[00:23:27.620] iteration 11302 : loss: 0.012863, loss_a: 0.007567
[00:23:28.953] iteration 11303 : loss: 0.021815, loss_a: 0.012833
[00:23:29.698] iteration 11304 : loss: 0.028413, loss_a: 0.016713
[00:23:31.041] iteration 11305 : loss: 0.015993, loss_a: 0.009407
[00:23:31.777] iteration 11306 : loss: 0.020135, loss_a: 0.011844
[00:23:33.133] iteration 11307 : loss: 0.016735, loss_a: 0.009844
[00:23:33.864] iteration 11308 : loss: 0.027035, loss_a: 0.015903
[00:23:35.186] iteration 11309 : loss: 0.029614, loss_a: 0.017420
[00:23:35.948] iteration 11310 : loss: 0.054223, loss_a: 0.031896
[00:23:37.298] iteration 11311 : loss: 0.028900, loss_a: 0.017000
[00:23:38.046] iteration 11312 : loss: 0.038064, loss_a: 0.022390
[00:23:39.396] iteration 11313 : loss: 0.026978, loss_a: 0.015869
[00:23:40.138] iteration 11314 : loss: 0.041205, loss_a: 0.024238
[00:23:41.504] iteration 11315 : loss: 0.020021, loss_a: 0.011777
[00:23:42.241] iteration 11316 : loss: 0.034008, loss_a: 0.020005
[00:23:43.593] iteration 11317 : loss: 0.022902, loss_a: 0.013472
[00:23:44.331] iteration 11318 : loss: 0.031491, loss_a: 0.018524
[00:23:45.654] iteration 11319 : loss: 0.015764, loss_a: 0.009273
[00:23:46.388] iteration 11320 : loss: 0.028610, loss_a: 0.016829
[00:23:47.819] iteration 11321 : loss: 0.029271, loss_a: 0.017218
[00:23:48.567] iteration 11322 : loss: 0.024449, loss_a: 0.014382
[00:23:49.936] iteration 11323 : loss: 0.016328, loss_a: 0.009605
[00:23:50.671] iteration 11324 : loss: 0.029804, loss_a: 0.017532
[00:23:52.024] iteration 11325 : loss: 0.035389, loss_a: 0.020817
[00:23:52.783] iteration 11326 : loss: 0.066556, loss_a: 0.039150
[00:23:54.132] iteration 11327 : loss: 0.017016, loss_a: 0.010010
[00:23:54.865] iteration 11328 : loss: 0.018737, loss_a: 0.011022
[00:23:56.200] iteration 11329 : loss: 0.022992, loss_a: 0.013525
[00:23:56.963] iteration 11330 : loss: 0.065896, loss_a: 0.038762
[00:23:58.321] iteration 11331 : loss: 0.019798, loss_a: 0.011646
[00:23:59.063] iteration 11332 : loss: 0.014406, loss_a: 0.008474
[00:24:00.400] iteration 11333 : loss: 0.021255, loss_a: 0.012503
[00:24:01.136] iteration 11334 : loss: 0.029814, loss_a: 0.017538
[00:24:02.460] iteration 11335 : loss: 0.027145, loss_a: 0.015968
[00:24:03.207] iteration 11336 : loss: 0.045891, loss_a: 0.026995
[00:24:04.576] iteration 11337 : loss: 0.031449, loss_a: 0.018499
[00:24:05.323] iteration 11338 : loss: 0.026605, loss_a: 0.015650
[00:24:06.682] iteration 11339 : loss: 0.026649, loss_a: 0.015676
[00:24:07.419] iteration 11340 : loss: 0.032509, loss_a: 0.019123
[00:24:08.742] iteration 11341 : loss: 0.016725, loss_a: 0.009838
[00:24:09.485] iteration 11342 : loss: 0.020191, loss_a: 0.011877
[00:24:10.842] iteration 11343 : loss: 0.034294, loss_a: 0.020173
[00:24:11.587] iteration 11344 : loss: 0.028872, loss_a: 0.016984
[00:24:12.930] iteration 11345 : loss: 0.027822, loss_a: 0.016366
[00:24:13.671] iteration 11346 : loss: 0.020648, loss_a: 0.012146
[00:24:15.009] iteration 11347 : loss: 0.033977, loss_a: 0.019986
[00:24:15.756] iteration 11348 : loss: 0.028736, loss_a: 0.016903
[00:24:17.101] iteration 11349 : loss: 0.026111, loss_a: 0.015359
[00:24:17.831] iteration 11350 : loss: 0.013115, loss_a: 0.007715
[00:24:19.166] iteration 11351 : loss: 0.020005, loss_a: 0.011768
[00:24:19.927] iteration 11352 : loss: 0.018445, loss_a: 0.010850
[00:24:21.248] iteration 11353 : loss: 0.025304, loss_a: 0.014885
[00:24:21.994] iteration 11354 : loss: 0.032412, loss_a: 0.019066
[00:24:23.306] iteration 11355 : loss: 0.035329, loss_a: 0.020782
[00:24:24.055] iteration 11356 : loss: 0.016193, loss_a: 0.009525
[00:24:25.393] iteration 11357 : loss: 0.035067, loss_a: 0.020628
[00:24:26.134] iteration 11358 : loss: 0.025673, loss_a: 0.015102
[00:24:27.474] iteration 11359 : loss: 0.020583, loss_a: 0.012108
[00:24:28.208] iteration 11360 : loss: 0.023026, loss_a: 0.013545
[00:24:29.575] iteration 11361 : loss: 0.033459, loss_a: 0.019682
[00:24:30.315] iteration 11362 : loss: 0.015742, loss_a: 0.009260
[00:24:31.646] iteration 11363 : loss: 0.034384, loss_a: 0.020226
[00:24:32.374] iteration 11364 : loss: 0.025491, loss_a: 0.014995
[00:24:33.730] iteration 11365 : loss: 0.045335, loss_a: 0.026668
[00:24:34.459] iteration 11366 : loss: 0.022714, loss_a: 0.013361
[00:24:35.777] iteration 11367 : loss: 0.038595, loss_a: 0.022703
[00:24:36.524] iteration 11368 : loss: 0.019747, loss_a: 0.011616
[00:24:37.875] iteration 11369 : loss: 0.036953, loss_a: 0.021737
[00:24:38.616] iteration 11370 : loss: 0.026371, loss_a: 0.015512
[00:24:39.978] iteration 11371 : loss: 0.033256, loss_a: 0.019563
[00:24:40.725] iteration 11372 : loss: 0.015736, loss_a: 0.009257
[00:24:42.073] iteration 11373 : loss: 0.020998, loss_a: 0.012352
[00:24:42.804] iteration 11374 : loss: 0.022577, loss_a: 0.013280
[00:24:44.165] iteration 11375 : loss: 0.017919, loss_a: 0.010541
[00:24:44.909] iteration 11376 : loss: 0.021070, loss_a: 0.012394
[00:24:46.250] iteration 11377 : loss: 0.015710, loss_a: 0.009241
[00:24:46.988] iteration 11378 : loss: 0.062586, loss_a: 0.036815
[00:24:48.306] iteration 11379 : loss: 0.014343, loss_a: 0.008437
[00:24:49.047] iteration 11380 : loss: 0.024325, loss_a: 0.014309
[00:24:50.362] iteration 11381 : loss: 0.013346, loss_a: 0.007851
[00:24:51.110] iteration 11382 : loss: 0.018483, loss_a: 0.010872
[00:24:52.479] iteration 11383 : loss: 0.029662, loss_a: 0.017449
[00:24:53.225] iteration 11384 : loss: 0.019906, loss_a: 0.011710
[00:24:54.585] iteration 11385 : loss: 0.030751, loss_a: 0.018089
[00:24:55.320] iteration 11386 : loss: 0.041549, loss_a: 0.024441
[00:24:56.655] iteration 11387 : loss: 0.037718, loss_a: 0.022187
[00:24:57.395] iteration 11388 : loss: 0.027967, loss_a: 0.016451
[00:24:58.732] iteration 11389 : loss: 0.017891, loss_a: 0.010524
[00:24:59.462] iteration 11390 : loss: 0.014040, loss_a: 0.008259
[00:25:00.824] iteration 11391 : loss: 0.077073, loss_a: 0.045337
[00:25:01.554] iteration 11392 : loss: 0.022410, loss_a: 0.013182
[00:25:02.873] iteration 11393 : loss: 0.038088, loss_a: 0.022405
[00:25:03.617] iteration 11394 : loss: 0.047471, loss_a: 0.027924
[00:25:04.955] iteration 11395 : loss: 0.027536, loss_a: 0.016198
[00:25:05.706] iteration 11396 : loss: 0.058806, loss_a: 0.034592
[00:25:07.042] iteration 11397 : loss: 0.026250, loss_a: 0.015441
[00:25:07.791] iteration 11398 : loss: 0.028134, loss_a: 0.016549
[00:25:09.115] iteration 11399 : loss: 0.030672, loss_a: 0.018042
[00:25:09.847] iteration 11400 : loss: 0.015592, loss_a: 0.009172
[00:25:34.483] iteration 11401 : loss: 0.026772, loss_a: 0.015748
[00:25:36.592] iteration 11402 : loss: 0.022765, loss_a: 0.013391
[00:25:37.963] iteration 11403 : loss: 0.047021, loss_a: 0.027659
[00:25:38.700] iteration 11404 : loss: 0.013184, loss_a: 0.007756
[00:25:40.013] iteration 11405 : loss: 0.018086, loss_a: 0.010639
[00:25:40.753] iteration 11406 : loss: 0.025509, loss_a: 0.015006
[00:25:42.081] iteration 11407 : loss: 0.018809, loss_a: 0.011064
[00:25:42.818] iteration 11408 : loss: 0.015279, loss_a: 0.008988
[00:25:44.131] iteration 11409 : loss: 0.037910, loss_a: 0.022300
[00:25:44.874] iteration 11410 : loss: 0.022786, loss_a: 0.013404
[00:25:46.185] iteration 11411 : loss: 0.020715, loss_a: 0.012185
[00:25:46.920] iteration 11412 : loss: 0.033076, loss_a: 0.019457
[00:25:48.275] iteration 11413 : loss: 0.021201, loss_a: 0.012471
[00:25:49.017] iteration 11414 : loss: 0.065942, loss_a: 0.038789
[00:25:50.331] iteration 11415 : loss: 0.028946, loss_a: 0.017027
[00:25:51.076] iteration 11416 : loss: 0.020471, loss_a: 0.012042
[00:25:52.395] iteration 11417 : loss: 0.020248, loss_a: 0.011910
[00:25:53.134] iteration 11418 : loss: 0.035512, loss_a: 0.020889
[00:25:54.445] iteration 11419 : loss: 0.016806, loss_a: 0.009886
[00:25:55.188] iteration 11420 : loss: 0.027290, loss_a: 0.016053
[00:25:56.514] iteration 11421 : loss: 0.020685, loss_a: 0.012168
[00:25:57.257] iteration 11422 : loss: 0.043148, loss_a: 0.025381
[00:25:58.578] iteration 11423 : loss: 0.030522, loss_a: 0.017954
[00:25:59.327] iteration 11424 : loss: 0.034202, loss_a: 0.020119
[00:26:00.675] iteration 11425 : loss: 0.009307, loss_a: 0.005475
[00:26:01.414] iteration 11426 : loss: 0.024380, loss_a: 0.014341
[00:26:02.750] iteration 11427 : loss: 0.017830, loss_a: 0.010488
[00:26:03.501] iteration 11428 : loss: 0.024290, loss_a: 0.014288
[00:26:04.852] iteration 11429 : loss: 0.034406, loss_a: 0.020239
[00:26:05.586] iteration 11430 : loss: 0.038555, loss_a: 0.022680
[00:26:06.925] iteration 11431 : loss: 0.024323, loss_a: 0.014307
[00:26:07.660] iteration 11432 : loss: 0.021610, loss_a: 0.012712
[00:26:09.015] iteration 11433 : loss: 0.022060, loss_a: 0.012977
[00:26:09.754] iteration 11434 : loss: 0.019341, loss_a: 0.011377
[00:26:11.110] iteration 11435 : loss: 0.024356, loss_a: 0.014327
[00:26:11.846] iteration 11436 : loss: 0.015878, loss_a: 0.009340
[00:26:13.205] iteration 11437 : loss: 0.025330, loss_a: 0.014900
[00:26:13.942] iteration 11438 : loss: 0.025869, loss_a: 0.015217
[00:26:15.250] iteration 11439 : loss: 0.023154, loss_a: 0.013620
[00:26:15.988] iteration 11440 : loss: 0.023182, loss_a: 0.013636
[00:26:17.330] iteration 11441 : loss: 0.035757, loss_a: 0.021033
[00:26:18.075] iteration 11442 : loss: 0.022211, loss_a: 0.013066
[00:26:19.391] iteration 11443 : loss: 0.011864, loss_a: 0.006979
[00:26:20.125] iteration 11444 : loss: 0.016760, loss_a: 0.009859
[00:26:21.475] iteration 11445 : loss: 0.022380, loss_a: 0.013165
[00:26:22.226] iteration 11446 : loss: 0.038802, loss_a: 0.022825
[00:26:23.575] iteration 11447 : loss: 0.016613, loss_a: 0.009772
[00:26:24.311] iteration 11448 : loss: 0.018293, loss_a: 0.010761
[00:26:25.636] iteration 11449 : loss: 0.011439, loss_a: 0.006729
[00:26:26.385] iteration 11450 : loss: 0.017871, loss_a: 0.010512
[00:26:27.733] iteration 11451 : loss: 0.016355, loss_a: 0.009621
[00:26:28.476] iteration 11452 : loss: 0.041600, loss_a: 0.024471
[00:26:29.847] iteration 11453 : loss: 0.036532, loss_a: 0.021489
[00:26:30.583] iteration 11454 : loss: 0.018540, loss_a: 0.010906
[00:26:31.925] iteration 11455 : loss: 0.028882, loss_a: 0.016989
[00:26:32.678] iteration 11456 : loss: 0.021236, loss_a: 0.012492
[00:26:34.019] iteration 11457 : loss: 0.038519, loss_a: 0.022658
[00:26:34.771] iteration 11458 : loss: 0.023263, loss_a: 0.013684
[00:26:36.128] iteration 11459 : loss: 0.016965, loss_a: 0.009980
[00:26:36.870] iteration 11460 : loss: 0.032463, loss_a: 0.019096
[00:26:38.213] iteration 11461 : loss: 0.034260, loss_a: 0.020153
[00:26:38.950] iteration 11462 : loss: 0.024931, loss_a: 0.014665
[00:26:40.308] iteration 11463 : loss: 0.024713, loss_a: 0.014537
[00:26:41.039] iteration 11464 : loss: 0.023687, loss_a: 0.013934
[00:26:42.382] iteration 11465 : loss: 0.025509, loss_a: 0.015005
[00:26:43.126] iteration 11466 : loss: 0.024312, loss_a: 0.014301
[00:26:44.459] iteration 11467 : loss: 0.016966, loss_a: 0.009980
[00:26:45.202] iteration 11468 : loss: 0.032191, loss_a: 0.018936
[00:26:46.518] iteration 11469 : loss: 0.018145, loss_a: 0.010674
[00:26:47.262] iteration 11470 : loss: 0.017903, loss_a: 0.010531
[00:26:48.633] iteration 11471 : loss: 0.022773, loss_a: 0.013396
[00:26:49.364] iteration 11472 : loss: 0.010599, loss_a: 0.006235
[00:26:50.722] iteration 11473 : loss: 0.032464, loss_a: 0.019096
[00:26:51.468] iteration 11474 : loss: 0.021809, loss_a: 0.012829
[00:26:52.789] iteration 11475 : loss: 0.015658, loss_a: 0.009210
[00:26:53.525] iteration 11476 : loss: 0.019135, loss_a: 0.011256
[00:26:54.857] iteration 11477 : loss: 0.012810, loss_a: 0.007535
[00:26:55.599] iteration 11478 : loss: 0.029299, loss_a: 0.017234
[00:26:56.943] iteration 11479 : loss: 0.017847, loss_a: 0.010498
[00:26:57.677] iteration 11480 : loss: 0.031705, loss_a: 0.018650
[00:26:59.035] iteration 11481 : loss: 0.017520, loss_a: 0.010306
[00:26:59.776] iteration 11482 : loss: 0.018222, loss_a: 0.010719
[00:27:01.131] iteration 11483 : loss: 0.020101, loss_a: 0.011824
[00:27:01.868] iteration 11484 : loss: 0.015024, loss_a: 0.008838
[00:27:03.227] iteration 11485 : loss: 0.043397, loss_a: 0.025527
[00:27:03.955] iteration 11486 : loss: 0.019346, loss_a: 0.011380
[00:27:05.293] iteration 11487 : loss: 0.022981, loss_a: 0.013519
[00:27:06.026] iteration 11488 : loss: 0.043616, loss_a: 0.025656
[00:27:07.390] iteration 11489 : loss: 0.067769, loss_a: 0.039864
[00:27:08.128] iteration 11490 : loss: 0.016629, loss_a: 0.009782
[00:27:09.468] iteration 11491 : loss: 0.011771, loss_a: 0.006924
[00:27:10.206] iteration 11492 : loss: 0.033454, loss_a: 0.019679
[00:27:11.530] iteration 11493 : loss: 0.018224, loss_a: 0.010720
[00:27:12.276] iteration 11494 : loss: 0.024932, loss_a: 0.014666
[00:27:13.642] iteration 11495 : loss: 0.033717, loss_a: 0.019833
[00:27:14.377] iteration 11496 : loss: 0.015858, loss_a: 0.009328
[00:27:15.703] iteration 11497 : loss: 0.012395, loss_a: 0.007291
[00:27:16.448] iteration 11498 : loss: 0.042404, loss_a: 0.024943
[00:27:17.784] iteration 11499 : loss: 0.019138, loss_a: 0.011257
[00:27:18.531] iteration 11500 : loss: 0.049004, loss_a: 0.028826
[00:27:19.864] iteration 11501 : loss: 0.058096, loss_a: 0.034174
[00:27:20.602] iteration 11502 : loss: 0.031268, loss_a: 0.018393
[00:27:21.970] iteration 11503 : loss: 0.028516, loss_a: 0.016774
[00:27:22.709] iteration 11504 : loss: 0.016803, loss_a: 0.009884
[00:27:24.062] iteration 11505 : loss: 0.019031, loss_a: 0.011195
[00:27:24.814] iteration 11506 : loss: 0.027639, loss_a: 0.016258
[00:27:26.173] iteration 11507 : loss: 0.016139, loss_a: 0.009493
[00:27:26.905] iteration 11508 : loss: 0.013428, loss_a: 0.007899
[00:27:28.244] iteration 11509 : loss: 0.017447, loss_a: 0.010263
[00:27:28.991] iteration 11510 : loss: 0.048050, loss_a: 0.028265
[00:27:30.359] iteration 11511 : loss: 0.024159, loss_a: 0.014211
[00:27:31.110] iteration 11512 : loss: 0.039563, loss_a: 0.023272
[00:27:32.453] iteration 11513 : loss: 0.035034, loss_a: 0.020608
[00:27:33.186] iteration 11514 : loss: 0.018350, loss_a: 0.010794
[00:27:34.548] iteration 11515 : loss: 0.026006, loss_a: 0.015297
[00:27:35.284] iteration 11516 : loss: 0.026672, loss_a: 0.015689
[00:27:36.623] iteration 11517 : loss: 0.041390, loss_a: 0.024347
[00:27:37.366] iteration 11518 : loss: 0.030571, loss_a: 0.017983
[00:27:38.727] iteration 11519 : loss: 0.033250, loss_a: 0.019559
[00:27:39.468] iteration 11520 : loss: 0.023910, loss_a: 0.014065
[00:27:40.796] iteration 11521 : loss: 0.051514, loss_a: 0.030302
[00:27:41.534] iteration 11522 : loss: 0.026343, loss_a: 0.015496
[00:27:42.884] iteration 11523 : loss: 0.025871, loss_a: 0.015218
[00:27:43.621] iteration 11524 : loss: 0.057408, loss_a: 0.033769
[00:27:44.968] iteration 11525 : loss: 0.032421, loss_a: 0.019071
[00:27:45.707] iteration 11526 : loss: 0.026363, loss_a: 0.015507
[00:27:47.069] iteration 11527 : loss: 0.033410, loss_a: 0.019653
[00:27:47.799] iteration 11528 : loss: 0.018380, loss_a: 0.010812
[00:27:49.120] iteration 11529 : loss: 0.016391, loss_a: 0.009642
[00:27:49.878] iteration 11530 : loss: 0.035847, loss_a: 0.021087
[00:27:51.180] iteration 11531 : loss: 0.013270, loss_a: 0.007806
[00:27:51.918] iteration 11532 : loss: 0.022452, loss_a: 0.013207
[00:27:53.235] iteration 11533 : loss: 0.017761, loss_a: 0.010448
[00:27:53.990] iteration 11534 : loss: 0.022613, loss_a: 0.013302
[00:27:55.316] iteration 11535 : loss: 0.025011, loss_a: 0.014712
[00:27:56.063] iteration 11536 : loss: 0.047025, loss_a: 0.027662
[00:27:57.409] iteration 11537 : loss: 0.026230, loss_a: 0.015430
[00:27:58.142] iteration 11538 : loss: 0.077955, loss_a: 0.045856
[00:27:59.461] iteration 11539 : loss: 0.022866, loss_a: 0.013450
[00:28:00.205] iteration 11540 : loss: 0.025898, loss_a: 0.015234
[00:28:01.530] iteration 11541 : loss: 0.021785, loss_a: 0.012815
[00:28:02.268] iteration 11542 : loss: 0.021267, loss_a: 0.012510
[00:28:03.604] iteration 11543 : loss: 0.026093, loss_a: 0.015349
[00:28:04.346] iteration 11544 : loss: 0.028853, loss_a: 0.016972
[00:28:05.659] iteration 11545 : loss: 0.032494, loss_a: 0.019114
[00:28:06.401] iteration 11546 : loss: 0.027929, loss_a: 0.016429
[00:28:07.722] iteration 11547 : loss: 0.046939, loss_a: 0.027611
[00:28:08.469] iteration 11548 : loss: 0.073831, loss_a: 0.043430
[00:28:09.782] iteration 11549 : loss: 0.028860, loss_a: 0.016976
[00:28:10.519] iteration 11550 : loss: 0.031516, loss_a: 0.018539
[00:28:11.874] iteration 11551 : loss: 0.067163, loss_a: 0.039508
[00:28:12.604] iteration 11552 : loss: 0.032629, loss_a: 0.019193
[00:28:13.972] iteration 11553 : loss: 0.032669, loss_a: 0.019217
[00:28:14.706] iteration 11554 : loss: 0.018803, loss_a: 0.011061
[00:28:16.047] iteration 11555 : loss: 0.021537, loss_a: 0.012669
[00:28:16.785] iteration 11556 : loss: 0.019389, loss_a: 0.011405
[00:28:18.094] iteration 11557 : loss: 0.012537, loss_a: 0.007375
[00:28:18.831] iteration 11558 : loss: 0.023307, loss_a: 0.013710
[00:28:20.179] iteration 11559 : loss: 0.017124, loss_a: 0.010073
[00:28:20.911] iteration 11560 : loss: 0.056128, loss_a: 0.033016
[00:28:22.263] iteration 11561 : loss: 0.012742, loss_a: 0.007496
[00:28:23.002] iteration 11562 : loss: 0.025598, loss_a: 0.015058
[00:28:24.328] iteration 11563 : loss: 0.022846, loss_a: 0.013439
[00:28:25.075] iteration 11564 : loss: 0.040316, loss_a: 0.023715
[00:28:26.403] iteration 11565 : loss: 0.022691, loss_a: 0.013348
[00:28:27.146] iteration 11566 : loss: 0.087912, loss_a: 0.051713
[00:28:28.481] iteration 11567 : loss: 0.027958, loss_a: 0.016446
[00:28:29.229] iteration 11568 : loss: 0.026537, loss_a: 0.015610
[00:28:30.547] iteration 11569 : loss: 0.026661, loss_a: 0.015683
[00:28:31.291] iteration 11570 : loss: 0.032037, loss_a: 0.018845
[00:28:32.650] iteration 11571 : loss: 0.026015, loss_a: 0.015303
[00:28:33.398] iteration 11572 : loss: 0.055560, loss_a: 0.032683
[00:28:34.743] iteration 11573 : loss: 0.031914, loss_a: 0.018773
[00:28:35.489] iteration 11574 : loss: 0.024965, loss_a: 0.014685
[00:28:36.824] iteration 11575 : loss: 0.064816, loss_a: 0.038127
[00:28:37.600] iteration 11576 : loss: 0.046930, loss_a: 0.027606
[00:28:38.920] iteration 11577 : loss: 0.075319, loss_a: 0.044306
[00:28:39.662] iteration 11578 : loss: 0.014551, loss_a: 0.008559
[00:28:40.988] iteration 11579 : loss: 0.043870, loss_a: 0.025806
[00:28:41.721] iteration 11580 : loss: 0.033027, loss_a: 0.019427
[00:28:43.048] iteration 11581 : loss: 0.048499, loss_a: 0.028529
[00:28:43.785] iteration 11582 : loss: 0.038438, loss_a: 0.022611
[00:28:45.131] iteration 11583 : loss: 0.057381, loss_a: 0.033753
[00:28:45.868] iteration 11584 : loss: 0.037292, loss_a: 0.021936
[00:28:47.219] iteration 11585 : loss: 0.028741, loss_a: 0.016907
[00:28:47.962] iteration 11586 : loss: 0.028505, loss_a: 0.016768
[00:28:49.288] iteration 11587 : loss: 0.026657, loss_a: 0.015681
[00:28:50.032] iteration 11588 : loss: 0.020366, loss_a: 0.011980
[00:28:51.349] iteration 11589 : loss: 0.023642, loss_a: 0.013907
[00:28:52.098] iteration 11590 : loss: 0.020974, loss_a: 0.012338
[00:28:53.477] iteration 11591 : loss: 0.044734, loss_a: 0.026314
[00:28:54.226] iteration 11592 : loss: 0.033192, loss_a: 0.019524
[00:28:55.548] iteration 11593 : loss: 0.031795, loss_a: 0.018703
[00:28:56.283] iteration 11594 : loss: 0.024679, loss_a: 0.014517
[00:28:57.623] iteration 11595 : loss: 0.040742, loss_a: 0.023966
[00:28:58.358] iteration 11596 : loss: 0.014268, loss_a: 0.008393
[00:28:59.681] iteration 11597 : loss: 0.046668, loss_a: 0.027452
[00:29:00.440] iteration 11598 : loss: 0.047556, loss_a: 0.027974
[00:29:01.775] iteration 11599 : loss: 0.017544, loss_a: 0.010320
[00:29:02.512] iteration 11600 : loss: 0.031536, loss_a: 0.018551
[00:29:27.145] iteration 11601 : loss: 0.015086, loss_a: 0.008874
[00:29:29.383] iteration 11602 : loss: 0.032565, loss_a: 0.019156
[00:29:30.745] iteration 11603 : loss: 0.040994, loss_a: 0.024114
[00:29:31.487] iteration 11604 : loss: 0.020702, loss_a: 0.012177
[00:29:32.786] iteration 11605 : loss: 0.018389, loss_a: 0.010817
[00:29:33.529] iteration 11606 : loss: 0.043527, loss_a: 0.025604
[00:29:34.882] iteration 11607 : loss: 0.059872, loss_a: 0.035219
[00:29:35.629] iteration 11608 : loss: 0.023489, loss_a: 0.013817
[00:29:36.955] iteration 11609 : loss: 0.023102, loss_a: 0.013589
[00:29:37.699] iteration 11610 : loss: 0.042394, loss_a: 0.024938
[00:29:39.017] iteration 11611 : loss: 0.036733, loss_a: 0.021608
[00:29:39.765] iteration 11612 : loss: 0.015275, loss_a: 0.008985
[00:29:41.115] iteration 11613 : loss: 0.035858, loss_a: 0.021093
[00:29:41.857] iteration 11614 : loss: 0.023722, loss_a: 0.013954
[00:29:43.169] iteration 11615 : loss: 0.017580, loss_a: 0.010341
[00:29:43.924] iteration 11616 : loss: 0.048007, loss_a: 0.028239
[00:29:45.270] iteration 11617 : loss: 0.019242, loss_a: 0.011319
[00:29:46.016] iteration 11618 : loss: 0.032948, loss_a: 0.019381
[00:29:47.352] iteration 11619 : loss: 0.030577, loss_a: 0.017986
[00:29:48.084] iteration 11620 : loss: 0.020525, loss_a: 0.012074
[00:29:49.446] iteration 11621 : loss: 0.030893, loss_a: 0.018172
[00:29:50.195] iteration 11622 : loss: 0.032475, loss_a: 0.019103
[00:29:51.507] iteration 11623 : loss: 0.024644, loss_a: 0.014497
[00:29:52.249] iteration 11624 : loss: 0.026250, loss_a: 0.015441
[00:29:53.586] iteration 11625 : loss: 0.042883, loss_a: 0.025225
[00:29:54.328] iteration 11626 : loss: 0.023863, loss_a: 0.014037
[00:29:55.664] iteration 11627 : loss: 0.013356, loss_a: 0.007856
[00:29:56.408] iteration 11628 : loss: 0.019877, loss_a: 0.011692
[00:29:57.720] iteration 11629 : loss: 0.040637, loss_a: 0.023904
[00:29:58.461] iteration 11630 : loss: 0.022657, loss_a: 0.013328
[00:29:59.783] iteration 11631 : loss: 0.027090, loss_a: 0.015935
[00:30:00.523] iteration 11632 : loss: 0.013824, loss_a: 0.008132
[00:30:01.836] iteration 11633 : loss: 0.013291, loss_a: 0.007818
[00:30:02.579] iteration 11634 : loss: 0.028395, loss_a: 0.016703
[00:30:03.931] iteration 11635 : loss: 0.022558, loss_a: 0.013269
[00:30:04.678] iteration 11636 : loss: 0.033749, loss_a: 0.019852
[00:30:06.032] iteration 11637 : loss: 0.018201, loss_a: 0.010706
[00:30:06.779] iteration 11638 : loss: 0.024862, loss_a: 0.014625
[00:30:08.105] iteration 11639 : loss: 0.017756, loss_a: 0.010445
[00:30:08.853] iteration 11640 : loss: 0.049477, loss_a: 0.029104
[00:30:10.196] iteration 11641 : loss: 0.021905, loss_a: 0.012885
[00:30:10.945] iteration 11642 : loss: 0.052242, loss_a: 0.030731
[00:30:12.311] iteration 11643 : loss: 0.023102, loss_a: 0.013589
[00:30:13.056] iteration 11644 : loss: 0.017359, loss_a: 0.010211
[00:30:14.414] iteration 11645 : loss: 0.026416, loss_a: 0.015539
[00:30:15.150] iteration 11646 : loss: 0.016708, loss_a: 0.009828
[00:30:16.505] iteration 11647 : loss: 0.040566, loss_a: 0.023862
[00:30:17.236] iteration 11648 : loss: 0.011027, loss_a: 0.006486
[00:30:18.592] iteration 11649 : loss: 0.017456, loss_a: 0.010268
[00:30:19.332] iteration 11650 : loss: 0.024666, loss_a: 0.014510
[00:30:20.670] iteration 11651 : loss: 0.020089, loss_a: 0.011817
[00:30:21.410] iteration 11652 : loss: 0.016561, loss_a: 0.009742
[00:30:22.729] iteration 11653 : loss: 0.014989, loss_a: 0.008817
[00:30:23.464] iteration 11654 : loss: 0.025652, loss_a: 0.015089
[00:30:24.791] iteration 11655 : loss: 0.019011, loss_a: 0.011183
[00:30:25.534] iteration 11656 : loss: 0.030052, loss_a: 0.017678
[00:30:26.890] iteration 11657 : loss: 0.021885, loss_a: 0.012873
[00:30:27.633] iteration 11658 : loss: 0.019125, loss_a: 0.011250
[00:30:28.974] iteration 11659 : loss: 0.019524, loss_a: 0.011484
[00:30:29.709] iteration 11660 : loss: 0.016086, loss_a: 0.009462
[00:30:31.039] iteration 11661 : loss: 0.028915, loss_a: 0.017009
[00:30:31.778] iteration 11662 : loss: 0.019167, loss_a: 0.011275
[00:30:33.095] iteration 11663 : loss: 0.020629, loss_a: 0.012135
[00:30:33.838] iteration 11664 : loss: 0.043636, loss_a: 0.025668
[00:30:35.175] iteration 11665 : loss: 0.016984, loss_a: 0.009990
[00:30:35.916] iteration 11666 : loss: 0.023138, loss_a: 0.013611
[00:30:37.251] iteration 11667 : loss: 0.053217, loss_a: 0.031304
[00:30:38.011] iteration 11668 : loss: 0.020419, loss_a: 0.012011
[00:30:39.331] iteration 11669 : loss: 0.025263, loss_a: 0.014861
[00:30:40.066] iteration 11670 : loss: 0.021204, loss_a: 0.012473
[00:30:41.418] iteration 11671 : loss: 0.045990, loss_a: 0.027053
[00:30:42.160] iteration 11672 : loss: 0.027645, loss_a: 0.016262
[00:30:43.471] iteration 11673 : loss: 0.018485, loss_a: 0.010874
[00:30:44.213] iteration 11674 : loss: 0.039883, loss_a: 0.023460
[00:30:45.551] iteration 11675 : loss: 0.020933, loss_a: 0.012314
[00:30:46.294] iteration 11676 : loss: 0.052930, loss_a: 0.031135
[00:30:47.609] iteration 11677 : loss: 0.034610, loss_a: 0.020359
[00:30:48.353] iteration 11678 : loss: 0.023352, loss_a: 0.013736
[00:30:49.706] iteration 11679 : loss: 0.029767, loss_a: 0.017510
[00:30:50.445] iteration 11680 : loss: 0.023464, loss_a: 0.013802
[00:30:51.772] iteration 11681 : loss: 0.032083, loss_a: 0.018872
[00:30:52.502] iteration 11682 : loss: 0.056134, loss_a: 0.033020
[00:30:53.849] iteration 11683 : loss: 0.032790, loss_a: 0.019288
[00:30:54.591] iteration 11684 : loss: 0.024315, loss_a: 0.014303
[00:30:55.915] iteration 11685 : loss: 0.020755, loss_a: 0.012209
[00:30:56.652] iteration 11686 : loss: 0.025787, loss_a: 0.015169
[00:30:58.006] iteration 11687 : loss: 0.028687, loss_a: 0.016875
[00:30:58.756] iteration 11688 : loss: 0.036831, loss_a: 0.021665
[00:31:00.071] iteration 11689 : loss: 0.014104, loss_a: 0.008296
[00:31:00.810] iteration 11690 : loss: 0.028983, loss_a: 0.017049
[00:31:02.115] iteration 11691 : loss: 0.012742, loss_a: 0.007495
[00:31:02.845] iteration 11692 : loss: 0.016669, loss_a: 0.009805
[00:31:04.189] iteration 11693 : loss: 0.013647, loss_a: 0.008028
[00:31:04.939] iteration 11694 : loss: 0.023387, loss_a: 0.013757
[00:31:06.293] iteration 11695 : loss: 0.029370, loss_a: 0.017276
[00:31:07.021] iteration 11696 : loss: 0.022917, loss_a: 0.013481
[00:31:08.351] iteration 11697 : loss: 0.019153, loss_a: 0.011267
[00:31:09.088] iteration 11698 : loss: 0.030271, loss_a: 0.017807
[00:31:10.412] iteration 11699 : loss: 0.028539, loss_a: 0.016787
[00:31:11.150] iteration 11700 : loss: 0.014649, loss_a: 0.008617
[00:31:12.497] iteration 11701 : loss: 0.029247, loss_a: 0.017204
[00:31:13.237] iteration 11702 : loss: 0.015055, loss_a: 0.008856
[00:31:14.551] iteration 11703 : loss: 0.018978, loss_a: 0.011164
[00:31:15.296] iteration 11704 : loss: 0.025796, loss_a: 0.015174
[00:31:16.630] iteration 11705 : loss: 0.019160, loss_a: 0.011270
[00:31:17.381] iteration 11706 : loss: 0.028212, loss_a: 0.016595
[00:31:18.706] iteration 11707 : loss: 0.028823, loss_a: 0.016955
[00:31:19.449] iteration 11708 : loss: 0.032721, loss_a: 0.019248
[00:31:20.775] iteration 11709 : loss: 0.045889, loss_a: 0.026993
[00:31:21.530] iteration 11710 : loss: 0.028869, loss_a: 0.016982
[00:31:22.850] iteration 11711 : loss: 0.027082, loss_a: 0.015930
[00:31:23.598] iteration 11712 : loss: 0.028144, loss_a: 0.016556
[00:31:24.907] iteration 11713 : loss: 0.023315, loss_a: 0.013715
[00:31:25.641] iteration 11714 : loss: 0.012506, loss_a: 0.007357
[00:31:26.980] iteration 11715 : loss: 0.037015, loss_a: 0.021773
[00:31:27.723] iteration 11716 : loss: 0.035441, loss_a: 0.020848
[00:31:29.047] iteration 11717 : loss: 0.012850, loss_a: 0.007559
[00:31:29.790] iteration 11718 : loss: 0.016709, loss_a: 0.009829
[00:31:31.109] iteration 11719 : loss: 0.013659, loss_a: 0.008035
[00:31:31.854] iteration 11720 : loss: 0.040591, loss_a: 0.023877
[00:31:33.199] iteration 11721 : loss: 0.023890, loss_a: 0.014053
[00:31:33.929] iteration 11722 : loss: 0.029316, loss_a: 0.017245
[00:31:35.250] iteration 11723 : loss: 0.039542, loss_a: 0.023260
[00:31:35.990] iteration 11724 : loss: 0.018519, loss_a: 0.010894
[00:31:37.311] iteration 11725 : loss: 0.017425, loss_a: 0.010250
[00:31:38.056] iteration 11726 : loss: 0.025034, loss_a: 0.014726
[00:31:39.386] iteration 11727 : loss: 0.025594, loss_a: 0.015055
[00:31:40.129] iteration 11728 : loss: 0.026297, loss_a: 0.015469
[00:31:41.447] iteration 11729 : loss: 0.049242, loss_a: 0.028966
[00:31:42.200] iteration 11730 : loss: 0.021153, loss_a: 0.012443
[00:31:43.500] iteration 11731 : loss: 0.007666, loss_a: 0.004509
[00:31:44.241] iteration 11732 : loss: 0.039740, loss_a: 0.023377
[00:31:45.570] iteration 11733 : loss: 0.027599, loss_a: 0.016235
[00:31:46.321] iteration 11734 : loss: 0.020027, loss_a: 0.011781
[00:31:47.686] iteration 11735 : loss: 0.020735, loss_a: 0.012197
[00:31:48.432] iteration 11736 : loss: 0.043344, loss_a: 0.025496
[00:31:49.736] iteration 11737 : loss: 0.021691, loss_a: 0.012760
[00:31:50.480] iteration 11738 : loss: 0.021105, loss_a: 0.012414
[00:31:51.804] iteration 11739 : loss: 0.018682, loss_a: 0.010989
[00:31:52.547] iteration 11740 : loss: 0.026298, loss_a: 0.015470
[00:31:53.880] iteration 11741 : loss: 0.016755, loss_a: 0.009856
[00:31:54.624] iteration 11742 : loss: 0.014757, loss_a: 0.008681
[00:31:55.960] iteration 11743 : loss: 0.032149, loss_a: 0.018911
[00:31:56.700] iteration 11744 : loss: 0.028642, loss_a: 0.016848
[00:31:58.051] iteration 11745 : loss: 0.022861, loss_a: 0.013448
[00:31:58.793] iteration 11746 : loss: 0.036305, loss_a: 0.021356
[00:32:00.117] iteration 11747 : loss: 0.023322, loss_a: 0.013719
[00:32:00.847] iteration 11748 : loss: 0.024358, loss_a: 0.014328
[00:32:02.198] iteration 11749 : loss: 0.048173, loss_a: 0.028337
[00:32:02.949] iteration 11750 : loss: 0.039894, loss_a: 0.023467
[00:32:04.278] iteration 11751 : loss: 0.017046, loss_a: 0.010027
[00:32:05.018] iteration 11752 : loss: 0.019509, loss_a: 0.011476
[00:32:06.343] iteration 11753 : loss: 0.028863, loss_a: 0.016978
[00:32:07.092] iteration 11754 : loss: 0.039933, loss_a: 0.023490
[00:32:08.413] iteration 11755 : loss: 0.029065, loss_a: 0.017097
[00:32:09.166] iteration 11756 : loss: 0.016863, loss_a: 0.009919
[00:32:10.531] iteration 11757 : loss: 0.020914, loss_a: 0.012302
[00:32:11.263] iteration 11758 : loss: 0.035493, loss_a: 0.020878
[00:32:12.604] iteration 11759 : loss: 0.026228, loss_a: 0.015428
[00:32:13.345] iteration 11760 : loss: 0.053349, loss_a: 0.031382
[00:32:14.665] iteration 11761 : loss: 0.027232, loss_a: 0.016019
[00:32:15.400] iteration 11762 : loss: 0.021389, loss_a: 0.012582
[00:32:16.756] iteration 11763 : loss: 0.062379, loss_a: 0.036693
[00:32:17.498] iteration 11764 : loss: 0.049278, loss_a: 0.028987
[00:32:18.870] iteration 11765 : loss: 0.033895, loss_a: 0.019938
[00:32:19.612] iteration 11766 : loss: 0.024997, loss_a: 0.014704
[00:32:20.927] iteration 11767 : loss: 0.028648, loss_a: 0.016852
[00:32:21.674] iteration 11768 : loss: 0.074893, loss_a: 0.044054
[00:32:23.017] iteration 11769 : loss: 0.017103, loss_a: 0.010061
[00:32:23.772] iteration 11770 : loss: 0.030281, loss_a: 0.017813
[00:32:25.104] iteration 11771 : loss: 0.065783, loss_a: 0.038696
[00:32:25.847] iteration 11772 : loss: 0.036749, loss_a: 0.021617
[00:32:27.191] iteration 11773 : loss: 0.057816, loss_a: 0.034009
[00:32:27.928] iteration 11774 : loss: 0.024246, loss_a: 0.014263
[00:32:29.289] iteration 11775 : loss: 0.029601, loss_a: 0.017412
[00:32:30.035] iteration 11776 : loss: 0.033951, loss_a: 0.019971
[00:32:31.349] iteration 11777 : loss: 0.013310, loss_a: 0.007829
[00:32:32.092] iteration 11778 : loss: 0.048581, loss_a: 0.028577
[00:32:33.409] iteration 11779 : loss: 0.014024, loss_a: 0.008249
[00:32:34.155] iteration 11780 : loss: 0.021290, loss_a: 0.012524
[00:32:35.497] iteration 11781 : loss: 0.030170, loss_a: 0.017747
[00:32:36.247] iteration 11782 : loss: 0.026961, loss_a: 0.015859
[00:32:37.610] iteration 11783 : loss: 0.031280, loss_a: 0.018400
[00:32:38.365] iteration 11784 : loss: 0.027546, loss_a: 0.016204
[00:32:39.713] iteration 11785 : loss: 0.026713, loss_a: 0.015713
[00:32:40.449] iteration 11786 : loss: 0.015369, loss_a: 0.009040
[00:32:41.803] iteration 11787 : loss: 0.043745, loss_a: 0.025732
[00:32:42.550] iteration 11788 : loss: 0.018152, loss_a: 0.010678
[00:32:43.869] iteration 11789 : loss: 0.030880, loss_a: 0.018165
[00:32:44.619] iteration 11790 : loss: 0.094442, loss_a: 0.055554
[00:32:45.962] iteration 11791 : loss: 0.034799, loss_a: 0.020470
[00:32:46.702] iteration 11792 : loss: 0.038542, loss_a: 0.022672
[00:32:48.056] iteration 11793 : loss: 0.027225, loss_a: 0.016014
[00:32:48.806] iteration 11794 : loss: 0.019587, loss_a: 0.011522
[00:32:50.165] iteration 11795 : loss: 0.028979, loss_a: 0.017046
[00:32:50.894] iteration 11796 : loss: 0.025572, loss_a: 0.015042
[00:32:52.229] iteration 11797 : loss: 0.019283, loss_a: 0.011343
[00:32:52.972] iteration 11798 : loss: 0.024513, loss_a: 0.014420
[00:32:54.312] iteration 11799 : loss: 0.026275, loss_a: 0.015456
[00:32:55.073] iteration 11800 : loss: 0.032681, loss_a: 0.019224
[00:33:19.743] iteration 11801 : loss: 0.017909, loss_a: 0.010535
[00:33:21.877] iteration 11802 : loss: 0.011451, loss_a: 0.006736
[00:33:23.191] iteration 11803 : loss: 0.014703, loss_a: 0.008649
[00:33:23.950] iteration 11804 : loss: 0.027613, loss_a: 0.016243
[00:33:25.277] iteration 11805 : loss: 0.021693, loss_a: 0.012761
[00:33:26.009] iteration 11806 : loss: 0.028138, loss_a: 0.016552
[00:33:27.326] iteration 11807 : loss: 0.038395, loss_a: 0.022585
[00:33:28.073] iteration 11808 : loss: 0.042548, loss_a: 0.025028
[00:33:29.395] iteration 11809 : loss: 0.011063, loss_a: 0.006508
[00:33:30.140] iteration 11810 : loss: 0.008063, loss_a: 0.004743
[00:33:31.487] iteration 11811 : loss: 0.023267, loss_a: 0.013687
[00:33:32.228] iteration 11812 : loss: 0.034246, loss_a: 0.020145
[00:33:33.561] iteration 11813 : loss: 0.009710, loss_a: 0.005712
[00:33:34.297] iteration 11814 : loss: 0.019227, loss_a: 0.011310
[00:33:35.631] iteration 11815 : loss: 0.036612, loss_a: 0.021536
[00:33:36.368] iteration 11816 : loss: 0.030686, loss_a: 0.018051
[00:33:37.719] iteration 11817 : loss: 0.026045, loss_a: 0.015320
[00:33:38.460] iteration 11818 : loss: 0.017564, loss_a: 0.010332
[00:33:39.780] iteration 11819 : loss: 0.017081, loss_a: 0.010047
[00:33:40.525] iteration 11820 : loss: 0.047511, loss_a: 0.027948
[00:33:41.911] iteration 11821 : loss: 0.046915, loss_a: 0.027597
[00:33:42.660] iteration 11822 : loss: 0.060704, loss_a: 0.035708
[00:33:44.003] iteration 11823 : loss: 0.015733, loss_a: 0.009255
[00:33:44.746] iteration 11824 : loss: 0.017335, loss_a: 0.010197
[00:33:46.092] iteration 11825 : loss: 0.033269, loss_a: 0.019570
[00:33:46.821] iteration 11826 : loss: 0.021118, loss_a: 0.012423
[00:33:48.155] iteration 11827 : loss: 0.037863, loss_a: 0.022273
[00:33:48.900] iteration 11828 : loss: 0.061290, loss_a: 0.036053
[00:33:50.231] iteration 11829 : loss: 0.021958, loss_a: 0.012917
[00:33:50.986] iteration 11830 : loss: 0.046013, loss_a: 0.027066
[00:33:52.325] iteration 11831 : loss: 0.013835, loss_a: 0.008138
[00:33:53.053] iteration 11832 : loss: 0.013570, loss_a: 0.007982
[00:33:54.402] iteration 11833 : loss: 0.024749, loss_a: 0.014558
[00:33:55.145] iteration 11834 : loss: 0.030129, loss_a: 0.017723
[00:33:56.465] iteration 11835 : loss: 0.015795, loss_a: 0.009291
[00:33:57.205] iteration 11836 : loss: 0.019467, loss_a: 0.011451
[00:33:58.573] iteration 11837 : loss: 0.026885, loss_a: 0.015815
[00:33:59.309] iteration 11838 : loss: 0.017170, loss_a: 0.010100
[00:34:00.632] iteration 11839 : loss: 0.035355, loss_a: 0.020797
[00:34:01.364] iteration 11840 : loss: 0.029785, loss_a: 0.017520
[00:34:02.682] iteration 11841 : loss: 0.016336, loss_a: 0.009609
[00:34:03.418] iteration 11842 : loss: 0.016596, loss_a: 0.009762
[00:34:04.768] iteration 11843 : loss: 0.028793, loss_a: 0.016937
[00:34:05.502] iteration 11844 : loss: 0.022331, loss_a: 0.013136
[00:34:06.831] iteration 11845 : loss: 0.023719, loss_a: 0.013952
[00:34:07.575] iteration 11846 : loss: 0.034756, loss_a: 0.020445
[00:34:08.929] iteration 11847 : loss: 0.032347, loss_a: 0.019028
[00:34:09.669] iteration 11848 : loss: 0.016925, loss_a: 0.009956
[00:34:10.997] iteration 11849 : loss: 0.018364, loss_a: 0.010802
[00:34:11.738] iteration 11850 : loss: 0.021753, loss_a: 0.012796
[00:34:13.083] iteration 11851 : loss: 0.031924, loss_a: 0.018779
[00:34:13.820] iteration 11852 : loss: 0.034487, loss_a: 0.020287
[00:34:15.137] iteration 11853 : loss: 0.024783, loss_a: 0.014578
[00:34:15.871] iteration 11854 : loss: 0.008068, loss_a: 0.004746
[00:34:17.229] iteration 11855 : loss: 0.034158, loss_a: 0.020093
[00:34:17.967] iteration 11856 : loss: 0.019642, loss_a: 0.011554
[00:34:19.311] iteration 11857 : loss: 0.021770, loss_a: 0.012806
[00:34:20.054] iteration 11858 : loss: 0.030746, loss_a: 0.018086
[00:34:21.405] iteration 11859 : loss: 0.026303, loss_a: 0.015472
[00:34:22.153] iteration 11860 : loss: 0.018923, loss_a: 0.011131
[00:34:23.460] iteration 11861 : loss: 0.010105, loss_a: 0.005944
[00:34:24.201] iteration 11862 : loss: 0.028980, loss_a: 0.017047
[00:34:25.550] iteration 11863 : loss: 0.041348, loss_a: 0.024322
[00:34:26.297] iteration 11864 : loss: 0.029663, loss_a: 0.017449
[00:34:27.609] iteration 11865 : loss: 0.038528, loss_a: 0.022663
[00:34:28.358] iteration 11866 : loss: 0.023130, loss_a: 0.013606
[00:34:29.711] iteration 11867 : loss: 0.028539, loss_a: 0.016788
[00:34:30.461] iteration 11868 : loss: 0.024965, loss_a: 0.014685
[00:34:31.831] iteration 11869 : loss: 0.019644, loss_a: 0.011555
[00:34:32.572] iteration 11870 : loss: 0.041286, loss_a: 0.024286
[00:34:33.942] iteration 11871 : loss: 0.016784, loss_a: 0.009873
[00:34:34.678] iteration 11872 : loss: 0.016881, loss_a: 0.009930
[00:34:36.024] iteration 11873 : loss: 0.041210, loss_a: 0.024241
[00:34:36.769] iteration 11874 : loss: 0.032241, loss_a: 0.018965
[00:34:38.100] iteration 11875 : loss: 0.028697, loss_a: 0.016881
[00:34:38.843] iteration 11876 : loss: 0.035606, loss_a: 0.020945
[00:34:40.197] iteration 11877 : loss: 0.024455, loss_a: 0.014385
[00:34:40.928] iteration 11878 : loss: 0.023009, loss_a: 0.013534
[00:34:42.276] iteration 11879 : loss: 0.016366, loss_a: 0.009627
[00:34:43.014] iteration 11880 : loss: 0.016430, loss_a: 0.009665
[00:34:44.393] iteration 11881 : loss: 0.035593, loss_a: 0.020937
[00:34:45.134] iteration 11882 : loss: 0.017149, loss_a: 0.010088
[00:34:46.461] iteration 11883 : loss: 0.024920, loss_a: 0.014659
[00:34:47.196] iteration 11884 : loss: 0.011811, loss_a: 0.006948
[00:34:48.528] iteration 11885 : loss: 0.018085, loss_a: 0.010638
[00:34:49.274] iteration 11886 : loss: 0.068146, loss_a: 0.040086
[00:34:50.642] iteration 11887 : loss: 0.034924, loss_a: 0.020544
[00:34:51.390] iteration 11888 : loss: 0.024509, loss_a: 0.014417
[00:34:52.721] iteration 11889 : loss: 0.021185, loss_a: 0.012462
[00:34:53.459] iteration 11890 : loss: 0.020519, loss_a: 0.012070
[00:34:54.825] iteration 11891 : loss: 0.043156, loss_a: 0.025386
[00:34:55.560] iteration 11892 : loss: 0.016761, loss_a: 0.009859
[00:34:56.908] iteration 11893 : loss: 0.040828, loss_a: 0.024016
[00:34:57.642] iteration 11894 : loss: 0.034908, loss_a: 0.020534
[00:34:59.014] iteration 11895 : loss: 0.023891, loss_a: 0.014054
[00:34:59.766] iteration 11896 : loss: 0.055650, loss_a: 0.032735
[00:35:01.121] iteration 11897 : loss: 0.025874, loss_a: 0.015220
[00:35:01.859] iteration 11898 : loss: 0.045514, loss_a: 0.026773
[00:35:03.189] iteration 11899 : loss: 0.021996, loss_a: 0.012939
[00:35:03.929] iteration 11900 : loss: 0.026376, loss_a: 0.015515
[00:35:05.287] iteration 11901 : loss: 0.047412, loss_a: 0.027889
[00:35:06.023] iteration 11902 : loss: 0.019518, loss_a: 0.011481
[00:35:07.377] iteration 11903 : loss: 0.033144, loss_a: 0.019496
[00:35:08.119] iteration 11904 : loss: 0.026947, loss_a: 0.015851
[00:35:09.448] iteration 11905 : loss: 0.019472, loss_a: 0.011454
[00:35:10.176] iteration 11906 : loss: 0.019641, loss_a: 0.011554
[00:35:11.504] iteration 11907 : loss: 0.017224, loss_a: 0.010132
[00:35:12.251] iteration 11908 : loss: 0.032374, loss_a: 0.019044
[00:35:13.582] iteration 11909 : loss: 0.056280, loss_a: 0.033106
[00:35:14.317] iteration 11910 : loss: 0.017497, loss_a: 0.010292
[00:35:15.672] iteration 11911 : loss: 0.054623, loss_a: 0.032131
[00:35:16.418] iteration 11912 : loss: 0.022090, loss_a: 0.012994
[00:35:17.757] iteration 11913 : loss: 0.016632, loss_a: 0.009784
[00:35:18.507] iteration 11914 : loss: 0.030104, loss_a: 0.017708
[00:35:19.870] iteration 11915 : loss: 0.014933, loss_a: 0.008784
[00:35:20.606] iteration 11916 : loss: 0.036299, loss_a: 0.021352
[00:35:21.945] iteration 11917 : loss: 0.017405, loss_a: 0.010238
[00:35:22.676] iteration 11918 : loss: 0.035586, loss_a: 0.020933
[00:35:24.023] iteration 11919 : loss: 0.012490, loss_a: 0.007347
[00:35:24.760] iteration 11920 : loss: 0.019869, loss_a: 0.011688
[00:35:26.082] iteration 11921 : loss: 0.064468, loss_a: 0.037922
[00:35:26.824] iteration 11922 : loss: 0.035775, loss_a: 0.021044
[00:35:28.152] iteration 11923 : loss: 0.020097, loss_a: 0.011821
[00:35:28.902] iteration 11924 : loss: 0.028181, loss_a: 0.016577
[00:35:30.229] iteration 11925 : loss: 0.014716, loss_a: 0.008657
[00:35:30.974] iteration 11926 : loss: 0.015555, loss_a: 0.009150
[00:35:32.311] iteration 11927 : loss: 0.013827, loss_a: 0.008134
[00:35:33.054] iteration 11928 : loss: 0.029469, loss_a: 0.017335
[00:35:34.392] iteration 11929 : loss: 0.031399, loss_a: 0.018470
[00:35:35.119] iteration 11930 : loss: 0.014005, loss_a: 0.008238
[00:35:36.448] iteration 11931 : loss: 0.023952, loss_a: 0.014089
[00:35:37.191] iteration 11932 : loss: 0.066023, loss_a: 0.038837
[00:35:38.523] iteration 11933 : loss: 0.032282, loss_a: 0.018989
[00:35:39.259] iteration 11934 : loss: 0.027776, loss_a: 0.016339
[00:35:40.610] iteration 11935 : loss: 0.036967, loss_a: 0.021745
[00:35:41.345] iteration 11936 : loss: 0.023434, loss_a: 0.013785
[00:35:42.710] iteration 11937 : loss: 0.038345, loss_a: 0.022556
[00:35:43.458] iteration 11938 : loss: 0.026126, loss_a: 0.015368
[00:35:44.800] iteration 11939 : loss: 0.023448, loss_a: 0.013793
[00:35:45.538] iteration 11940 : loss: 0.029319, loss_a: 0.017246
[00:35:46.853] iteration 11941 : loss: 0.021430, loss_a: 0.012606
[00:35:47.602] iteration 11942 : loss: 0.028154, loss_a: 0.016561
[00:35:48.968] iteration 11943 : loss: 0.027680, loss_a: 0.016282
[00:35:49.712] iteration 11944 : loss: 0.029030, loss_a: 0.017077
[00:35:51.060] iteration 11945 : loss: 0.034038, loss_a: 0.020022
[00:35:51.802] iteration 11946 : loss: 0.019716, loss_a: 0.011597
[00:35:53.148] iteration 11947 : loss: 0.023784, loss_a: 0.013990
[00:35:53.886] iteration 11948 : loss: 0.028562, loss_a: 0.016801
[00:35:55.236] iteration 11949 : loss: 0.024444, loss_a: 0.014379
[00:35:55.983] iteration 11950 : loss: 0.021231, loss_a: 0.012489
[00:35:57.328] iteration 11951 : loss: 0.031257, loss_a: 0.018386
[00:35:58.061] iteration 11952 : loss: 0.042115, loss_a: 0.024773
[00:35:59.415] iteration 11953 : loss: 0.015519, loss_a: 0.009129
[00:36:00.165] iteration 11954 : loss: 0.019207, loss_a: 0.011298
[00:36:01.519] iteration 11955 : loss: 0.017707, loss_a: 0.010416
[00:36:02.252] iteration 11956 : loss: 0.028817, loss_a: 0.016951
[00:36:03.581] iteration 11957 : loss: 0.018478, loss_a: 0.010869
[00:36:04.332] iteration 11958 : loss: 0.019190, loss_a: 0.011288
[00:36:05.648] iteration 11959 : loss: 0.011818, loss_a: 0.006952
[00:36:06.391] iteration 11960 : loss: 0.020186, loss_a: 0.011874
[00:36:07.735] iteration 11961 : loss: 0.019733, loss_a: 0.011608
[00:36:08.483] iteration 11962 : loss: 0.017124, loss_a: 0.010073
[00:36:09.843] iteration 11963 : loss: 0.024511, loss_a: 0.014418
[00:36:10.580] iteration 11964 : loss: 0.023290, loss_a: 0.013700
[00:36:11.953] iteration 11965 : loss: 0.067155, loss_a: 0.039503
[00:36:12.700] iteration 11966 : loss: 0.029050, loss_a: 0.017088
[00:36:14.035] iteration 11967 : loss: 0.080778, loss_a: 0.047516
[00:36:14.778] iteration 11968 : loss: 0.014070, loss_a: 0.008276
[00:36:16.135] iteration 11969 : loss: 0.020931, loss_a: 0.012312
[00:36:16.877] iteration 11970 : loss: 0.024365, loss_a: 0.014332
[00:36:18.193] iteration 11971 : loss: 0.018626, loss_a: 0.010956
[00:36:18.937] iteration 11972 : loss: 0.019868, loss_a: 0.011687
[00:36:20.274] iteration 11973 : loss: 0.035864, loss_a: 0.021097
[00:36:21.017] iteration 11974 : loss: 0.023072, loss_a: 0.013572
[00:36:22.338] iteration 11975 : loss: 0.022669, loss_a: 0.013335
[00:36:23.084] iteration 11976 : loss: 0.041487, loss_a: 0.024404
[00:36:24.440] iteration 11977 : loss: 0.023390, loss_a: 0.013759
[00:36:25.174] iteration 11978 : loss: 0.020081, loss_a: 0.011812
[00:36:26.503] iteration 11979 : loss: 0.016648, loss_a: 0.009793
[00:36:27.237] iteration 11980 : loss: 0.020981, loss_a: 0.012342
[00:36:28.555] iteration 11981 : loss: 0.018918, loss_a: 0.011128
[00:36:29.292] iteration 11982 : loss: 0.018840, loss_a: 0.011083
[00:36:30.608] iteration 11983 : loss: 0.024122, loss_a: 0.014189
[00:36:31.344] iteration 11984 : loss: 0.013633, loss_a: 0.008019
[00:36:32.698] iteration 11985 : loss: 0.011962, loss_a: 0.007036
[00:36:33.433] iteration 11986 : loss: 0.029372, loss_a: 0.017278
[00:36:34.762] iteration 11987 : loss: 0.015094, loss_a: 0.008879
[00:36:35.496] iteration 11988 : loss: 0.017556, loss_a: 0.010327
[00:36:36.835] iteration 11989 : loss: 0.026561, loss_a: 0.015624
[00:36:37.592] iteration 11990 : loss: 0.022151, loss_a: 0.013030
[00:36:38.909] iteration 11991 : loss: 0.054520, loss_a: 0.032071
[00:36:39.654] iteration 11992 : loss: 0.021023, loss_a: 0.012367
[00:36:40.974] iteration 11993 : loss: 0.015168, loss_a: 0.008923
[00:36:41.742] iteration 11994 : loss: 0.055862, loss_a: 0.032860
[00:36:43.097] iteration 11995 : loss: 0.032442, loss_a: 0.019084
[00:36:43.831] iteration 11996 : loss: 0.014428, loss_a: 0.008487
[00:36:45.170] iteration 11997 : loss: 0.036557, loss_a: 0.021504
[00:36:45.922] iteration 11998 : loss: 0.028586, loss_a: 0.016815
[00:36:47.300] iteration 11999 : loss: 0.107209, loss_a: 0.063064
[00:36:48.041] iteration 12000 : loss: 0.030433, loss_a: 0.017902
[00:37:12.692] iteration 12001 : loss: 0.026818, loss_a: 0.015775
[00:37:14.828] iteration 12002 : loss: 0.022330, loss_a: 0.013135
[00:37:16.206] iteration 12003 : loss: 0.040266, loss_a: 0.023686
[00:37:16.946] iteration 12004 : loss: 0.035225, loss_a: 0.020721
[00:37:18.295] iteration 12005 : loss: 0.016683, loss_a: 0.009814
[00:37:19.049] iteration 12006 : loss: 0.022676, loss_a: 0.013339
[00:37:20.408] iteration 12007 : loss: 0.017918, loss_a: 0.010540
[00:37:21.148] iteration 12008 : loss: 0.018524, loss_a: 0.010897
[00:37:22.470] iteration 12009 : loss: 0.017254, loss_a: 0.010150
[00:37:23.218] iteration 12010 : loss: 0.034377, loss_a: 0.020222
[00:37:24.567] iteration 12011 : loss: 0.039240, loss_a: 0.023083
[00:37:25.306] iteration 12012 : loss: 0.017216, loss_a: 0.010127
[00:37:26.655] iteration 12013 : loss: 0.020188, loss_a: 0.011875
[00:37:27.406] iteration 12014 : loss: 0.013372, loss_a: 0.007866
[00:37:28.719] iteration 12015 : loss: 0.022914, loss_a: 0.013479
[00:37:29.478] iteration 12016 : loss: 0.034710, loss_a: 0.020418
[00:37:30.829] iteration 12017 : loss: 0.030674, loss_a: 0.018044
[00:37:31.570] iteration 12018 : loss: 0.061581, loss_a: 0.036224
[00:37:32.902] iteration 12019 : loss: 0.019426, loss_a: 0.011427
[00:37:33.637] iteration 12020 : loss: 0.030257, loss_a: 0.017799
[00:37:34.998] iteration 12021 : loss: 0.026326, loss_a: 0.015486
[00:37:35.745] iteration 12022 : loss: 0.037142, loss_a: 0.021848
[00:37:37.104] iteration 12023 : loss: 0.036658, loss_a: 0.021563
[00:37:37.858] iteration 12024 : loss: 0.023453, loss_a: 0.013796
[00:37:39.184] iteration 12025 : loss: 0.014325, loss_a: 0.008427
[00:37:39.934] iteration 12026 : loss: 0.029999, loss_a: 0.017647
[00:37:41.270] iteration 12027 : loss: 0.039186, loss_a: 0.023050
[00:37:42.017] iteration 12028 : loss: 0.015936, loss_a: 0.009374
[00:37:43.392] iteration 12029 : loss: 0.022614, loss_a: 0.013302
[00:37:44.129] iteration 12030 : loss: 0.012360, loss_a: 0.007271
[00:37:45.500] iteration 12031 : loss: 0.034220, loss_a: 0.020129
[00:37:46.244] iteration 12032 : loss: 0.029175, loss_a: 0.017162
[00:37:47.581] iteration 12033 : loss: 0.022653, loss_a: 0.013325
[00:37:48.329] iteration 12034 : loss: 0.023989, loss_a: 0.014111
[00:37:49.659] iteration 12035 : loss: 0.022327, loss_a: 0.013133
[00:37:50.407] iteration 12036 : loss: 0.040391, loss_a: 0.023759
[00:37:51.795] iteration 12037 : loss: 0.047597, loss_a: 0.027998
[00:37:52.535] iteration 12038 : loss: 0.028260, loss_a: 0.016623
[00:37:53.869] iteration 12039 : loss: 0.024397, loss_a: 0.014351
[00:37:54.617] iteration 12040 : loss: 0.026480, loss_a: 0.015577
[00:37:55.938] iteration 12041 : loss: 0.031203, loss_a: 0.018354
[00:37:56.685] iteration 12042 : loss: 0.084578, loss_a: 0.049752
[00:37:58.010] iteration 12043 : loss: 0.023285, loss_a: 0.013697
[00:37:58.755] iteration 12044 : loss: 0.043157, loss_a: 0.025386
[00:38:00.104] iteration 12045 : loss: 0.030756, loss_a: 0.018092
[00:38:00.862] iteration 12046 : loss: 0.024829, loss_a: 0.014605
[00:38:02.212] iteration 12047 : loss: 0.014715, loss_a: 0.008656
[00:38:02.956] iteration 12048 : loss: 0.027168, loss_a: 0.015981
[00:38:04.327] iteration 12049 : loss: 0.017695, loss_a: 0.010409
[00:38:05.083] iteration 12050 : loss: 0.058002, loss_a: 0.034119
[00:38:06.417] iteration 12051 : loss: 0.027083, loss_a: 0.015931
[00:38:07.149] iteration 12052 : loss: 0.017518, loss_a: 0.010305
[00:38:08.514] iteration 12053 : loss: 0.035228, loss_a: 0.020722
[00:38:09.253] iteration 12054 : loss: 0.029331, loss_a: 0.017253
[00:38:10.591] iteration 12055 : loss: 0.013250, loss_a: 0.007794
[00:38:11.325] iteration 12056 : loss: 0.019976, loss_a: 0.011750
[00:38:12.654] iteration 12057 : loss: 0.029767, loss_a: 0.017510
[00:38:13.392] iteration 12058 : loss: 0.028969, loss_a: 0.017040
[00:38:14.711] iteration 12059 : loss: 0.053616, loss_a: 0.031539
[00:38:15.457] iteration 12060 : loss: 0.018633, loss_a: 0.010961
[00:38:16.786] iteration 12061 : loss: 0.020795, loss_a: 0.012232
[00:38:17.526] iteration 12062 : loss: 0.036025, loss_a: 0.021191
[00:38:18.861] iteration 12063 : loss: 0.052181, loss_a: 0.030695
[00:38:19.604] iteration 12064 : loss: 0.040183, loss_a: 0.023637
[00:38:20.944] iteration 12065 : loss: 0.041103, loss_a: 0.024178
[00:38:21.690] iteration 12066 : loss: 0.034080, loss_a: 0.020047
[00:38:23.028] iteration 12067 : loss: 0.017272, loss_a: 0.010160
[00:38:23.771] iteration 12068 : loss: 0.020176, loss_a: 0.011869
[00:38:25.091] iteration 12069 : loss: 0.034069, loss_a: 0.020040
[00:38:25.834] iteration 12070 : loss: 0.033453, loss_a: 0.019678
[00:38:27.152] iteration 12071 : loss: 0.020642, loss_a: 0.012142
[00:38:27.893] iteration 12072 : loss: 0.016866, loss_a: 0.009921
[00:38:29.252] iteration 12073 : loss: 0.034589, loss_a: 0.020346
[00:38:29.995] iteration 12074 : loss: 0.023513, loss_a: 0.013831
[00:38:31.354] iteration 12075 : loss: 0.020295, loss_a: 0.011939
[00:38:32.089] iteration 12076 : loss: 0.051147, loss_a: 0.030087
[00:38:33.458] iteration 12077 : loss: 0.025655, loss_a: 0.015091
[00:38:34.193] iteration 12078 : loss: 0.015701, loss_a: 0.009236
[00:38:35.552] iteration 12079 : loss: 0.020583, loss_a: 0.012107
[00:38:36.291] iteration 12080 : loss: 0.024002, loss_a: 0.014119
[00:38:37.643] iteration 12081 : loss: 0.035578, loss_a: 0.020928
[00:38:38.393] iteration 12082 : loss: 0.065796, loss_a: 0.038704
[00:38:39.719] iteration 12083 : loss: 0.058561, loss_a: 0.034448
[00:38:40.476] iteration 12084 : loss: 0.038961, loss_a: 0.022918
[00:38:41.816] iteration 12085 : loss: 0.024180, loss_a: 0.014224
[00:38:42.565] iteration 12086 : loss: 0.016561, loss_a: 0.009742
[00:38:43.930] iteration 12087 : loss: 0.014457, loss_a: 0.008504
[00:38:44.678] iteration 12088 : loss: 0.027490, loss_a: 0.016171
[00:38:46.034] iteration 12089 : loss: 0.034575, loss_a: 0.020338
[00:38:46.786] iteration 12090 : loss: 0.042593, loss_a: 0.025055
[00:38:48.125] iteration 12091 : loss: 0.021016, loss_a: 0.012362
[00:38:48.871] iteration 12092 : loss: 0.020942, loss_a: 0.012319
[00:38:50.187] iteration 12093 : loss: 0.029258, loss_a: 0.017211
[00:38:50.944] iteration 12094 : loss: 0.055036, loss_a: 0.032374
[00:38:52.298] iteration 12095 : loss: 0.057187, loss_a: 0.033639
[00:38:53.054] iteration 12096 : loss: 0.070392, loss_a: 0.041407
[00:38:54.401] iteration 12097 : loss: 0.028083, loss_a: 0.016519
[00:38:55.143] iteration 12098 : loss: 0.022148, loss_a: 0.013028
[00:38:56.485] iteration 12099 : loss: 0.048571, loss_a: 0.028571
[00:38:57.233] iteration 12100 : loss: 0.050337, loss_a: 0.029610
[00:38:58.588] iteration 12101 : loss: 0.021628, loss_a: 0.012722
[00:38:59.327] iteration 12102 : loss: 0.030793, loss_a: 0.018114
[00:39:00.689] iteration 12103 : loss: 0.018811, loss_a: 0.011065
[00:39:01.435] iteration 12104 : loss: 0.053674, loss_a: 0.031573
[00:39:02.782] iteration 12105 : loss: 0.025304, loss_a: 0.014885
[00:39:03.523] iteration 12106 : loss: 0.017125, loss_a: 0.010073
[00:39:04.871] iteration 12107 : loss: 0.027878, loss_a: 0.016399
[00:39:05.633] iteration 12108 : loss: 0.039047, loss_a: 0.022969
[00:39:06.985] iteration 12109 : loss: 0.030697, loss_a: 0.018057
[00:39:07.727] iteration 12110 : loss: 0.037901, loss_a: 0.022295
[00:39:09.064] iteration 12111 : loss: 0.014762, loss_a: 0.008683
[00:39:09.804] iteration 12112 : loss: 0.013780, loss_a: 0.008106
[00:39:11.133] iteration 12113 : loss: 0.016627, loss_a: 0.009781
[00:39:11.876] iteration 12114 : loss: 0.034731, loss_a: 0.020430
[00:39:13.228] iteration 12115 : loss: 0.020620, loss_a: 0.012129
[00:39:13.970] iteration 12116 : loss: 0.037258, loss_a: 0.021917
[00:39:15.339] iteration 12117 : loss: 0.030285, loss_a: 0.017814
[00:39:16.079] iteration 12118 : loss: 0.017991, loss_a: 0.010583
[00:39:17.461] iteration 12119 : loss: 0.064811, loss_a: 0.038124
[00:39:18.197] iteration 12120 : loss: 0.020888, loss_a: 0.012287
[00:39:19.546] iteration 12121 : loss: 0.022982, loss_a: 0.013519
[00:39:20.284] iteration 12122 : loss: 0.033685, loss_a: 0.019815
[00:39:21.639] iteration 12123 : loss: 0.016510, loss_a: 0.009712
[00:39:22.386] iteration 12124 : loss: 0.027038, loss_a: 0.015905
[00:39:23.753] iteration 12125 : loss: 0.022224, loss_a: 0.013073
[00:39:24.504] iteration 12126 : loss: 0.053146, loss_a: 0.031263
[00:39:25.827] iteration 12127 : loss: 0.017339, loss_a: 0.010200
[00:39:26.573] iteration 12128 : loss: 0.029418, loss_a: 0.017305
[00:39:27.912] iteration 12129 : loss: 0.023458, loss_a: 0.013799
[00:39:28.652] iteration 12130 : loss: 0.048339, loss_a: 0.028434
[00:39:30.024] iteration 12131 : loss: 0.016069, loss_a: 0.009453
[00:39:30.765] iteration 12132 : loss: 0.046586, loss_a: 0.027404
[00:39:32.134] iteration 12133 : loss: 0.038964, loss_a: 0.022920
[00:39:32.872] iteration 12134 : loss: 0.020810, loss_a: 0.012241
[00:39:34.210] iteration 12135 : loss: 0.019241, loss_a: 0.011318
[00:39:34.953] iteration 12136 : loss: 0.013910, loss_a: 0.008182
[00:39:36.298] iteration 12137 : loss: 0.020993, loss_a: 0.012349
[00:39:37.062] iteration 12138 : loss: 0.074179, loss_a: 0.043634
[00:39:38.432] iteration 12139 : loss: 0.075703, loss_a: 0.044531
[00:39:39.182] iteration 12140 : loss: 0.025599, loss_a: 0.015058
[00:39:40.540] iteration 12141 : loss: 0.014388, loss_a: 0.008463
[00:39:41.274] iteration 12142 : loss: 0.012018, loss_a: 0.007069
[00:39:42.619] iteration 12143 : loss: 0.023715, loss_a: 0.013950
[00:39:43.369] iteration 12144 : loss: 0.050931, loss_a: 0.029959
[00:39:44.707] iteration 12145 : loss: 0.038840, loss_a: 0.022847
[00:39:45.452] iteration 12146 : loss: 0.039064, loss_a: 0.022979
[00:39:46.776] iteration 12147 : loss: 0.015636, loss_a: 0.009197
[00:39:47.535] iteration 12148 : loss: 0.040263, loss_a: 0.023684
[00:39:48.876] iteration 12149 : loss: 0.020334, loss_a: 0.011961
[00:39:49.624] iteration 12150 : loss: 0.017705, loss_a: 0.010415
[00:39:50.948] iteration 12151 : loss: 0.043776, loss_a: 0.025751
[00:39:51.705] iteration 12152 : loss: 0.032217, loss_a: 0.018951
[00:39:53.069] iteration 12153 : loss: 0.025260, loss_a: 0.014859
[00:39:53.822] iteration 12154 : loss: 0.019039, loss_a: 0.011199
[00:39:55.179] iteration 12155 : loss: 0.020320, loss_a: 0.011953
[00:39:55.926] iteration 12156 : loss: 0.024363, loss_a: 0.014331
[00:39:57.256] iteration 12157 : loss: 0.029922, loss_a: 0.017601
[00:39:57.986] iteration 12158 : loss: 0.023723, loss_a: 0.013955
[00:39:59.348] iteration 12159 : loss: 0.061038, loss_a: 0.035904
[00:40:00.094] iteration 12160 : loss: 0.033963, loss_a: 0.019978
[00:40:01.437] iteration 12161 : loss: 0.029213, loss_a: 0.017184
[00:40:02.177] iteration 12162 : loss: 0.033701, loss_a: 0.019824
[00:40:03.523] iteration 12163 : loss: 0.027984, loss_a: 0.016461
[00:40:04.269] iteration 12164 : loss: 0.026001, loss_a: 0.015294
[00:40:05.622] iteration 12165 : loss: 0.020363, loss_a: 0.011978
[00:40:06.358] iteration 12166 : loss: 0.016528, loss_a: 0.009722
[00:40:07.713] iteration 12167 : loss: 0.017014, loss_a: 0.010008
[00:40:08.443] iteration 12168 : loss: 0.011614, loss_a: 0.006832
[00:40:09.804] iteration 12169 : loss: 0.040491, loss_a: 0.023818
[00:40:10.552] iteration 12170 : loss: 0.023941, loss_a: 0.014083
[00:40:11.898] iteration 12171 : loss: 0.026596, loss_a: 0.015644
[00:40:12.651] iteration 12172 : loss: 0.031841, loss_a: 0.018730
[00:40:13.984] iteration 12173 : loss: 0.034093, loss_a: 0.020055
[00:40:14.739] iteration 12174 : loss: 0.015245, loss_a: 0.008968
[00:40:16.117] iteration 12175 : loss: 0.027060, loss_a: 0.015918
[00:40:16.854] iteration 12176 : loss: 0.017143, loss_a: 0.010084
[00:40:18.219] iteration 12177 : loss: 0.021541, loss_a: 0.012671
[00:40:18.959] iteration 12178 : loss: 0.022949, loss_a: 0.013499
[00:40:20.277] iteration 12179 : loss: 0.017822, loss_a: 0.010483
[00:40:21.039] iteration 12180 : loss: 0.036685, loss_a: 0.021579
[00:40:22.357] iteration 12181 : loss: 0.025355, loss_a: 0.014914
[00:40:23.091] iteration 12182 : loss: 0.021007, loss_a: 0.012357
[00:40:24.450] iteration 12183 : loss: 0.017330, loss_a: 0.010194
[00:40:25.187] iteration 12184 : loss: 0.023639, loss_a: 0.013906
[00:40:26.525] iteration 12185 : loss: 0.031411, loss_a: 0.018477
[00:40:27.259] iteration 12186 : loss: 0.011472, loss_a: 0.006748
[00:40:28.620] iteration 12187 : loss: 0.015486, loss_a: 0.009109
[00:40:29.358] iteration 12188 : loss: 0.015945, loss_a: 0.009379
[00:40:30.736] iteration 12189 : loss: 0.065520, loss_a: 0.038541
[00:40:31.491] iteration 12190 : loss: 0.033466, loss_a: 0.019686
[00:40:32.840] iteration 12191 : loss: 0.022771, loss_a: 0.013395
[00:40:33.583] iteration 12192 : loss: 0.031866, loss_a: 0.018745
[00:40:34.927] iteration 12193 : loss: 0.030886, loss_a: 0.018168
[00:40:35.686] iteration 12194 : loss: 0.047410, loss_a: 0.027888
[00:40:37.044] iteration 12195 : loss: 0.023149, loss_a: 0.013617
[00:40:37.776] iteration 12196 : loss: 0.020986, loss_a: 0.012345
[00:40:39.110] iteration 12197 : loss: 0.014860, loss_a: 0.008741
[00:40:39.858] iteration 12198 : loss: 0.023338, loss_a: 0.013728
[00:40:41.206] iteration 12199 : loss: 0.036747, loss_a: 0.021616
[00:40:41.955] iteration 12200 : loss: 0.033071, loss_a: 0.019454
[00:41:06.653] iteration 12201 : loss: 0.016466, loss_a: 0.009686
[00:41:08.937] iteration 12202 : loss: 0.067875, loss_a: 0.039926
[00:41:10.295] iteration 12203 : loss: 0.032368, loss_a: 0.019040
[00:41:11.032] iteration 12204 : loss: 0.014919, loss_a: 0.008776
[00:41:12.353] iteration 12205 : loss: 0.053371, loss_a: 0.031394
[00:41:13.093] iteration 12206 : loss: 0.023298, loss_a: 0.013705
[00:41:14.449] iteration 12207 : loss: 0.022757, loss_a: 0.013387
[00:41:15.201] iteration 12208 : loss: 0.023139, loss_a: 0.013611
[00:41:16.530] iteration 12209 : loss: 0.022690, loss_a: 0.013347
[00:41:17.270] iteration 12210 : loss: 0.025333, loss_a: 0.014902
[00:41:18.632] iteration 12211 : loss: 0.018206, loss_a: 0.010709
[00:41:19.370] iteration 12212 : loss: 0.023905, loss_a: 0.014062
[00:41:20.692] iteration 12213 : loss: 0.016806, loss_a: 0.009886
[00:41:21.432] iteration 12214 : loss: 0.010538, loss_a: 0.006199
[00:41:22.799] iteration 12215 : loss: 0.031785, loss_a: 0.018697
[00:41:23.537] iteration 12216 : loss: 0.030817, loss_a: 0.018128
[00:41:24.893] iteration 12217 : loss: 0.027665, loss_a: 0.016274
[00:41:25.624] iteration 12218 : loss: 0.033580, loss_a: 0.019753
[00:41:26.971] iteration 12219 : loss: 0.016804, loss_a: 0.009885
[00:41:27.708] iteration 12220 : loss: 0.012203, loss_a: 0.007179
[00:41:29.067] iteration 12221 : loss: 0.044229, loss_a: 0.026017
[00:41:29.800] iteration 12222 : loss: 0.019003, loss_a: 0.011178
[00:41:31.151] iteration 12223 : loss: 0.028428, loss_a: 0.016722
[00:41:31.896] iteration 12224 : loss: 0.024798, loss_a: 0.014587
[00:41:33.237] iteration 12225 : loss: 0.021731, loss_a: 0.012783
[00:41:33.985] iteration 12226 : loss: 0.026720, loss_a: 0.015718
[00:41:35.371] iteration 12227 : loss: 0.035189, loss_a: 0.020699
[00:41:36.112] iteration 12228 : loss: 0.020504, loss_a: 0.012061
[00:41:37.455] iteration 12229 : loss: 0.023525, loss_a: 0.013838
[00:41:38.197] iteration 12230 : loss: 0.021715, loss_a: 0.012774
[00:41:39.546] iteration 12231 : loss: 0.014249, loss_a: 0.008382
[00:41:40.282] iteration 12232 : loss: 0.040816, loss_a: 0.024009
[00:41:41.614] iteration 12233 : loss: 0.039407, loss_a: 0.023181
[00:41:42.348] iteration 12234 : loss: 0.029495, loss_a: 0.017350
[00:41:43.689] iteration 12235 : loss: 0.022566, loss_a: 0.013274
[00:41:44.432] iteration 12236 : loss: 0.080216, loss_a: 0.047186
[00:41:45.777] iteration 12237 : loss: 0.041776, loss_a: 0.024574
[00:41:46.519] iteration 12238 : loss: 0.018209, loss_a: 0.010711
[00:41:47.870] iteration 12239 : loss: 0.027710, loss_a: 0.016300
[00:41:48.605] iteration 12240 : loss: 0.016984, loss_a: 0.009991
[00:41:49.955] iteration 12241 : loss: 0.031555, loss_a: 0.018561
[00:41:50.688] iteration 12242 : loss: 0.016289, loss_a: 0.009582
[00:41:51.989] iteration 12243 : loss: 0.032485, loss_a: 0.019109
[00:41:52.731] iteration 12244 : loss: 0.038498, loss_a: 0.022646
[00:41:54.084] iteration 12245 : loss: 0.057556, loss_a: 0.033857
[00:41:54.820] iteration 12246 : loss: 0.017831, loss_a: 0.010489
[00:41:56.193] iteration 12247 : loss: 0.027196, loss_a: 0.015998
[00:41:56.951] iteration 12248 : loss: 0.036696, loss_a: 0.021586
[00:41:58.293] iteration 12249 : loss: 0.055969, loss_a: 0.032923
[00:41:59.026] iteration 12250 : loss: 0.021877, loss_a: 0.012869
[00:42:00.388] iteration 12251 : loss: 0.051467, loss_a: 0.030275
[00:42:01.125] iteration 12252 : loss: 0.021502, loss_a: 0.012648
[00:42:02.490] iteration 12253 : loss: 0.046238, loss_a: 0.027199
[00:42:03.222] iteration 12254 : loss: 0.013211, loss_a: 0.007771
[00:42:04.557] iteration 12255 : loss: 0.026134, loss_a: 0.015373
[00:42:05.296] iteration 12256 : loss: 0.023007, loss_a: 0.013534
[00:42:06.653] iteration 12257 : loss: 0.020835, loss_a: 0.012256
[00:42:07.391] iteration 12258 : loss: 0.018360, loss_a: 0.010800
[00:42:08.747] iteration 12259 : loss: 0.036060, loss_a: 0.021212
[00:42:09.485] iteration 12260 : loss: 0.025737, loss_a: 0.015139
[00:42:10.847] iteration 12261 : loss: 0.014898, loss_a: 0.008763
[00:42:11.591] iteration 12262 : loss: 0.028839, loss_a: 0.016964
[00:42:12.933] iteration 12263 : loss: 0.014234, loss_a: 0.008373
[00:42:13.682] iteration 12264 : loss: 0.026048, loss_a: 0.015322
[00:42:15.044] iteration 12265 : loss: 0.026666, loss_a: 0.015686
[00:42:15.789] iteration 12266 : loss: 0.079446, loss_a: 0.046733
[00:42:17.143] iteration 12267 : loss: 0.018864, loss_a: 0.011096
[00:42:17.885] iteration 12268 : loss: 0.034848, loss_a: 0.020499
[00:42:19.215] iteration 12269 : loss: 0.043343, loss_a: 0.025496
[00:42:19.953] iteration 12270 : loss: 0.024735, loss_a: 0.014550
[00:42:21.289] iteration 12271 : loss: 0.024599, loss_a: 0.014470
[00:42:22.021] iteration 12272 : loss: 0.033853, loss_a: 0.019914
[00:42:23.393] iteration 12273 : loss: 0.033890, loss_a: 0.019935
[00:42:24.126] iteration 12274 : loss: 0.023971, loss_a: 0.014101
[00:42:25.470] iteration 12275 : loss: 0.026345, loss_a: 0.015497
[00:42:26.220] iteration 12276 : loss: 0.031205, loss_a: 0.018356
[00:42:27.573] iteration 12277 : loss: 0.024448, loss_a: 0.014381
[00:42:28.312] iteration 12278 : loss: 0.033241, loss_a: 0.019553
[00:42:29.699] iteration 12279 : loss: 0.015779, loss_a: 0.009282
[00:42:30.438] iteration 12280 : loss: 0.041476, loss_a: 0.024397
[00:42:31.805] iteration 12281 : loss: 0.012578, loss_a: 0.007399
[00:42:32.559] iteration 12282 : loss: 0.023938, loss_a: 0.014081
[00:42:33.927] iteration 12283 : loss: 0.046893, loss_a: 0.027584
[00:42:34.670] iteration 12284 : loss: 0.032937, loss_a: 0.019375
[00:42:36.035] iteration 12285 : loss: 0.016427, loss_a: 0.009663
[00:42:36.791] iteration 12286 : loss: 0.028450, loss_a: 0.016735
[00:42:38.127] iteration 12287 : loss: 0.020663, loss_a: 0.012155
[00:42:38.858] iteration 12288 : loss: 0.023971, loss_a: 0.014100
[00:42:40.204] iteration 12289 : loss: 0.015118, loss_a: 0.008893
[00:42:40.947] iteration 12290 : loss: 0.037787, loss_a: 0.022228
[00:42:42.294] iteration 12291 : loss: 0.023597, loss_a: 0.013881
[00:42:43.032] iteration 12292 : loss: 0.017798, loss_a: 0.010469
[00:42:44.376] iteration 12293 : loss: 0.030344, loss_a: 0.017850
[00:42:45.111] iteration 12294 : loss: 0.022245, loss_a: 0.013085
[00:42:46.444] iteration 12295 : loss: 0.030823, loss_a: 0.018131
[00:42:47.173] iteration 12296 : loss: 0.021958, loss_a: 0.012916
[00:42:48.530] iteration 12297 : loss: 0.056725, loss_a: 0.033367
[00:42:49.271] iteration 12298 : loss: 0.015847, loss_a: 0.009322
[00:42:50.605] iteration 12299 : loss: 0.019407, loss_a: 0.011416
[00:42:51.342] iteration 12300 : loss: 0.034679, loss_a: 0.020399
[00:42:52.700] iteration 12301 : loss: 0.028870, loss_a: 0.016982
[00:42:53.442] iteration 12302 : loss: 0.027139, loss_a: 0.015964
[00:42:54.803] iteration 12303 : loss: 0.053274, loss_a: 0.031337
[00:42:55.540] iteration 12304 : loss: 0.029552, loss_a: 0.017383
[00:42:56.882] iteration 12305 : loss: 0.024912, loss_a: 0.014654
[00:42:57.614] iteration 12306 : loss: 0.031333, loss_a: 0.018431
[00:42:58.966] iteration 12307 : loss: 0.022846, loss_a: 0.013439
[00:42:59.717] iteration 12308 : loss: 0.041543, loss_a: 0.024437
[00:43:01.063] iteration 12309 : loss: 0.181368, loss_a: 0.106687
[00:43:01.806] iteration 12310 : loss: 0.037661, loss_a: 0.022153
[00:43:03.152] iteration 12311 : loss: 0.022817, loss_a: 0.013422
[00:43:03.896] iteration 12312 : loss: 0.046356, loss_a: 0.027268
[00:43:05.254] iteration 12313 : loss: 0.021895, loss_a: 0.012879
[00:43:06.002] iteration 12314 : loss: 0.015894, loss_a: 0.009349
[00:43:07.340] iteration 12315 : loss: 0.038121, loss_a: 0.022424
[00:43:08.087] iteration 12316 : loss: 0.048315, loss_a: 0.028421
[00:43:09.413] iteration 12317 : loss: 0.036966, loss_a: 0.021745
[00:43:10.155] iteration 12318 : loss: 0.021462, loss_a: 0.012625
[00:43:11.487] iteration 12319 : loss: 0.030551, loss_a: 0.017971
[00:43:12.244] iteration 12320 : loss: 0.062359, loss_a: 0.036682
[00:43:13.560] iteration 12321 : loss: 0.015691, loss_a: 0.009230
[00:43:14.297] iteration 12322 : loss: 0.024258, loss_a: 0.014269
[00:43:15.657] iteration 12323 : loss: 0.022313, loss_a: 0.013125
[00:43:16.399] iteration 12324 : loss: 0.018951, loss_a: 0.011148
[00:43:17.724] iteration 12325 : loss: 0.027978, loss_a: 0.016458
[00:43:18.469] iteration 12326 : loss: 0.042590, loss_a: 0.025053
[00:43:19.799] iteration 12327 : loss: 0.019092, loss_a: 0.011230
[00:43:20.532] iteration 12328 : loss: 0.017260, loss_a: 0.010153
[00:43:21.875] iteration 12329 : loss: 0.054100, loss_a: 0.031823
[00:43:22.611] iteration 12330 : loss: 0.048707, loss_a: 0.028651
[00:43:23.922] iteration 12331 : loss: 0.027909, loss_a: 0.016417
[00:43:24.665] iteration 12332 : loss: 0.024755, loss_a: 0.014562
[00:43:26.020] iteration 12333 : loss: 0.022083, loss_a: 0.012990
[00:43:26.753] iteration 12334 : loss: 0.020474, loss_a: 0.012043
[00:43:28.081] iteration 12335 : loss: 0.044621, loss_a: 0.026248
[00:43:28.816] iteration 12336 : loss: 0.023313, loss_a: 0.013713
[00:43:30.140] iteration 12337 : loss: 0.019350, loss_a: 0.011382
[00:43:30.883] iteration 12338 : loss: 0.030825, loss_a: 0.018133
[00:43:32.202] iteration 12339 : loss: 0.015661, loss_a: 0.009213
[00:43:32.951] iteration 12340 : loss: 0.016281, loss_a: 0.009577
[00:43:34.273] iteration 12341 : loss: 0.039119, loss_a: 0.023011
[00:43:35.017] iteration 12342 : loss: 0.017878, loss_a: 0.010517
[00:43:36.344] iteration 12343 : loss: 0.025425, loss_a: 0.014956
[00:43:37.075] iteration 12344 : loss: 0.029460, loss_a: 0.017330
[00:43:38.402] iteration 12345 : loss: 0.022630, loss_a: 0.013312
[00:43:39.151] iteration 12346 : loss: 0.019594, loss_a: 0.011526
[00:43:40.497] iteration 12347 : loss: 0.027268, loss_a: 0.016040
[00:43:41.231] iteration 12348 : loss: 0.017630, loss_a: 0.010371
[00:43:42.556] iteration 12349 : loss: 0.043260, loss_a: 0.025447
[00:43:43.301] iteration 12350 : loss: 0.024204, loss_a: 0.014238
[00:43:44.632] iteration 12351 : loss: 0.046259, loss_a: 0.027211
[00:43:45.375] iteration 12352 : loss: 0.088579, loss_a: 0.052105
[00:43:46.711] iteration 12353 : loss: 0.020790, loss_a: 0.012230
[00:43:47.447] iteration 12354 : loss: 0.047734, loss_a: 0.028079
[00:43:48.782] iteration 12355 : loss: 0.026810, loss_a: 0.015771
[00:43:49.529] iteration 12356 : loss: 0.041775, loss_a: 0.024573
[00:43:50.864] iteration 12357 : loss: 0.039646, loss_a: 0.023321
[00:43:51.604] iteration 12358 : loss: 0.013453, loss_a: 0.007913
[00:43:52.950] iteration 12359 : loss: 0.017591, loss_a: 0.010348
[00:43:53.697] iteration 12360 : loss: 0.027534, loss_a: 0.016197
[00:43:55.019] iteration 12361 : loss: 0.032840, loss_a: 0.019318
[00:43:55.763] iteration 12362 : loss: 0.013209, loss_a: 0.007770
[00:43:57.085] iteration 12363 : loss: 0.027394, loss_a: 0.016114
[00:43:57.827] iteration 12364 : loss: 0.040212, loss_a: 0.023654
[00:43:59.173] iteration 12365 : loss: 0.015569, loss_a: 0.009158
[00:43:59.912] iteration 12366 : loss: 0.024166, loss_a: 0.014215
[00:44:01.277] iteration 12367 : loss: 0.021659, loss_a: 0.012740
[00:44:02.011] iteration 12368 : loss: 0.013205, loss_a: 0.007768
[00:44:03.359] iteration 12369 : loss: 0.039952, loss_a: 0.023501
[00:44:04.094] iteration 12370 : loss: 0.024531, loss_a: 0.014430
[00:44:05.433] iteration 12371 : loss: 0.036628, loss_a: 0.021546
[00:44:06.181] iteration 12372 : loss: 0.065190, loss_a: 0.038347
[00:44:07.537] iteration 12373 : loss: 0.052038, loss_a: 0.030611
[00:44:08.277] iteration 12374 : loss: 0.023822, loss_a: 0.014013
[00:44:09.597] iteration 12375 : loss: 0.017191, loss_a: 0.010112
[00:44:10.346] iteration 12376 : loss: 0.057768, loss_a: 0.033981
[00:44:11.704] iteration 12377 : loss: 0.022220, loss_a: 0.013070
[00:44:12.440] iteration 12378 : loss: 0.018843, loss_a: 0.011084
[00:44:13.788] iteration 12379 : loss: 0.019959, loss_a: 0.011740
[00:44:14.529] iteration 12380 : loss: 0.029576, loss_a: 0.017398
[00:44:15.871] iteration 12381 : loss: 0.018561, loss_a: 0.010918
[00:44:16.614] iteration 12382 : loss: 0.036626, loss_a: 0.021544
[00:44:17.943] iteration 12383 : loss: 0.025088, loss_a: 0.014758
[00:44:18.681] iteration 12384 : loss: 0.015446, loss_a: 0.009086
[00:44:20.012] iteration 12385 : loss: 0.017696, loss_a: 0.010409
[00:44:20.751] iteration 12386 : loss: 0.019673, loss_a: 0.011572
[00:44:22.080] iteration 12387 : loss: 0.026986, loss_a: 0.015874
[00:44:22.821] iteration 12388 : loss: 0.019187, loss_a: 0.011286
[00:44:24.192] iteration 12389 : loss: 0.022443, loss_a: 0.013202
[00:44:24.940] iteration 12390 : loss: 0.071308, loss_a: 0.041946
[00:44:26.248] iteration 12391 : loss: 0.022765, loss_a: 0.013391
[00:44:26.975] iteration 12392 : loss: 0.012431, loss_a: 0.007312
[00:44:28.335] iteration 12393 : loss: 0.029680, loss_a: 0.017459
[00:44:29.079] iteration 12394 : loss: 0.040616, loss_a: 0.023892
[00:44:30.441] iteration 12395 : loss: 0.017340, loss_a: 0.010200
[00:44:31.178] iteration 12396 : loss: 0.032890, loss_a: 0.019347
[00:44:32.514] iteration 12397 : loss: 0.014476, loss_a: 0.008515
[00:44:33.256] iteration 12398 : loss: 0.029112, loss_a: 0.017125
[00:44:34.604] iteration 12399 : loss: 0.027048, loss_a: 0.015911
[00:44:35.342] iteration 12400 : loss: 0.024841, loss_a: 0.014612
[00:45:00.041] iteration 12401 : loss: 0.023170, loss_a: 0.013629
[00:45:02.172] iteration 12402 : loss: 0.008832, loss_a: 0.005195
[00:45:03.551] iteration 12403 : loss: 0.017586, loss_a: 0.010345
[00:45:04.288] iteration 12404 : loss: 0.022015, loss_a: 0.012950
[00:45:05.618] iteration 12405 : loss: 0.033362, loss_a: 0.019624
[00:45:06.367] iteration 12406 : loss: 0.038159, loss_a: 0.022446
[00:45:07.698] iteration 12407 : loss: 0.022579, loss_a: 0.013282
[00:45:08.444] iteration 12408 : loss: 0.023395, loss_a: 0.013762
[00:45:09.814] iteration 12409 : loss: 0.057750, loss_a: 0.033971
[00:45:10.556] iteration 12410 : loss: 0.018571, loss_a: 0.010924
[00:45:11.883] iteration 12411 : loss: 0.018293, loss_a: 0.010761
[00:45:12.623] iteration 12412 : loss: 0.019553, loss_a: 0.011502
[00:45:13.991] iteration 12413 : loss: 0.017706, loss_a: 0.010415
[00:45:14.732] iteration 12414 : loss: 0.023683, loss_a: 0.013931
[00:45:16.076] iteration 12415 : loss: 0.014185, loss_a: 0.008344
[00:45:16.829] iteration 12416 : loss: 0.013555, loss_a: 0.007974
[00:45:18.155] iteration 12417 : loss: 0.015770, loss_a: 0.009276
[00:45:18.899] iteration 12418 : loss: 0.042013, loss_a: 0.024713
[00:45:20.221] iteration 12419 : loss: 0.022207, loss_a: 0.013063
[00:45:20.957] iteration 12420 : loss: 0.017878, loss_a: 0.010517
[00:45:22.283] iteration 12421 : loss: 0.014770, loss_a: 0.008688
[00:45:23.023] iteration 12422 : loss: 0.013725, loss_a: 0.008073
[00:45:24.369] iteration 12423 : loss: 0.025875, loss_a: 0.015221
[00:45:25.122] iteration 12424 : loss: 0.026507, loss_a: 0.015592
[00:45:26.484] iteration 12425 : loss: 0.015427, loss_a: 0.009075
[00:45:27.236] iteration 12426 : loss: 0.044184, loss_a: 0.025990
[00:45:28.555] iteration 12427 : loss: 0.016277, loss_a: 0.009574
[00:45:29.304] iteration 12428 : loss: 0.034213, loss_a: 0.020125
[00:45:30.642] iteration 12429 : loss: 0.020708, loss_a: 0.012181
[00:45:31.382] iteration 12430 : loss: 0.015223, loss_a: 0.008955
[00:45:32.705] iteration 12431 : loss: 0.011238, loss_a: 0.006611
[00:45:33.455] iteration 12432 : loss: 0.030564, loss_a: 0.017979
[00:45:34.825] iteration 12433 : loss: 0.026816, loss_a: 0.015774
[00:45:35.575] iteration 12434 : loss: 0.057534, loss_a: 0.033844
[00:45:36.888] iteration 12435 : loss: 0.017601, loss_a: 0.010354
[00:45:37.628] iteration 12436 : loss: 0.040442, loss_a: 0.023789
[00:45:39.008] iteration 12437 : loss: 0.031762, loss_a: 0.018684
[00:45:39.744] iteration 12438 : loss: 0.044171, loss_a: 0.025983
[00:45:41.109] iteration 12439 : loss: 0.043791, loss_a: 0.025759
[00:45:41.851] iteration 12440 : loss: 0.035336, loss_a: 0.020786
[00:45:43.172] iteration 12441 : loss: 0.012617, loss_a: 0.007422
[00:45:43.917] iteration 12442 : loss: 0.039339, loss_a: 0.023141
[00:45:45.256] iteration 12443 : loss: 0.037030, loss_a: 0.021783
[00:45:45.992] iteration 12444 : loss: 0.022852, loss_a: 0.013443
[00:45:47.334] iteration 12445 : loss: 0.069548, loss_a: 0.040910
[00:45:48.085] iteration 12446 : loss: 0.069019, loss_a: 0.040600
[00:45:49.435] iteration 12447 : loss: 0.019107, loss_a: 0.011239
[00:45:50.177] iteration 12448 : loss: 0.021835, loss_a: 0.012844
[00:45:51.487] iteration 12449 : loss: 0.021735, loss_a: 0.012785
[00:45:52.231] iteration 12450 : loss: 0.028428, loss_a: 0.016722
[00:45:53.580] iteration 12451 : loss: 0.021508, loss_a: 0.012652
[00:45:54.328] iteration 12452 : loss: 0.043508, loss_a: 0.025593
[00:45:55.639] iteration 12453 : loss: 0.023068, loss_a: 0.013570
[00:45:56.371] iteration 12454 : loss: 0.013052, loss_a: 0.007678
[00:45:57.686] iteration 12455 : loss: 0.039867, loss_a: 0.023451
[00:45:58.430] iteration 12456 : loss: 0.034076, loss_a: 0.020045
[00:45:59.750] iteration 12457 : loss: 0.027417, loss_a: 0.016127
[00:46:00.492] iteration 12458 : loss: 0.090946, loss_a: 0.053497
[00:46:01.859] iteration 12459 : loss: 0.047284, loss_a: 0.027814
[00:46:02.601] iteration 12460 : loss: 0.026188, loss_a: 0.015405
[00:46:03.938] iteration 12461 : loss: 0.023217, loss_a: 0.013657
[00:46:04.676] iteration 12462 : loss: 0.018046, loss_a: 0.010615
[00:46:06.002] iteration 12463 : loss: 0.016554, loss_a: 0.009738
[00:46:06.739] iteration 12464 : loss: 0.013521, loss_a: 0.007953
[00:46:08.093] iteration 12465 : loss: 0.028709, loss_a: 0.016888
[00:46:08.846] iteration 12466 : loss: 0.026189, loss_a: 0.015405
[00:46:10.209] iteration 12467 : loss: 0.048430, loss_a: 0.028488
[00:46:10.950] iteration 12468 : loss: 0.024753, loss_a: 0.014561
[00:46:12.321] iteration 12469 : loss: 0.029661, loss_a: 0.017448
[00:46:13.062] iteration 12470 : loss: 0.023277, loss_a: 0.013692
[00:46:14.386] iteration 12471 : loss: 0.011801, loss_a: 0.006942
[00:46:15.133] iteration 12472 : loss: 0.025702, loss_a: 0.015119
[00:46:16.482] iteration 12473 : loss: 0.063847, loss_a: 0.037557
[00:46:17.223] iteration 12474 : loss: 0.019913, loss_a: 0.011713
[00:46:18.578] iteration 12475 : loss: 0.011727, loss_a: 0.006898
[00:46:19.312] iteration 12476 : loss: 0.017604, loss_a: 0.010355
[00:46:20.636] iteration 12477 : loss: 0.031423, loss_a: 0.018484
[00:46:21.384] iteration 12478 : loss: 0.023737, loss_a: 0.013963
[00:46:22.732] iteration 12479 : loss: 0.026805, loss_a: 0.015768
[00:46:23.466] iteration 12480 : loss: 0.023240, loss_a: 0.013671
[00:46:24.790] iteration 12481 : loss: 0.030527, loss_a: 0.017957
[00:46:25.520] iteration 12482 : loss: 0.064871, loss_a: 0.038160
[00:46:26.845] iteration 12483 : loss: 0.073377, loss_a: 0.043163
[00:46:27.580] iteration 12484 : loss: 0.021735, loss_a: 0.012785
[00:46:28.894] iteration 12485 : loss: 0.021701, loss_a: 0.012765
[00:46:29.628] iteration 12486 : loss: 0.025786, loss_a: 0.015168
[00:46:30.976] iteration 12487 : loss: 0.036346, loss_a: 0.021380
[00:46:31.714] iteration 12488 : loss: 0.038315, loss_a: 0.022538
[00:46:33.031] iteration 12489 : loss: 0.016431, loss_a: 0.009665
[00:46:33.764] iteration 12490 : loss: 0.016336, loss_a: 0.009609
[00:46:35.094] iteration 12491 : loss: 0.022778, loss_a: 0.013399
[00:46:35.844] iteration 12492 : loss: 0.025218, loss_a: 0.014834
[00:46:37.200] iteration 12493 : loss: 0.014722, loss_a: 0.008660
[00:46:37.937] iteration 12494 : loss: 0.029211, loss_a: 0.017183
[00:46:39.278] iteration 12495 : loss: 0.017836, loss_a: 0.010492
[00:46:40.017] iteration 12496 : loss: 0.030084, loss_a: 0.017696
[00:46:41.366] iteration 12497 : loss: 0.036887, loss_a: 0.021698
[00:46:42.116] iteration 12498 : loss: 0.022364, loss_a: 0.013155
[00:46:43.439] iteration 12499 : loss: 0.030539, loss_a: 0.017964
[00:46:44.181] iteration 12500 : loss: 0.029005, loss_a: 0.017062
[00:46:45.533] iteration 12501 : loss: 0.023091, loss_a: 0.013583
[00:46:46.277] iteration 12502 : loss: 0.020866, loss_a: 0.012274
[00:46:47.607] iteration 12503 : loss: 0.024340, loss_a: 0.014318
[00:46:48.346] iteration 12504 : loss: 0.021974, loss_a: 0.012926
[00:46:49.704] iteration 12505 : loss: 0.032267, loss_a: 0.018980
[00:46:50.459] iteration 12506 : loss: 0.030491, loss_a: 0.017936
[00:46:51.829] iteration 12507 : loss: 0.026263, loss_a: 0.015449
[00:46:52.573] iteration 12508 : loss: 0.032370, loss_a: 0.019041
[00:46:53.902] iteration 12509 : loss: 0.030104, loss_a: 0.017708
[00:46:54.635] iteration 12510 : loss: 0.018764, loss_a: 0.011038
[00:46:55.975] iteration 12511 : loss: 0.035440, loss_a: 0.020847
[00:46:56.719] iteration 12512 : loss: 0.021718, loss_a: 0.012775
[00:46:58.056] iteration 12513 : loss: 0.022520, loss_a: 0.013247
[00:46:58.792] iteration 12514 : loss: 0.025319, loss_a: 0.014893
[00:47:00.165] iteration 12515 : loss: 0.026361, loss_a: 0.015506
[00:47:00.910] iteration 12516 : loss: 0.017153, loss_a: 0.010090
[00:47:02.271] iteration 12517 : loss: 0.019336, loss_a: 0.011374
[00:47:03.020] iteration 12518 : loss: 0.042253, loss_a: 0.024855
[00:47:04.376] iteration 12519 : loss: 0.021967, loss_a: 0.012922
[00:47:05.125] iteration 12520 : loss: 0.024353, loss_a: 0.014325
[00:47:06.484] iteration 12521 : loss: 0.037443, loss_a: 0.022026
[00:47:07.236] iteration 12522 : loss: 0.023845, loss_a: 0.014027
[00:47:08.620] iteration 12523 : loss: 0.038253, loss_a: 0.022502
[00:47:09.368] iteration 12524 : loss: 0.027863, loss_a: 0.016390
[00:47:10.696] iteration 12525 : loss: 0.018036, loss_a: 0.010609
[00:47:11.446] iteration 12526 : loss: 0.023188, loss_a: 0.013640
[00:47:12.760] iteration 12527 : loss: 0.016621, loss_a: 0.009777
[00:47:13.490] iteration 12528 : loss: 0.014450, loss_a: 0.008500
[00:47:14.841] iteration 12529 : loss: 0.019593, loss_a: 0.011525
[00:47:15.581] iteration 12530 : loss: 0.023532, loss_a: 0.013843
[00:47:16.919] iteration 12531 : loss: 0.019360, loss_a: 0.011388
[00:47:17.660] iteration 12532 : loss: 0.023335, loss_a: 0.013726
[00:47:18.975] iteration 12533 : loss: 0.032591, loss_a: 0.019171
[00:47:19.716] iteration 12534 : loss: 0.013359, loss_a: 0.007858
[00:47:21.077] iteration 12535 : loss: 0.010504, loss_a: 0.006179
[00:47:21.821] iteration 12536 : loss: 0.031903, loss_a: 0.018766
[00:47:23.139] iteration 12537 : loss: 0.019917, loss_a: 0.011716
[00:47:23.884] iteration 12538 : loss: 0.040337, loss_a: 0.023728
[00:47:25.234] iteration 12539 : loss: 0.082621, loss_a: 0.048600
[00:47:25.980] iteration 12540 : loss: 0.027723, loss_a: 0.016308
[00:47:27.297] iteration 12541 : loss: 0.027461, loss_a: 0.016154
[00:47:28.024] iteration 12542 : loss: 0.046396, loss_a: 0.027292
[00:47:29.345] iteration 12543 : loss: 0.016286, loss_a: 0.009580
[00:47:30.100] iteration 12544 : loss: 0.035037, loss_a: 0.020610
[00:47:31.429] iteration 12545 : loss: 0.030661, loss_a: 0.018036
[00:47:32.169] iteration 12546 : loss: 0.031847, loss_a: 0.018734
[00:47:33.504] iteration 12547 : loss: 0.027410, loss_a: 0.016123
[00:47:34.243] iteration 12548 : loss: 0.015959, loss_a: 0.009388
[00:47:35.597] iteration 12549 : loss: 0.021720, loss_a: 0.012777
[00:47:36.341] iteration 12550 : loss: 0.021683, loss_a: 0.012755
[00:47:37.706] iteration 12551 : loss: 0.021694, loss_a: 0.012761
[00:47:38.440] iteration 12552 : loss: 0.010564, loss_a: 0.006214
[00:47:39.767] iteration 12553 : loss: 0.014885, loss_a: 0.008756
[00:47:40.515] iteration 12554 : loss: 0.024714, loss_a: 0.014538
[00:47:41.849] iteration 12555 : loss: 0.015563, loss_a: 0.009155
[00:47:42.597] iteration 12556 : loss: 0.013917, loss_a: 0.008186
[00:47:43.930] iteration 12557 : loss: 0.018716, loss_a: 0.011009
[00:47:44.683] iteration 12558 : loss: 0.022309, loss_a: 0.013123
[00:47:46.043] iteration 12559 : loss: 0.042745, loss_a: 0.025144
[00:47:46.783] iteration 12560 : loss: 0.020389, loss_a: 0.011993
[00:47:48.096] iteration 12561 : loss: 0.028722, loss_a: 0.016895
[00:47:48.835] iteration 12562 : loss: 0.049456, loss_a: 0.029092
[00:47:50.206] iteration 12563 : loss: 0.017895, loss_a: 0.010527
[00:47:50.944] iteration 12564 : loss: 0.034669, loss_a: 0.020393
[00:47:52.283] iteration 12565 : loss: 0.030629, loss_a: 0.018017
[00:47:53.025] iteration 12566 : loss: 0.023995, loss_a: 0.014115
[00:47:54.350] iteration 12567 : loss: 0.011789, loss_a: 0.006935
[00:47:55.094] iteration 12568 : loss: 0.037507, loss_a: 0.022063
[00:47:56.417] iteration 12569 : loss: 0.044504, loss_a: 0.026179
[00:47:57.165] iteration 12570 : loss: 0.029093, loss_a: 0.017114
[00:47:58.506] iteration 12571 : loss: 0.020651, loss_a: 0.012148
[00:47:59.255] iteration 12572 : loss: 0.037564, loss_a: 0.022097
[00:48:00.628] iteration 12573 : loss: 0.032198, loss_a: 0.018940
[00:48:01.363] iteration 12574 : loss: 0.021409, loss_a: 0.012594
[00:48:02.686] iteration 12575 : loss: 0.024153, loss_a: 0.014208
[00:48:03.418] iteration 12576 : loss: 0.016899, loss_a: 0.009941
[00:48:04.737] iteration 12577 : loss: 0.024915, loss_a: 0.014656
[00:48:05.487] iteration 12578 : loss: 0.017608, loss_a: 0.010358
[00:48:06.859] iteration 12579 : loss: 0.031749, loss_a: 0.018676
[00:48:07.595] iteration 12580 : loss: 0.015413, loss_a: 0.009066
[00:48:08.951] iteration 12581 : loss: 0.031438, loss_a: 0.018493
[00:48:09.689] iteration 12582 : loss: 0.029896, loss_a: 0.017586
[00:48:11.023] iteration 12583 : loss: 0.032125, loss_a: 0.018897
[00:48:11.769] iteration 12584 : loss: 0.045866, loss_a: 0.026980
[00:48:13.119] iteration 12585 : loss: 0.014343, loss_a: 0.008437
[00:48:13.863] iteration 12586 : loss: 0.021835, loss_a: 0.012844
[00:48:15.193] iteration 12587 : loss: 0.018187, loss_a: 0.010698
[00:48:15.928] iteration 12588 : loss: 0.026352, loss_a: 0.015501
[00:48:17.268] iteration 12589 : loss: 0.013956, loss_a: 0.008210
[00:48:18.014] iteration 12590 : loss: 0.011900, loss_a: 0.007000
[00:48:19.370] iteration 12591 : loss: 0.029415, loss_a: 0.017303
[00:48:20.102] iteration 12592 : loss: 0.025109, loss_a: 0.014770
[00:48:21.435] iteration 12593 : loss: 0.013451, loss_a: 0.007912
[00:48:22.169] iteration 12594 : loss: 0.036070, loss_a: 0.021217
[00:48:23.497] iteration 12595 : loss: 0.015041, loss_a: 0.008848
[00:48:24.231] iteration 12596 : loss: 0.020284, loss_a: 0.011932
[00:48:25.577] iteration 12597 : loss: 0.031540, loss_a: 0.018553
[00:48:26.322] iteration 12598 : loss: 0.018909, loss_a: 0.011123
[00:48:27.680] iteration 12599 : loss: 0.046053, loss_a: 0.027090
[00:48:28.420] iteration 12600 : loss: 0.045799, loss_a: 0.026941
[00:48:53.103] iteration 12601 : loss: 0.038932, loss_a: 0.022901
[00:48:55.266] iteration 12602 : loss: 0.011522, loss_a: 0.006778
[00:48:56.617] iteration 12603 : loss: 0.026748, loss_a: 0.015734
[00:48:57.359] iteration 12604 : loss: 0.019494, loss_a: 0.011467
[00:48:58.690] iteration 12605 : loss: 0.019958, loss_a: 0.011740
[00:48:59.441] iteration 12606 : loss: 0.022885, loss_a: 0.013462
[00:49:00.767] iteration 12607 : loss: 0.035737, loss_a: 0.021022
[00:49:01.514] iteration 12608 : loss: 0.020289, loss_a: 0.011935
[00:49:02.837] iteration 12609 : loss: 0.041513, loss_a: 0.024419
[00:49:03.577] iteration 12610 : loss: 0.015925, loss_a: 0.009368
[00:49:04.939] iteration 12611 : loss: 0.029158, loss_a: 0.017152
[00:49:05.675] iteration 12612 : loss: 0.030441, loss_a: 0.017906
[00:49:07.037] iteration 12613 : loss: 0.060835, loss_a: 0.035785
[00:49:07.783] iteration 12614 : loss: 0.038697, loss_a: 0.022763
[00:49:09.157] iteration 12615 : loss: 0.035920, loss_a: 0.021130
[00:49:09.895] iteration 12616 : loss: 0.023975, loss_a: 0.014103
[00:49:11.261] iteration 12617 : loss: 0.047341, loss_a: 0.027848
[00:49:12.009] iteration 12618 : loss: 0.023294, loss_a: 0.013702
[00:49:13.330] iteration 12619 : loss: 0.013009, loss_a: 0.007653
[00:49:14.078] iteration 12620 : loss: 0.018401, loss_a: 0.010824
[00:49:15.440] iteration 12621 : loss: 0.035182, loss_a: 0.020695
[00:49:16.186] iteration 12622 : loss: 0.035650, loss_a: 0.020971
[00:49:17.548] iteration 12623 : loss: 0.029451, loss_a: 0.017324
[00:49:18.296] iteration 12624 : loss: 0.021283, loss_a: 0.012519
[00:49:19.698] iteration 12625 : loss: 0.027910, loss_a: 0.016418
[00:49:20.453] iteration 12626 : loss: 0.024834, loss_a: 0.014608
[00:49:21.812] iteration 12627 : loss: 0.027256, loss_a: 0.016033
[00:49:22.544] iteration 12628 : loss: 0.023766, loss_a: 0.013980
[00:49:23.896] iteration 12629 : loss: 0.027642, loss_a: 0.016260
[00:49:24.646] iteration 12630 : loss: 0.035396, loss_a: 0.020821
[00:49:25.982] iteration 12631 : loss: 0.021997, loss_a: 0.012939
[00:49:26.728] iteration 12632 : loss: 0.036608, loss_a: 0.021534
[00:49:28.046] iteration 12633 : loss: 0.020536, loss_a: 0.012080
[00:49:28.781] iteration 12634 : loss: 0.030771, loss_a: 0.018100
[00:49:30.138] iteration 12635 : loss: 0.036284, loss_a: 0.021343
[00:49:30.890] iteration 12636 : loss: 0.072275, loss_a: 0.042515
[00:49:32.226] iteration 12637 : loss: 0.033100, loss_a: 0.019471
[00:49:32.976] iteration 12638 : loss: 0.076979, loss_a: 0.045282
[00:49:34.322] iteration 12639 : loss: 0.035743, loss_a: 0.021025
[00:49:35.060] iteration 12640 : loss: 0.023523, loss_a: 0.013837
[00:49:36.428] iteration 12641 : loss: 0.036754, loss_a: 0.021620
[00:49:37.160] iteration 12642 : loss: 0.043308, loss_a: 0.025475
[00:49:38.492] iteration 12643 : loss: 0.018213, loss_a: 0.010713
[00:49:39.234] iteration 12644 : loss: 0.017656, loss_a: 0.010386
[00:49:40.573] iteration 12645 : loss: 0.015199, loss_a: 0.008940
[00:49:41.315] iteration 12646 : loss: 0.028831, loss_a: 0.016959
[00:49:42.675] iteration 12647 : loss: 0.027637, loss_a: 0.016257
[00:49:43.418] iteration 12648 : loss: 0.020996, loss_a: 0.012351
[00:49:44.786] iteration 12649 : loss: 0.013977, loss_a: 0.008222
[00:49:45.523] iteration 12650 : loss: 0.023240, loss_a: 0.013671
[00:49:46.844] iteration 12651 : loss: 0.030144, loss_a: 0.017732
[00:49:47.584] iteration 12652 : loss: 0.019528, loss_a: 0.011487
[00:49:48.924] iteration 12653 : loss: 0.020511, loss_a: 0.012065
[00:49:49.662] iteration 12654 : loss: 0.026853, loss_a: 0.015796
[00:49:51.016] iteration 12655 : loss: 0.026748, loss_a: 0.015734
[00:49:51.752] iteration 12656 : loss: 0.008716, loss_a: 0.005127
[00:49:53.112] iteration 12657 : loss: 0.009698, loss_a: 0.005704
[00:49:53.857] iteration 12658 : loss: 0.032671, loss_a: 0.019218
[00:49:55.234] iteration 12659 : loss: 0.017643, loss_a: 0.010378
[00:49:55.972] iteration 12660 : loss: 0.040756, loss_a: 0.023974
[00:49:57.338] iteration 12661 : loss: 0.043088, loss_a: 0.025346
[00:49:58.075] iteration 12662 : loss: 0.013514, loss_a: 0.007949
[00:49:59.401] iteration 12663 : loss: 0.027099, loss_a: 0.015941
[00:50:00.144] iteration 12664 : loss: 0.025017, loss_a: 0.014716
[00:50:01.515] iteration 12665 : loss: 0.018083, loss_a: 0.010637
[00:50:02.250] iteration 12666 : loss: 0.016330, loss_a: 0.009606
[00:50:03.647] iteration 12667 : loss: 0.494066, loss_a: 0.290627
[00:50:04.386] iteration 12668 : loss: 0.016853, loss_a: 0.009914
[00:50:05.714] iteration 12669 : loss: 0.042081, loss_a: 0.024753
[00:50:06.455] iteration 12670 : loss: 0.020165, loss_a: 0.011862
[00:50:07.786] iteration 12671 : loss: 0.021598, loss_a: 0.012705
[00:50:08.540] iteration 12672 : loss: 0.044114, loss_a: 0.025949
[00:50:09.899] iteration 12673 : loss: 0.033535, loss_a: 0.019726
[00:50:10.645] iteration 12674 : loss: 0.030978, loss_a: 0.018222
[00:50:11.944] iteration 12675 : loss: 0.015157, loss_a: 0.008916
[00:50:12.684] iteration 12676 : loss: 0.015800, loss_a: 0.009294
[00:50:14.044] iteration 12677 : loss: 0.054109, loss_a: 0.031829
[00:50:14.773] iteration 12678 : loss: 0.024502, loss_a: 0.014413
[00:50:16.102] iteration 12679 : loss: 0.032337, loss_a: 0.019022
[00:50:16.835] iteration 12680 : loss: 0.015152, loss_a: 0.008913
[00:50:18.197] iteration 12681 : loss: 0.016058, loss_a: 0.009446
[00:50:18.943] iteration 12682 : loss: 0.029698, loss_a: 0.017469
[00:50:20.280] iteration 12683 : loss: 0.017568, loss_a: 0.010334
[00:50:21.028] iteration 12684 : loss: 0.031334, loss_a: 0.018432
[00:50:22.381] iteration 12685 : loss: 0.088425, loss_a: 0.052014
[00:50:23.122] iteration 12686 : loss: 0.047717, loss_a: 0.028069
[00:50:24.466] iteration 12687 : loss: 0.023738, loss_a: 0.013964
[00:50:25.205] iteration 12688 : loss: 0.016147, loss_a: 0.009498
[00:50:26.550] iteration 12689 : loss: 0.025801, loss_a: 0.015177
[00:50:27.287] iteration 12690 : loss: 0.018118, loss_a: 0.010658
[00:50:28.604] iteration 12691 : loss: 0.019115, loss_a: 0.011244
[00:50:29.344] iteration 12692 : loss: 0.031636, loss_a: 0.018609
[00:50:30.673] iteration 12693 : loss: 0.028471, loss_a: 0.016747
[00:50:31.414] iteration 12694 : loss: 0.018151, loss_a: 0.010677
[00:50:32.782] iteration 12695 : loss: 0.037501, loss_a: 0.022060
[00:50:33.525] iteration 12696 : loss: 0.046206, loss_a: 0.027180
[00:50:34.875] iteration 12697 : loss: 0.063167, loss_a: 0.037157
[00:50:35.626] iteration 12698 : loss: 0.030249, loss_a: 0.017793
[00:50:36.977] iteration 12699 : loss: 0.033072, loss_a: 0.019454
[00:50:37.719] iteration 12700 : loss: 0.028708, loss_a: 0.016887
[00:50:39.059] iteration 12701 : loss: 0.014577, loss_a: 0.008574
[00:50:39.797] iteration 12702 : loss: 0.022676, loss_a: 0.013339
[00:50:41.133] iteration 12703 : loss: 0.034412, loss_a: 0.020242
[00:50:41.869] iteration 12704 : loss: 0.019697, loss_a: 0.011586
[00:50:43.208] iteration 12705 : loss: 0.049136, loss_a: 0.028904
[00:50:43.947] iteration 12706 : loss: 0.019018, loss_a: 0.011187
[00:50:45.277] iteration 12707 : loss: 0.050450, loss_a: 0.029677
[00:50:46.029] iteration 12708 : loss: 0.014771, loss_a: 0.008689
[00:50:47.382] iteration 12709 : loss: 0.052330, loss_a: 0.030782
[00:50:48.126] iteration 12710 : loss: 0.015921, loss_a: 0.009365
[00:50:49.471] iteration 12711 : loss: 0.018189, loss_a: 0.010699
[00:50:50.221] iteration 12712 : loss: 0.044726, loss_a: 0.026309
[00:50:51.541] iteration 12713 : loss: 0.030975, loss_a: 0.018220
[00:50:52.282] iteration 12714 : loss: 0.024939, loss_a: 0.014670
[00:50:53.595] iteration 12715 : loss: 0.037927, loss_a: 0.022310
[00:50:54.343] iteration 12716 : loss: 0.015655, loss_a: 0.009209
[00:50:55.707] iteration 12717 : loss: 0.022857, loss_a: 0.013446
[00:50:56.447] iteration 12718 : loss: 0.024076, loss_a: 0.014162
[00:50:57.805] iteration 12719 : loss: 0.022386, loss_a: 0.013168
[00:50:58.548] iteration 12720 : loss: 0.027161, loss_a: 0.015977
[00:50:59.907] iteration 12721 : loss: 0.037653, loss_a: 0.022149
[00:51:00.647] iteration 12722 : loss: 0.018295, loss_a: 0.010762
[00:51:01.980] iteration 12723 : loss: 0.029163, loss_a: 0.017154
[00:51:02.728] iteration 12724 : loss: 0.025004, loss_a: 0.014708
[00:51:04.106] iteration 12725 : loss: 0.029890, loss_a: 0.017582
[00:51:04.854] iteration 12726 : loss: 0.016856, loss_a: 0.009915
[00:51:06.196] iteration 12727 : loss: 0.016245, loss_a: 0.009556
[00:51:06.937] iteration 12728 : loss: 0.018294, loss_a: 0.010761
[00:51:08.269] iteration 12729 : loss: 0.014055, loss_a: 0.008268
[00:51:09.019] iteration 12730 : loss: 0.035348, loss_a: 0.020793
[00:51:10.363] iteration 12731 : loss: 0.035053, loss_a: 0.020619
[00:51:11.101] iteration 12732 : loss: 0.020986, loss_a: 0.012345
[00:51:12.455] iteration 12733 : loss: 0.040386, loss_a: 0.023756
[00:51:13.202] iteration 12734 : loss: 0.020162, loss_a: 0.011860
[00:51:14.533] iteration 12735 : loss: 0.013971, loss_a: 0.008218
[00:51:15.281] iteration 12736 : loss: 0.015237, loss_a: 0.008963
[00:51:16.616] iteration 12737 : loss: 0.023180, loss_a: 0.013635
[00:51:17.357] iteration 12738 : loss: 0.020679, loss_a: 0.012164
[00:51:18.669] iteration 12739 : loss: 0.020105, loss_a: 0.011826
[00:51:19.413] iteration 12740 : loss: 0.018928, loss_a: 0.011134
[00:51:20.775] iteration 12741 : loss: 0.331237, loss_a: 0.194845
[00:51:21.514] iteration 12742 : loss: 0.021773, loss_a: 0.012807
[00:51:22.839] iteration 12743 : loss: 0.042683, loss_a: 0.025107
[00:51:23.574] iteration 12744 : loss: 0.013335, loss_a: 0.007844
[00:51:24.938] iteration 12745 : loss: 0.041562, loss_a: 0.024448
[00:51:25.681] iteration 12746 : loss: 0.016240, loss_a: 0.009553
[00:51:27.028] iteration 12747 : loss: 0.034691, loss_a: 0.020407
[00:51:27.767] iteration 12748 : loss: 0.036170, loss_a: 0.021276
[00:51:29.087] iteration 12749 : loss: 0.014741, loss_a: 0.008671
[00:51:29.838] iteration 12750 : loss: 0.016088, loss_a: 0.009464
[00:51:31.155] iteration 12751 : loss: 0.020459, loss_a: 0.012035
[00:51:31.888] iteration 12752 : loss: 0.018312, loss_a: 0.010772
[00:51:33.208] iteration 12753 : loss: 0.019600, loss_a: 0.011529
[00:51:33.939] iteration 12754 : loss: 0.013709, loss_a: 0.008064
[00:51:35.269] iteration 12755 : loss: 0.021476, loss_a: 0.012633
[00:51:36.009] iteration 12756 : loss: 0.025926, loss_a: 0.015251
[00:51:37.354] iteration 12757 : loss: 0.017246, loss_a: 0.010145
[00:51:38.091] iteration 12758 : loss: 0.017607, loss_a: 0.010357
[00:51:39.412] iteration 12759 : loss: 0.036071, loss_a: 0.021218
[00:51:40.150] iteration 12760 : loss: 0.026241, loss_a: 0.015436
[00:51:41.474] iteration 12761 : loss: 0.017843, loss_a: 0.010496
[00:51:42.225] iteration 12762 : loss: 0.024505, loss_a: 0.014415
[00:51:43.548] iteration 12763 : loss: 0.030379, loss_a: 0.017870
[00:51:44.289] iteration 12764 : loss: 0.069696, loss_a: 0.040998
[00:51:45.607] iteration 12765 : loss: 0.022038, loss_a: 0.012963
[00:51:46.340] iteration 12766 : loss: 0.016787, loss_a: 0.009875
[00:51:47.699] iteration 12767 : loss: 0.042374, loss_a: 0.024926
[00:51:48.439] iteration 12768 : loss: 0.020365, loss_a: 0.011979
[00:51:49.772] iteration 12769 : loss: 0.014545, loss_a: 0.008556
[00:51:50.516] iteration 12770 : loss: 0.033174, loss_a: 0.019514
[00:51:51.837] iteration 12771 : loss: 0.063571, loss_a: 0.037395
[00:51:52.577] iteration 12772 : loss: 0.051002, loss_a: 0.030001
[00:51:53.914] iteration 12773 : loss: 0.048079, loss_a: 0.028282
[00:51:54.659] iteration 12774 : loss: 0.036018, loss_a: 0.021187
[00:51:56.007] iteration 12775 : loss: 0.038676, loss_a: 0.022750
[00:51:56.746] iteration 12776 : loss: 0.032505, loss_a: 0.019120
[00:51:58.100] iteration 12777 : loss: 0.027150, loss_a: 0.015971
[00:51:58.837] iteration 12778 : loss: 0.014466, loss_a: 0.008509
[00:52:00.178] iteration 12779 : loss: 0.015309, loss_a: 0.009006
[00:52:00.913] iteration 12780 : loss: 0.031077, loss_a: 0.018281
[00:52:02.272] iteration 12781 : loss: 0.030259, loss_a: 0.017799
[00:52:03.014] iteration 12782 : loss: 0.013233, loss_a: 0.007784
[00:52:04.359] iteration 12783 : loss: 0.060383, loss_a: 0.035520
[00:52:05.095] iteration 12784 : loss: 0.023779, loss_a: 0.013988
[00:52:06.454] iteration 12785 : loss: 0.019432, loss_a: 0.011431
[00:52:07.195] iteration 12786 : loss: 0.020245, loss_a: 0.011909
[00:52:08.521] iteration 12787 : loss: 0.018163, loss_a: 0.010684
[00:52:09.265] iteration 12788 : loss: 0.066092, loss_a: 0.038878
[00:52:10.596] iteration 12789 : loss: 0.030942, loss_a: 0.018201
[00:52:11.345] iteration 12790 : loss: 0.033792, loss_a: 0.019878
[00:52:12.687] iteration 12791 : loss: 0.038429, loss_a: 0.022605
[00:52:13.434] iteration 12792 : loss: 0.018717, loss_a: 0.011010
[00:52:14.783] iteration 12793 : loss: 0.019305, loss_a: 0.011356
[00:52:15.525] iteration 12794 : loss: 0.021305, loss_a: 0.012532
[00:52:16.898] iteration 12795 : loss: 0.019290, loss_a: 0.011347
[00:52:17.649] iteration 12796 : loss: 0.029277, loss_a: 0.017222
[00:52:18.967] iteration 12797 : loss: 0.013992, loss_a: 0.008231
[00:52:19.706] iteration 12798 : loss: 0.021331, loss_a: 0.012548
[00:52:21.045] iteration 12799 : loss: 0.033330, loss_a: 0.019606
[00:52:21.799] iteration 12800 : loss: 0.035803, loss_a: 0.021060
[00:52:46.496] iteration 12801 : loss: 0.030031, loss_a: 0.017665
[00:52:48.627] iteration 12802 : loss: 0.031836, loss_a: 0.018727
[00:52:49.979] iteration 12803 : loss: 0.030869, loss_a: 0.018158
[00:52:50.723] iteration 12804 : loss: 0.020952, loss_a: 0.012324
[00:52:52.045] iteration 12805 : loss: 0.014628, loss_a: 0.008605
[00:52:52.790] iteration 12806 : loss: 0.022049, loss_a: 0.012970
[00:52:54.129] iteration 12807 : loss: 0.020787, loss_a: 0.012228
[00:52:54.872] iteration 12808 : loss: 0.038588, loss_a: 0.022699
[00:52:56.261] iteration 12809 : loss: 0.022857, loss_a: 0.013445
[00:52:57.005] iteration 12810 : loss: 0.033097, loss_a: 0.019469
[00:52:58.327] iteration 12811 : loss: 0.019308, loss_a: 0.011357
[00:52:59.067] iteration 12812 : loss: 0.021380, loss_a: 0.012577
[00:53:00.433] iteration 12813 : loss: 0.024976, loss_a: 0.014692
[00:53:01.177] iteration 12814 : loss: 0.020799, loss_a: 0.012234
[00:53:02.534] iteration 12815 : loss: 0.024412, loss_a: 0.014360
[00:53:03.276] iteration 12816 : loss: 0.026468, loss_a: 0.015569
[00:53:04.638] iteration 12817 : loss: 0.018693, loss_a: 0.010996
[00:53:05.386] iteration 12818 : loss: 0.012936, loss_a: 0.007609
[00:53:06.723] iteration 12819 : loss: 0.033123, loss_a: 0.019484
[00:53:07.479] iteration 12820 : loss: 0.044452, loss_a: 0.026148
[00:53:08.825] iteration 12821 : loss: 0.018795, loss_a: 0.011056
[00:53:09.567] iteration 12822 : loss: 0.073885, loss_a: 0.043462
[00:53:10.931] iteration 12823 : loss: 0.038888, loss_a: 0.022875
[00:53:11.675] iteration 12824 : loss: 0.018161, loss_a: 0.010683
[00:53:13.031] iteration 12825 : loss: 0.023677, loss_a: 0.013927
[00:53:13.772] iteration 12826 : loss: 0.017725, loss_a: 0.010427
[00:53:15.110] iteration 12827 : loss: 0.014532, loss_a: 0.008548
[00:53:15.853] iteration 12828 : loss: 0.036140, loss_a: 0.021259
[00:53:17.181] iteration 12829 : loss: 0.019320, loss_a: 0.011365
[00:53:17.924] iteration 12830 : loss: 0.037044, loss_a: 0.021791
[00:53:19.292] iteration 12831 : loss: 0.038834, loss_a: 0.022844
[00:53:20.026] iteration 12832 : loss: 0.037312, loss_a: 0.021948
[00:53:21.360] iteration 12833 : loss: 0.023396, loss_a: 0.013763
[00:53:22.096] iteration 12834 : loss: 0.030809, loss_a: 0.018123
[00:53:23.455] iteration 12835 : loss: 0.017597, loss_a: 0.010351
[00:53:24.193] iteration 12836 : loss: 0.030317, loss_a: 0.017833
[00:53:25.534] iteration 12837 : loss: 0.022805, loss_a: 0.013415
[00:53:26.277] iteration 12838 : loss: 0.022080, loss_a: 0.012988
[00:53:27.596] iteration 12839 : loss: 0.060843, loss_a: 0.035790
[00:53:28.341] iteration 12840 : loss: 0.018176, loss_a: 0.010692
[00:53:29.696] iteration 12841 : loss: 0.035634, loss_a: 0.020961
[00:53:30.444] iteration 12842 : loss: 0.022463, loss_a: 0.013214
[00:53:31.810] iteration 12843 : loss: 0.027113, loss_a: 0.015949
[00:53:32.549] iteration 12844 : loss: 0.015940, loss_a: 0.009377
[00:53:33.891] iteration 12845 : loss: 0.026475, loss_a: 0.015574
[00:53:34.629] iteration 12846 : loss: 0.016768, loss_a: 0.009864
[00:53:35.974] iteration 12847 : loss: 0.043804, loss_a: 0.025767
[00:53:36.712] iteration 12848 : loss: 0.013003, loss_a: 0.007649
[00:53:38.043] iteration 12849 : loss: 0.017771, loss_a: 0.010453
[00:53:38.787] iteration 12850 : loss: 0.020571, loss_a: 0.012101
[00:53:40.136] iteration 12851 : loss: 0.019079, loss_a: 0.011223
[00:53:40.884] iteration 12852 : loss: 0.032502, loss_a: 0.019119
[00:53:42.248] iteration 12853 : loss: 0.013289, loss_a: 0.007817
[00:53:42.993] iteration 12854 : loss: 0.022914, loss_a: 0.013479
[00:53:44.342] iteration 12855 : loss: 0.015192, loss_a: 0.008937
[00:53:45.079] iteration 12856 : loss: 0.016138, loss_a: 0.009493
[00:53:46.414] iteration 12857 : loss: 0.021427, loss_a: 0.012604
[00:53:47.160] iteration 12858 : loss: 0.052179, loss_a: 0.030693
[00:53:48.509] iteration 12859 : loss: 0.027616, loss_a: 0.016244
[00:53:49.241] iteration 12860 : loss: 0.034623, loss_a: 0.020366
[00:53:50.571] iteration 12861 : loss: 0.017792, loss_a: 0.010466
[00:53:51.308] iteration 12862 : loss: 0.016062, loss_a: 0.009448
[00:53:52.664] iteration 12863 : loss: 0.044985, loss_a: 0.026462
[00:53:53.413] iteration 12864 : loss: 0.017370, loss_a: 0.010218
[00:53:54.765] iteration 12865 : loss: 0.023395, loss_a: 0.013762
[00:53:55.500] iteration 12866 : loss: 0.022148, loss_a: 0.013028
[00:53:56.844] iteration 12867 : loss: 0.035078, loss_a: 0.020634
[00:53:57.584] iteration 12868 : loss: 0.024313, loss_a: 0.014301
[00:53:58.928] iteration 12869 : loss: 0.022398, loss_a: 0.013175
[00:53:59.662] iteration 12870 : loss: 0.013754, loss_a: 0.008090
[00:54:01.037] iteration 12871 : loss: 0.035514, loss_a: 0.020891
[00:54:01.776] iteration 12872 : loss: 0.037200, loss_a: 0.021882
[00:54:03.122] iteration 12873 : loss: 0.017193, loss_a: 0.010113
[00:54:03.871] iteration 12874 : loss: 0.024227, loss_a: 0.014251
[00:54:05.212] iteration 12875 : loss: 0.015543, loss_a: 0.009143
[00:54:05.950] iteration 12876 : loss: 0.015427, loss_a: 0.009074
[00:54:07.305] iteration 12877 : loss: 0.029383, loss_a: 0.017284
[00:54:08.037] iteration 12878 : loss: 0.011626, loss_a: 0.006839
[00:54:09.368] iteration 12879 : loss: 0.041781, loss_a: 0.024577
[00:54:10.106] iteration 12880 : loss: 0.041403, loss_a: 0.024355
[00:54:11.451] iteration 12881 : loss: 0.009923, loss_a: 0.005837
[00:54:12.190] iteration 12882 : loss: 0.027503, loss_a: 0.016178
[00:54:13.539] iteration 12883 : loss: 0.023537, loss_a: 0.013845
[00:54:14.274] iteration 12884 : loss: 0.037311, loss_a: 0.021948
[00:54:15.617] iteration 12885 : loss: 0.015245, loss_a: 0.008967
[00:54:16.363] iteration 12886 : loss: 0.046253, loss_a: 0.027208
[00:54:17.729] iteration 12887 : loss: 0.033768, loss_a: 0.019863
[00:54:18.458] iteration 12888 : loss: 0.018118, loss_a: 0.010658
[00:54:19.827] iteration 12889 : loss: 0.022912, loss_a: 0.013477
[00:54:20.578] iteration 12890 : loss: 0.024440, loss_a: 0.014377
[00:54:21.930] iteration 12891 : loss: 0.018018, loss_a: 0.010599
[00:54:22.670] iteration 12892 : loss: 0.012968, loss_a: 0.007628
[00:54:23.990] iteration 12893 : loss: 0.014302, loss_a: 0.008413
[00:54:24.736] iteration 12894 : loss: 0.017832, loss_a: 0.010489
[00:54:26.082] iteration 12895 : loss: 0.016529, loss_a: 0.009723
[00:54:26.820] iteration 12896 : loss: 0.018336, loss_a: 0.010786
[00:54:28.185] iteration 12897 : loss: 0.021696, loss_a: 0.012762
[00:54:28.920] iteration 12898 : loss: 0.043160, loss_a: 0.025388
[00:54:30.274] iteration 12899 : loss: 0.061440, loss_a: 0.036141
[00:54:31.007] iteration 12900 : loss: 0.016806, loss_a: 0.009886
[00:54:32.361] iteration 12901 : loss: 0.039431, loss_a: 0.023194
[00:54:33.105] iteration 12902 : loss: 0.025469, loss_a: 0.014982
[00:54:34.448] iteration 12903 : loss: 0.008606, loss_a: 0.005063
[00:54:35.199] iteration 12904 : loss: 0.020107, loss_a: 0.011828
[00:54:36.531] iteration 12905 : loss: 0.035928, loss_a: 0.021134
[00:54:37.271] iteration 12906 : loss: 0.015293, loss_a: 0.008996
[00:54:38.627] iteration 12907 : loss: 0.029821, loss_a: 0.017542
[00:54:39.379] iteration 12908 : loss: 0.020603, loss_a: 0.012119
[00:54:40.716] iteration 12909 : loss: 0.017301, loss_a: 0.010177
[00:54:41.462] iteration 12910 : loss: 0.051834, loss_a: 0.030490
[00:54:42.823] iteration 12911 : loss: 0.032907, loss_a: 0.019357
[00:54:43.567] iteration 12912 : loss: 0.050836, loss_a: 0.029904
[00:54:44.918] iteration 12913 : loss: 0.016961, loss_a: 0.009977
[00:54:45.666] iteration 12914 : loss: 0.023528, loss_a: 0.013840
[00:54:47.000] iteration 12915 : loss: 0.029646, loss_a: 0.017439
[00:54:47.740] iteration 12916 : loss: 0.016080, loss_a: 0.009459
[00:54:49.098] iteration 12917 : loss: 0.074857, loss_a: 0.044033
[00:54:49.843] iteration 12918 : loss: 0.038335, loss_a: 0.022550
[00:54:51.156] iteration 12919 : loss: 0.014272, loss_a: 0.008395
[00:54:51.907] iteration 12920 : loss: 0.019547, loss_a: 0.011498
[00:54:53.264] iteration 12921 : loss: 0.022276, loss_a: 0.013104
[00:54:54.010] iteration 12922 : loss: 0.020815, loss_a: 0.012244
[00:54:55.360] iteration 12923 : loss: 0.020022, loss_a: 0.011777
[00:54:56.106] iteration 12924 : loss: 0.030987, loss_a: 0.018227
[00:54:57.405] iteration 12925 : loss: 0.013874, loss_a: 0.008161
[00:54:58.153] iteration 12926 : loss: 0.026103, loss_a: 0.015355
[00:54:59.459] iteration 12927 : loss: 0.024262, loss_a: 0.014272
[00:55:00.192] iteration 12928 : loss: 0.022925, loss_a: 0.013485
[00:55:01.542] iteration 12929 : loss: 0.025392, loss_a: 0.014936
[00:55:02.287] iteration 12930 : loss: 0.032288, loss_a: 0.018993
[00:55:03.632] iteration 12931 : loss: 0.024499, loss_a: 0.014411
[00:55:04.371] iteration 12932 : loss: 0.029670, loss_a: 0.017453
[00:55:05.715] iteration 12933 : loss: 0.012805, loss_a: 0.007532
[00:55:06.462] iteration 12934 : loss: 0.024192, loss_a: 0.014230
[00:55:07.808] iteration 12935 : loss: 0.011336, loss_a: 0.006668
[00:55:08.545] iteration 12936 : loss: 0.025373, loss_a: 0.014925
[00:55:09.881] iteration 12937 : loss: 0.038769, loss_a: 0.022805
[00:55:10.632] iteration 12938 : loss: 0.039963, loss_a: 0.023508
[00:55:11.963] iteration 12939 : loss: 0.013789, loss_a: 0.008111
[00:55:12.708] iteration 12940 : loss: 0.030978, loss_a: 0.018223
[00:55:14.067] iteration 12941 : loss: 0.042064, loss_a: 0.024744
[00:55:14.814] iteration 12942 : loss: 0.011726, loss_a: 0.006898
[00:55:16.122] iteration 12943 : loss: 0.033353, loss_a: 0.019620
[00:55:16.875] iteration 12944 : loss: 0.020070, loss_a: 0.011806
[00:55:18.248] iteration 12945 : loss: 0.043905, loss_a: 0.025827
[00:55:18.996] iteration 12946 : loss: 0.028052, loss_a: 0.016501
[00:55:20.367] iteration 12947 : loss: 0.069354, loss_a: 0.040796
[00:55:21.109] iteration 12948 : loss: 0.071328, loss_a: 0.041958
[00:55:22.446] iteration 12949 : loss: 0.025901, loss_a: 0.015236
[00:55:23.187] iteration 12950 : loss: 0.023933, loss_a: 0.014078
[00:55:24.512] iteration 12951 : loss: 0.032407, loss_a: 0.019063
[00:55:25.251] iteration 12952 : loss: 0.020921, loss_a: 0.012306
[00:55:26.590] iteration 12953 : loss: 0.017117, loss_a: 0.010069
[00:55:27.352] iteration 12954 : loss: 0.016365, loss_a: 0.009626
[00:55:28.702] iteration 12955 : loss: 0.029219, loss_a: 0.017188
[00:55:29.441] iteration 12956 : loss: 0.027696, loss_a: 0.016292
[00:55:30.755] iteration 12957 : loss: 0.016600, loss_a: 0.009765
[00:55:31.495] iteration 12958 : loss: 0.024255, loss_a: 0.014268
[00:55:32.806] iteration 12959 : loss: 0.009163, loss_a: 0.005390
[00:55:33.543] iteration 12960 : loss: 0.018040, loss_a: 0.010612
[00:55:34.893] iteration 12961 : loss: 0.033657, loss_a: 0.019798
[00:55:35.678] iteration 12962 : loss: 0.439583, loss_a: 0.258578
[00:55:37.047] iteration 12963 : loss: 0.020757, loss_a: 0.012210
[00:55:37.796] iteration 12964 : loss: 0.038770, loss_a: 0.022806
[00:55:39.164] iteration 12965 : loss: 0.026583, loss_a: 0.015637
[00:55:39.898] iteration 12966 : loss: 0.033814, loss_a: 0.019891
[00:55:41.252] iteration 12967 : loss: 0.022060, loss_a: 0.012976
[00:55:41.992] iteration 12968 : loss: 0.022749, loss_a: 0.013382
[00:55:43.324] iteration 12969 : loss: 0.019126, loss_a: 0.011251
[00:55:44.061] iteration 12970 : loss: 0.033977, loss_a: 0.019986
[00:55:45.411] iteration 12971 : loss: 0.019920, loss_a: 0.011718
[00:55:46.152] iteration 12972 : loss: 0.011452, loss_a: 0.006736
[00:55:47.534] iteration 12973 : loss: 0.010650, loss_a: 0.006265
[00:55:48.272] iteration 12974 : loss: 0.013930, loss_a: 0.008194
[00:55:49.595] iteration 12975 : loss: 0.012893, loss_a: 0.007584
[00:55:50.343] iteration 12976 : loss: 0.028149, loss_a: 0.016558
[00:55:51.669] iteration 12977 : loss: 0.014338, loss_a: 0.008434
[00:55:52.417] iteration 12978 : loss: 0.017595, loss_a: 0.010350
[00:55:53.767] iteration 12979 : loss: 0.013058, loss_a: 0.007681
[00:55:54.516] iteration 12980 : loss: 0.022839, loss_a: 0.013435
[00:55:55.848] iteration 12981 : loss: 0.021956, loss_a: 0.012915
[00:55:56.589] iteration 12982 : loss: 0.019744, loss_a: 0.011614
[00:55:57.916] iteration 12983 : loss: 0.042663, loss_a: 0.025096
[00:55:58.658] iteration 12984 : loss: 0.027727, loss_a: 0.016310
[00:55:59.978] iteration 12985 : loss: 0.020024, loss_a: 0.011779
[00:56:00.724] iteration 12986 : loss: 0.018876, loss_a: 0.011104
[00:56:02.038] iteration 12987 : loss: 0.040595, loss_a: 0.023879
[00:56:02.772] iteration 12988 : loss: 0.033955, loss_a: 0.019974
[00:56:04.137] iteration 12989 : loss: 0.017600, loss_a: 0.010353
[00:56:04.882] iteration 12990 : loss: 0.016162, loss_a: 0.009507
[00:56:06.245] iteration 12991 : loss: 0.030621, loss_a: 0.018013
[00:56:06.985] iteration 12992 : loss: 0.015817, loss_a: 0.009304
[00:56:08.321] iteration 12993 : loss: 0.027949, loss_a: 0.016441
[00:56:09.060] iteration 12994 : loss: 0.016541, loss_a: 0.009730
[00:56:10.386] iteration 12995 : loss: 0.057782, loss_a: 0.033989
[00:56:11.136] iteration 12996 : loss: 0.025900, loss_a: 0.015235
[00:56:12.453] iteration 12997 : loss: 0.023801, loss_a: 0.014000
[00:56:13.188] iteration 12998 : loss: 0.012051, loss_a: 0.007089
[00:56:14.533] iteration 12999 : loss: 0.019822, loss_a: 0.011660
[00:56:15.273] iteration 13000 : loss: 0.026544, loss_a: 0.015614
[00:56:38.888] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_13000_dice_0.901.pth
[00:56:40.252] iteration 13001 : loss: 0.022717, loss_a: 0.013363
[00:56:42.541] iteration 13002 : loss: 0.035603, loss_a: 0.020943
[00:56:43.875] iteration 13003 : loss: 0.034055, loss_a: 0.020032
[00:56:44.612] iteration 13004 : loss: 0.017012, loss_a: 0.010007
[00:56:45.931] iteration 13005 : loss: 0.015697, loss_a: 0.009234
[00:56:46.676] iteration 13006 : loss: 0.015177, loss_a: 0.008928
[00:56:48.014] iteration 13007 : loss: 0.019792, loss_a: 0.011643
[00:56:48.752] iteration 13008 : loss: 0.016217, loss_a: 0.009540
[00:56:50.092] iteration 13009 : loss: 0.026843, loss_a: 0.015790
[00:56:50.834] iteration 13010 : loss: 0.023965, loss_a: 0.014097
[00:56:52.157] iteration 13011 : loss: 0.016574, loss_a: 0.009749
[00:56:52.907] iteration 13012 : loss: 0.021407, loss_a: 0.012593
[00:56:54.231] iteration 13013 : loss: 0.027024, loss_a: 0.015896
[00:56:54.964] iteration 13014 : loss: 0.014426, loss_a: 0.008486
[00:56:56.308] iteration 13015 : loss: 0.017709, loss_a: 0.010417
[00:56:57.053] iteration 13016 : loss: 0.027763, loss_a: 0.016331
[00:56:58.416] iteration 13017 : loss: 0.034394, loss_a: 0.020232
[00:56:59.165] iteration 13018 : loss: 0.011948, loss_a: 0.007028
[00:57:00.518] iteration 13019 : loss: 0.029059, loss_a: 0.017094
[00:57:01.267] iteration 13020 : loss: 0.022646, loss_a: 0.013321
[00:57:02.637] iteration 13021 : loss: 0.024457, loss_a: 0.014386
[00:57:03.377] iteration 13022 : loss: 0.027648, loss_a: 0.016264
[00:57:04.718] iteration 13023 : loss: 0.050213, loss_a: 0.029537
[00:57:05.455] iteration 13024 : loss: 0.013953, loss_a: 0.008207
[00:57:06.777] iteration 13025 : loss: 0.007937, loss_a: 0.004669
[00:57:07.520] iteration 13026 : loss: 0.056991, loss_a: 0.033524
[00:57:08.840] iteration 13027 : loss: 0.016532, loss_a: 0.009725
[00:57:09.582] iteration 13028 : loss: 0.027429, loss_a: 0.016135
[00:57:10.923] iteration 13029 : loss: 0.040027, loss_a: 0.023546
[00:57:11.671] iteration 13030 : loss: 0.032387, loss_a: 0.019051
[00:57:13.010] iteration 13031 : loss: 0.023009, loss_a: 0.013535
[00:57:13.742] iteration 13032 : loss: 0.023448, loss_a: 0.013793
[00:57:15.101] iteration 13033 : loss: 0.023068, loss_a: 0.013570
[00:57:15.841] iteration 13034 : loss: 0.012198, loss_a: 0.007175
[00:57:17.172] iteration 13035 : loss: 0.036440, loss_a: 0.021435
[00:57:17.917] iteration 13036 : loss: 0.038230, loss_a: 0.022489
[00:57:19.233] iteration 13037 : loss: 0.023486, loss_a: 0.013815
[00:57:19.981] iteration 13038 : loss: 0.043716, loss_a: 0.025715
[00:57:21.316] iteration 13039 : loss: 0.034574, loss_a: 0.020338
[00:57:22.058] iteration 13040 : loss: 0.018200, loss_a: 0.010706
[00:57:23.449] iteration 13041 : loss: 0.030760, loss_a: 0.018094
[00:57:24.207] iteration 13042 : loss: 0.020119, loss_a: 0.011835
[00:57:25.538] iteration 13043 : loss: 0.027644, loss_a: 0.016261
[00:57:26.290] iteration 13044 : loss: 0.013899, loss_a: 0.008176
[00:57:27.615] iteration 13045 : loss: 0.052437, loss_a: 0.030845
[00:57:28.345] iteration 13046 : loss: 0.030098, loss_a: 0.017705
[00:57:29.712] iteration 13047 : loss: 0.028170, loss_a: 0.016570
[00:57:30.448] iteration 13048 : loss: 0.016804, loss_a: 0.009885
[00:57:31.781] iteration 13049 : loss: 0.020762, loss_a: 0.012213
[00:57:32.526] iteration 13050 : loss: 0.025910, loss_a: 0.015241
[00:57:33.864] iteration 13051 : loss: 0.014758, loss_a: 0.008681
[00:57:34.616] iteration 13052 : loss: 0.069085, loss_a: 0.040638
[00:57:35.950] iteration 13053 : loss: 0.009630, loss_a: 0.005665
[00:57:36.708] iteration 13054 : loss: 0.016363, loss_a: 0.009625
[00:57:38.033] iteration 13055 : loss: 0.023386, loss_a: 0.013757
[00:57:38.773] iteration 13056 : loss: 0.030062, loss_a: 0.017684
[00:57:40.139] iteration 13057 : loss: 0.020087, loss_a: 0.011816
[00:57:40.888] iteration 13058 : loss: 0.018781, loss_a: 0.011048
[00:57:42.252] iteration 13059 : loss: 0.063356, loss_a: 0.037268
[00:57:42.995] iteration 13060 : loss: 0.017245, loss_a: 0.010144
[00:57:44.319] iteration 13061 : loss: 0.025319, loss_a: 0.014894
[00:57:45.057] iteration 13062 : loss: 0.025001, loss_a: 0.014706
[00:57:46.386] iteration 13063 : loss: 0.039103, loss_a: 0.023002
[00:57:47.134] iteration 13064 : loss: 0.034864, loss_a: 0.020508
[00:57:48.453] iteration 13065 : loss: 0.038329, loss_a: 0.022547
[00:57:49.193] iteration 13066 : loss: 0.026347, loss_a: 0.015498
[00:57:50.530] iteration 13067 : loss: 0.029842, loss_a: 0.017554
[00:57:51.276] iteration 13068 : loss: 0.036103, loss_a: 0.021237
[00:57:52.648] iteration 13069 : loss: 0.035194, loss_a: 0.020702
[00:57:53.386] iteration 13070 : loss: 0.024192, loss_a: 0.014231
[00:57:54.712] iteration 13071 : loss: 0.021525, loss_a: 0.012662
[00:57:55.446] iteration 13072 : loss: 0.015894, loss_a: 0.009349
[00:57:56.772] iteration 13073 : loss: 0.018659, loss_a: 0.010976
[00:57:57.517] iteration 13074 : loss: 0.053269, loss_a: 0.031335
[00:57:58.851] iteration 13075 : loss: 0.016111, loss_a: 0.009477
[00:57:59.596] iteration 13076 : loss: 0.018978, loss_a: 0.011163
[00:58:00.906] iteration 13077 : loss: 0.014659, loss_a: 0.008623
[00:58:01.666] iteration 13078 : loss: 0.034354, loss_a: 0.020208
[00:58:03.003] iteration 13079 : loss: 0.033676, loss_a: 0.019809
[00:58:03.746] iteration 13080 : loss: 0.017113, loss_a: 0.010067
[00:58:05.110] iteration 13081 : loss: 0.020829, loss_a: 0.012253
[00:58:05.857] iteration 13082 : loss: 0.025231, loss_a: 0.014842
[00:58:07.177] iteration 13083 : loss: 0.019507, loss_a: 0.011475
[00:58:07.926] iteration 13084 : loss: 0.022189, loss_a: 0.013053
[00:58:09.276] iteration 13085 : loss: 0.027800, loss_a: 0.016353
[00:58:10.018] iteration 13086 : loss: 0.039084, loss_a: 0.022991
[00:58:11.379] iteration 13087 : loss: 0.013660, loss_a: 0.008035
[00:58:12.132] iteration 13088 : loss: 0.037980, loss_a: 0.022341
[00:58:13.463] iteration 13089 : loss: 0.014201, loss_a: 0.008354
[00:58:14.211] iteration 13090 : loss: 0.025009, loss_a: 0.014711
[00:58:15.523] iteration 13091 : loss: 0.038214, loss_a: 0.022479
[00:58:16.278] iteration 13092 : loss: 0.036120, loss_a: 0.021247
[00:58:17.620] iteration 13093 : loss: 0.026127, loss_a: 0.015369
[00:58:18.363] iteration 13094 : loss: 0.032178, loss_a: 0.018928
[00:58:19.718] iteration 13095 : loss: 0.059831, loss_a: 0.035194
[00:58:20.465] iteration 13096 : loss: 0.029716, loss_a: 0.017480
[00:58:21.816] iteration 13097 : loss: 0.048343, loss_a: 0.028437
[00:58:22.551] iteration 13098 : loss: 0.016486, loss_a: 0.009698
[00:58:23.919] iteration 13099 : loss: 0.027944, loss_a: 0.016438
[00:58:24.658] iteration 13100 : loss: 0.016622, loss_a: 0.009777
[00:58:26.017] iteration 13101 : loss: 0.014730, loss_a: 0.008665
[00:58:26.766] iteration 13102 : loss: 0.023095, loss_a: 0.013585
[00:58:28.109] iteration 13103 : loss: 0.028950, loss_a: 0.017029
[00:58:28.864] iteration 13104 : loss: 0.038191, loss_a: 0.022465
[00:58:30.194] iteration 13105 : loss: 0.013649, loss_a: 0.008029
[00:58:30.943] iteration 13106 : loss: 0.047127, loss_a: 0.027722
[00:58:32.274] iteration 13107 : loss: 0.027117, loss_a: 0.015951
[00:58:33.014] iteration 13108 : loss: 0.028735, loss_a: 0.016903
[00:58:34.350] iteration 13109 : loss: 0.015919, loss_a: 0.009364
[00:58:35.088] iteration 13110 : loss: 0.008384, loss_a: 0.004932
[00:58:36.434] iteration 13111 : loss: 0.018451, loss_a: 0.010854
[00:58:37.186] iteration 13112 : loss: 0.030680, loss_a: 0.018047
[00:58:38.527] iteration 13113 : loss: 0.057478, loss_a: 0.033811
[00:58:39.275] iteration 13114 : loss: 0.011923, loss_a: 0.007014
[00:58:40.619] iteration 13115 : loss: 0.030916, loss_a: 0.018186
[00:58:41.360] iteration 13116 : loss: 0.015874, loss_a: 0.009338
[00:58:42.734] iteration 13117 : loss: 0.015563, loss_a: 0.009155
[00:58:43.487] iteration 13118 : loss: 0.021677, loss_a: 0.012751
[00:58:44.859] iteration 13119 : loss: 0.044270, loss_a: 0.026041
[00:58:45.600] iteration 13120 : loss: 0.014190, loss_a: 0.008347
[00:58:46.973] iteration 13121 : loss: 0.015788, loss_a: 0.009287
[00:58:47.713] iteration 13122 : loss: 0.034303, loss_a: 0.020178
[00:58:49.052] iteration 13123 : loss: 0.018334, loss_a: 0.010785
[00:58:49.806] iteration 13124 : loss: 0.029546, loss_a: 0.017380
[00:58:51.187] iteration 13125 : loss: 0.032488, loss_a: 0.019111
[00:58:51.946] iteration 13126 : loss: 0.025318, loss_a: 0.014893
[00:58:53.294] iteration 13127 : loss: 0.015312, loss_a: 0.009007
[00:58:54.041] iteration 13128 : loss: 0.021356, loss_a: 0.012563
[00:58:55.398] iteration 13129 : loss: 0.019592, loss_a: 0.011525
[00:58:56.141] iteration 13130 : loss: 0.020158, loss_a: 0.011858
[00:58:57.497] iteration 13131 : loss: 0.022272, loss_a: 0.013101
[00:58:58.239] iteration 13132 : loss: 0.031931, loss_a: 0.018783
[00:58:59.589] iteration 13133 : loss: 0.016407, loss_a: 0.009651
[00:59:00.326] iteration 13134 : loss: 0.030317, loss_a: 0.017833
[00:59:01.679] iteration 13135 : loss: 0.013792, loss_a: 0.008113
[00:59:02.418] iteration 13136 : loss: 0.013443, loss_a: 0.007908
[00:59:03.778] iteration 13137 : loss: 0.022943, loss_a: 0.013496
[00:59:04.523] iteration 13138 : loss: 0.015469, loss_a: 0.009100
[00:59:05.869] iteration 13139 : loss: 0.018779, loss_a: 0.011047
[00:59:06.606] iteration 13140 : loss: 0.020663, loss_a: 0.012155
[00:59:07.931] iteration 13141 : loss: 0.036963, loss_a: 0.021743
[00:59:08.679] iteration 13142 : loss: 0.042069, loss_a: 0.024747
[00:59:10.040] iteration 13143 : loss: 0.021984, loss_a: 0.012932
[00:59:10.778] iteration 13144 : loss: 0.013724, loss_a: 0.008073
[00:59:12.132] iteration 13145 : loss: 0.014683, loss_a: 0.008637
[00:59:12.890] iteration 13146 : loss: 0.029803, loss_a: 0.017531
[00:59:14.211] iteration 13147 : loss: 0.027751, loss_a: 0.016324
[00:59:14.949] iteration 13148 : loss: 0.021965, loss_a: 0.012921
[00:59:16.265] iteration 13149 : loss: 0.060908, loss_a: 0.035828
[00:59:17.040] iteration 13150 : loss: 0.043695, loss_a: 0.025703
[00:59:18.375] iteration 13151 : loss: 0.052697, loss_a: 0.030998
[00:59:19.124] iteration 13152 : loss: 0.030466, loss_a: 0.017921
[00:59:20.437] iteration 13153 : loss: 0.015527, loss_a: 0.009133
[00:59:21.181] iteration 13154 : loss: 0.041890, loss_a: 0.024641
[00:59:22.552] iteration 13155 : loss: 0.017358, loss_a: 0.010211
[00:59:23.308] iteration 13156 : loss: 0.032333, loss_a: 0.019019
[00:59:24.635] iteration 13157 : loss: 0.029737, loss_a: 0.017492
[00:59:25.375] iteration 13158 : loss: 0.046361, loss_a: 0.027271
[00:59:26.722] iteration 13159 : loss: 0.037962, loss_a: 0.022331
[00:59:27.466] iteration 13160 : loss: 0.038696, loss_a: 0.022762
[00:59:28.794] iteration 13161 : loss: 0.030608, loss_a: 0.018005
[00:59:29.532] iteration 13162 : loss: 0.015732, loss_a: 0.009254
[00:59:30.910] iteration 13163 : loss: 0.032873, loss_a: 0.019337
[00:59:31.658] iteration 13164 : loss: 0.024032, loss_a: 0.014136
[00:59:32.969] iteration 13165 : loss: 0.017594, loss_a: 0.010349
[00:59:33.705] iteration 13166 : loss: 0.020632, loss_a: 0.012137
[00:59:35.082] iteration 13167 : loss: 0.022973, loss_a: 0.013514
[00:59:35.823] iteration 13168 : loss: 0.024392, loss_a: 0.014348
[00:59:37.154] iteration 13169 : loss: 0.014117, loss_a: 0.008304
[00:59:37.891] iteration 13170 : loss: 0.018928, loss_a: 0.011134
[00:59:39.250] iteration 13171 : loss: 0.021661, loss_a: 0.012742
[00:59:40.002] iteration 13172 : loss: 0.028637, loss_a: 0.016845
[00:59:41.335] iteration 13173 : loss: 0.021571, loss_a: 0.012689
[00:59:42.087] iteration 13174 : loss: 0.079337, loss_a: 0.046669
[00:59:43.403] iteration 13175 : loss: 0.027533, loss_a: 0.016196
[00:59:44.155] iteration 13176 : loss: 0.020219, loss_a: 0.011893
[00:59:45.487] iteration 13177 : loss: 0.056217, loss_a: 0.033069
[00:59:46.231] iteration 13178 : loss: 0.074396, loss_a: 0.043762
[00:59:47.577] iteration 13179 : loss: 0.026594, loss_a: 0.015644
[00:59:48.320] iteration 13180 : loss: 0.021576, loss_a: 0.012692
[00:59:49.662] iteration 13181 : loss: 0.026865, loss_a: 0.015803
[00:59:50.412] iteration 13182 : loss: 0.038551, loss_a: 0.022677
[00:59:51.754] iteration 13183 : loss: 0.047036, loss_a: 0.027668
[00:59:52.499] iteration 13184 : loss: 0.027643, loss_a: 0.016260
[00:59:53.844] iteration 13185 : loss: 0.050390, loss_a: 0.029641
[00:59:54.582] iteration 13186 : loss: 0.044445, loss_a: 0.026144
[00:59:55.949] iteration 13187 : loss: 0.037903, loss_a: 0.022296
[00:59:56.700] iteration 13188 : loss: 0.041273, loss_a: 0.024278
[00:59:58.042] iteration 13189 : loss: 0.019404, loss_a: 0.011414
[00:59:58.800] iteration 13190 : loss: 0.041328, loss_a: 0.024311
[01:00:00.184] iteration 13191 : loss: 0.017805, loss_a: 0.010473
[01:00:00.937] iteration 13192 : loss: 0.039412, loss_a: 0.023184
[01:00:02.265] iteration 13193 : loss: 0.015621, loss_a: 0.009189
[01:00:03.003] iteration 13194 : loss: 0.026643, loss_a: 0.015672
[01:00:04.351] iteration 13195 : loss: 0.014885, loss_a: 0.008756
[01:00:05.091] iteration 13196 : loss: 0.037971, loss_a: 0.022336
[01:00:06.445] iteration 13197 : loss: 0.055953, loss_a: 0.032913
[01:00:07.202] iteration 13198 : loss: 0.025643, loss_a: 0.015084
[01:00:08.530] iteration 13199 : loss: 0.017014, loss_a: 0.010009
[01:00:09.273] iteration 13200 : loss: 0.034037, loss_a: 0.020022
[01:00:33.942] iteration 13201 : loss: 0.026659, loss_a: 0.015682
[01:00:36.101] iteration 13202 : loss: 0.013026, loss_a: 0.007662
[01:00:37.466] iteration 13203 : loss: 0.014022, loss_a: 0.008248
[01:00:38.223] iteration 13204 : loss: 0.020023, loss_a: 0.011778
[01:00:39.577] iteration 13205 : loss: 0.044307, loss_a: 0.026063
[01:00:40.312] iteration 13206 : loss: 0.021143, loss_a: 0.012437
[01:00:41.678] iteration 13207 : loss: 0.062287, loss_a: 0.036640
[01:00:42.418] iteration 13208 : loss: 0.044056, loss_a: 0.025915
[01:00:43.734] iteration 13209 : loss: 0.027804, loss_a: 0.016355
[01:00:44.477] iteration 13210 : loss: 0.034659, loss_a: 0.020388
[01:00:45.820] iteration 13211 : loss: 0.026842, loss_a: 0.015789
[01:00:46.547] iteration 13212 : loss: 0.018481, loss_a: 0.010871
[01:00:47.874] iteration 13213 : loss: 0.023423, loss_a: 0.013778
[01:00:48.607] iteration 13214 : loss: 0.009961, loss_a: 0.005860
[01:00:49.948] iteration 13215 : loss: 0.024370, loss_a: 0.014335
[01:00:50.683] iteration 13216 : loss: 0.011985, loss_a: 0.007050
[01:00:52.033] iteration 13217 : loss: 0.024512, loss_a: 0.014419
[01:00:52.776] iteration 13218 : loss: 0.020126, loss_a: 0.011839
[01:00:54.100] iteration 13219 : loss: 0.014053, loss_a: 0.008267
[01:00:54.836] iteration 13220 : loss: 0.012570, loss_a: 0.007394
[01:00:56.199] iteration 13221 : loss: 0.056619, loss_a: 0.033305
[01:00:56.938] iteration 13222 : loss: 0.011855, loss_a: 0.006973
[01:00:58.287] iteration 13223 : loss: 0.030942, loss_a: 0.018201
[01:00:59.044] iteration 13224 : loss: 0.017857, loss_a: 0.010504
[01:01:00.392] iteration 13225 : loss: 0.029264, loss_a: 0.017214
[01:01:01.143] iteration 13226 : loss: 0.048107, loss_a: 0.028298
[01:01:02.474] iteration 13227 : loss: 0.016593, loss_a: 0.009761
[01:01:03.222] iteration 13228 : loss: 0.032470, loss_a: 0.019100
[01:01:04.570] iteration 13229 : loss: 0.025810, loss_a: 0.015183
[01:01:05.311] iteration 13230 : loss: 0.022749, loss_a: 0.013382
[01:01:06.671] iteration 13231 : loss: 0.025810, loss_a: 0.015183
[01:01:07.427] iteration 13232 : loss: 0.026780, loss_a: 0.015753
[01:01:08.786] iteration 13233 : loss: 0.046391, loss_a: 0.027289
[01:01:09.519] iteration 13234 : loss: 0.038889, loss_a: 0.022876
[01:01:10.846] iteration 13235 : loss: 0.029476, loss_a: 0.017339
[01:01:11.583] iteration 13236 : loss: 0.043068, loss_a: 0.025334
[01:01:12.924] iteration 13237 : loss: 0.024403, loss_a: 0.014354
[01:01:13.677] iteration 13238 : loss: 0.013873, loss_a: 0.008161
[01:01:15.008] iteration 13239 : loss: 0.018774, loss_a: 0.011043
[01:01:15.746] iteration 13240 : loss: 0.020820, loss_a: 0.012247
[01:01:17.089] iteration 13241 : loss: 0.016007, loss_a: 0.009416
[01:01:17.840] iteration 13242 : loss: 0.046294, loss_a: 0.027232
[01:01:19.167] iteration 13243 : loss: 0.024956, loss_a: 0.014680
[01:01:19.903] iteration 13244 : loss: 0.023015, loss_a: 0.013538
[01:01:21.251] iteration 13245 : loss: 0.022261, loss_a: 0.013095
[01:01:21.991] iteration 13246 : loss: 0.042065, loss_a: 0.024744
[01:01:23.305] iteration 13247 : loss: 0.030478, loss_a: 0.017928
[01:01:24.045] iteration 13248 : loss: 0.046036, loss_a: 0.027080
[01:01:25.401] iteration 13249 : loss: 0.061542, loss_a: 0.036201
[01:01:26.142] iteration 13250 : loss: 0.018176, loss_a: 0.010692
[01:01:27.449] iteration 13251 : loss: 0.027512, loss_a: 0.016183
[01:01:28.198] iteration 13252 : loss: 0.019018, loss_a: 0.011187
[01:01:29.519] iteration 13253 : loss: 0.022398, loss_a: 0.013175
[01:01:30.260] iteration 13254 : loss: 0.024458, loss_a: 0.014387
[01:01:31.570] iteration 13255 : loss: 0.010973, loss_a: 0.006455
[01:01:32.315] iteration 13256 : loss: 0.039145, loss_a: 0.023027
[01:01:33.646] iteration 13257 : loss: 0.010349, loss_a: 0.006088
[01:01:34.386] iteration 13258 : loss: 0.033839, loss_a: 0.019906
[01:01:35.739] iteration 13259 : loss: 0.020356, loss_a: 0.011974
[01:01:36.484] iteration 13260 : loss: 0.042290, loss_a: 0.024877
[01:01:37.823] iteration 13261 : loss: 0.018543, loss_a: 0.010908
[01:01:38.556] iteration 13262 : loss: 0.031708, loss_a: 0.018652
[01:01:39.874] iteration 13263 : loss: 0.019793, loss_a: 0.011643
[01:01:40.624] iteration 13264 : loss: 0.042160, loss_a: 0.024800
[01:01:41.951] iteration 13265 : loss: 0.024889, loss_a: 0.014641
[01:01:42.683] iteration 13266 : loss: 0.021024, loss_a: 0.012367
[01:01:44.025] iteration 13267 : loss: 0.024628, loss_a: 0.014487
[01:01:44.768] iteration 13268 : loss: 0.034333, loss_a: 0.020196
[01:01:46.100] iteration 13269 : loss: 0.056227, loss_a: 0.033075
[01:01:46.848] iteration 13270 : loss: 0.024324, loss_a: 0.014308
[01:01:48.184] iteration 13271 : loss: 0.015039, loss_a: 0.008847
[01:01:48.923] iteration 13272 : loss: 0.019813, loss_a: 0.011655
[01:01:50.271] iteration 13273 : loss: 0.038177, loss_a: 0.022457
[01:01:51.016] iteration 13274 : loss: 0.014604, loss_a: 0.008591
[01:01:52.339] iteration 13275 : loss: 0.022063, loss_a: 0.012978
[01:01:53.084] iteration 13276 : loss: 0.039158, loss_a: 0.023034
[01:01:54.440] iteration 13277 : loss: 0.015752, loss_a: 0.009266
[01:01:55.169] iteration 13278 : loss: 0.025201, loss_a: 0.014824
[01:01:56.519] iteration 13279 : loss: 0.015778, loss_a: 0.009281
[01:01:57.259] iteration 13280 : loss: 0.025835, loss_a: 0.015197
[01:01:58.583] iteration 13281 : loss: 0.022369, loss_a: 0.013158
[01:01:59.321] iteration 13282 : loss: 0.072370, loss_a: 0.042571
[01:02:00.641] iteration 13283 : loss: 0.015780, loss_a: 0.009282
[01:02:01.384] iteration 13284 : loss: 0.033563, loss_a: 0.019743
[01:02:02.764] iteration 13285 : loss: 0.030501, loss_a: 0.017942
[01:02:03.522] iteration 13286 : loss: 0.025089, loss_a: 0.014758
[01:02:04.834] iteration 13287 : loss: 0.023027, loss_a: 0.013545
[01:02:05.566] iteration 13288 : loss: 0.019068, loss_a: 0.011216
[01:02:06.895] iteration 13289 : loss: 0.016582, loss_a: 0.009754
[01:02:07.634] iteration 13290 : loss: 0.015142, loss_a: 0.008907
[01:02:08.989] iteration 13291 : loss: 0.042345, loss_a: 0.024909
[01:02:09.733] iteration 13292 : loss: 0.032038, loss_a: 0.018846
[01:02:11.097] iteration 13293 : loss: 0.024532, loss_a: 0.014431
[01:02:11.842] iteration 13294 : loss: 0.021502, loss_a: 0.012648
[01:02:13.205] iteration 13295 : loss: 0.049743, loss_a: 0.029260
[01:02:13.948] iteration 13296 : loss: 0.028463, loss_a: 0.016743
[01:02:15.275] iteration 13297 : loss: 0.020597, loss_a: 0.012116
[01:02:16.025] iteration 13298 : loss: 0.019631, loss_a: 0.011547
[01:02:17.312] iteration 13299 : loss: 0.016485, loss_a: 0.009697
[01:02:18.056] iteration 13300 : loss: 0.047616, loss_a: 0.028009
[01:02:19.420] iteration 13301 : loss: 0.023037, loss_a: 0.013551
[01:02:20.157] iteration 13302 : loss: 0.030411, loss_a: 0.017889
[01:02:21.498] iteration 13303 : loss: 0.018529, loss_a: 0.010899
[01:02:22.240] iteration 13304 : loss: 0.011138, loss_a: 0.006552
[01:02:23.597] iteration 13305 : loss: 0.035007, loss_a: 0.020592
[01:02:24.339] iteration 13306 : loss: 0.010222, loss_a: 0.006013
[01:02:25.702] iteration 13307 : loss: 0.016772, loss_a: 0.009866
[01:02:26.451] iteration 13308 : loss: 0.020506, loss_a: 0.012063
[01:02:27.792] iteration 13309 : loss: 0.060370, loss_a: 0.035512
[01:02:28.534] iteration 13310 : loss: 0.027437, loss_a: 0.016140
[01:02:29.889] iteration 13311 : loss: 0.029035, loss_a: 0.017079
[01:02:30.623] iteration 13312 : loss: 0.013587, loss_a: 0.007992
[01:02:31.963] iteration 13313 : loss: 0.011369, loss_a: 0.006687
[01:02:32.701] iteration 13314 : loss: 0.029600, loss_a: 0.017412
[01:02:34.037] iteration 13315 : loss: 0.031506, loss_a: 0.018533
[01:02:34.784] iteration 13316 : loss: 0.029210, loss_a: 0.017182
[01:02:36.148] iteration 13317 : loss: 0.024826, loss_a: 0.014604
[01:02:36.881] iteration 13318 : loss: 0.021110, loss_a: 0.012418
[01:02:38.223] iteration 13319 : loss: 0.027302, loss_a: 0.016060
[01:02:38.959] iteration 13320 : loss: 0.024354, loss_a: 0.014326
[01:02:40.288] iteration 13321 : loss: 0.022006, loss_a: 0.012945
[01:02:41.033] iteration 13322 : loss: 0.027239, loss_a: 0.016023
[01:02:42.391] iteration 13323 : loss: 0.017573, loss_a: 0.010337
[01:02:43.134] iteration 13324 : loss: 0.033529, loss_a: 0.019723
[01:02:44.483] iteration 13325 : loss: 0.026093, loss_a: 0.015349
[01:02:45.220] iteration 13326 : loss: 0.046437, loss_a: 0.027316
[01:02:46.565] iteration 13327 : loss: 0.016958, loss_a: 0.009975
[01:02:47.298] iteration 13328 : loss: 0.016370, loss_a: 0.009630
[01:02:48.668] iteration 13329 : loss: 0.029420, loss_a: 0.017306
[01:02:49.400] iteration 13330 : loss: 0.038784, loss_a: 0.022814
[01:02:50.746] iteration 13331 : loss: 0.023938, loss_a: 0.014081
[01:02:51.478] iteration 13332 : loss: 0.015627, loss_a: 0.009193
[01:02:52.860] iteration 13333 : loss: 0.021175, loss_a: 0.012456
[01:02:53.601] iteration 13334 : loss: 0.018753, loss_a: 0.011031
[01:02:54.938] iteration 13335 : loss: 0.022231, loss_a: 0.013077
[01:02:55.675] iteration 13336 : loss: 0.031724, loss_a: 0.018661
[01:02:57.014] iteration 13337 : loss: 0.012981, loss_a: 0.007636
[01:02:57.751] iteration 13338 : loss: 0.044307, loss_a: 0.026063
[01:02:59.120] iteration 13339 : loss: 0.026250, loss_a: 0.015441
[01:02:59.857] iteration 13340 : loss: 0.016481, loss_a: 0.009694
[01:03:01.198] iteration 13341 : loss: 0.026194, loss_a: 0.015408
[01:03:01.932] iteration 13342 : loss: 0.022633, loss_a: 0.013314
[01:03:03.274] iteration 13343 : loss: 0.028431, loss_a: 0.016724
[01:03:04.015] iteration 13344 : loss: 0.016117, loss_a: 0.009480
[01:03:05.342] iteration 13345 : loss: 0.013839, loss_a: 0.008140
[01:03:06.086] iteration 13346 : loss: 0.033538, loss_a: 0.019728
[01:03:07.445] iteration 13347 : loss: 0.023067, loss_a: 0.013569
[01:03:08.202] iteration 13348 : loss: 0.052195, loss_a: 0.030703
[01:03:09.557] iteration 13349 : loss: 0.017986, loss_a: 0.010580
[01:03:10.289] iteration 13350 : loss: 0.025519, loss_a: 0.015011
[01:03:11.659] iteration 13351 : loss: 0.021353, loss_a: 0.012560
[01:03:12.407] iteration 13352 : loss: 0.024573, loss_a: 0.014455
[01:03:13.756] iteration 13353 : loss: 0.027744, loss_a: 0.016320
[01:03:14.508] iteration 13354 : loss: 0.028370, loss_a: 0.016688
[01:03:15.867] iteration 13355 : loss: 0.021135, loss_a: 0.012432
[01:03:16.601] iteration 13356 : loss: 0.026683, loss_a: 0.015696
[01:03:17.959] iteration 13357 : loss: 0.022771, loss_a: 0.013395
[01:03:18.697] iteration 13358 : loss: 0.040504, loss_a: 0.023826
[01:03:20.037] iteration 13359 : loss: 0.010556, loss_a: 0.006209
[01:03:20.804] iteration 13360 : loss: 0.033626, loss_a: 0.019780
[01:03:22.147] iteration 13361 : loss: 0.058383, loss_a: 0.034343
[01:03:22.880] iteration 13362 : loss: 0.009826, loss_a: 0.005780
[01:03:24.225] iteration 13363 : loss: 0.025080, loss_a: 0.014753
[01:03:24.969] iteration 13364 : loss: 0.016459, loss_a: 0.009682
[01:03:26.307] iteration 13365 : loss: 0.019380, loss_a: 0.011400
[01:03:27.053] iteration 13366 : loss: 0.029403, loss_a: 0.017296
[01:03:28.366] iteration 13367 : loss: 0.016483, loss_a: 0.009696
[01:03:29.112] iteration 13368 : loss: 0.040763, loss_a: 0.023978
[01:03:30.444] iteration 13369 : loss: 0.011732, loss_a: 0.006901
[01:03:31.192] iteration 13370 : loss: 0.019122, loss_a: 0.011248
[01:03:32.571] iteration 13371 : loss: 0.032638, loss_a: 0.019199
[01:03:33.319] iteration 13372 : loss: 0.027783, loss_a: 0.016343
[01:03:34.654] iteration 13373 : loss: 0.027803, loss_a: 0.016355
[01:03:35.389] iteration 13374 : loss: 0.012884, loss_a: 0.007579
[01:03:36.717] iteration 13375 : loss: 0.036801, loss_a: 0.021648
[01:03:37.453] iteration 13376 : loss: 0.021731, loss_a: 0.012783
[01:03:38.787] iteration 13377 : loss: 0.027624, loss_a: 0.016250
[01:03:39.535] iteration 13378 : loss: 0.040753, loss_a: 0.023972
[01:03:40.863] iteration 13379 : loss: 0.015780, loss_a: 0.009282
[01:03:41.604] iteration 13380 : loss: 0.028215, loss_a: 0.016597
[01:03:42.924] iteration 13381 : loss: 0.027993, loss_a: 0.016466
[01:03:43.670] iteration 13382 : loss: 0.034843, loss_a: 0.020496
[01:03:45.025] iteration 13383 : loss: 0.027567, loss_a: 0.016216
[01:03:45.771] iteration 13384 : loss: 0.018866, loss_a: 0.011098
[01:03:47.115] iteration 13385 : loss: 0.017345, loss_a: 0.010203
[01:03:47.856] iteration 13386 : loss: 0.025751, loss_a: 0.015147
[01:03:49.253] iteration 13387 : loss: 0.038105, loss_a: 0.022415
[01:03:49.990] iteration 13388 : loss: 0.021942, loss_a: 0.012907
[01:03:51.357] iteration 13389 : loss: 0.025690, loss_a: 0.015112
[01:03:52.090] iteration 13390 : loss: 0.024843, loss_a: 0.014614
[01:03:53.400] iteration 13391 : loss: 0.025893, loss_a: 0.015231
[01:03:54.136] iteration 13392 : loss: 0.033593, loss_a: 0.019761
[01:03:55.503] iteration 13393 : loss: 0.017455, loss_a: 0.010268
[01:03:56.244] iteration 13394 : loss: 0.045271, loss_a: 0.026630
[01:03:57.579] iteration 13395 : loss: 0.042926, loss_a: 0.025250
[01:03:58.325] iteration 13396 : loss: 0.024607, loss_a: 0.014475
[01:03:59.642] iteration 13397 : loss: 0.013133, loss_a: 0.007725
[01:04:00.384] iteration 13398 : loss: 0.061452, loss_a: 0.036148
[01:04:01.724] iteration 13399 : loss: 0.022065, loss_a: 0.012979
[01:04:02.459] iteration 13400 : loss: 0.033524, loss_a: 0.019720
[01:04:26.004] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_13400_dice_0.9015.pth
[01:04:27.324] iteration 13401 : loss: 0.022395, loss_a: 0.013174
[01:04:29.409] iteration 13402 : loss: 0.038026, loss_a: 0.022368
[01:04:30.750] iteration 13403 : loss: 0.014513, loss_a: 0.008537
[01:04:31.487] iteration 13404 : loss: 0.026739, loss_a: 0.015729
[01:04:32.848] iteration 13405 : loss: 0.028534, loss_a: 0.016785
[01:04:33.579] iteration 13406 : loss: 0.014706, loss_a: 0.008650
[01:04:34.874] iteration 13407 : loss: 0.010881, loss_a: 0.006400
[01:04:35.622] iteration 13408 : loss: 0.024386, loss_a: 0.014345
[01:04:37.003] iteration 13409 : loss: 0.026044, loss_a: 0.015320
[01:04:37.758] iteration 13410 : loss: 0.047100, loss_a: 0.027706
[01:04:39.103] iteration 13411 : loss: 0.024508, loss_a: 0.014417
[01:04:39.847] iteration 13412 : loss: 0.015888, loss_a: 0.009346
[01:04:41.192] iteration 13413 : loss: 0.016239, loss_a: 0.009552
[01:04:41.939] iteration 13414 : loss: 0.029471, loss_a: 0.017336
[01:04:43.295] iteration 13415 : loss: 0.018601, loss_a: 0.010942
[01:04:44.050] iteration 13416 : loss: 0.015901, loss_a: 0.009354
[01:04:45.396] iteration 13417 : loss: 0.051850, loss_a: 0.030500
[01:04:46.142] iteration 13418 : loss: 0.029298, loss_a: 0.017234
[01:04:47.471] iteration 13419 : loss: 0.024019, loss_a: 0.014129
[01:04:48.207] iteration 13420 : loss: 0.021699, loss_a: 0.012764
[01:04:49.549] iteration 13421 : loss: 0.016891, loss_a: 0.009936
[01:04:50.288] iteration 13422 : loss: 0.033353, loss_a: 0.019619
[01:04:51.626] iteration 13423 : loss: 0.021689, loss_a: 0.012758
[01:04:52.375] iteration 13424 : loss: 0.033337, loss_a: 0.019610
[01:04:53.709] iteration 13425 : loss: 0.016026, loss_a: 0.009427
[01:04:54.445] iteration 13426 : loss: 0.023808, loss_a: 0.014005
[01:04:55.817] iteration 13427 : loss: 0.047212, loss_a: 0.027772
[01:04:56.556] iteration 13428 : loss: 0.014176, loss_a: 0.008339
[01:04:57.893] iteration 13429 : loss: 0.013798, loss_a: 0.008116
[01:04:58.633] iteration 13430 : loss: 0.021247, loss_a: 0.012498
[01:04:59.990] iteration 13431 : loss: 0.018737, loss_a: 0.011022
[01:05:00.734] iteration 13432 : loss: 0.025597, loss_a: 0.015057
[01:05:02.089] iteration 13433 : loss: 0.046950, loss_a: 0.027617
[01:05:02.828] iteration 13434 : loss: 0.025609, loss_a: 0.015064
[01:05:04.139] iteration 13435 : loss: 0.011499, loss_a: 0.006764
[01:05:04.877] iteration 13436 : loss: 0.019538, loss_a: 0.011493
[01:05:06.198] iteration 13437 : loss: 0.040372, loss_a: 0.023748
[01:05:06.955] iteration 13438 : loss: 0.035720, loss_a: 0.021012
[01:05:08.278] iteration 13439 : loss: 0.050439, loss_a: 0.029670
[01:05:09.022] iteration 13440 : loss: 0.012573, loss_a: 0.007396
[01:05:10.358] iteration 13441 : loss: 0.025562, loss_a: 0.015036
[01:05:11.110] iteration 13442 : loss: 0.017102, loss_a: 0.010060
[01:05:12.467] iteration 13443 : loss: 0.029923, loss_a: 0.017602
[01:05:13.211] iteration 13444 : loss: 0.016123, loss_a: 0.009484
[01:05:14.577] iteration 13445 : loss: 0.017163, loss_a: 0.010096
[01:05:15.323] iteration 13446 : loss: 0.060342, loss_a: 0.035495
[01:05:16.695] iteration 13447 : loss: 0.040255, loss_a: 0.023680
[01:05:17.440] iteration 13448 : loss: 0.012549, loss_a: 0.007382
[01:05:18.786] iteration 13449 : loss: 0.037203, loss_a: 0.021884
[01:05:19.529] iteration 13450 : loss: 0.023409, loss_a: 0.013770
[01:05:20.877] iteration 13451 : loss: 0.028614, loss_a: 0.016832
[01:05:21.612] iteration 13452 : loss: 0.030532, loss_a: 0.017960
[01:05:22.962] iteration 13453 : loss: 0.050885, loss_a: 0.029932
[01:05:23.701] iteration 13454 : loss: 0.026948, loss_a: 0.015852
[01:05:25.024] iteration 13455 : loss: 0.013928, loss_a: 0.008193
[01:05:25.783] iteration 13456 : loss: 0.035371, loss_a: 0.020807
[01:05:27.140] iteration 13457 : loss: 0.015626, loss_a: 0.009192
[01:05:27.880] iteration 13458 : loss: 0.032258, loss_a: 0.018975
[01:05:29.249] iteration 13459 : loss: 0.022660, loss_a: 0.013329
[01:05:29.994] iteration 13460 : loss: 0.034139, loss_a: 0.020082
[01:05:31.352] iteration 13461 : loss: 0.020242, loss_a: 0.011907
[01:05:32.091] iteration 13462 : loss: 0.012755, loss_a: 0.007503
[01:05:33.464] iteration 13463 : loss: 0.030059, loss_a: 0.017682
[01:05:34.208] iteration 13464 : loss: 0.015090, loss_a: 0.008877
[01:05:35.534] iteration 13465 : loss: 0.017792, loss_a: 0.010466
[01:05:36.270] iteration 13466 : loss: 0.016547, loss_a: 0.009733
[01:05:37.591] iteration 13467 : loss: 0.037242, loss_a: 0.021907
[01:05:38.340] iteration 13468 : loss: 0.025378, loss_a: 0.014928
[01:05:39.692] iteration 13469 : loss: 0.018007, loss_a: 0.010593
[01:05:40.438] iteration 13470 : loss: 0.040464, loss_a: 0.023802
[01:05:41.803] iteration 13471 : loss: 0.022043, loss_a: 0.012967
[01:05:42.553] iteration 13472 : loss: 0.012969, loss_a: 0.007629
[01:05:43.899] iteration 13473 : loss: 0.024883, loss_a: 0.014637
[01:05:44.641] iteration 13474 : loss: 0.037069, loss_a: 0.021805
[01:05:45.973] iteration 13475 : loss: 0.024897, loss_a: 0.014645
[01:05:46.711] iteration 13476 : loss: 0.029048, loss_a: 0.017087
[01:05:48.062] iteration 13477 : loss: 0.029325, loss_a: 0.017250
[01:05:48.798] iteration 13478 : loss: 0.016694, loss_a: 0.009820
[01:05:50.137] iteration 13479 : loss: 0.017077, loss_a: 0.010045
[01:05:50.895] iteration 13480 : loss: 0.042200, loss_a: 0.024823
[01:05:52.235] iteration 13481 : loss: 0.026391, loss_a: 0.015524
[01:05:52.987] iteration 13482 : loss: 0.025586, loss_a: 0.015050
[01:05:54.311] iteration 13483 : loss: 0.022102, loss_a: 0.013001
[01:05:55.082] iteration 13484 : loss: 0.044522, loss_a: 0.026189
[01:05:56.422] iteration 13485 : loss: 0.043060, loss_a: 0.025329
[01:05:57.177] iteration 13486 : loss: 0.022421, loss_a: 0.013189
[01:05:58.538] iteration 13487 : loss: 0.059439, loss_a: 0.034964
[01:05:59.278] iteration 13488 : loss: 0.015243, loss_a: 0.008967
[01:06:00.650] iteration 13489 : loss: 0.031882, loss_a: 0.018754
[01:06:01.389] iteration 13490 : loss: 0.021778, loss_a: 0.012811
[01:06:02.751] iteration 13491 : loss: 0.021962, loss_a: 0.012919
[01:06:03.496] iteration 13492 : loss: 0.052317, loss_a: 0.030775
[01:06:04.830] iteration 13493 : loss: 0.040355, loss_a: 0.023738
[01:06:05.589] iteration 13494 : loss: 0.025818, loss_a: 0.015187
[01:06:06.963] iteration 13495 : loss: 0.033817, loss_a: 0.019892
[01:06:07.694] iteration 13496 : loss: 0.022225, loss_a: 0.013074
[01:06:09.042] iteration 13497 : loss: 0.023989, loss_a: 0.014111
[01:06:09.787] iteration 13498 : loss: 0.031095, loss_a: 0.018291
[01:06:11.158] iteration 13499 : loss: 0.015179, loss_a: 0.008929
[01:06:11.907] iteration 13500 : loss: 0.030332, loss_a: 0.017842
[01:06:13.239] iteration 13501 : loss: 0.019930, loss_a: 0.011723
[01:06:13.983] iteration 13502 : loss: 0.013205, loss_a: 0.007768
[01:06:15.330] iteration 13503 : loss: 0.042304, loss_a: 0.024885
[01:06:16.075] iteration 13504 : loss: 0.019287, loss_a: 0.011346
[01:06:17.430] iteration 13505 : loss: 0.049495, loss_a: 0.029114
[01:06:18.182] iteration 13506 : loss: 0.059094, loss_a: 0.034761
[01:06:19.547] iteration 13507 : loss: 0.021070, loss_a: 0.012394
[01:06:20.282] iteration 13508 : loss: 0.047133, loss_a: 0.027725
[01:06:21.639] iteration 13509 : loss: 0.031490, loss_a: 0.018523
[01:06:22.381] iteration 13510 : loss: 0.039698, loss_a: 0.023351
[01:06:23.711] iteration 13511 : loss: 0.023003, loss_a: 0.013531
[01:06:24.458] iteration 13512 : loss: 0.065686, loss_a: 0.038639
[01:06:25.777] iteration 13513 : loss: 0.020468, loss_a: 0.012040
[01:06:26.529] iteration 13514 : loss: 0.029387, loss_a: 0.017287
[01:06:27.886] iteration 13515 : loss: 0.027380, loss_a: 0.016106
[01:06:28.624] iteration 13516 : loss: 0.016196, loss_a: 0.009527
[01:06:29.963] iteration 13517 : loss: 0.014666, loss_a: 0.008627
[01:06:30.695] iteration 13518 : loss: 0.032629, loss_a: 0.019194
[01:06:32.018] iteration 13519 : loss: 0.021969, loss_a: 0.012923
[01:06:32.754] iteration 13520 : loss: 0.023944, loss_a: 0.014085
[01:06:34.081] iteration 13521 : loss: 0.027172, loss_a: 0.015983
[01:06:34.817] iteration 13522 : loss: 0.019209, loss_a: 0.011300
[01:06:36.144] iteration 13523 : loss: 0.014987, loss_a: 0.008816
[01:06:36.889] iteration 13524 : loss: 0.024055, loss_a: 0.014150
[01:06:38.215] iteration 13525 : loss: 0.024455, loss_a: 0.014385
[01:06:38.951] iteration 13526 : loss: 0.034094, loss_a: 0.020055
[01:06:40.279] iteration 13527 : loss: 0.024320, loss_a: 0.014306
[01:06:41.019] iteration 13528 : loss: 0.053820, loss_a: 0.031659
[01:06:42.350] iteration 13529 : loss: 0.024751, loss_a: 0.014559
[01:06:43.093] iteration 13530 : loss: 0.021646, loss_a: 0.012733
[01:06:44.411] iteration 13531 : loss: 0.027014, loss_a: 0.015891
[01:06:45.152] iteration 13532 : loss: 0.028201, loss_a: 0.016589
[01:06:46.474] iteration 13533 : loss: 0.022531, loss_a: 0.013253
[01:06:47.207] iteration 13534 : loss: 0.047588, loss_a: 0.027993
[01:06:48.540] iteration 13535 : loss: 0.025861, loss_a: 0.015212
[01:06:49.283] iteration 13536 : loss: 0.021004, loss_a: 0.012355
[01:06:50.638] iteration 13537 : loss: 0.020097, loss_a: 0.011822
[01:06:51.376] iteration 13538 : loss: 0.026096, loss_a: 0.015351
[01:06:52.744] iteration 13539 : loss: 0.026159, loss_a: 0.015388
[01:06:53.478] iteration 13540 : loss: 0.042810, loss_a: 0.025182
[01:06:54.833] iteration 13541 : loss: 0.023640, loss_a: 0.013906
[01:06:55.583] iteration 13542 : loss: 0.014537, loss_a: 0.008551
[01:06:56.937] iteration 13543 : loss: 0.035259, loss_a: 0.020741
[01:06:57.674] iteration 13544 : loss: 0.018586, loss_a: 0.010933
[01:06:58.997] iteration 13545 : loss: 0.015620, loss_a: 0.009188
[01:06:59.745] iteration 13546 : loss: 0.034021, loss_a: 0.020012
[01:07:01.111] iteration 13547 : loss: 0.018397, loss_a: 0.010822
[01:07:01.853] iteration 13548 : loss: 0.030651, loss_a: 0.018030
[01:07:03.187] iteration 13549 : loss: 0.017269, loss_a: 0.010158
[01:07:03.935] iteration 13550 : loss: 0.020299, loss_a: 0.011940
[01:07:05.263] iteration 13551 : loss: 0.025581, loss_a: 0.015048
[01:07:06.004] iteration 13552 : loss: 0.048338, loss_a: 0.028434
[01:07:07.350] iteration 13553 : loss: 0.033221, loss_a: 0.019542
[01:07:08.103] iteration 13554 : loss: 0.029646, loss_a: 0.017439
[01:07:09.457] iteration 13555 : loss: 0.036501, loss_a: 0.021471
[01:07:10.205] iteration 13556 : loss: 0.032831, loss_a: 0.019312
[01:07:11.558] iteration 13557 : loss: 0.019540, loss_a: 0.011494
[01:07:12.300] iteration 13558 : loss: 0.019545, loss_a: 0.011497
[01:07:13.623] iteration 13559 : loss: 0.011662, loss_a: 0.006860
[01:07:14.357] iteration 13560 : loss: 0.042950, loss_a: 0.025265
[01:07:15.671] iteration 13561 : loss: 0.045320, loss_a: 0.026659
[01:07:16.407] iteration 13562 : loss: 0.018674, loss_a: 0.010985
[01:07:17.776] iteration 13563 : loss: 0.033039, loss_a: 0.019435
[01:07:18.524] iteration 13564 : loss: 0.017322, loss_a: 0.010189
[01:07:19.836] iteration 13565 : loss: 0.017704, loss_a: 0.010414
[01:07:20.569] iteration 13566 : loss: 0.011004, loss_a: 0.006473
[01:07:21.928] iteration 13567 : loss: 0.036280, loss_a: 0.021341
[01:07:22.659] iteration 13568 : loss: 0.013115, loss_a: 0.007715
[01:07:23.985] iteration 13569 : loss: 0.012147, loss_a: 0.007145
[01:07:24.729] iteration 13570 : loss: 0.034067, loss_a: 0.020039
[01:07:26.063] iteration 13571 : loss: 0.023513, loss_a: 0.013831
[01:07:26.807] iteration 13572 : loss: 0.039664, loss_a: 0.023332
[01:07:28.160] iteration 13573 : loss: 0.019622, loss_a: 0.011542
[01:07:28.911] iteration 13574 : loss: 0.039005, loss_a: 0.022944
[01:07:30.236] iteration 13575 : loss: 0.036496, loss_a: 0.021469
[01:07:30.961] iteration 13576 : loss: 0.008972, loss_a: 0.005278
[01:07:32.308] iteration 13577 : loss: 0.020551, loss_a: 0.012089
[01:07:33.055] iteration 13578 : loss: 0.067893, loss_a: 0.039937
[01:07:34.397] iteration 13579 : loss: 0.022471, loss_a: 0.013218
[01:07:35.138] iteration 13580 : loss: 0.028234, loss_a: 0.016608
[01:07:36.463] iteration 13581 : loss: 0.012945, loss_a: 0.007615
[01:07:37.214] iteration 13582 : loss: 0.030929, loss_a: 0.018193
[01:07:38.538] iteration 13583 : loss: 0.026323, loss_a: 0.015484
[01:07:39.289] iteration 13584 : loss: 0.043046, loss_a: 0.025321
[01:07:40.613] iteration 13585 : loss: 0.013348, loss_a: 0.007852
[01:07:41.360] iteration 13586 : loss: 0.015439, loss_a: 0.009082
[01:07:42.701] iteration 13587 : loss: 0.023590, loss_a: 0.013877
[01:07:43.456] iteration 13588 : loss: 0.056582, loss_a: 0.033284
[01:07:44.807] iteration 13589 : loss: 0.014040, loss_a: 0.008259
[01:07:45.549] iteration 13590 : loss: 0.025153, loss_a: 0.014796
[01:07:46.901] iteration 13591 : loss: 0.023438, loss_a: 0.013787
[01:07:47.635] iteration 13592 : loss: 0.028371, loss_a: 0.016689
[01:07:49.004] iteration 13593 : loss: 0.027132, loss_a: 0.015960
[01:07:49.748] iteration 13594 : loss: 0.018240, loss_a: 0.010729
[01:07:51.104] iteration 13595 : loss: 0.020725, loss_a: 0.012191
[01:07:51.838] iteration 13596 : loss: 0.035645, loss_a: 0.020967
[01:07:53.187] iteration 13597 : loss: 0.029257, loss_a: 0.017210
[01:07:53.927] iteration 13598 : loss: 0.030574, loss_a: 0.017985
[01:07:55.248] iteration 13599 : loss: 0.022285, loss_a: 0.013109
[01:07:55.988] iteration 13600 : loss: 0.051089, loss_a: 0.030052
[01:08:20.633] iteration 13601 : loss: 0.022723, loss_a: 0.013367
[01:08:22.772] iteration 13602 : loss: 0.055071, loss_a: 0.032394
[01:08:24.135] iteration 13603 : loss: 0.053039, loss_a: 0.031199
[01:08:24.874] iteration 13604 : loss: 0.026754, loss_a: 0.015738
[01:08:26.226] iteration 13605 : loss: 0.071497, loss_a: 0.042057
[01:08:26.959] iteration 13606 : loss: 0.018160, loss_a: 0.010682
[01:08:28.311] iteration 13607 : loss: 0.025088, loss_a: 0.014757
[01:08:29.049] iteration 13608 : loss: 0.021586, loss_a: 0.012698
[01:08:30.380] iteration 13609 : loss: 0.022571, loss_a: 0.013277
[01:08:31.114] iteration 13610 : loss: 0.017927, loss_a: 0.010545
[01:08:32.464] iteration 13611 : loss: 0.040883, loss_a: 0.024049
[01:08:33.198] iteration 13612 : loss: 0.020094, loss_a: 0.011820
[01:08:34.555] iteration 13613 : loss: 0.013435, loss_a: 0.007903
[01:08:35.299] iteration 13614 : loss: 0.024565, loss_a: 0.014450
[01:08:36.624] iteration 13615 : loss: 0.021991, loss_a: 0.012936
[01:08:37.366] iteration 13616 : loss: 0.038813, loss_a: 0.022831
[01:08:38.688] iteration 13617 : loss: 0.018740, loss_a: 0.011024
[01:08:39.430] iteration 13618 : loss: 0.023929, loss_a: 0.014076
[01:08:40.759] iteration 13619 : loss: 0.312686, loss_a: 0.183933
[01:08:41.495] iteration 13620 : loss: 0.019048, loss_a: 0.011205
[01:08:42.854] iteration 13621 : loss: 0.016928, loss_a: 0.009957
[01:08:43.600] iteration 13622 : loss: 0.022049, loss_a: 0.012970
[01:08:44.958] iteration 13623 : loss: 0.013339, loss_a: 0.007847
[01:08:45.694] iteration 13624 : loss: 0.014782, loss_a: 0.008695
[01:08:47.015] iteration 13625 : loss: 0.015878, loss_a: 0.009340
[01:08:47.750] iteration 13626 : loss: 0.020056, loss_a: 0.011797
[01:08:49.099] iteration 13627 : loss: 0.012286, loss_a: 0.007227
[01:08:49.836] iteration 13628 : loss: 0.023662, loss_a: 0.013919
[01:08:51.177] iteration 13629 : loss: 0.014562, loss_a: 0.008566
[01:08:51.909] iteration 13630 : loss: 0.021465, loss_a: 0.012627
[01:08:53.257] iteration 13631 : loss: 0.017324, loss_a: 0.010191
[01:08:53.995] iteration 13632 : loss: 0.024204, loss_a: 0.014237
[01:08:55.353] iteration 13633 : loss: 0.017511, loss_a: 0.010301
[01:08:56.100] iteration 13634 : loss: 0.012590, loss_a: 0.007406
[01:08:57.424] iteration 13635 : loss: 0.052005, loss_a: 0.030591
[01:08:58.163] iteration 13636 : loss: 0.028124, loss_a: 0.016543
[01:08:59.521] iteration 13637 : loss: 0.028372, loss_a: 0.016690
[01:09:00.267] iteration 13638 : loss: 0.019387, loss_a: 0.011404
[01:09:01.610] iteration 13639 : loss: 0.038432, loss_a: 0.022607
[01:09:02.347] iteration 13640 : loss: 0.013313, loss_a: 0.007831
[01:09:03.683] iteration 13641 : loss: 0.038265, loss_a: 0.022509
[01:09:04.418] iteration 13642 : loss: 0.038406, loss_a: 0.022591
[01:09:05.779] iteration 13643 : loss: 0.037371, loss_a: 0.021983
[01:09:06.518] iteration 13644 : loss: 0.015126, loss_a: 0.008898
[01:09:07.844] iteration 13645 : loss: 0.015779, loss_a: 0.009282
[01:09:08.578] iteration 13646 : loss: 0.019890, loss_a: 0.011700
[01:09:09.935] iteration 13647 : loss: 0.015792, loss_a: 0.009289
[01:09:10.679] iteration 13648 : loss: 0.015870, loss_a: 0.009335
[01:09:12.043] iteration 13649 : loss: 0.048412, loss_a: 0.028478
[01:09:12.776] iteration 13650 : loss: 0.017560, loss_a: 0.010329
[01:09:14.114] iteration 13651 : loss: 0.046762, loss_a: 0.027507
[01:09:14.843] iteration 13652 : loss: 0.014770, loss_a: 0.008688
[01:09:16.197] iteration 13653 : loss: 0.019878, loss_a: 0.011693
[01:09:16.932] iteration 13654 : loss: 0.051907, loss_a: 0.030534
[01:09:18.239] iteration 13655 : loss: 0.016100, loss_a: 0.009470
[01:09:18.981] iteration 13656 : loss: 0.021912, loss_a: 0.012890
[01:09:20.339] iteration 13657 : loss: 0.024629, loss_a: 0.014488
[01:09:21.087] iteration 13658 : loss: 0.022498, loss_a: 0.013234
[01:09:22.420] iteration 13659 : loss: 0.028953, loss_a: 0.017031
[01:09:23.170] iteration 13660 : loss: 0.020577, loss_a: 0.012104
[01:09:24.512] iteration 13661 : loss: 0.016589, loss_a: 0.009758
[01:09:25.249] iteration 13662 : loss: 0.014689, loss_a: 0.008641
[01:09:26.607] iteration 13663 : loss: 0.018517, loss_a: 0.010892
[01:09:27.337] iteration 13664 : loss: 0.018236, loss_a: 0.010727
[01:09:28.647] iteration 13665 : loss: 0.018824, loss_a: 0.011073
[01:09:29.391] iteration 13666 : loss: 0.039750, loss_a: 0.023382
[01:09:30.711] iteration 13667 : loss: 0.029215, loss_a: 0.017186
[01:09:31.455] iteration 13668 : loss: 0.029465, loss_a: 0.017332
[01:09:32.822] iteration 13669 : loss: 0.019864, loss_a: 0.011685
[01:09:33.554] iteration 13670 : loss: 0.017983, loss_a: 0.010578
[01:09:34.922] iteration 13671 : loss: 0.039131, loss_a: 0.023018
[01:09:35.673] iteration 13672 : loss: 0.018550, loss_a: 0.010912
[01:09:37.044] iteration 13673 : loss: 0.034649, loss_a: 0.020382
[01:09:37.772] iteration 13674 : loss: 0.013927, loss_a: 0.008192
[01:09:39.102] iteration 13675 : loss: 0.016001, loss_a: 0.009412
[01:09:39.839] iteration 13676 : loss: 0.026137, loss_a: 0.015375
[01:09:41.186] iteration 13677 : loss: 0.014559, loss_a: 0.008564
[01:09:41.927] iteration 13678 : loss: 0.056248, loss_a: 0.033087
[01:09:43.249] iteration 13679 : loss: 0.020642, loss_a: 0.012142
[01:09:43.991] iteration 13680 : loss: 0.039395, loss_a: 0.023173
[01:09:45.323] iteration 13681 : loss: 0.017753, loss_a: 0.010443
[01:09:46.066] iteration 13682 : loss: 0.018265, loss_a: 0.010744
[01:09:47.368] iteration 13683 : loss: 0.038262, loss_a: 0.022507
[01:09:48.110] iteration 13684 : loss: 0.037585, loss_a: 0.022109
[01:09:49.441] iteration 13685 : loss: 0.036740, loss_a: 0.021612
[01:09:50.178] iteration 13686 : loss: 0.036362, loss_a: 0.021389
[01:09:51.490] iteration 13687 : loss: 0.014617, loss_a: 0.008598
[01:09:52.238] iteration 13688 : loss: 0.018399, loss_a: 0.010823
[01:09:53.590] iteration 13689 : loss: 0.014515, loss_a: 0.008538
[01:09:54.320] iteration 13690 : loss: 0.012148, loss_a: 0.007146
[01:09:55.688] iteration 13691 : loss: 0.024395, loss_a: 0.014350
[01:09:56.431] iteration 13692 : loss: 0.035123, loss_a: 0.020661
[01:09:57.752] iteration 13693 : loss: 0.016432, loss_a: 0.009666
[01:09:58.489] iteration 13694 : loss: 0.010925, loss_a: 0.006426
[01:09:59.855] iteration 13695 : loss: 0.040669, loss_a: 0.023923
[01:10:00.591] iteration 13696 : loss: 0.014808, loss_a: 0.008710
[01:10:01.910] iteration 13697 : loss: 0.015997, loss_a: 0.009410
[01:10:02.653] iteration 13698 : loss: 0.019826, loss_a: 0.011662
[01:10:03.976] iteration 13699 : loss: 0.020077, loss_a: 0.011810
[01:10:04.715] iteration 13700 : loss: 0.031703, loss_a: 0.018649
[01:10:06.067] iteration 13701 : loss: 0.019939, loss_a: 0.011729
[01:10:06.806] iteration 13702 : loss: 0.021073, loss_a: 0.012396
[01:10:08.168] iteration 13703 : loss: 0.029837, loss_a: 0.017551
[01:10:08.903] iteration 13704 : loss: 0.023225, loss_a: 0.013662
[01:10:10.267] iteration 13705 : loss: 0.048301, loss_a: 0.028412
[01:10:11.003] iteration 13706 : loss: 0.029081, loss_a: 0.017106
[01:10:12.366] iteration 13707 : loss: 0.020578, loss_a: 0.012105
[01:10:13.099] iteration 13708 : loss: 0.048469, loss_a: 0.028511
[01:10:14.422] iteration 13709 : loss: 0.013067, loss_a: 0.007687
[01:10:15.172] iteration 13710 : loss: 0.021324, loss_a: 0.012544
[01:10:16.519] iteration 13711 : loss: 0.045518, loss_a: 0.026775
[01:10:17.264] iteration 13712 : loss: 0.019032, loss_a: 0.011195
[01:10:18.612] iteration 13713 : loss: 0.117830, loss_a: 0.069312
[01:10:19.342] iteration 13714 : loss: 0.019067, loss_a: 0.011216
[01:10:20.687] iteration 13715 : loss: 0.030970, loss_a: 0.018218
[01:10:21.419] iteration 13716 : loss: 0.014830, loss_a: 0.008724
[01:10:22.754] iteration 13717 : loss: 0.025521, loss_a: 0.015012
[01:10:23.496] iteration 13718 : loss: 0.025986, loss_a: 0.015286
[01:10:24.850] iteration 13719 : loss: 0.016562, loss_a: 0.009742
[01:10:25.588] iteration 13720 : loss: 0.015068, loss_a: 0.008864
[01:10:26.952] iteration 13721 : loss: 0.037966, loss_a: 0.022333
[01:10:27.692] iteration 13722 : loss: 0.025941, loss_a: 0.015259
[01:10:29.014] iteration 13723 : loss: 0.015657, loss_a: 0.009210
[01:10:29.759] iteration 13724 : loss: 0.038150, loss_a: 0.022441
[01:10:31.085] iteration 13725 : loss: 0.024594, loss_a: 0.014467
[01:10:31.820] iteration 13726 : loss: 0.030418, loss_a: 0.017893
[01:10:33.146] iteration 13727 : loss: 0.018529, loss_a: 0.010899
[01:10:33.880] iteration 13728 : loss: 0.023653, loss_a: 0.013913
[01:10:35.230] iteration 13729 : loss: 0.022321, loss_a: 0.013130
[01:10:35.963] iteration 13730 : loss: 0.017931, loss_a: 0.010548
[01:10:37.295] iteration 13731 : loss: 0.016469, loss_a: 0.009687
[01:10:38.046] iteration 13732 : loss: 0.024213, loss_a: 0.014243
[01:10:39.400] iteration 13733 : loss: 0.021352, loss_a: 0.012560
[01:10:40.141] iteration 13734 : loss: 0.025680, loss_a: 0.015106
[01:10:41.452] iteration 13735 : loss: 0.024935, loss_a: 0.014667
[01:10:42.199] iteration 13736 : loss: 0.027371, loss_a: 0.016101
[01:10:43.505] iteration 13737 : loss: 0.058191, loss_a: 0.034230
[01:10:44.257] iteration 13738 : loss: 0.042501, loss_a: 0.025001
[01:10:45.573] iteration 13739 : loss: 0.017554, loss_a: 0.010326
[01:10:46.311] iteration 13740 : loss: 0.044760, loss_a: 0.026329
[01:10:47.652] iteration 13741 : loss: 0.026261, loss_a: 0.015448
[01:10:48.391] iteration 13742 : loss: 0.023553, loss_a: 0.013855
[01:10:49.737] iteration 13743 : loss: 0.065141, loss_a: 0.038318
[01:10:50.483] iteration 13744 : loss: 0.024286, loss_a: 0.014286
[01:10:51.813] iteration 13745 : loss: 0.048853, loss_a: 0.028737
[01:10:52.550] iteration 13746 : loss: 0.029068, loss_a: 0.017099
[01:10:53.908] iteration 13747 : loss: 0.019009, loss_a: 0.011182
[01:10:54.657] iteration 13748 : loss: 0.030439, loss_a: 0.017905
[01:10:56.008] iteration 13749 : loss: 0.028155, loss_a: 0.016562
[01:10:56.740] iteration 13750 : loss: 0.009553, loss_a: 0.005619
[01:10:58.057] iteration 13751 : loss: 0.015410, loss_a: 0.009064
[01:10:58.794] iteration 13752 : loss: 0.040055, loss_a: 0.023562
[01:11:00.137] iteration 13753 : loss: 0.014066, loss_a: 0.008274
[01:11:00.878] iteration 13754 : loss: 0.036599, loss_a: 0.021529
[01:11:02.205] iteration 13755 : loss: 0.055699, loss_a: 0.032764
[01:11:02.947] iteration 13756 : loss: 0.044683, loss_a: 0.026284
[01:11:04.277] iteration 13757 : loss: 0.025338, loss_a: 0.014905
[01:11:05.004] iteration 13758 : loss: 0.029081, loss_a: 0.017107
[01:11:06.351] iteration 13759 : loss: 0.019964, loss_a: 0.011744
[01:11:07.089] iteration 13760 : loss: 0.019730, loss_a: 0.011606
[01:11:08.446] iteration 13761 : loss: 0.020803, loss_a: 0.012237
[01:11:09.188] iteration 13762 : loss: 0.020566, loss_a: 0.012098
[01:11:10.532] iteration 13763 : loss: 0.018587, loss_a: 0.010933
[01:11:11.276] iteration 13764 : loss: 0.029520, loss_a: 0.017365
[01:11:12.599] iteration 13765 : loss: 0.013939, loss_a: 0.008200
[01:11:13.338] iteration 13766 : loss: 0.021303, loss_a: 0.012531
[01:11:14.672] iteration 13767 : loss: 0.018631, loss_a: 0.010959
[01:11:15.413] iteration 13768 : loss: 0.027396, loss_a: 0.016116
[01:11:16.770] iteration 13769 : loss: 0.024090, loss_a: 0.014170
[01:11:17.500] iteration 13770 : loss: 0.040050, loss_a: 0.023559
[01:11:18.845] iteration 13771 : loss: 0.029775, loss_a: 0.017515
[01:11:19.577] iteration 13772 : loss: 0.013888, loss_a: 0.008169
[01:11:20.930] iteration 13773 : loss: 0.013326, loss_a: 0.007839
[01:11:21.666] iteration 13774 : loss: 0.019233, loss_a: 0.011314
[01:11:23.019] iteration 13775 : loss: 0.039488, loss_a: 0.023228
[01:11:23.758] iteration 13776 : loss: 0.025476, loss_a: 0.014986
[01:11:25.102] iteration 13777 : loss: 0.039135, loss_a: 0.023021
[01:11:25.844] iteration 13778 : loss: 0.014782, loss_a: 0.008695
[01:11:27.167] iteration 13779 : loss: 0.028590, loss_a: 0.016817
[01:11:27.906] iteration 13780 : loss: 0.034873, loss_a: 0.020514
[01:11:29.230] iteration 13781 : loss: 0.012571, loss_a: 0.007395
[01:11:29.970] iteration 13782 : loss: 0.056037, loss_a: 0.032963
[01:11:31.319] iteration 13783 : loss: 0.017552, loss_a: 0.010325
[01:11:32.054] iteration 13784 : loss: 0.024114, loss_a: 0.014185
[01:11:33.400] iteration 13785 : loss: 0.018201, loss_a: 0.010706
[01:11:34.137] iteration 13786 : loss: 0.032455, loss_a: 0.019091
[01:11:35.491] iteration 13787 : loss: 0.026321, loss_a: 0.015483
[01:11:36.249] iteration 13788 : loss: 0.039726, loss_a: 0.023368
[01:11:37.569] iteration 13789 : loss: 0.012405, loss_a: 0.007297
[01:11:38.310] iteration 13790 : loss: 0.044754, loss_a: 0.026326
[01:11:39.637] iteration 13791 : loss: 0.031941, loss_a: 0.018789
[01:11:40.377] iteration 13792 : loss: 0.033650, loss_a: 0.019794
[01:11:41.721] iteration 13793 : loss: 0.019691, loss_a: 0.011583
[01:11:42.466] iteration 13794 : loss: 0.039464, loss_a: 0.023214
[01:11:43.814] iteration 13795 : loss: 0.020278, loss_a: 0.011928
[01:11:44.551] iteration 13796 : loss: 0.019501, loss_a: 0.011471
[01:11:45.885] iteration 13797 : loss: 0.011771, loss_a: 0.006924
[01:11:46.643] iteration 13798 : loss: 0.016030, loss_a: 0.009429
[01:11:47.986] iteration 13799 : loss: 0.016961, loss_a: 0.009977
[01:11:48.726] iteration 13800 : loss: 0.041180, loss_a: 0.024224
[01:12:13.397] iteration 13801 : loss: 0.013586, loss_a: 0.007992
[01:12:15.618] iteration 13802 : loss: 0.019098, loss_a: 0.011234
[01:12:16.973] iteration 13803 : loss: 0.016959, loss_a: 0.009976
[01:12:17.719] iteration 13804 : loss: 0.014081, loss_a: 0.008283
[01:12:19.060] iteration 13805 : loss: 0.030245, loss_a: 0.017791
[01:12:19.809] iteration 13806 : loss: 0.026456, loss_a: 0.015562
[01:12:21.123] iteration 13807 : loss: 0.025157, loss_a: 0.014798
[01:12:21.868] iteration 13808 : loss: 0.039128, loss_a: 0.023016
[01:12:23.213] iteration 13809 : loss: 0.011400, loss_a: 0.006706
[01:12:23.950] iteration 13810 : loss: 0.034950, loss_a: 0.020559
[01:12:25.303] iteration 13811 : loss: 0.013867, loss_a: 0.008157
[01:12:26.049] iteration 13812 : loss: 0.028245, loss_a: 0.016615
[01:12:27.405] iteration 13813 : loss: 0.019014, loss_a: 0.011185
[01:12:28.157] iteration 13814 : loss: 0.042507, loss_a: 0.025004
[01:12:29.484] iteration 13815 : loss: 0.017822, loss_a: 0.010483
[01:12:30.229] iteration 13816 : loss: 0.018216, loss_a: 0.010716
[01:12:31.552] iteration 13817 : loss: 0.019498, loss_a: 0.011470
[01:12:32.294] iteration 13818 : loss: 0.035157, loss_a: 0.020681
[01:12:33.633] iteration 13819 : loss: 0.016131, loss_a: 0.009489
[01:12:34.368] iteration 13820 : loss: 0.025612, loss_a: 0.015066
[01:12:35.717] iteration 13821 : loss: 0.047842, loss_a: 0.028142
[01:12:36.449] iteration 13822 : loss: 0.021620, loss_a: 0.012717
[01:12:37.762] iteration 13823 : loss: 0.012039, loss_a: 0.007082
[01:12:38.502] iteration 13824 : loss: 0.016270, loss_a: 0.009571
[01:12:39.831] iteration 13825 : loss: 0.020414, loss_a: 0.012008
[01:12:40.565] iteration 13826 : loss: 0.021355, loss_a: 0.012562
[01:12:41.900] iteration 13827 : loss: 0.023991, loss_a: 0.014113
[01:12:42.641] iteration 13828 : loss: 0.013757, loss_a: 0.008092
[01:12:43.963] iteration 13829 : loss: 0.025128, loss_a: 0.014781
[01:12:44.721] iteration 13830 : loss: 0.036199, loss_a: 0.021294
[01:12:46.083] iteration 13831 : loss: 0.016631, loss_a: 0.009783
[01:12:46.821] iteration 13832 : loss: 0.018918, loss_a: 0.011128
[01:12:48.168] iteration 13833 : loss: 0.028628, loss_a: 0.016840
[01:12:48.910] iteration 13834 : loss: 0.020302, loss_a: 0.011942
[01:12:50.253] iteration 13835 : loss: 0.015463, loss_a: 0.009096
[01:12:50.994] iteration 13836 : loss: 0.015278, loss_a: 0.008987
[01:12:52.342] iteration 13837 : loss: 0.023696, loss_a: 0.013939
[01:12:53.075] iteration 13838 : loss: 0.062979, loss_a: 0.037047
[01:12:54.409] iteration 13839 : loss: 0.036564, loss_a: 0.021508
[01:12:55.154] iteration 13840 : loss: 0.029447, loss_a: 0.017322
[01:12:56.504] iteration 13841 : loss: 0.022836, loss_a: 0.013433
[01:12:57.246] iteration 13842 : loss: 0.034802, loss_a: 0.020472
[01:12:58.597] iteration 13843 : loss: 0.019485, loss_a: 0.011462
[01:12:59.355] iteration 13844 : loss: 0.042048, loss_a: 0.024734
[01:13:00.719] iteration 13845 : loss: 0.024267, loss_a: 0.014275
[01:13:01.456] iteration 13846 : loss: 0.027712, loss_a: 0.016301
[01:13:02.798] iteration 13847 : loss: 0.026584, loss_a: 0.015638
[01:13:03.547] iteration 13848 : loss: 0.031577, loss_a: 0.018575
[01:13:04.864] iteration 13849 : loss: 0.007445, loss_a: 0.004379
[01:13:05.610] iteration 13850 : loss: 0.014615, loss_a: 0.008597
[01:13:06.936] iteration 13851 : loss: 0.013843, loss_a: 0.008143
[01:13:07.673] iteration 13852 : loss: 0.026513, loss_a: 0.015596
[01:13:08.985] iteration 13853 : loss: 0.027722, loss_a: 0.016307
[01:13:09.731] iteration 13854 : loss: 0.019497, loss_a: 0.011469
[01:13:11.092] iteration 13855 : loss: 0.066403, loss_a: 0.039061
[01:13:11.830] iteration 13856 : loss: 0.010520, loss_a: 0.006188
[01:13:13.146] iteration 13857 : loss: 0.023172, loss_a: 0.013631
[01:13:13.897] iteration 13858 : loss: 0.047291, loss_a: 0.027818
[01:13:15.217] iteration 13859 : loss: 0.042061, loss_a: 0.024742
[01:13:15.951] iteration 13860 : loss: 0.012387, loss_a: 0.007286
[01:13:17.267] iteration 13861 : loss: 0.018401, loss_a: 0.010824
[01:13:18.022] iteration 13862 : loss: 0.030062, loss_a: 0.017683
[01:13:19.369] iteration 13863 : loss: 0.041106, loss_a: 0.024180
[01:13:20.109] iteration 13864 : loss: 0.035019, loss_a: 0.020599
[01:13:21.447] iteration 13865 : loss: 0.025450, loss_a: 0.014970
[01:13:22.196] iteration 13866 : loss: 0.056687, loss_a: 0.033346
[01:13:23.518] iteration 13867 : loss: 0.017721, loss_a: 0.010424
[01:13:24.250] iteration 13868 : loss: 0.026168, loss_a: 0.015393
[01:13:25.578] iteration 13869 : loss: 0.026050, loss_a: 0.015323
[01:13:26.320] iteration 13870 : loss: 0.020861, loss_a: 0.012271
[01:13:27.660] iteration 13871 : loss: 0.020162, loss_a: 0.011860
[01:13:28.404] iteration 13872 : loss: 0.034297, loss_a: 0.020175
[01:13:29.726] iteration 13873 : loss: 0.026999, loss_a: 0.015881
[01:13:30.466] iteration 13874 : loss: 0.026738, loss_a: 0.015728
[01:13:31.791] iteration 13875 : loss: 0.016269, loss_a: 0.009570
[01:13:32.534] iteration 13876 : loss: 0.017997, loss_a: 0.010587
[01:13:33.866] iteration 13877 : loss: 0.020746, loss_a: 0.012204
[01:13:34.615] iteration 13878 : loss: 0.019114, loss_a: 0.011244
[01:13:35.949] iteration 13879 : loss: 0.028031, loss_a: 0.016489
[01:13:36.684] iteration 13880 : loss: 0.017808, loss_a: 0.010475
[01:13:38.040] iteration 13881 : loss: 0.017287, loss_a: 0.010169
[01:13:38.778] iteration 13882 : loss: 0.013456, loss_a: 0.007915
[01:13:40.090] iteration 13883 : loss: 0.019325, loss_a: 0.011368
[01:13:40.829] iteration 13884 : loss: 0.013019, loss_a: 0.007658
[01:13:42.145] iteration 13885 : loss: 0.017214, loss_a: 0.010126
[01:13:42.879] iteration 13886 : loss: 0.015175, loss_a: 0.008926
[01:13:44.213] iteration 13887 : loss: 0.043810, loss_a: 0.025771
[01:13:44.962] iteration 13888 : loss: 0.009443, loss_a: 0.005555
[01:13:46.314] iteration 13889 : loss: 0.016736, loss_a: 0.009845
[01:13:47.053] iteration 13890 : loss: 0.033072, loss_a: 0.019454
[01:13:48.415] iteration 13891 : loss: 0.031551, loss_a: 0.018560
[01:13:49.158] iteration 13892 : loss: 0.037156, loss_a: 0.021856
[01:13:50.488] iteration 13893 : loss: 0.045635, loss_a: 0.026844
[01:13:51.232] iteration 13894 : loss: 0.029969, loss_a: 0.017629
[01:13:52.582] iteration 13895 : loss: 0.016196, loss_a: 0.009527
[01:13:53.318] iteration 13896 : loss: 0.028049, loss_a: 0.016499
[01:13:54.654] iteration 13897 : loss: 0.008128, loss_a: 0.004781
[01:13:55.405] iteration 13898 : loss: 0.016926, loss_a: 0.009956
[01:13:56.761] iteration 13899 : loss: 0.027672, loss_a: 0.016278
[01:13:57.506] iteration 13900 : loss: 0.032845, loss_a: 0.019320
[01:13:58.829] iteration 13901 : loss: 0.014672, loss_a: 0.008631
[01:13:59.567] iteration 13902 : loss: 0.017987, loss_a: 0.010581
[01:14:00.878] iteration 13903 : loss: 0.024627, loss_a: 0.014486
[01:14:01.616] iteration 13904 : loss: 0.013514, loss_a: 0.007949
[01:14:02.982] iteration 13905 : loss: 0.026132, loss_a: 0.015372
[01:14:03.735] iteration 13906 : loss: 0.020979, loss_a: 0.012341
[01:14:05.073] iteration 13907 : loss: 0.014275, loss_a: 0.008397
[01:14:05.813] iteration 13908 : loss: 0.017741, loss_a: 0.010436
[01:14:07.151] iteration 13909 : loss: 0.043540, loss_a: 0.025612
[01:14:07.891] iteration 13910 : loss: 0.015420, loss_a: 0.009070
[01:14:09.243] iteration 13911 : loss: 0.019604, loss_a: 0.011532
[01:14:09.993] iteration 13912 : loss: 0.021029, loss_a: 0.012370
[01:14:11.342] iteration 13913 : loss: 0.015805, loss_a: 0.009297
[01:14:12.083] iteration 13914 : loss: 0.017488, loss_a: 0.010287
[01:14:13.413] iteration 13915 : loss: 0.017797, loss_a: 0.010469
[01:14:14.147] iteration 13916 : loss: 0.009676, loss_a: 0.005692
[01:14:15.466] iteration 13917 : loss: 0.019974, loss_a: 0.011750
[01:14:16.212] iteration 13918 : loss: 0.030695, loss_a: 0.018056
[01:14:17.561] iteration 13919 : loss: 0.021975, loss_a: 0.012926
[01:14:18.300] iteration 13920 : loss: 0.018124, loss_a: 0.010661
[01:14:19.657] iteration 13921 : loss: 0.018527, loss_a: 0.010898
[01:14:20.409] iteration 13922 : loss: 0.017326, loss_a: 0.010192
[01:14:21.768] iteration 13923 : loss: 0.036486, loss_a: 0.021462
[01:14:22.498] iteration 13924 : loss: 0.018970, loss_a: 0.011159
[01:14:23.870] iteration 13925 : loss: 0.017126, loss_a: 0.010074
[01:14:24.610] iteration 13926 : loss: 0.018498, loss_a: 0.010881
[01:14:25.956] iteration 13927 : loss: 0.023068, loss_a: 0.013570
[01:14:26.689] iteration 13928 : loss: 0.009264, loss_a: 0.005449
[01:14:28.017] iteration 13929 : loss: 0.014907, loss_a: 0.008769
[01:14:28.767] iteration 13930 : loss: 0.027863, loss_a: 0.016390
[01:14:30.134] iteration 13931 : loss: 0.034219, loss_a: 0.020129
[01:14:30.880] iteration 13932 : loss: 0.014558, loss_a: 0.008564
[01:14:32.213] iteration 13933 : loss: 0.031729, loss_a: 0.018664
[01:14:32.950] iteration 13934 : loss: 0.019298, loss_a: 0.011352
[01:14:34.306] iteration 13935 : loss: 0.013884, loss_a: 0.008167
[01:14:35.043] iteration 13936 : loss: 0.024869, loss_a: 0.014629
[01:14:36.375] iteration 13937 : loss: 0.011134, loss_a: 0.006550
[01:14:37.120] iteration 13938 : loss: 0.038867, loss_a: 0.022863
[01:14:38.492] iteration 13939 : loss: 0.029152, loss_a: 0.017148
[01:14:39.226] iteration 13940 : loss: 0.010045, loss_a: 0.005909
[01:14:40.542] iteration 13941 : loss: 0.016900, loss_a: 0.009941
[01:14:41.292] iteration 13942 : loss: 0.023844, loss_a: 0.014026
[01:14:42.643] iteration 13943 : loss: 0.015574, loss_a: 0.009161
[01:14:43.384] iteration 13944 : loss: 0.022435, loss_a: 0.013197
[01:14:44.690] iteration 13945 : loss: 0.014892, loss_a: 0.008760
[01:14:45.442] iteration 13946 : loss: 0.016156, loss_a: 0.009503
[01:14:46.811] iteration 13947 : loss: 0.058750, loss_a: 0.034559
[01:14:47.549] iteration 13948 : loss: 0.050099, loss_a: 0.029470
[01:14:48.914] iteration 13949 : loss: 0.066548, loss_a: 0.039146
[01:14:49.656] iteration 13950 : loss: 0.033986, loss_a: 0.019992
[01:14:50.997] iteration 13951 : loss: 0.016676, loss_a: 0.009810
[01:14:51.746] iteration 13952 : loss: 0.037553, loss_a: 0.022090
[01:14:53.110] iteration 13953 : loss: 0.019034, loss_a: 0.011197
[01:14:53.853] iteration 13954 : loss: 0.020021, loss_a: 0.011777
[01:14:55.207] iteration 13955 : loss: 0.019847, loss_a: 0.011675
[01:14:55.947] iteration 13956 : loss: 0.054765, loss_a: 0.032214
[01:14:57.297] iteration 13957 : loss: 0.031852, loss_a: 0.018736
[01:14:58.045] iteration 13958 : loss: 0.026509, loss_a: 0.015593
[01:14:59.393] iteration 13959 : loss: 0.027830, loss_a: 0.016371
[01:15:00.134] iteration 13960 : loss: 0.023836, loss_a: 0.014021
[01:15:01.467] iteration 13961 : loss: 0.047523, loss_a: 0.027955
[01:15:02.199] iteration 13962 : loss: 0.025816, loss_a: 0.015186
[01:15:03.525] iteration 13963 : loss: 0.021678, loss_a: 0.012752
[01:15:04.270] iteration 13964 : loss: 0.026956, loss_a: 0.015857
[01:15:05.623] iteration 13965 : loss: 0.043655, loss_a: 0.025680
[01:15:06.376] iteration 13966 : loss: 0.020410, loss_a: 0.012006
[01:15:07.721] iteration 13967 : loss: 0.022576, loss_a: 0.013280
[01:15:08.467] iteration 13968 : loss: 0.033235, loss_a: 0.019550
[01:15:09.826] iteration 13969 : loss: 0.033161, loss_a: 0.019507
[01:15:10.583] iteration 13970 : loss: 0.043759, loss_a: 0.025741
[01:15:11.944] iteration 13971 : loss: 0.029608, loss_a: 0.017416
[01:15:12.684] iteration 13972 : loss: 0.081920, loss_a: 0.048188
[01:15:14.041] iteration 13973 : loss: 0.026139, loss_a: 0.015376
[01:15:14.776] iteration 13974 : loss: 0.021805, loss_a: 0.012827
[01:15:16.183] iteration 13975 : loss: 0.035721, loss_a: 0.021012
[01:15:16.920] iteration 13976 : loss: 0.021798, loss_a: 0.012822
[01:15:18.254] iteration 13977 : loss: 0.024808, loss_a: 0.014593
[01:15:18.994] iteration 13978 : loss: 0.034918, loss_a: 0.020540
[01:15:20.316] iteration 13979 : loss: 0.038179, loss_a: 0.022458
[01:15:21.056] iteration 13980 : loss: 0.019672, loss_a: 0.011572
[01:15:22.405] iteration 13981 : loss: 0.032266, loss_a: 0.018980
[01:15:23.143] iteration 13982 : loss: 0.016168, loss_a: 0.009511
[01:15:24.493] iteration 13983 : loss: 0.027657, loss_a: 0.016269
[01:15:25.233] iteration 13984 : loss: 0.021069, loss_a: 0.012394
[01:15:26.537] iteration 13985 : loss: 0.030123, loss_a: 0.017720
[01:15:27.278] iteration 13986 : loss: 0.018998, loss_a: 0.011175
[01:15:28.616] iteration 13987 : loss: 0.065372, loss_a: 0.038454
[01:15:29.364] iteration 13988 : loss: 0.022839, loss_a: 0.013435
[01:15:30.677] iteration 13989 : loss: 0.020660, loss_a: 0.012153
[01:15:31.423] iteration 13990 : loss: 0.018772, loss_a: 0.011042
[01:15:32.752] iteration 13991 : loss: 0.015498, loss_a: 0.009116
[01:15:33.490] iteration 13992 : loss: 0.022083, loss_a: 0.012990
[01:15:34.836] iteration 13993 : loss: 0.010852, loss_a: 0.006383
[01:15:35.573] iteration 13994 : loss: 0.034500, loss_a: 0.020294
[01:15:36.921] iteration 13995 : loss: 0.022194, loss_a: 0.013056
[01:15:37.702] iteration 13996 : loss: 0.064559, loss_a: 0.037976
[01:15:39.107] iteration 13997 : loss: 0.029944, loss_a: 0.017614
[01:15:39.857] iteration 13998 : loss: 0.084745, loss_a: 0.049850
[01:15:41.198] iteration 13999 : loss: 0.017364, loss_a: 0.010214
[01:15:41.939] iteration 14000 : loss: 0.014572, loss_a: 0.008571
[01:16:06.604] iteration 14001 : loss: 0.026468, loss_a: 0.015569
[01:16:08.811] iteration 14002 : loss: 0.032296, loss_a: 0.018998
[01:16:10.142] iteration 14003 : loss: 0.018531, loss_a: 0.010901
[01:16:10.891] iteration 14004 : loss: 0.019295, loss_a: 0.011350
[01:16:12.255] iteration 14005 : loss: 0.030321, loss_a: 0.017836
[01:16:12.996] iteration 14006 : loss: 0.016073, loss_a: 0.009455
[01:16:14.358] iteration 14007 : loss: 0.021831, loss_a: 0.012842
[01:16:15.096] iteration 14008 : loss: 0.021721, loss_a: 0.012777
[01:16:16.418] iteration 14009 : loss: 0.017937, loss_a: 0.010551
[01:16:17.167] iteration 14010 : loss: 0.027020, loss_a: 0.015894
[01:16:18.509] iteration 14011 : loss: 0.024030, loss_a: 0.014136
[01:16:19.257] iteration 14012 : loss: 0.047740, loss_a: 0.028082
[01:16:20.575] iteration 14013 : loss: 0.011000, loss_a: 0.006470
[01:16:21.311] iteration 14014 : loss: 0.017562, loss_a: 0.010331
[01:16:22.676] iteration 14015 : loss: 0.059147, loss_a: 0.034792
[01:16:23.419] iteration 14016 : loss: 0.016788, loss_a: 0.009875
[01:16:24.780] iteration 14017 : loss: 0.036524, loss_a: 0.021485
[01:16:25.512] iteration 14018 : loss: 0.014078, loss_a: 0.008281
[01:16:26.872] iteration 14019 : loss: 0.022396, loss_a: 0.013174
[01:16:27.612] iteration 14020 : loss: 0.021051, loss_a: 0.012383
[01:16:28.963] iteration 14021 : loss: 0.030823, loss_a: 0.018131
[01:16:29.704] iteration 14022 : loss: 0.021716, loss_a: 0.012774
[01:16:31.058] iteration 14023 : loss: 0.026239, loss_a: 0.015435
[01:16:31.799] iteration 14024 : loss: 0.019608, loss_a: 0.011534
[01:16:33.123] iteration 14025 : loss: 0.036988, loss_a: 0.021758
[01:16:33.880] iteration 14026 : loss: 0.023256, loss_a: 0.013680
[01:16:35.228] iteration 14027 : loss: 0.051135, loss_a: 0.030080
[01:16:35.971] iteration 14028 : loss: 0.049508, loss_a: 0.029123
[01:16:37.306] iteration 14029 : loss: 0.019865, loss_a: 0.011685
[01:16:38.056] iteration 14030 : loss: 0.019455, loss_a: 0.011444
[01:16:39.414] iteration 14031 : loss: 0.048371, loss_a: 0.028453
[01:16:40.159] iteration 14032 : loss: 0.034909, loss_a: 0.020534
[01:16:41.506] iteration 14033 : loss: 0.023276, loss_a: 0.013691
[01:16:42.252] iteration 14034 : loss: 0.027142, loss_a: 0.015966
[01:16:43.551] iteration 14035 : loss: 0.013719, loss_a: 0.008070
[01:16:44.293] iteration 14036 : loss: 0.038026, loss_a: 0.022368
[01:16:45.658] iteration 14037 : loss: 0.015471, loss_a: 0.009101
[01:16:46.397] iteration 14038 : loss: 0.014053, loss_a: 0.008267
[01:16:47.731] iteration 14039 : loss: 0.017653, loss_a: 0.010384
[01:16:48.471] iteration 14040 : loss: 0.052812, loss_a: 0.031066
[01:16:49.808] iteration 14041 : loss: 0.017608, loss_a: 0.010357
[01:16:50.550] iteration 14042 : loss: 0.020760, loss_a: 0.012212
[01:16:51.866] iteration 14043 : loss: 0.011545, loss_a: 0.006791
[01:16:52.604] iteration 14044 : loss: 0.018021, loss_a: 0.010601
[01:16:53.927] iteration 14045 : loss: 0.013707, loss_a: 0.008063
[01:16:54.669] iteration 14046 : loss: 0.021798, loss_a: 0.012822
[01:16:56.008] iteration 14047 : loss: 0.040384, loss_a: 0.023755
[01:16:56.763] iteration 14048 : loss: 0.028334, loss_a: 0.016667
[01:16:58.096] iteration 14049 : loss: 0.022927, loss_a: 0.013486
[01:16:58.859] iteration 14050 : loss: 0.024813, loss_a: 0.014596
[01:17:00.180] iteration 14051 : loss: 0.012574, loss_a: 0.007397
[01:17:00.928] iteration 14052 : loss: 0.011900, loss_a: 0.007000
[01:17:02.249] iteration 14053 : loss: 0.018357, loss_a: 0.010798
[01:17:02.996] iteration 14054 : loss: 0.033768, loss_a: 0.019863
[01:17:04.343] iteration 14055 : loss: 0.017857, loss_a: 0.010504
[01:17:05.086] iteration 14056 : loss: 0.015096, loss_a: 0.008880
[01:17:06.452] iteration 14057 : loss: 0.035727, loss_a: 0.021016
[01:17:07.197] iteration 14058 : loss: 0.018447, loss_a: 0.010851
[01:17:08.528] iteration 14059 : loss: 0.027432, loss_a: 0.016137
[01:17:09.262] iteration 14060 : loss: 0.047414, loss_a: 0.027891
[01:17:10.597] iteration 14061 : loss: 0.020144, loss_a: 0.011849
[01:17:11.336] iteration 14062 : loss: 0.018119, loss_a: 0.010658
[01:17:12.696] iteration 14063 : loss: 0.026855, loss_a: 0.015797
[01:17:13.436] iteration 14064 : loss: 0.017468, loss_a: 0.010275
[01:17:14.775] iteration 14065 : loss: 0.030855, loss_a: 0.018150
[01:17:15.523] iteration 14066 : loss: 0.023431, loss_a: 0.013783
[01:17:16.845] iteration 14067 : loss: 0.023285, loss_a: 0.013697
[01:17:17.599] iteration 14068 : loss: 0.030429, loss_a: 0.017900
[01:17:18.919] iteration 14069 : loss: 0.010887, loss_a: 0.006404
[01:17:19.652] iteration 14070 : loss: 0.020418, loss_a: 0.012011
[01:17:20.988] iteration 14071 : loss: 0.026074, loss_a: 0.015338
[01:17:21.730] iteration 14072 : loss: 0.033261, loss_a: 0.019565
[01:17:23.081] iteration 14073 : loss: 0.025845, loss_a: 0.015203
[01:17:23.814] iteration 14074 : loss: 0.009545, loss_a: 0.005615
[01:17:25.209] iteration 14075 : loss: 0.024934, loss_a: 0.014667
[01:17:25.955] iteration 14076 : loss: 0.043672, loss_a: 0.025689
[01:17:27.281] iteration 14077 : loss: 0.015469, loss_a: 0.009099
[01:17:28.047] iteration 14078 : loss: 0.028381, loss_a: 0.016695
[01:17:29.391] iteration 14079 : loss: 0.014619, loss_a: 0.008599
[01:17:30.134] iteration 14080 : loss: 0.020450, loss_a: 0.012029
[01:17:31.468] iteration 14081 : loss: 0.015186, loss_a: 0.008933
[01:17:32.201] iteration 14082 : loss: 0.015417, loss_a: 0.009069
[01:17:33.522] iteration 14083 : loss: 0.027267, loss_a: 0.016040
[01:17:34.268] iteration 14084 : loss: 0.031500, loss_a: 0.018530
[01:17:35.608] iteration 14085 : loss: 0.035978, loss_a: 0.021164
[01:17:36.349] iteration 14086 : loss: 0.022881, loss_a: 0.013459
[01:17:37.694] iteration 14087 : loss: 0.014367, loss_a: 0.008451
[01:17:38.436] iteration 14088 : loss: 0.028312, loss_a: 0.016654
[01:17:39.782] iteration 14089 : loss: 0.015348, loss_a: 0.009028
[01:17:40.533] iteration 14090 : loss: 0.035848, loss_a: 0.021087
[01:17:41.858] iteration 14091 : loss: 0.019916, loss_a: 0.011715
[01:17:42.606] iteration 14092 : loss: 0.035370, loss_a: 0.020806
[01:17:43.960] iteration 14093 : loss: 0.026111, loss_a: 0.015359
[01:17:44.705] iteration 14094 : loss: 0.046398, loss_a: 0.027293
[01:17:46.037] iteration 14095 : loss: 0.054672, loss_a: 0.032160
[01:17:46.789] iteration 14096 : loss: 0.035192, loss_a: 0.020701
[01:17:48.140] iteration 14097 : loss: 0.026873, loss_a: 0.015807
[01:17:48.883] iteration 14098 : loss: 0.036499, loss_a: 0.021470
[01:17:50.236] iteration 14099 : loss: 0.022701, loss_a: 0.013354
[01:17:50.981] iteration 14100 : loss: 0.023647, loss_a: 0.013910
[01:17:52.326] iteration 14101 : loss: 0.011456, loss_a: 0.006739
[01:17:53.070] iteration 14102 : loss: 0.014011, loss_a: 0.008242
[01:17:54.386] iteration 14103 : loss: 0.029002, loss_a: 0.017060
[01:17:55.140] iteration 14104 : loss: 0.028768, loss_a: 0.016922
[01:17:56.488] iteration 14105 : loss: 0.020742, loss_a: 0.012201
[01:17:57.220] iteration 14106 : loss: 0.030339, loss_a: 0.017846
[01:17:58.546] iteration 14107 : loss: 0.018609, loss_a: 0.010946
[01:17:59.282] iteration 14108 : loss: 0.019959, loss_a: 0.011740
[01:18:00.618] iteration 14109 : loss: 0.036791, loss_a: 0.021642
[01:18:01.364] iteration 14110 : loss: 0.030802, loss_a: 0.018119
[01:18:02.714] iteration 14111 : loss: 0.020246, loss_a: 0.011910
[01:18:03.462] iteration 14112 : loss: 0.037905, loss_a: 0.022297
[01:18:04.801] iteration 14113 : loss: 0.023290, loss_a: 0.013700
[01:18:05.535] iteration 14114 : loss: 0.015017, loss_a: 0.008834
[01:18:06.862] iteration 14115 : loss: 0.013828, loss_a: 0.008134
[01:18:07.604] iteration 14116 : loss: 0.015882, loss_a: 0.009342
[01:18:08.951] iteration 14117 : loss: 0.020349, loss_a: 0.011970
[01:18:09.709] iteration 14118 : loss: 0.055887, loss_a: 0.032875
[01:18:11.050] iteration 14119 : loss: 0.022087, loss_a: 0.012992
[01:18:11.796] iteration 14120 : loss: 0.029742, loss_a: 0.017496
[01:18:13.116] iteration 14121 : loss: 0.010300, loss_a: 0.006059
[01:18:13.849] iteration 14122 : loss: 0.016710, loss_a: 0.009829
[01:18:15.162] iteration 14123 : loss: 0.029196, loss_a: 0.017174
[01:18:15.908] iteration 14124 : loss: 0.012138, loss_a: 0.007140
[01:18:17.227] iteration 14125 : loss: 0.025182, loss_a: 0.014813
[01:18:17.960] iteration 14126 : loss: 0.052011, loss_a: 0.030595
[01:18:19.283] iteration 14127 : loss: 0.025986, loss_a: 0.015286
[01:18:20.024] iteration 14128 : loss: 0.030674, loss_a: 0.018044
[01:18:21.384] iteration 14129 : loss: 0.020240, loss_a: 0.011906
[01:18:22.121] iteration 14130 : loss: 0.018951, loss_a: 0.011148
[01:18:23.455] iteration 14131 : loss: 0.012362, loss_a: 0.007272
[01:18:24.197] iteration 14132 : loss: 0.032485, loss_a: 0.019109
[01:18:25.570] iteration 14133 : loss: 0.019595, loss_a: 0.011527
[01:18:26.318] iteration 14134 : loss: 0.025830, loss_a: 0.015194
[01:18:27.673] iteration 14135 : loss: 0.056400, loss_a: 0.033176
[01:18:28.408] iteration 14136 : loss: 0.015556, loss_a: 0.009151
[01:18:29.757] iteration 14137 : loss: 0.037536, loss_a: 0.022080
[01:18:30.494] iteration 14138 : loss: 0.016069, loss_a: 0.009453
[01:18:31.862] iteration 14139 : loss: 0.061213, loss_a: 0.036008
[01:18:32.611] iteration 14140 : loss: 0.017383, loss_a: 0.010225
[01:18:33.939] iteration 14141 : loss: 0.044440, loss_a: 0.026141
[01:18:34.679] iteration 14142 : loss: 0.024597, loss_a: 0.014469
[01:18:36.024] iteration 14143 : loss: 0.020805, loss_a: 0.012238
[01:18:36.761] iteration 14144 : loss: 0.018100, loss_a: 0.010647
[01:18:38.121] iteration 14145 : loss: 0.031436, loss_a: 0.018492
[01:18:38.868] iteration 14146 : loss: 0.036516, loss_a: 0.021480
[01:18:40.229] iteration 14147 : loss: 0.026680, loss_a: 0.015694
[01:18:40.966] iteration 14148 : loss: 0.023056, loss_a: 0.013563
[01:18:42.334] iteration 14149 : loss: 0.035412, loss_a: 0.020830
[01:18:43.070] iteration 14150 : loss: 0.043925, loss_a: 0.025838
[01:18:44.397] iteration 14151 : loss: 0.029969, loss_a: 0.017629
[01:18:45.136] iteration 14152 : loss: 0.023129, loss_a: 0.013605
[01:18:46.466] iteration 14153 : loss: 0.018713, loss_a: 0.011008
[01:18:47.214] iteration 14154 : loss: 0.028207, loss_a: 0.016592
[01:18:48.570] iteration 14155 : loss: 0.022601, loss_a: 0.013295
[01:18:49.325] iteration 14156 : loss: 0.044126, loss_a: 0.025956
[01:18:50.677] iteration 14157 : loss: 0.056333, loss_a: 0.033137
[01:18:51.415] iteration 14158 : loss: 0.018036, loss_a: 0.010610
[01:18:52.781] iteration 14159 : loss: 0.041437, loss_a: 0.024375
[01:18:53.519] iteration 14160 : loss: 0.020216, loss_a: 0.011892
[01:18:54.832] iteration 14161 : loss: 0.022610, loss_a: 0.013300
[01:18:55.569] iteration 14162 : loss: 0.020892, loss_a: 0.012289
[01:18:56.915] iteration 14163 : loss: 0.020465, loss_a: 0.012038
[01:18:57.663] iteration 14164 : loss: 0.027954, loss_a: 0.016443
[01:18:58.983] iteration 14165 : loss: 0.029008, loss_a: 0.017064
[01:18:59.727] iteration 14166 : loss: 0.034760, loss_a: 0.020447
[01:19:01.055] iteration 14167 : loss: 0.013201, loss_a: 0.007765
[01:19:01.795] iteration 14168 : loss: 0.020600, loss_a: 0.012117
[01:19:03.111] iteration 14169 : loss: 0.016142, loss_a: 0.009495
[01:19:03.850] iteration 14170 : loss: 0.019419, loss_a: 0.011423
[01:19:05.210] iteration 14171 : loss: 0.035962, loss_a: 0.021154
[01:19:05.956] iteration 14172 : loss: 0.036835, loss_a: 0.021668
[01:19:07.309] iteration 14173 : loss: 0.065082, loss_a: 0.038284
[01:19:08.050] iteration 14174 : loss: 0.018714, loss_a: 0.011008
[01:19:09.406] iteration 14175 : loss: 0.027024, loss_a: 0.015897
[01:19:10.150] iteration 14176 : loss: 0.035419, loss_a: 0.020835
[01:19:11.508] iteration 14177 : loss: 0.014989, loss_a: 0.008817
[01:19:12.241] iteration 14178 : loss: 0.016308, loss_a: 0.009593
[01:19:13.569] iteration 14179 : loss: 0.021476, loss_a: 0.012633
[01:19:14.306] iteration 14180 : loss: 0.056106, loss_a: 0.033003
[01:19:15.644] iteration 14181 : loss: 0.022887, loss_a: 0.013463
[01:19:16.392] iteration 14182 : loss: 0.022277, loss_a: 0.013104
[01:19:17.748] iteration 14183 : loss: 0.020525, loss_a: 0.012074
[01:19:18.493] iteration 14184 : loss: 0.028846, loss_a: 0.016969
[01:19:19.848] iteration 14185 : loss: 0.012589, loss_a: 0.007405
[01:19:20.583] iteration 14186 : loss: 0.057807, loss_a: 0.034004
[01:19:21.946] iteration 14187 : loss: 0.064289, loss_a: 0.037817
[01:19:22.694] iteration 14188 : loss: 0.032466, loss_a: 0.019097
[01:19:24.058] iteration 14189 : loss: 0.019240, loss_a: 0.011318
[01:19:24.811] iteration 14190 : loss: 0.028220, loss_a: 0.016600
[01:19:26.168] iteration 14191 : loss: 0.020462, loss_a: 0.012037
[01:19:26.908] iteration 14192 : loss: 0.013261, loss_a: 0.007801
[01:19:28.240] iteration 14193 : loss: 0.031569, loss_a: 0.018570
[01:19:28.983] iteration 14194 : loss: 0.026209, loss_a: 0.015417
[01:19:30.308] iteration 14195 : loss: 0.010918, loss_a: 0.006423
[01:19:31.045] iteration 14196 : loss: 0.063279, loss_a: 0.037223
[01:19:32.381] iteration 14197 : loss: 0.037851, loss_a: 0.022265
[01:19:33.130] iteration 14198 : loss: 0.021580, loss_a: 0.012694
[01:19:34.492] iteration 14199 : loss: 0.028119, loss_a: 0.016540
[01:19:35.236] iteration 14200 : loss: 0.022961, loss_a: 0.013507
[01:19:59.894] iteration 14201 : loss: 0.016132, loss_a: 0.009489
[01:20:02.064] iteration 14202 : loss: 0.035978, loss_a: 0.021164
[01:20:03.394] iteration 14203 : loss: 0.045594, loss_a: 0.026820
[01:20:04.141] iteration 14204 : loss: 0.042411, loss_a: 0.024947
[01:20:05.496] iteration 14205 : loss: 0.024633, loss_a: 0.014490
[01:20:06.250] iteration 14206 : loss: 0.029268, loss_a: 0.017216
[01:20:07.587] iteration 14207 : loss: 0.023338, loss_a: 0.013728
[01:20:08.331] iteration 14208 : loss: 0.020700, loss_a: 0.012176
[01:20:09.701] iteration 14209 : loss: 0.024826, loss_a: 0.014603
[01:20:10.438] iteration 14210 : loss: 0.039687, loss_a: 0.023345
[01:20:11.809] iteration 14211 : loss: 0.021405, loss_a: 0.012591
[01:20:12.551] iteration 14212 : loss: 0.015377, loss_a: 0.009045
[01:20:13.890] iteration 14213 : loss: 0.056900, loss_a: 0.033471
[01:20:14.630] iteration 14214 : loss: 0.017047, loss_a: 0.010028
[01:20:15.978] iteration 14215 : loss: 0.045357, loss_a: 0.026681
[01:20:16.722] iteration 14216 : loss: 0.025388, loss_a: 0.014934
[01:20:18.052] iteration 14217 : loss: 0.029245, loss_a: 0.017203
[01:20:18.799] iteration 14218 : loss: 0.021006, loss_a: 0.012357
[01:20:20.140] iteration 14219 : loss: 0.031801, loss_a: 0.018706
[01:20:20.880] iteration 14220 : loss: 0.017756, loss_a: 0.010445
[01:20:22.230] iteration 14221 : loss: 0.015281, loss_a: 0.008989
[01:20:22.971] iteration 14222 : loss: 0.013682, loss_a: 0.008048
[01:20:24.302] iteration 14223 : loss: 0.018183, loss_a: 0.010696
[01:20:25.030] iteration 14224 : loss: 0.011696, loss_a: 0.006880
[01:20:26.361] iteration 14225 : loss: 0.029223, loss_a: 0.017190
[01:20:27.108] iteration 14226 : loss: 0.031747, loss_a: 0.018675
[01:20:28.479] iteration 14227 : loss: 0.036068, loss_a: 0.021216
[01:20:29.224] iteration 14228 : loss: 0.017076, loss_a: 0.010045
[01:20:30.549] iteration 14229 : loss: 0.019477, loss_a: 0.011457
[01:20:31.289] iteration 14230 : loss: 0.032398, loss_a: 0.019057
[01:20:32.622] iteration 14231 : loss: 0.058942, loss_a: 0.034672
[01:20:33.363] iteration 14232 : loss: 0.025003, loss_a: 0.014708
[01:20:34.714] iteration 14233 : loss: 0.012275, loss_a: 0.007221
[01:20:35.457] iteration 14234 : loss: 0.015832, loss_a: 0.009313
[01:20:36.804] iteration 14235 : loss: 0.038382, loss_a: 0.022578
[01:20:37.535] iteration 14236 : loss: 0.021948, loss_a: 0.012910
[01:20:38.894] iteration 14237 : loss: 0.014982, loss_a: 0.008813
[01:20:39.632] iteration 14238 : loss: 0.019764, loss_a: 0.011626
[01:20:40.981] iteration 14239 : loss: 0.028302, loss_a: 0.016648
[01:20:41.719] iteration 14240 : loss: 0.011811, loss_a: 0.006948
[01:20:43.038] iteration 14241 : loss: 0.028393, loss_a: 0.016702
[01:20:43.780] iteration 14242 : loss: 0.017613, loss_a: 0.010361
[01:20:45.138] iteration 14243 : loss: 0.029343, loss_a: 0.017261
[01:20:45.884] iteration 14244 : loss: 0.029827, loss_a: 0.017546
[01:20:47.203] iteration 14245 : loss: 0.016656, loss_a: 0.009797
[01:20:47.955] iteration 14246 : loss: 0.026888, loss_a: 0.015816
[01:20:49.268] iteration 14247 : loss: 0.031853, loss_a: 0.018737
[01:20:50.019] iteration 14248 : loss: 0.044478, loss_a: 0.026163
[01:20:51.339] iteration 14249 : loss: 0.015794, loss_a: 0.009291
[01:20:52.081] iteration 14250 : loss: 0.027860, loss_a: 0.016388
[01:20:53.383] iteration 14251 : loss: 0.018174, loss_a: 0.010690
[01:20:54.115] iteration 14252 : loss: 0.021205, loss_a: 0.012473
[01:20:55.445] iteration 14253 : loss: 0.048513, loss_a: 0.028537
[01:20:56.180] iteration 14254 : loss: 0.018404, loss_a: 0.010826
[01:20:57.510] iteration 14255 : loss: 0.029338, loss_a: 0.017258
[01:20:58.263] iteration 14256 : loss: 0.025754, loss_a: 0.015149
[01:20:59.621] iteration 14257 : loss: 0.033347, loss_a: 0.019616
[01:21:00.348] iteration 14258 : loss: 0.019348, loss_a: 0.011381
[01:21:01.686] iteration 14259 : loss: 0.029470, loss_a: 0.017335
[01:21:02.421] iteration 14260 : loss: 0.019083, loss_a: 0.011226
[01:21:03.778] iteration 14261 : loss: 0.022590, loss_a: 0.013289
[01:21:04.511] iteration 14262 : loss: 0.030122, loss_a: 0.017719
[01:21:05.848] iteration 14263 : loss: 0.018662, loss_a: 0.010978
[01:21:06.579] iteration 14264 : loss: 0.028952, loss_a: 0.017030
[01:21:07.942] iteration 14265 : loss: 0.025325, loss_a: 0.014897
[01:21:08.682] iteration 14266 : loss: 0.015182, loss_a: 0.008931
[01:21:10.039] iteration 14267 : loss: 0.037900, loss_a: 0.022294
[01:21:10.790] iteration 14268 : loss: 0.017309, loss_a: 0.010182
[01:21:12.177] iteration 14269 : loss: 0.041638, loss_a: 0.024493
[01:21:12.913] iteration 14270 : loss: 0.016402, loss_a: 0.009648
[01:21:14.270] iteration 14271 : loss: 0.027794, loss_a: 0.016349
[01:21:15.018] iteration 14272 : loss: 0.021265, loss_a: 0.012509
[01:21:16.345] iteration 14273 : loss: 0.022302, loss_a: 0.013119
[01:21:17.080] iteration 14274 : loss: 0.025131, loss_a: 0.014783
[01:21:18.454] iteration 14275 : loss: 0.103024, loss_a: 0.060603
[01:21:19.199] iteration 14276 : loss: 0.023523, loss_a: 0.013837
[01:21:20.522] iteration 14277 : loss: 0.014264, loss_a: 0.008391
[01:21:21.273] iteration 14278 : loss: 0.041038, loss_a: 0.024140
[01:21:22.617] iteration 14279 : loss: 0.022327, loss_a: 0.013133
[01:21:23.368] iteration 14280 : loss: 0.050353, loss_a: 0.029620
[01:21:24.732] iteration 14281 : loss: 0.020618, loss_a: 0.012128
[01:21:25.466] iteration 14282 : loss: 0.021681, loss_a: 0.012754
[01:21:26.780] iteration 14283 : loss: 0.027122, loss_a: 0.015954
[01:21:27.523] iteration 14284 : loss: 0.035455, loss_a: 0.020856
[01:21:28.861] iteration 14285 : loss: 0.015459, loss_a: 0.009094
[01:21:29.597] iteration 14286 : loss: 0.012234, loss_a: 0.007196
[01:21:30.946] iteration 14287 : loss: 0.028638, loss_a: 0.016846
[01:21:31.682] iteration 14288 : loss: 0.024395, loss_a: 0.014350
[01:21:33.035] iteration 14289 : loss: 0.049808, loss_a: 0.029299
[01:21:33.780] iteration 14290 : loss: 0.017060, loss_a: 0.010035
[01:21:35.118] iteration 14291 : loss: 0.042913, loss_a: 0.025243
[01:21:35.876] iteration 14292 : loss: 0.023958, loss_a: 0.014093
[01:21:37.228] iteration 14293 : loss: 0.044537, loss_a: 0.026198
[01:21:37.965] iteration 14294 : loss: 0.030097, loss_a: 0.017704
[01:21:39.318] iteration 14295 : loss: 0.022483, loss_a: 0.013225
[01:21:40.065] iteration 14296 : loss: 0.030538, loss_a: 0.017964
[01:21:41.393] iteration 14297 : loss: 0.020706, loss_a: 0.012180
[01:21:42.128] iteration 14298 : loss: 0.015343, loss_a: 0.009025
[01:21:43.485] iteration 14299 : loss: 0.027536, loss_a: 0.016197
[01:21:44.221] iteration 14300 : loss: 0.017994, loss_a: 0.010585
[01:21:45.551] iteration 14301 : loss: 0.014917, loss_a: 0.008775
[01:21:46.289] iteration 14302 : loss: 0.016242, loss_a: 0.009554
[01:21:47.629] iteration 14303 : loss: 0.012405, loss_a: 0.007297
[01:21:48.374] iteration 14304 : loss: 0.041220, loss_a: 0.024247
[01:21:49.734] iteration 14305 : loss: 0.029176, loss_a: 0.017163
[01:21:50.482] iteration 14306 : loss: 0.060069, loss_a: 0.035334
[01:21:51.838] iteration 14307 : loss: 0.031666, loss_a: 0.018627
[01:21:52.584] iteration 14308 : loss: 0.017731, loss_a: 0.010430
[01:21:53.968] iteration 14309 : loss: 0.017644, loss_a: 0.010379
[01:21:54.713] iteration 14310 : loss: 0.024091, loss_a: 0.014171
[01:21:56.078] iteration 14311 : loss: 0.047953, loss_a: 0.028208
[01:21:56.845] iteration 14312 : loss: 0.031606, loss_a: 0.018592
[01:21:58.177] iteration 14313 : loss: 0.011734, loss_a: 0.006902
[01:21:58.926] iteration 14314 : loss: 0.031343, loss_a: 0.018437
[01:22:00.258] iteration 14315 : loss: 0.014548, loss_a: 0.008558
[01:22:01.018] iteration 14316 : loss: 0.033805, loss_a: 0.019885
[01:22:02.344] iteration 14317 : loss: 0.026458, loss_a: 0.015563
[01:22:03.089] iteration 14318 : loss: 0.038602, loss_a: 0.022707
[01:22:04.448] iteration 14319 : loss: 0.030511, loss_a: 0.017948
[01:22:05.189] iteration 14320 : loss: 0.016943, loss_a: 0.009966
[01:22:06.520] iteration 14321 : loss: 0.013056, loss_a: 0.007680
[01:22:07.267] iteration 14322 : loss: 0.020909, loss_a: 0.012299
[01:22:08.594] iteration 14323 : loss: 0.018729, loss_a: 0.011017
[01:22:09.337] iteration 14324 : loss: 0.015850, loss_a: 0.009324
[01:22:10.693] iteration 14325 : loss: 0.015658, loss_a: 0.009211
[01:22:11.451] iteration 14326 : loss: 0.013528, loss_a: 0.007957
[01:22:12.803] iteration 14327 : loss: 0.031288, loss_a: 0.018405
[01:22:13.538] iteration 14328 : loss: 0.020595, loss_a: 0.012115
[01:22:14.887] iteration 14329 : loss: 0.019481, loss_a: 0.011460
[01:22:15.634] iteration 14330 : loss: 0.022710, loss_a: 0.013359
[01:22:16.955] iteration 14331 : loss: 0.062490, loss_a: 0.036759
[01:22:17.701] iteration 14332 : loss: 0.020345, loss_a: 0.011968
[01:22:19.066] iteration 14333 : loss: 0.031728, loss_a: 0.018663
[01:22:19.810] iteration 14334 : loss: 0.024065, loss_a: 0.014156
[01:22:21.162] iteration 14335 : loss: 0.026702, loss_a: 0.015707
[01:22:21.901] iteration 14336 : loss: 0.027548, loss_a: 0.016205
[01:22:23.264] iteration 14337 : loss: 0.022834, loss_a: 0.013432
[01:22:23.996] iteration 14338 : loss: 0.049601, loss_a: 0.029177
[01:22:25.331] iteration 14339 : loss: 0.055366, loss_a: 0.032569
[01:22:26.068] iteration 14340 : loss: 0.021138, loss_a: 0.012434
[01:22:27.423] iteration 14341 : loss: 0.012696, loss_a: 0.007468
[01:22:28.161] iteration 14342 : loss: 0.034518, loss_a: 0.020305
[01:22:29.519] iteration 14343 : loss: 0.053235, loss_a: 0.031314
[01:22:30.252] iteration 14344 : loss: 0.016365, loss_a: 0.009627
[01:22:31.570] iteration 14345 : loss: 0.018456, loss_a: 0.010856
[01:22:32.308] iteration 14346 : loss: 0.027066, loss_a: 0.015921
[01:22:33.630] iteration 14347 : loss: 0.022451, loss_a: 0.013207
[01:22:34.380] iteration 14348 : loss: 0.018642, loss_a: 0.010966
[01:22:35.747] iteration 14349 : loss: 0.052206, loss_a: 0.030710
[01:22:36.485] iteration 14350 : loss: 0.009668, loss_a: 0.005687
[01:22:37.816] iteration 14351 : loss: 0.041161, loss_a: 0.024213
[01:22:38.547] iteration 14352 : loss: 0.017308, loss_a: 0.010181
[01:22:39.882] iteration 14353 : loss: 0.029541, loss_a: 0.017377
[01:22:40.638] iteration 14354 : loss: 0.034064, loss_a: 0.020038
[01:22:41.988] iteration 14355 : loss: 0.019886, loss_a: 0.011698
[01:22:42.714] iteration 14356 : loss: 0.028292, loss_a: 0.016642
[01:22:44.034] iteration 14357 : loss: 0.054037, loss_a: 0.031787
[01:22:44.766] iteration 14358 : loss: 0.029907, loss_a: 0.017592
[01:22:46.141] iteration 14359 : loss: 0.025364, loss_a: 0.014920
[01:22:46.889] iteration 14360 : loss: 0.050307, loss_a: 0.029592
[01:22:48.248] iteration 14361 : loss: 0.023019, loss_a: 0.013541
[01:22:48.989] iteration 14362 : loss: 0.022117, loss_a: 0.013010
[01:22:50.355] iteration 14363 : loss: 0.023054, loss_a: 0.013561
[01:22:51.099] iteration 14364 : loss: 0.021288, loss_a: 0.012522
[01:22:52.451] iteration 14365 : loss: 0.018548, loss_a: 0.010910
[01:22:53.183] iteration 14366 : loss: 0.020766, loss_a: 0.012215
[01:22:54.493] iteration 14367 : loss: 0.013156, loss_a: 0.007739
[01:22:55.236] iteration 14368 : loss: 0.039737, loss_a: 0.023375
[01:22:56.600] iteration 14369 : loss: 0.020841, loss_a: 0.012259
[01:22:57.355] iteration 14370 : loss: 0.039851, loss_a: 0.023442
[01:22:58.679] iteration 14371 : loss: 0.021546, loss_a: 0.012674
[01:22:59.429] iteration 14372 : loss: 0.015501, loss_a: 0.009118
[01:23:00.757] iteration 14373 : loss: 0.015343, loss_a: 0.009025
[01:23:01.497] iteration 14374 : loss: 0.016458, loss_a: 0.009681
[01:23:02.851] iteration 14375 : loss: 0.014034, loss_a: 0.008255
[01:23:03.585] iteration 14376 : loss: 0.028237, loss_a: 0.016610
[01:23:04.937] iteration 14377 : loss: 0.036536, loss_a: 0.021492
[01:23:05.683] iteration 14378 : loss: 0.011988, loss_a: 0.007052
[01:23:07.067] iteration 14379 : loss: 0.064743, loss_a: 0.038084
[01:23:07.815] iteration 14380 : loss: 0.042800, loss_a: 0.025176
[01:23:09.164] iteration 14381 : loss: 0.030370, loss_a: 0.017864
[01:23:09.906] iteration 14382 : loss: 0.015844, loss_a: 0.009320
[01:23:11.223] iteration 14383 : loss: 0.025577, loss_a: 0.015045
[01:23:11.981] iteration 14384 : loss: 0.031454, loss_a: 0.018502
[01:23:13.336] iteration 14385 : loss: 0.019027, loss_a: 0.011192
[01:23:14.088] iteration 14386 : loss: 0.023030, loss_a: 0.013547
[01:23:15.436] iteration 14387 : loss: 0.031232, loss_a: 0.018372
[01:23:16.175] iteration 14388 : loss: 0.016742, loss_a: 0.009848
[01:23:17.502] iteration 14389 : loss: 0.023643, loss_a: 0.013907
[01:23:18.246] iteration 14390 : loss: 0.019923, loss_a: 0.011719
[01:23:19.585] iteration 14391 : loss: 0.019592, loss_a: 0.011525
[01:23:20.323] iteration 14392 : loss: 0.012603, loss_a: 0.007413
[01:23:21.685] iteration 14393 : loss: 0.051864, loss_a: 0.030508
[01:23:22.427] iteration 14394 : loss: 0.016715, loss_a: 0.009832
[01:23:23.763] iteration 14395 : loss: 0.042302, loss_a: 0.024883
[01:23:24.508] iteration 14396 : loss: 0.022695, loss_a: 0.013350
[01:23:25.853] iteration 14397 : loss: 0.020922, loss_a: 0.012307
[01:23:26.598] iteration 14398 : loss: 0.036545, loss_a: 0.021497
[01:23:27.911] iteration 14399 : loss: 0.015224, loss_a: 0.008955
[01:23:28.672] iteration 14400 : loss: 0.039187, loss_a: 0.023051
[01:23:52.200] save best model to /home/vigil/Desktop/BCP-main/model/BCP/LA_label8_win+3loss-superfluous_8_labeled/self_train/iter_14400_dice_0.9017.pth
[01:23:53.519] iteration 14401 : loss: 0.032058, loss_a: 0.018858
[01:23:55.671] iteration 14402 : loss: 0.040590, loss_a: 0.023876
[01:23:57.018] iteration 14403 : loss: 0.039557, loss_a: 0.023269
[01:23:57.772] iteration 14404 : loss: 0.050719, loss_a: 0.029835
[01:23:59.121] iteration 14405 : loss: 0.014327, loss_a: 0.008427
[01:23:59.857] iteration 14406 : loss: 0.013449, loss_a: 0.007911
[01:24:01.217] iteration 14407 : loss: 0.023159, loss_a: 0.013623
[01:24:01.958] iteration 14408 : loss: 0.016000, loss_a: 0.009412
[01:24:03.274] iteration 14409 : loss: 0.025663, loss_a: 0.015096
[01:24:04.021] iteration 14410 : loss: 0.040809, loss_a: 0.024005
[01:24:05.399] iteration 14411 : loss: 0.039924, loss_a: 0.023485
[01:24:06.146] iteration 14412 : loss: 0.028265, loss_a: 0.016626
[01:24:07.512] iteration 14413 : loss: 0.055733, loss_a: 0.032784
[01:24:08.256] iteration 14414 : loss: 0.017225, loss_a: 0.010132
[01:24:09.628] iteration 14415 : loss: 0.032811, loss_a: 0.019300
[01:24:10.378] iteration 14416 : loss: 0.026271, loss_a: 0.015454
[01:24:11.738] iteration 14417 : loss: 0.028862, loss_a: 0.016978
[01:24:12.481] iteration 14418 : loss: 0.021040, loss_a: 0.012376
[01:24:13.819] iteration 14419 : loss: 0.036103, loss_a: 0.021237
[01:24:14.559] iteration 14420 : loss: 0.019263, loss_a: 0.011331
[01:24:15.896] iteration 14421 : loss: 0.020697, loss_a: 0.012175
[01:24:16.633] iteration 14422 : loss: 0.048201, loss_a: 0.028354
[01:24:17.987] iteration 14423 : loss: 0.024664, loss_a: 0.014508
[01:24:18.742] iteration 14424 : loss: 0.049171, loss_a: 0.028924
[01:24:20.074] iteration 14425 : loss: 0.025181, loss_a: 0.014812
[01:24:20.822] iteration 14426 : loss: 0.028291, loss_a: 0.016642
[01:24:22.181] iteration 14427 : loss: 0.051808, loss_a: 0.030475
[01:24:22.923] iteration 14428 : loss: 0.019370, loss_a: 0.011394
[01:24:24.248] iteration 14429 : loss: 0.021412, loss_a: 0.012596
[01:24:25.005] iteration 14430 : loss: 0.020020, loss_a: 0.011776
[01:24:26.373] iteration 14431 : loss: 0.022292, loss_a: 0.013113
[01:24:27.111] iteration 14432 : loss: 0.042170, loss_a: 0.024806
[01:24:28.433] iteration 14433 : loss: 0.015395, loss_a: 0.009056
[01:24:29.171] iteration 14434 : loss: 0.026971, loss_a: 0.015865
[01:24:30.524] iteration 14435 : loss: 0.072521, loss_a: 0.042660
[01:24:31.266] iteration 14436 : loss: 0.041864, loss_a: 0.024626
[01:24:32.632] iteration 14437 : loss: 0.033654, loss_a: 0.019797
[01:24:33.366] iteration 14438 : loss: 0.013779, loss_a: 0.008105
[01:24:34.701] iteration 14439 : loss: 0.037858, loss_a: 0.022270
[01:24:35.433] iteration 14440 : loss: 0.008819, loss_a: 0.005188
[01:24:36.798] iteration 14441 : loss: 0.028197, loss_a: 0.016586
[01:24:37.538] iteration 14442 : loss: 0.032290, loss_a: 0.018994
[01:24:38.858] iteration 14443 : loss: 0.020191, loss_a: 0.011877
[01:24:39.597] iteration 14444 : loss: 0.042244, loss_a: 0.024849
[01:24:40.966] iteration 14445 : loss: 0.018572, loss_a: 0.010925
[01:24:41.699] iteration 14446 : loss: 0.014416, loss_a: 0.008480
[01:24:43.022] iteration 14447 : loss: 0.036949, loss_a: 0.021735
[01:24:43.760] iteration 14448 : loss: 0.014740, loss_a: 0.008671
[01:24:45.122] iteration 14449 : loss: 0.027198, loss_a: 0.015999
[01:24:45.862] iteration 14450 : loss: 0.027060, loss_a: 0.015917
[01:24:47.188] iteration 14451 : loss: 0.043175, loss_a: 0.025397
[01:24:47.935] iteration 14452 : loss: 0.048928, loss_a: 0.028781
[01:24:49.314] iteration 14453 : loss: 0.034400, loss_a: 0.020235
[01:24:50.061] iteration 14454 : loss: 0.021322, loss_a: 0.012543
[01:24:51.384] iteration 14455 : loss: 0.015260, loss_a: 0.008976
[01:24:52.126] iteration 14456 : loss: 0.031373, loss_a: 0.018455
[01:24:53.454] iteration 14457 : loss: 0.029450, loss_a: 0.017324
[01:24:54.205] iteration 14458 : loss: 0.035713, loss_a: 0.021008
[01:24:55.538] iteration 14459 : loss: 0.027960, loss_a: 0.016447
[01:24:56.277] iteration 14460 : loss: 0.016175, loss_a: 0.009515
[01:24:57.640] iteration 14461 : loss: 0.026887, loss_a: 0.015816
[01:24:58.399] iteration 14462 : loss: 0.030814, loss_a: 0.018126
[01:24:59.732] iteration 14463 : loss: 0.038915, loss_a: 0.022891
[01:25:00.478] iteration 14464 : loss: 0.015351, loss_a: 0.009030
[01:25:01.851] iteration 14465 : loss: 0.017891, loss_a: 0.010524
[01:25:02.600] iteration 14466 : loss: 0.037757, loss_a: 0.022210
[01:25:03.922] iteration 14467 : loss: 0.013221, loss_a: 0.007777
[01:25:04.663] iteration 14468 : loss: 0.035331, loss_a: 0.020783
[01:25:06.024] iteration 14469 : loss: 0.023574, loss_a: 0.013867
[01:25:06.764] iteration 14470 : loss: 0.021391, loss_a: 0.012583
[01:25:08.130] iteration 14471 : loss: 0.037365, loss_a: 0.021980
[01:25:08.876] iteration 14472 : loss: 0.054895, loss_a: 0.032291
[01:25:10.238] iteration 14473 : loss: 0.032312, loss_a: 0.019007
[01:25:10.982] iteration 14474 : loss: 0.022450, loss_a: 0.013206
[01:25:12.331] iteration 14475 : loss: 0.015010, loss_a: 0.008829
[01:25:13.062] iteration 14476 : loss: 0.021581, loss_a: 0.012695
[01:25:14.406] iteration 14477 : loss: 0.016248, loss_a: 0.009558
[01:25:15.148] iteration 14478 : loss: 0.032339, loss_a: 0.019023
[01:25:16.487] iteration 14479 : loss: 0.027569, loss_a: 0.016217
[01:25:17.235] iteration 14480 : loss: 0.018028, loss_a: 0.010605
[01:25:18.557] iteration 14481 : loss: 0.013322, loss_a: 0.007837
[01:25:19.296] iteration 14482 : loss: 0.020373, loss_a: 0.011984
[01:25:20.624] iteration 14483 : loss: 0.035497, loss_a: 0.020881
[01:25:21.369] iteration 14484 : loss: 0.021113, loss_a: 0.012419
[01:25:22.694] iteration 14485 : loss: 0.017452, loss_a: 0.010266
[01:25:23.440] iteration 14486 : loss: 0.016440, loss_a: 0.009671
[01:25:24.778] iteration 14487 : loss: 0.018640, loss_a: 0.010965
[01:25:25.527] iteration 14488 : loss: 0.021295, loss_a: 0.012526
[01:25:26.888] iteration 14489 : loss: 0.017259, loss_a: 0.010152
[01:25:27.625] iteration 14490 : loss: 0.012104, loss_a: 0.007120
[01:25:28.966] iteration 14491 : loss: 0.053438, loss_a: 0.031434
[01:25:29.708] iteration 14492 : loss: 0.036380, loss_a: 0.021400
[01:25:31.073] iteration 14493 : loss: 0.039817, loss_a: 0.023422
[01:25:31.807] iteration 14494 : loss: 0.056110, loss_a: 0.033006
[01:25:33.159] iteration 14495 : loss: 0.022394, loss_a: 0.013173
[01:25:33.898] iteration 14496 : loss: 0.042124, loss_a: 0.024779
[01:25:35.221] iteration 14497 : loss: 0.011394, loss_a: 0.006702
[01:25:35.968] iteration 14498 : loss: 0.013750, loss_a: 0.008088
[01:25:37.319] iteration 14499 : loss: 0.019747, loss_a: 0.011616
[01:25:38.052] iteration 14500 : loss: 0.014338, loss_a: 0.008434
[01:25:39.414] iteration 14501 : loss: 0.022317, loss_a: 0.013128
[01:25:40.151] iteration 14502 : loss: 0.019362, loss_a: 0.011389
[01:25:41.497] iteration 14503 : loss: 0.019940, loss_a: 0.011730
[01:25:42.228] iteration 14504 : loss: 0.027388, loss_a: 0.016111
[01:25:43.583] iteration 14505 : loss: 0.012805, loss_a: 0.007532
[01:25:44.329] iteration 14506 : loss: 0.019514, loss_a: 0.011479
[01:25:45.683] iteration 14507 : loss: 0.017786, loss_a: 0.010462
[01:25:46.433] iteration 14508 : loss: 0.034050, loss_a: 0.020029
[01:25:47.792] iteration 14509 : loss: 0.022288, loss_a: 0.013110
[01:25:48.549] iteration 14510 : loss: 0.041265, loss_a: 0.024273
[01:25:49.909] iteration 14511 : loss: 0.025185, loss_a: 0.014814
[01:25:50.641] iteration 14512 : loss: 0.019400, loss_a: 0.011412
[01:25:52.015] iteration 14513 : loss: 0.050432, loss_a: 0.029666
[01:25:52.764] iteration 14514 : loss: 0.048287, loss_a: 0.028404
[01:25:54.088] iteration 14515 : loss: 0.052280, loss_a: 0.030753
[01:25:54.830] iteration 14516 : loss: 0.050293, loss_a: 0.029584
[01:25:56.186] iteration 14517 : loss: 0.026241, loss_a: 0.015436
[01:25:56.931] iteration 14518 : loss: 0.017503, loss_a: 0.010296
[01:25:58.293] iteration 14519 : loss: 0.037854, loss_a: 0.022267
[01:25:59.037] iteration 14520 : loss: 0.013853, loss_a: 0.008149
[01:26:00.401] iteration 14521 : loss: 0.032734, loss_a: 0.019255
[01:26:01.138] iteration 14522 : loss: 0.012677, loss_a: 0.007457
[01:26:02.496] iteration 14523 : loss: 0.034090, loss_a: 0.020053
[01:26:03.236] iteration 14524 : loss: 0.046955, loss_a: 0.027620
[01:26:04.579] iteration 14525 : loss: 0.030225, loss_a: 0.017779
[01:26:05.324] iteration 14526 : loss: 0.031786, loss_a: 0.018698
[01:26:06.685] iteration 14527 : loss: 0.019132, loss_a: 0.011254
[01:26:07.423] iteration 14528 : loss: 0.024042, loss_a: 0.014142
[01:26:08.782] iteration 14529 : loss: 0.013281, loss_a: 0.007812
[01:26:09.521] iteration 14530 : loss: 0.033708, loss_a: 0.019828
[01:26:10.856] iteration 14531 : loss: 0.014818, loss_a: 0.008716
[01:26:11.604] iteration 14532 : loss: 0.024624, loss_a: 0.014485
[01:26:12.953] iteration 14533 : loss: 0.048567, loss_a: 0.028569
[01:26:13.699] iteration 14534 : loss: 0.014921, loss_a: 0.008777
[01:26:15.034] iteration 14535 : loss: 0.011559, loss_a: 0.006800
[01:26:15.783] iteration 14536 : loss: 0.044870, loss_a: 0.026394
[01:26:17.101] iteration 14537 : loss: 0.022826, loss_a: 0.013427
[01:26:17.854] iteration 14538 : loss: 0.031821, loss_a: 0.018718
[01:26:19.208] iteration 14539 : loss: 0.019963, loss_a: 0.011743
[01:26:19.945] iteration 14540 : loss: 0.066321, loss_a: 0.039012
[01:26:21.304] iteration 14541 : loss: 0.023072, loss_a: 0.013572
[01:26:22.045] iteration 14542 : loss: 0.022935, loss_a: 0.013491
[01:26:23.378] iteration 14543 : loss: 0.017374, loss_a: 0.010220
[01:26:24.118] iteration 14544 : loss: 0.014820, loss_a: 0.008718
[01:26:25.441] iteration 14545 : loss: 0.016572, loss_a: 0.009748
[01:26:26.192] iteration 14546 : loss: 0.032057, loss_a: 0.018857
[01:26:27.565] iteration 14547 : loss: 0.044766, loss_a: 0.026333
[01:26:28.301] iteration 14548 : loss: 0.018983, loss_a: 0.011167
[01:26:29.670] iteration 14549 : loss: 0.047714, loss_a: 0.028067
[01:26:30.408] iteration 14550 : loss: 0.019005, loss_a: 0.011179
[01:26:31.762] iteration 14551 : loss: 0.022421, loss_a: 0.013189
[01:26:32.504] iteration 14552 : loss: 0.050461, loss_a: 0.029683
[01:26:33.865] iteration 14553 : loss: 0.018527, loss_a: 0.010898
[01:26:34.602] iteration 14554 : loss: 0.025633, loss_a: 0.015078
[01:26:35.968] iteration 14555 : loss: 0.021030, loss_a: 0.012371
[01:26:36.710] iteration 14556 : loss: 0.026989, loss_a: 0.015876
[01:26:38.086] iteration 14557 : loss: 0.021006, loss_a: 0.012356
[01:26:38.822] iteration 14558 : loss: 0.031824, loss_a: 0.018720
[01:26:40.148] iteration 14559 : loss: 0.030498, loss_a: 0.017940
[01:26:40.888] iteration 14560 : loss: 0.019567, loss_a: 0.011510
[01:26:42.227] iteration 14561 : loss: 0.050044, loss_a: 0.029438
[01:26:42.966] iteration 14562 : loss: 0.020083, loss_a: 0.011814
[01:26:44.302] iteration 14563 : loss: 0.023964, loss_a: 0.014097
[01:26:45.035] iteration 14564 : loss: 0.012285, loss_a: 0.007226
[01:26:46.389] iteration 14565 : loss: 0.032193, loss_a: 0.018937
[01:26:47.139] iteration 14566 : loss: 0.053724, loss_a: 0.031602
[01:26:48.459] iteration 14567 : loss: 0.030248, loss_a: 0.017793
[01:26:49.191] iteration 14568 : loss: 0.019802, loss_a: 0.011648
[01:26:50.535] iteration 14569 : loss: 0.017589, loss_a: 0.010347
[01:26:51.285] iteration 14570 : loss: 0.029287, loss_a: 0.017227
[01:26:52.654] iteration 14571 : loss: 0.012697, loss_a: 0.007469
[01:26:53.393] iteration 14572 : loss: 0.014815, loss_a: 0.008715
[01:26:54.730] iteration 14573 : loss: 0.079759, loss_a: 0.046917
[01:26:55.470] iteration 14574 : loss: 0.025439, loss_a: 0.014964
[01:26:56.805] iteration 14575 : loss: 0.018399, loss_a: 0.010823
[01:26:57.551] iteration 14576 : loss: 0.014097, loss_a: 0.008292
[01:26:58.869] iteration 14577 : loss: 0.021252, loss_a: 0.012501
[01:26:59.606] iteration 14578 : loss: 0.023316, loss_a: 0.013715
[01:27:00.964] iteration 14579 : loss: 0.026956, loss_a: 0.015856
[01:27:01.713] iteration 14580 : loss: 0.024887, loss_a: 0.014639
[01:27:03.040] iteration 14581 : loss: 0.022021, loss_a: 0.012953
[01:27:03.783] iteration 14582 : loss: 0.022882, loss_a: 0.013460
[01:27:05.133] iteration 14583 : loss: 0.029996, loss_a: 0.017645
[01:27:05.872] iteration 14584 : loss: 0.022683, loss_a: 0.013343
[01:27:07.200] iteration 14585 : loss: 0.018594, loss_a: 0.010938
[01:27:07.956] iteration 14586 : loss: 0.018661, loss_a: 0.010977
[01:27:09.320] iteration 14587 : loss: 0.020591, loss_a: 0.012112
[01:27:10.062] iteration 14588 : loss: 0.022577, loss_a: 0.013281
[01:27:11.404] iteration 14589 : loss: 0.057357, loss_a: 0.033739
[01:27:12.161] iteration 14590 : loss: 0.078305, loss_a: 0.046062
[01:27:13.474] iteration 14591 : loss: 0.027564, loss_a: 0.016214
[01:27:14.214] iteration 14592 : loss: 0.036464, loss_a: 0.021450
[01:27:15.588] iteration 14593 : loss: 0.040930, loss_a: 0.024077
[01:27:16.325] iteration 14594 : loss: 0.014916, loss_a: 0.008774
[01:27:17.656] iteration 14595 : loss: 0.036943, loss_a: 0.021731
[01:27:18.394] iteration 14596 : loss: 0.016131, loss_a: 0.009489
[01:27:19.770] iteration 14597 : loss: 0.023797, loss_a: 0.013998
[01:27:20.507] iteration 14598 : loss: 0.028928, loss_a: 0.017017
[01:27:21.837] iteration 14599 : loss: 0.057086, loss_a: 0.033580
[01:27:22.583] iteration 14600 : loss: 0.021914, loss_a: 0.012891
[01:27:47.209] iteration 14601 : loss: 0.024734, loss_a: 0.014550
[01:27:49.348] iteration 14602 : loss: 0.022697, loss_a: 0.013351
[01:27:50.703] iteration 14603 : loss: 0.014505, loss_a: 0.008532
[01:27:51.456] iteration 14604 : loss: 0.053882, loss_a: 0.031695
[01:27:52.775] iteration 14605 : loss: 0.012204, loss_a: 0.007179
[01:27:53.519] iteration 14606 : loss: 0.013643, loss_a: 0.008025
[01:27:54.872] iteration 14607 : loss: 0.030815, loss_a: 0.018127
[01:27:55.614] iteration 14608 : loss: 0.029657, loss_a: 0.017445
[01:27:56.940] iteration 14609 : loss: 0.025923, loss_a: 0.015249
[01:27:57.690] iteration 14610 : loss: 0.020855, loss_a: 0.012268
[01:27:59.015] iteration 14611 : loss: 0.018241, loss_a: 0.010730
[01:27:59.764] iteration 14612 : loss: 0.027595, loss_a: 0.016232
[01:28:01.125] iteration 14613 : loss: 0.020292, loss_a: 0.011936
[01:28:01.863] iteration 14614 : loss: 0.009840, loss_a: 0.005788
[01:28:03.200] iteration 14615 : loss: 0.044348, loss_a: 0.026087
[01:28:03.943] iteration 14616 : loss: 0.022976, loss_a: 0.013515
[01:28:05.309] iteration 14617 : loss: 0.025311, loss_a: 0.014889
[01:28:06.051] iteration 14618 : loss: 0.019148, loss_a: 0.011264
[01:28:07.406] iteration 14619 : loss: 0.023305, loss_a: 0.013709
[01:28:08.154] iteration 14620 : loss: 0.013223, loss_a: 0.007778
[01:28:09.470] iteration 14621 : loss: 0.035559, loss_a: 0.020917
[01:28:10.212] iteration 14622 : loss: 0.016118, loss_a: 0.009481
[01:28:11.547] iteration 14623 : loss: 0.016014, loss_a: 0.009420
[01:28:12.284] iteration 14624 : loss: 0.013021, loss_a: 0.007660
[01:28:13.641] iteration 14625 : loss: 0.042601, loss_a: 0.025060
[01:28:14.389] iteration 14626 : loss: 0.032033, loss_a: 0.018843
[01:28:15.712] iteration 14627 : loss: 0.026399, loss_a: 0.015529
[01:28:16.455] iteration 14628 : loss: 0.022192, loss_a: 0.013054
[01:28:17.784] iteration 14629 : loss: 0.020754, loss_a: 0.012208
[01:28:18.535] iteration 14630 : loss: 0.054044, loss_a: 0.031790
[01:28:19.913] iteration 14631 : loss: 0.015488, loss_a: 0.009111
[01:28:20.656] iteration 14632 : loss: 0.018613, loss_a: 0.010949
[01:28:21.997] iteration 14633 : loss: 0.024056, loss_a: 0.014151
[01:28:22.742] iteration 14634 : loss: 0.025566, loss_a: 0.015039
[01:28:24.101] iteration 14635 : loss: 0.072662, loss_a: 0.042742
[01:28:24.846] iteration 14636 : loss: 0.024608, loss_a: 0.014475
[01:28:26.217] iteration 14637 : loss: 0.029035, loss_a: 0.017079
[01:28:26.964] iteration 14638 : loss: 0.012359, loss_a: 0.007270
[01:28:28.305] iteration 14639 : loss: 0.025412, loss_a: 0.014949
[01:28:29.033] iteration 14640 : loss: 0.020035, loss_a: 0.011786
[01:28:30.385] iteration 14641 : loss: 0.015858, loss_a: 0.009328
[01:28:31.140] iteration 14642 : loss: 0.037582, loss_a: 0.022107
[01:28:32.479] iteration 14643 : loss: 0.026117, loss_a: 0.015363
[01:28:33.228] iteration 14644 : loss: 0.014435, loss_a: 0.008491
[01:28:34.578] iteration 14645 : loss: 0.034251, loss_a: 0.020148
[01:28:35.336] iteration 14646 : loss: 0.059463, loss_a: 0.034978
[01:28:36.679] iteration 14647 : loss: 0.031862, loss_a: 0.018743
[01:28:37.434] iteration 14648 : loss: 0.034506, loss_a: 0.020298
[01:28:38.821] iteration 14649 : loss: 0.048607, loss_a: 0.028592
[01:28:39.571] iteration 14650 : loss: 0.024867, loss_a: 0.014628
[01:28:40.939] iteration 14651 : loss: 0.022034, loss_a: 0.012961
[01:28:41.671] iteration 14652 : loss: 0.017261, loss_a: 0.010153
[01:28:42.996] iteration 14653 : loss: 0.034533, loss_a: 0.020314
[01:28:43.743] iteration 14654 : loss: 0.043604, loss_a: 0.025650
[01:28:45.097] iteration 14655 : loss: 0.022473, loss_a: 0.013219
[01:28:45.848] iteration 14656 : loss: 0.051072, loss_a: 0.030042
[01:28:47.155] iteration 14657 : loss: 0.030352, loss_a: 0.017854
[01:28:47.896] iteration 14658 : loss: 0.024580, loss_a: 0.014459
[01:28:49.254] iteration 14659 : loss: 0.019049, loss_a: 0.011206
[01:28:49.991] iteration 14660 : loss: 0.027893, loss_a: 0.016408
[01:28:51.308] iteration 14661 : loss: 0.033012, loss_a: 0.019419
[01:28:52.052] iteration 14662 : loss: 0.028404, loss_a: 0.016708
[01:28:53.404] iteration 14663 : loss: 0.016300, loss_a: 0.009588
[01:28:54.151] iteration 14664 : loss: 0.021104, loss_a: 0.012414
[01:28:55.493] iteration 14665 : loss: 0.018202, loss_a: 0.010707
[01:28:56.247] iteration 14666 : loss: 0.016457, loss_a: 0.009681
[01:28:57.601] iteration 14667 : loss: 0.015494, loss_a: 0.009114
[01:28:58.346] iteration 14668 : loss: 0.023319, loss_a: 0.013717
[01:28:59.700] iteration 14669 : loss: 0.014086, loss_a: 0.008286
[01:29:00.437] iteration 14670 : loss: 0.027493, loss_a: 0.016172
[01:29:01.771] iteration 14671 : loss: 0.024167, loss_a: 0.014216
[01:29:02.530] iteration 14672 : loss: 0.054509, loss_a: 0.032064
[01:29:03.885] iteration 14673 : loss: 0.013731, loss_a: 0.008077
[01:29:04.631] iteration 14674 : loss: 0.015410, loss_a: 0.009064
[01:29:05.963] iteration 14675 : loss: 0.072359, loss_a: 0.042564
[01:29:06.727] iteration 14676 : loss: 0.034407, loss_a: 0.020239
[01:29:08.049] iteration 14677 : loss: 0.032162, loss_a: 0.018919
[01:29:08.798] iteration 14678 : loss: 0.037925, loss_a: 0.022309
[01:29:10.150] iteration 14679 : loss: 0.022577, loss_a: 0.013281
[01:29:10.883] iteration 14680 : loss: 0.022837, loss_a: 0.013433
[01:29:12.236] iteration 14681 : loss: 0.044297, loss_a: 0.026057
[01:29:12.983] iteration 14682 : loss: 0.030858, loss_a: 0.018151
[01:29:14.301] iteration 14683 : loss: 0.029586, loss_a: 0.017403
[01:29:15.040] iteration 14684 : loss: 0.013426, loss_a: 0.007898
[01:29:16.416] iteration 14685 : loss: 0.032122, loss_a: 0.018895
[01:29:17.159] iteration 14686 : loss: 0.023100, loss_a: 0.013589
[01:29:18.498] iteration 14687 : loss: 0.029476, loss_a: 0.017339
[01:29:19.235] iteration 14688 : loss: 0.014618, loss_a: 0.008599
[01:29:20.603] iteration 14689 : loss: 0.034508, loss_a: 0.020299
[01:29:21.346] iteration 14690 : loss: 0.012861, loss_a: 0.007566
[01:29:22.678] iteration 14691 : loss: 0.027313, loss_a: 0.016066
[01:29:23.426] iteration 14692 : loss: 0.028069, loss_a: 0.016511
[01:29:24.773] iteration 14693 : loss: 0.050153, loss_a: 0.029502
[01:29:25.529] iteration 14694 : loss: 0.039928, loss_a: 0.023487
[01:29:26.849] iteration 14695 : loss: 0.020756, loss_a: 0.012209
[01:29:27.612] iteration 14696 : loss: 0.033360, loss_a: 0.019623
[01:29:28.928] iteration 14697 : loss: 0.019547, loss_a: 0.011498
[01:29:29.667] iteration 14698 : loss: 0.023870, loss_a: 0.014041
[01:29:31.008] iteration 14699 : loss: 0.018894, loss_a: 0.011114
[01:29:31.753] iteration 14700 : loss: 0.026845, loss_a: 0.015791
[01:29:33.074] iteration 14701 : loss: 0.030231, loss_a: 0.017783
[01:29:33.820] iteration 14702 : loss: 0.034644, loss_a: 0.020379
[01:29:35.163] iteration 14703 : loss: 0.056708, loss_a: 0.033358
[01:29:35.914] iteration 14704 : loss: 0.016890, loss_a: 0.009935
[01:29:37.275] iteration 14705 : loss: 0.023460, loss_a: 0.013800
[01:29:38.013] iteration 14706 : loss: 0.033062, loss_a: 0.019448
[01:29:39.338] iteration 14707 : loss: 0.019845, loss_a: 0.011674
[01:29:40.075] iteration 14708 : loss: 0.027535, loss_a: 0.016197
[01:29:41.407] iteration 14709 : loss: 0.036832, loss_a: 0.021666
[01:29:42.140] iteration 14710 : loss: 0.017114, loss_a: 0.010067
[01:29:43.473] iteration 14711 : loss: 0.037832, loss_a: 0.022254
[01:29:44.207] iteration 14712 : loss: 0.009671, loss_a: 0.005689
[01:29:45.559] iteration 14713 : loss: 0.048557, loss_a: 0.028563
[01:29:46.318] iteration 14714 : loss: 0.022301, loss_a: 0.013118
[01:29:47.662] iteration 14715 : loss: 0.017786, loss_a: 0.010463
[01:29:48.411] iteration 14716 : loss: 0.019141, loss_a: 0.011259
[01:29:49.733] iteration 14717 : loss: 0.013051, loss_a: 0.007677
[01:29:50.479] iteration 14718 : loss: 0.027225, loss_a: 0.016015
[01:29:51.844] iteration 14719 : loss: 0.019862, loss_a: 0.011684
[01:29:52.582] iteration 14720 : loss: 0.032126, loss_a: 0.018898
[01:29:53.901] iteration 14721 : loss: 0.011817, loss_a: 0.006951
[01:29:54.659] iteration 14722 : loss: 0.027169, loss_a: 0.015982
[01:29:56.024] iteration 14723 : loss: 0.038521, loss_a: 0.022659
[01:29:56.772] iteration 14724 : loss: 0.019774, loss_a: 0.011631
[01:29:58.131] iteration 14725 : loss: 0.011038, loss_a: 0.006493
[01:29:58.867] iteration 14726 : loss: 0.016991, loss_a: 0.009995
[01:30:00.239] iteration 14727 : loss: 0.086773, loss_a: 0.051043
[01:30:00.981] iteration 14728 : loss: 0.019908, loss_a: 0.011711
[01:30:02.333] iteration 14729 : loss: 0.017272, loss_a: 0.010160
[01:30:03.068] iteration 14730 : loss: 0.037654, loss_a: 0.022150
[01:30:04.379] iteration 14731 : loss: 0.011610, loss_a: 0.006830
[01:30:05.149] iteration 14732 : loss: 0.503843, loss_a: 0.296378
[01:30:06.505] iteration 14733 : loss: 0.028014, loss_a: 0.016479
[01:30:07.241] iteration 14734 : loss: 0.014312, loss_a: 0.008419
[01:30:08.589] iteration 14735 : loss: 0.031837, loss_a: 0.018727
[01:30:09.343] iteration 14736 : loss: 0.021372, loss_a: 0.012572
[01:30:10.691] iteration 14737 : loss: 0.015376, loss_a: 0.009045
[01:30:11.440] iteration 14738 : loss: 0.014481, loss_a: 0.008518
[01:30:12.789] iteration 14739 : loss: 0.023456, loss_a: 0.013798
[01:30:13.534] iteration 14740 : loss: 0.028564, loss_a: 0.016802
[01:30:14.880] iteration 14741 : loss: 0.013736, loss_a: 0.008080
[01:30:15.623] iteration 14742 : loss: 0.021873, loss_a: 0.012866
[01:30:17.006] iteration 14743 : loss: 0.025320, loss_a: 0.014894
[01:30:17.750] iteration 14744 : loss: 0.017762, loss_a: 0.010448
[01:30:19.116] iteration 14745 : loss: 0.033988, loss_a: 0.019993
[01:30:19.850] iteration 14746 : loss: 0.024491, loss_a: 0.014407
[01:30:21.210] iteration 14747 : loss: 0.013810, loss_a: 0.008124
[01:30:21.951] iteration 14748 : loss: 0.028856, loss_a: 0.016974
[01:30:23.288] iteration 14749 : loss: 0.025861, loss_a: 0.015212
[01:30:24.020] iteration 14750 : loss: 0.022136, loss_a: 0.013021
[01:30:25.357] iteration 14751 : loss: 0.022029, loss_a: 0.012958
[01:30:26.101] iteration 14752 : loss: 0.024190, loss_a: 0.014230
[01:30:27.460] iteration 14753 : loss: 0.041729, loss_a: 0.024546
[01:30:28.198] iteration 14754 : loss: 0.023471, loss_a: 0.013806
[01:30:29.544] iteration 14755 : loss: 0.018135, loss_a: 0.010668
[01:30:30.301] iteration 14756 : loss: 0.036034, loss_a: 0.021196
[01:30:31.627] iteration 14757 : loss: 0.016819, loss_a: 0.009894
[01:30:32.366] iteration 14758 : loss: 0.015435, loss_a: 0.009079
[01:30:33.731] iteration 14759 : loss: 0.039957, loss_a: 0.023504
[01:30:34.482] iteration 14760 : loss: 0.038010, loss_a: 0.022359
[01:30:35.835] iteration 14761 : loss: 0.040008, loss_a: 0.023534
[01:30:36.594] iteration 14762 : loss: 0.024388, loss_a: 0.014346
[01:30:37.927] iteration 14763 : loss: 0.024020, loss_a: 0.014129
[01:30:38.674] iteration 14764 : loss: 0.030222, loss_a: 0.017778
[01:30:40.004] iteration 14765 : loss: 0.057962, loss_a: 0.034095
[01:30:40.738] iteration 14766 : loss: 0.017462, loss_a: 0.010272
[01:30:42.077] iteration 14767 : loss: 0.020973, loss_a: 0.012337
[01:30:42.823] iteration 14768 : loss: 0.019470, loss_a: 0.011453
[01:30:44.179] iteration 14769 : loss: 0.020945, loss_a: 0.012320
[01:30:44.931] iteration 14770 : loss: 0.042295, loss_a: 0.024880
[01:30:46.271] iteration 14771 : loss: 0.021706, loss_a: 0.012769
[01:30:47.016] iteration 14772 : loss: 0.026556, loss_a: 0.015621
[01:30:48.404] iteration 14773 : loss: 0.012740, loss_a: 0.007494
[01:30:49.147] iteration 14774 : loss: 0.009820, loss_a: 0.005776
[01:30:50.513] iteration 14775 : loss: 0.030106, loss_a: 0.017709
[01:30:51.248] iteration 14776 : loss: 0.046168, loss_a: 0.027158
[01:30:52.610] iteration 14777 : loss: 0.019399, loss_a: 0.011411
[01:30:53.341] iteration 14778 : loss: 0.035082, loss_a: 0.020636
[01:30:54.684] iteration 14779 : loss: 0.035567, loss_a: 0.020922
[01:30:55.424] iteration 14780 : loss: 0.030585, loss_a: 0.017991
[01:30:56.739] iteration 14781 : loss: 0.027204, loss_a: 0.016002
[01:30:57.489] iteration 14782 : loss: 0.038383, loss_a: 0.022578
[01:30:58.827] iteration 14783 : loss: 0.023937, loss_a: 0.014080
[01:30:59.569] iteration 14784 : loss: 0.029995, loss_a: 0.017644
[01:31:00.917] iteration 14785 : loss: 0.018070, loss_a: 0.010630
[01:31:01.654] iteration 14786 : loss: 0.018941, loss_a: 0.011142
[01:31:02.975] iteration 14787 : loss: 0.024424, loss_a: 0.014367
[01:31:03.706] iteration 14788 : loss: 0.019239, loss_a: 0.011317
[01:31:05.048] iteration 14789 : loss: 0.029852, loss_a: 0.017560
[01:31:05.785] iteration 14790 : loss: 0.011691, loss_a: 0.006877
[01:31:07.134] iteration 14791 : loss: 0.016380, loss_a: 0.009635
[01:31:07.878] iteration 14792 : loss: 0.028856, loss_a: 0.016974
[01:31:09.208] iteration 14793 : loss: 0.023828, loss_a: 0.014017
[01:31:09.947] iteration 14794 : loss: 0.013815, loss_a: 0.008127
[01:31:11.304] iteration 14795 : loss: 0.018548, loss_a: 0.010910
[01:31:12.041] iteration 14796 : loss: 0.018624, loss_a: 0.010955
[01:31:13.382] iteration 14797 : loss: 0.034322, loss_a: 0.020190
[01:31:14.125] iteration 14798 : loss: 0.020002, loss_a: 0.011766
[01:31:15.480] iteration 14799 : loss: 0.042314, loss_a: 0.024890
[01:31:16.216] iteration 14800 : loss: 0.022319, loss_a: 0.013129
[01:31:40.842] iteration 14801 : loss: 0.030172, loss_a: 0.017748
[01:31:43.056] iteration 14802 : loss: 0.060884, loss_a: 0.035814
[01:31:44.421] iteration 14803 : loss: 0.023609, loss_a: 0.013888
[01:31:45.149] iteration 14804 : loss: 0.029441, loss_a: 0.017318
[01:31:46.486] iteration 14805 : loss: 0.015274, loss_a: 0.008984
[01:31:47.228] iteration 14806 : loss: 0.028774, loss_a: 0.016926
[01:31:48.557] iteration 14807 : loss: 0.018634, loss_a: 0.010961
[01:31:49.296] iteration 14808 : loss: 0.017873, loss_a: 0.010514
[01:31:50.697] iteration 14809 : loss: 0.186127, loss_a: 0.109487
[01:31:51.445] iteration 14810 : loss: 0.045547, loss_a: 0.026793
[01:31:52.807] iteration 14811 : loss: 0.024007, loss_a: 0.014122
[01:31:53.553] iteration 14812 : loss: 0.025125, loss_a: 0.014780
[01:31:54.909] iteration 14813 : loss: 0.027117, loss_a: 0.015951
[01:31:55.661] iteration 14814 : loss: 0.036625, loss_a: 0.021544
[01:31:57.023] iteration 14815 : loss: 0.052847, loss_a: 0.031087
[01:31:57.752] iteration 14816 : loss: 0.014942, loss_a: 0.008789
[01:31:59.105] iteration 14817 : loss: 0.017173, loss_a: 0.010102
[01:31:59.849] iteration 14818 : loss: 0.017662, loss_a: 0.010389
[01:32:01.184] iteration 14819 : loss: 0.025171, loss_a: 0.014806
[01:32:01.922] iteration 14820 : loss: 0.032959, loss_a: 0.019387
[01:32:03.274] iteration 14821 : loss: 0.026534, loss_a: 0.015608
[01:32:04.016] iteration 14822 : loss: 0.030054, loss_a: 0.017679
[01:32:05.330] iteration 14823 : loss: 0.014148, loss_a: 0.008322
[01:32:06.089] iteration 14824 : loss: 0.020095, loss_a: 0.011821
[01:32:07.457] iteration 14825 : loss: 0.055125, loss_a: 0.032427
[01:32:08.191] iteration 14826 : loss: 0.015562, loss_a: 0.009154
[01:32:09.534] iteration 14827 : loss: 0.014802, loss_a: 0.008707
[01:32:10.270] iteration 14828 : loss: 0.017347, loss_a: 0.010204
[01:32:11.621] iteration 14829 : loss: 0.017015, loss_a: 0.010009
[01:32:12.360] iteration 14830 : loss: 0.024030, loss_a: 0.014135
[01:32:13.698] iteration 14831 : loss: 0.023665, loss_a: 0.013921
[01:32:14.444] iteration 14832 : loss: 0.015543, loss_a: 0.009143
[01:32:15.811] iteration 14833 : loss: 0.031037, loss_a: 0.018257
[01:32:16.544] iteration 14834 : loss: 0.016394, loss_a: 0.009644
[01:32:17.894] iteration 14835 : loss: 0.029547, loss_a: 0.017381
[01:32:18.639] iteration 14836 : loss: 0.023015, loss_a: 0.013538
[01:32:19.960] iteration 14837 : loss: 0.020451, loss_a: 0.012030
[01:32:20.702] iteration 14838 : loss: 0.037664, loss_a: 0.022156
[01:32:22.053] iteration 14839 : loss: 0.021761, loss_a: 0.012801
[01:32:22.784] iteration 14840 : loss: 0.072325, loss_a: 0.042544
[01:32:24.140] iteration 14841 : loss: 0.034596, loss_a: 0.020351
[01:32:24.880] iteration 14842 : loss: 0.015617, loss_a: 0.009186
[01:32:26.234] iteration 14843 : loss: 0.020529, loss_a: 0.012076
[01:32:26.979] iteration 14844 : loss: 0.014719, loss_a: 0.008658
[01:32:28.341] iteration 14845 : loss: 0.013332, loss_a: 0.007843
[01:32:29.071] iteration 14846 : loss: 0.017151, loss_a: 0.010089
[01:32:30.440] iteration 14847 : loss: 0.034508, loss_a: 0.020299
[01:32:31.173] iteration 14848 : loss: 0.014586, loss_a: 0.008580
[01:32:32.536] iteration 14849 : loss: 0.024064, loss_a: 0.014155
[01:32:33.273] iteration 14850 : loss: 0.018202, loss_a: 0.010707
[01:32:34.601] iteration 14851 : loss: 0.012913, loss_a: 0.007596
[01:32:35.349] iteration 14852 : loss: 0.026350, loss_a: 0.015500
[01:32:36.667] iteration 14853 : loss: 0.048438, loss_a: 0.028493
[01:32:37.400] iteration 14854 : loss: 0.044287, loss_a: 0.026051
[01:32:38.717] iteration 14855 : loss: 0.021477, loss_a: 0.012633
[01:32:39.468] iteration 14856 : loss: 0.037721, loss_a: 0.022189
[01:32:40.782] iteration 14857 : loss: 0.015546, loss_a: 0.009145
[01:32:41.519] iteration 14858 : loss: 0.019452, loss_a: 0.011442
[01:32:42.867] iteration 14859 : loss: 0.022628, loss_a: 0.013311
[01:32:43.607] iteration 14860 : loss: 0.022795, loss_a: 0.013409
[01:32:44.962] iteration 14861 : loss: 0.054921, loss_a: 0.032307
[01:32:45.701] iteration 14862 : loss: 0.018078, loss_a: 0.010634
[01:32:47.043] iteration 14863 : loss: 0.020904, loss_a: 0.012296
[01:32:47.782] iteration 14864 : loss: 0.024658, loss_a: 0.014505
[01:32:49.145] iteration 14865 : loss: 0.026305, loss_a: 0.015474
[01:32:49.901] iteration 14866 : loss: 0.056107, loss_a: 0.033004
[01:32:51.223] iteration 14867 : loss: 0.012524, loss_a: 0.007367
[01:32:51.966] iteration 14868 : loss: 0.067486, loss_a: 0.039698
[01:32:53.301] iteration 14869 : loss: 0.044549, loss_a: 0.026206
[01:32:54.043] iteration 14870 : loss: 0.022874, loss_a: 0.013455
[01:32:55.383] iteration 14871 : loss: 0.030470, loss_a: 0.017923
[01:32:56.121] iteration 14872 : loss: 0.024493, loss_a: 0.014407
[01:32:57.471] iteration 14873 : loss: 0.038009, loss_a: 0.022358
[01:32:58.203] iteration 14874 : loss: 0.018576, loss_a: 0.010927
[01:32:59.550] iteration 14875 : loss: 0.016779, loss_a: 0.009870
[01:33:00.292] iteration 14876 : loss: 0.018289, loss_a: 0.010758
[01:33:01.624] iteration 14877 : loss: 0.021403, loss_a: 0.012590
[01:33:02.366] iteration 14878 : loss: 0.027607, loss_a: 0.016239
[01:33:03.724] iteration 14879 : loss: 0.037675, loss_a: 0.022162
[01:33:04.464] iteration 14880 : loss: 0.034132, loss_a: 0.020078
[01:33:05.799] iteration 14881 : loss: 0.016251, loss_a: 0.009559
[01:33:06.540] iteration 14882 : loss: 0.018639, loss_a: 0.010964
[01:33:07.871] iteration 14883 : loss: 0.029158, loss_a: 0.017152
[01:33:08.608] iteration 14884 : loss: 0.019407, loss_a: 0.011416
[01:33:09.972] iteration 14885 : loss: 0.027264, loss_a: 0.016037
[01:33:10.710] iteration 14886 : loss: 0.016026, loss_a: 0.009427
[01:33:12.039] iteration 14887 : loss: 0.033193, loss_a: 0.019525
[01:33:12.780] iteration 14888 : loss: 0.047481, loss_a: 0.027930
[01:33:14.110] iteration 14889 : loss: 0.027922, loss_a: 0.016425
[01:33:14.846] iteration 14890 : loss: 0.017112, loss_a: 0.010066
[01:33:16.193] iteration 14891 : loss: 0.018755, loss_a: 0.011033
[01:33:16.939] iteration 14892 : loss: 0.024019, loss_a: 0.014129
[01:33:18.295] iteration 14893 : loss: 0.058654, loss_a: 0.034502
[01:33:19.038] iteration 14894 : loss: 0.029854, loss_a: 0.017561
[01:33:20.369] iteration 14895 : loss: 0.018157, loss_a: 0.010681
[01:33:21.108] iteration 14896 : loss: 0.036295, loss_a: 0.021350
[01:33:22.490] iteration 14897 : loss: 0.031826, loss_a: 0.018721
[01:33:23.234] iteration 14898 : loss: 0.016461, loss_a: 0.009683
[01:33:24.588] iteration 14899 : loss: 0.024143, loss_a: 0.014202
[01:33:25.322] iteration 14900 : loss: 0.015636, loss_a: 0.009198
[01:33:26.668] iteration 14901 : loss: 0.016966, loss_a: 0.009980
[01:33:27.405] iteration 14902 : loss: 0.028314, loss_a: 0.016656
[01:33:28.757] iteration 14903 : loss: 0.020326, loss_a: 0.011956
[01:33:29.501] iteration 14904 : loss: 0.021733, loss_a: 0.012784
[01:33:30.838] iteration 14905 : loss: 0.015765, loss_a: 0.009274
[01:33:31.580] iteration 14906 : loss: 0.035374, loss_a: 0.020808
[01:33:32.944] iteration 14907 : loss: 0.013937, loss_a: 0.008199
[01:33:33.684] iteration 14908 : loss: 0.038918, loss_a: 0.022893
[01:33:35.021] iteration 14909 : loss: 0.019299, loss_a: 0.011352
[01:33:35.772] iteration 14910 : loss: 0.027890, loss_a: 0.016406
[01:33:37.121] iteration 14911 : loss: 0.014601, loss_a: 0.008589
[01:33:37.856] iteration 14912 : loss: 0.044520, loss_a: 0.026188
[01:33:39.204] iteration 14913 : loss: 0.022982, loss_a: 0.013519
[01:33:39.948] iteration 14914 : loss: 0.054411, loss_a: 0.032006
[01:33:41.281] iteration 14915 : loss: 0.023882, loss_a: 0.014048
[01:33:42.024] iteration 14916 : loss: 0.035289, loss_a: 0.020758
[01:33:43.378] iteration 14917 : loss: 0.033567, loss_a: 0.019745
[01:33:44.116] iteration 14918 : loss: 0.022131, loss_a: 0.013018
[01:33:45.439] iteration 14919 : loss: 0.023708, loss_a: 0.013946
[01:33:46.177] iteration 14920 : loss: 0.027075, loss_a: 0.015926
[01:33:47.514] iteration 14921 : loss: 0.014971, loss_a: 0.008806
[01:33:48.253] iteration 14922 : loss: 0.013689, loss_a: 0.008052
[01:33:49.585] iteration 14923 : loss: 0.020104, loss_a: 0.011826
[01:33:50.320] iteration 14924 : loss: 0.014063, loss_a: 0.008272
[01:33:51.685] iteration 14925 : loss: 0.023979, loss_a: 0.014105
[01:33:52.420] iteration 14926 : loss: 0.014635, loss_a: 0.008609
[01:33:53.780] iteration 14927 : loss: 0.032731, loss_a: 0.019254
[01:33:54.516] iteration 14928 : loss: 0.012790, loss_a: 0.007524
[01:33:55.879] iteration 14929 : loss: 0.024735, loss_a: 0.014550
[01:33:56.620] iteration 14930 : loss: 0.032003, loss_a: 0.018825
[01:33:57.972] iteration 14931 : loss: 0.017577, loss_a: 0.010339
[01:33:58.713] iteration 14932 : loss: 0.037023, loss_a: 0.021778
[01:34:00.042] iteration 14933 : loss: 0.017428, loss_a: 0.010252
[01:34:00.782] iteration 14934 : loss: 0.025969, loss_a: 0.015276
[01:34:02.125] iteration 14935 : loss: 0.022643, loss_a: 0.013319
[01:34:02.862] iteration 14936 : loss: 0.025570, loss_a: 0.015041
[01:34:04.219] iteration 14937 : loss: 0.031807, loss_a: 0.018710
[01:34:04.963] iteration 14938 : loss: 0.018867, loss_a: 0.011098
[01:34:06.325] iteration 14939 : loss: 0.027838, loss_a: 0.016375
[01:34:07.061] iteration 14940 : loss: 0.022266, loss_a: 0.013097
[01:34:08.426] iteration 14941 : loss: 0.023859, loss_a: 0.014034
[01:34:09.161] iteration 14942 : loss: 0.014615, loss_a: 0.008597
[01:34:10.507] iteration 14943 : loss: 0.013732, loss_a: 0.008078
[01:34:11.250] iteration 14944 : loss: 0.023225, loss_a: 0.013662
[01:34:12.616] iteration 14945 : loss: 0.037206, loss_a: 0.021886
[01:34:13.351] iteration 14946 : loss: 0.028021, loss_a: 0.016483
[01:34:14.674] iteration 14947 : loss: 0.012140, loss_a: 0.007141
[01:34:15.425] iteration 14948 : loss: 0.025284, loss_a: 0.014873
[01:34:16.741] iteration 14949 : loss: 0.032190, loss_a: 0.018935
[01:34:17.474] iteration 14950 : loss: 0.015574, loss_a: 0.009161
[01:34:18.846] iteration 14951 : loss: 0.031876, loss_a: 0.018751
[01:34:19.585] iteration 14952 : loss: 0.016773, loss_a: 0.009866
[01:34:20.941] iteration 14953 : loss: 0.020266, loss_a: 0.011921
[01:34:21.683] iteration 14954 : loss: 0.020267, loss_a: 0.011922
[01:34:23.039] iteration 14955 : loss: 0.034386, loss_a: 0.020227
[01:34:23.775] iteration 14956 : loss: 0.020723, loss_a: 0.012190
[01:34:25.122] iteration 14957 : loss: 0.035014, loss_a: 0.020597
[01:34:25.859] iteration 14958 : loss: 0.018711, loss_a: 0.011006
[01:34:27.221] iteration 14959 : loss: 0.021187, loss_a: 0.012463
[01:34:27.952] iteration 14960 : loss: 0.008932, loss_a: 0.005254
[01:34:29.269] iteration 14961 : loss: 0.034791, loss_a: 0.020465
[01:34:30.011] iteration 14962 : loss: 0.015457, loss_a: 0.009092
[01:34:31.362] iteration 14963 : loss: 0.054163, loss_a: 0.031861
[01:34:32.101] iteration 14964 : loss: 0.015492, loss_a: 0.009113
[01:34:33.472] iteration 14965 : loss: 0.023282, loss_a: 0.013695
[01:34:34.234] iteration 14966 : loss: 0.032699, loss_a: 0.019235
[01:34:35.556] iteration 14967 : loss: 0.015012, loss_a: 0.008831
[01:34:36.300] iteration 14968 : loss: 0.032425, loss_a: 0.019073
[01:34:37.663] iteration 14969 : loss: 0.057156, loss_a: 0.033621
[01:34:38.403] iteration 14970 : loss: 0.031276, loss_a: 0.018398
[01:34:39.772] iteration 14971 : loss: 0.022751, loss_a: 0.013383
[01:34:40.508] iteration 14972 : loss: 0.015688, loss_a: 0.009228
[01:34:41.864] iteration 14973 : loss: 0.030546, loss_a: 0.017968
[01:34:42.623] iteration 14974 : loss: 0.047586, loss_a: 0.027992
[01:34:43.983] iteration 14975 : loss: 0.035752, loss_a: 0.021031
[01:34:44.724] iteration 14976 : loss: 0.035485, loss_a: 0.020874
[01:34:46.074] iteration 14977 : loss: 0.020190, loss_a: 0.011876
[01:34:46.826] iteration 14978 : loss: 0.062478, loss_a: 0.036752
[01:34:48.146] iteration 14979 : loss: 0.031478, loss_a: 0.018516
[01:34:48.884] iteration 14980 : loss: 0.046199, loss_a: 0.027176
[01:34:50.217] iteration 14981 : loss: 0.036585, loss_a: 0.021520
[01:34:50.952] iteration 14982 : loss: 0.010513, loss_a: 0.006184
[01:34:52.276] iteration 14983 : loss: 0.023061, loss_a: 0.013566
[01:34:53.014] iteration 14984 : loss: 0.010340, loss_a: 0.006082
[01:34:54.344] iteration 14985 : loss: 0.025705, loss_a: 0.015121
[01:34:55.083] iteration 14986 : loss: 0.022260, loss_a: 0.013094
[01:34:56.417] iteration 14987 : loss: 0.023603, loss_a: 0.013884
[01:34:57.154] iteration 14988 : loss: 0.020586, loss_a: 0.012109
[01:34:58.478] iteration 14989 : loss: 0.021193, loss_a: 0.012466
[01:34:59.214] iteration 14990 : loss: 0.025211, loss_a: 0.014830
[01:35:00.580] iteration 14991 : loss: 0.021432, loss_a: 0.012607
[01:35:01.318] iteration 14992 : loss: 0.018141, loss_a: 0.010671
[01:35:02.637] iteration 14993 : loss: 0.013623, loss_a: 0.008014
[01:35:03.379] iteration 14994 : loss: 0.030905, loss_a: 0.018179
[01:35:04.746] iteration 14995 : loss: 0.032390, loss_a: 0.019053
[01:35:05.487] iteration 14996 : loss: 0.023011, loss_a: 0.013536
[01:35:06.837] iteration 14997 : loss: 0.022300, loss_a: 0.013118
[01:35:07.572] iteration 14998 : loss: 0.019029, loss_a: 0.011193
[01:35:08.906] iteration 14999 : loss: 0.073349, loss_a: 0.043147
[01:35:09.641] iteration 15000 : loss: 0.029939, loss_a: 0.017611
